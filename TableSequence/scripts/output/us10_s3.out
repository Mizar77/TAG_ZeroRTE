/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_3', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 67228
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 67328, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/model', pretrained_wv='outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=67328, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.310, loss:52590.5345
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.042, loss:2504.0905
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.047, loss:2121.7363
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.045, loss:2112.9683
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.049, loss:2057.5129
>> valid entity prec:0.3460, rec:0.3953, f1:0.3690
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 2.519, loss:2016.2848
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.047, loss:1827.5655
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 1.024, loss:1728.4662
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 1.026, loss:1660.6917
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 1.026, loss:1563.9251
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4745, rec:0.3885, f1:0.4272
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 2.433, loss:1512.5236
g_step 1200, step 1200, avg_time 1.026, loss:1424.9451
g_step 1300, step 1300, avg_time 1.022, loss:1442.5319
g_step 1400, step 1400, avg_time 1.026, loss:1305.5725
g_step 1500, step 1500, avg_time 1.018, loss:1319.2885
>> valid entity prec:0.5805, rec:0.5748, f1:0.5776
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 1600, avg_time 2.439, loss:1253.0754
g_step 1700, step 1700, avg_time 1.030, loss:1191.0490
g_step 1800, step 1800, avg_time 1.020, loss:1221.7724
g_step 1900, step 75, avg_time 1.024, loss:1170.0093
g_step 2000, step 175, avg_time 1.023, loss:1134.5132
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5222, rec:0.6001, f1:0.5585
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 275, avg_time 2.435, loss:1106.4148
g_step 2200, step 375, avg_time 1.027, loss:1140.2664
g_step 2300, step 475, avg_time 1.023, loss:1054.7711
g_step 2400, step 575, avg_time 1.024, loss:1060.5362
g_step 2500, step 675, avg_time 1.022, loss:1070.0518
>> valid entity prec:0.5198, rec:0.6761, f1:0.5877
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 775, avg_time 2.450, loss:1031.3628
g_step 2700, step 875, avg_time 1.019, loss:1090.0753
g_step 2800, step 975, avg_time 1.026, loss:1044.3084
g_step 2900, step 1075, avg_time 1.026, loss:1013.4512
g_step 3000, step 1175, avg_time 1.021, loss:1023.7233
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6033, rec:0.4646, f1:0.5250
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 1275, avg_time 2.427, loss:1016.1022
g_step 3200, step 1375, avg_time 1.028, loss:991.5282
g_step 3300, step 1475, avg_time 1.030, loss:974.9998
g_step 3400, step 1575, avg_time 1.028, loss:1013.1732
g_step 3500, step 1675, avg_time 1.024, loss:961.2008
>> valid entity prec:0.5657, rec:0.6179, f1:0.5906
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 1775, avg_time 2.442, loss:984.2565
g_step 3700, step 50, avg_time 1.020, loss:930.2625
g_step 3800, step 150, avg_time 1.015, loss:953.6462
g_step 3900, step 250, avg_time 1.018, loss:949.5599
g_step 4000, step 350, avg_time 1.027, loss:903.9304
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4934, rec:0.6639, f1:0.5661
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 450, avg_time 2.448, loss:922.6201
g_step 4200, step 550, avg_time 1.026, loss:925.9430
g_step 4300, step 650, avg_time 1.026, loss:966.6227
g_step 4400, step 750, avg_time 1.024, loss:918.6590
g_step 4500, step 850, avg_time 1.027, loss:878.5983
>> valid entity prec:0.5998, rec:0.5196, f1:0.5568
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 950, avg_time 2.430, loss:913.1002
g_step 4700, step 1050, avg_time 1.024, loss:913.4178
g_step 4800, step 1150, avg_time 1.024, loss:916.7278
g_step 4900, step 1250, avg_time 1.020, loss:912.9272
g_step 5000, step 1350, avg_time 1.021, loss:895.5403
learning rate was adjusted to 0.0008
>> valid entity prec:0.5208, rec:0.5007, f1:0.5106
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 1450, avg_time 2.431, loss:880.6297
g_step 5200, step 1550, avg_time 1.028, loss:872.5797
g_step 5300, step 1650, avg_time 1.017, loss:877.7439
g_step 5400, step 1750, avg_time 1.018, loss:871.1575
g_step 5500, step 25, avg_time 1.023, loss:883.2616
>> valid entity prec:0.5110, rec:0.5404, f1:0.5253
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 125, avg_time 2.431, loss:867.0216
g_step 5700, step 225, avg_time 1.026, loss:833.0522
g_step 5800, step 325, avg_time 1.015, loss:862.2739
g_step 5900, step 425, avg_time 1.021, loss:837.7850
g_step 6000, step 525, avg_time 1.025, loss:850.9807
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5330, rec:0.4743, f1:0.5019
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 625, avg_time 2.429, loss:832.6483
g_step 6200, step 725, avg_time 1.025, loss:874.6928
g_step 6300, step 825, avg_time 1.014, loss:838.2701
g_step 6400, step 925, avg_time 1.027, loss:815.3725
g_step 6500, step 1025, avg_time 1.029, loss:829.6858
>> valid entity prec:0.5304, rec:0.5864, f1:0.5570
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 1125, avg_time 2.440, loss:843.6020
g_step 6700, step 1225, avg_time 1.021, loss:834.7633
g_step 6800, step 1325, avg_time 1.017, loss:843.9606
g_step 6900, step 1425, avg_time 1.023, loss:849.0818
g_step 7000, step 1525, avg_time 1.022, loss:827.4991
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6031, rec:0.4508, f1:0.5159
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 1625, avg_time 2.436, loss:818.2936
g_step 7200, step 1725, avg_time 1.019, loss:812.5975
g_step 7300, step 1825, avg_time 1.027, loss:845.0878
g_step 7400, step 100, avg_time 1.024, loss:769.6227
g_step 7500, step 200, avg_time 1.023, loss:798.6517
>> valid entity prec:0.5256, rec:0.4652, f1:0.4936
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 300, avg_time 2.426, loss:797.0292
g_step 7700, step 400, avg_time 1.026, loss:783.2198
g_step 7800, step 500, avg_time 1.023, loss:811.1674
g_step 7900, step 600, avg_time 1.018, loss:795.6017
g_step 8000, step 700, avg_time 1.028, loss:823.0662
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5140, rec:0.4760, f1:0.4943
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 800, avg_time 2.433, loss:783.1192
g_step 8200, step 900, avg_time 1.015, loss:795.4368
g_step 8300, step 1000, avg_time 1.025, loss:797.4293
g_step 8400, step 1100, avg_time 1.027, loss:797.9800
g_step 8500, step 1200, avg_time 1.015, loss:821.6568
>> valid entity prec:0.5276, rec:0.6284, f1:0.5736
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 1300, avg_time 2.442, loss:788.5242
g_step 8700, step 1400, avg_time 1.025, loss:798.5552
g_step 8800, step 1500, avg_time 1.025, loss:785.2785
g_step 8900, step 1600, avg_time 1.025, loss:776.7679
g_step 9000, step 1700, avg_time 1.027, loss:777.4063
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5820, rec:0.4588, f1:0.5131
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 1800, avg_time 2.422, loss:794.0299
g_step 9200, step 75, avg_time 1.022, loss:751.2529
g_step 9300, step 175, avg_time 1.020, loss:775.2606
g_step 9400, step 275, avg_time 1.025, loss:781.3797
g_step 9500, step 375, avg_time 1.017, loss:765.8940
>> valid entity prec:0.5870, rec:0.4973, f1:0.5384
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 475, avg_time 2.427, loss:752.9804
g_step 9700, step 575, avg_time 1.027, loss:780.5328
g_step 9800, step 675, avg_time 1.019, loss:738.4091
g_step 9900, step 775, avg_time 1.029, loss:750.8754
g_step 10000, step 875, avg_time 1.021, loss:737.2310
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.5392, rec:0.5195, f1:0.5292
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.41s/it]Extractor Predicting: 2it [00:08,  3.47s/it]Extractor Predicting: 3it [00:08,  2.19s/it]Extractor Predicting: 4it [00:09,  1.58s/it]Extractor Predicting: 5it [00:10,  1.24s/it]Extractor Predicting: 6it [00:10,  1.04s/it]Extractor Predicting: 7it [00:11,  1.11it/s]Extractor Predicting: 8it [00:11,  1.24it/s]Extractor Predicting: 9it [00:12,  1.30it/s]Extractor Predicting: 10it [00:13,  1.38it/s]Extractor Predicting: 11it [00:13,  1.43it/s]Extractor Predicting: 12it [00:14,  1.44it/s]Extractor Predicting: 13it [00:15,  1.47it/s]Extractor Predicting: 14it [00:15,  1.47it/s]Extractor Predicting: 15it [00:16,  1.51it/s]Extractor Predicting: 16it [00:17,  1.50it/s]Extractor Predicting: 17it [00:17,  1.50it/s]Extractor Predicting: 18it [00:18,  1.52it/s]Extractor Predicting: 19it [00:19,  1.55it/s]Extractor Predicting: 20it [00:19,  1.55it/s]Extractor Predicting: 21it [00:20,  1.53it/s]Extractor Predicting: 22it [00:21,  1.55it/s]Extractor Predicting: 23it [00:21,  1.55it/s]Extractor Predicting: 24it [00:22,  1.56it/s]Extractor Predicting: 25it [00:23,  1.53it/s]Extractor Predicting: 26it [00:23,  1.53it/s]Extractor Predicting: 27it [00:24,  1.53it/s]Extractor Predicting: 28it [00:24,  1.54it/s]Extractor Predicting: 29it [00:25,  1.52it/s]Extractor Predicting: 30it [00:26,  1.51it/s]Extractor Predicting: 31it [00:27,  1.50it/s]Extractor Predicting: 32it [00:27,  1.51it/s]Extractor Predicting: 33it [00:28,  1.49it/s]Extractor Predicting: 34it [00:29,  1.48it/s]Extractor Predicting: 35it [00:29,  1.46it/s]Extractor Predicting: 36it [00:30,  1.45it/s]Extractor Predicting: 37it [00:31,  1.46it/s]Extractor Predicting: 38it [00:31,  1.46it/s]Extractor Predicting: 39it [00:32,  1.47it/s]Extractor Predicting: 40it [00:33,  1.45it/s]Extractor Predicting: 41it [00:33,  1.46it/s]Extractor Predicting: 42it [00:34,  1.45it/s]Extractor Predicting: 43it [00:35,  1.44it/s]Extractor Predicting: 44it [00:35,  1.44it/s]Extractor Predicting: 45it [00:36,  1.43it/s]Extractor Predicting: 46it [00:37,  1.42it/s]Extractor Predicting: 47it [00:38,  1.46it/s]Extractor Predicting: 48it [00:38,  1.46it/s]Extractor Predicting: 49it [00:39,  1.49it/s]Extractor Predicting: 50it [00:40,  1.45it/s]Extractor Predicting: 51it [00:40,  1.48it/s]Extractor Predicting: 52it [00:41,  1.49it/s]Extractor Predicting: 53it [00:42,  1.48it/s]Extractor Predicting: 54it [00:42,  1.47it/s]Extractor Predicting: 55it [00:43,  1.47it/s]Extractor Predicting: 56it [00:44,  1.46it/s]Extractor Predicting: 57it [00:44,  1.49it/s]Extractor Predicting: 58it [00:45,  1.49it/s]Extractor Predicting: 59it [00:46,  1.46it/s]Extractor Predicting: 60it [00:46,  1.45it/s]Extractor Predicting: 61it [00:47,  1.43it/s]Extractor Predicting: 62it [00:48,  1.43it/s]Extractor Predicting: 63it [00:48,  1.44it/s]Extractor Predicting: 64it [00:49,  1.43it/s]Extractor Predicting: 65it [00:50,  1.46it/s]Extractor Predicting: 66it [00:50,  1.47it/s]Extractor Predicting: 67it [00:51,  1.46it/s]Extractor Predicting: 68it [00:52,  1.48it/s]Extractor Predicting: 69it [00:53,  1.36it/s]Extractor Predicting: 70it [00:53,  1.39it/s]Extractor Predicting: 71it [00:54,  1.38it/s]Extractor Predicting: 72it [00:55,  1.41it/s]Extractor Predicting: 73it [00:56,  1.42it/s]Extractor Predicting: 74it [00:56,  1.44it/s]Extractor Predicting: 75it [00:57,  1.45it/s]Extractor Predicting: 76it [00:58,  1.46it/s]Extractor Predicting: 77it [00:58,  1.48it/s]Extractor Predicting: 78it [00:59,  1.45it/s]Extractor Predicting: 79it [01:00,  1.45it/s]Extractor Predicting: 80it [01:00,  1.44it/s]Extractor Predicting: 81it [01:01,  1.43it/s]Extractor Predicting: 82it [01:02,  1.44it/s]Extractor Predicting: 83it [01:02,  1.48it/s]Extractor Predicting: 84it [01:03,  1.45it/s]Extractor Predicting: 85it [01:04,  1.44it/s]Extractor Predicting: 86it [01:04,  1.45it/s]Extractor Predicting: 87it [01:05,  1.46it/s]Extractor Predicting: 88it [01:06,  1.49it/s]Extractor Predicting: 89it [01:06,  1.48it/s]Extractor Predicting: 90it [01:07,  1.51it/s]Extractor Predicting: 91it [01:08,  1.55it/s]Extractor Predicting: 92it [01:08,  1.55it/s]Extractor Predicting: 93it [01:09,  1.52it/s]Extractor Predicting: 94it [01:10,  1.54it/s]Extractor Predicting: 95it [01:10,  1.54it/s]Extractor Predicting: 96it [01:11,  1.53it/s]Extractor Predicting: 97it [01:12,  1.52it/s]Extractor Predicting: 98it [01:12,  1.50it/s]Extractor Predicting: 99it [01:13,  1.47it/s]Extractor Predicting: 100it [01:14,  1.46it/s]Extractor Predicting: 101it [01:14,  1.51it/s]Extractor Predicting: 102it [01:15,  1.55it/s]Extractor Predicting: 103it [01:16,  1.53it/s]Extractor Predicting: 104it [01:16,  1.55it/s]Extractor Predicting: 105it [01:17,  1.54it/s]Extractor Predicting: 106it [01:18,  1.55it/s]Extractor Predicting: 107it [01:18,  1.53it/s]Extractor Predicting: 108it [01:19,  1.53it/s]Extractor Predicting: 109it [01:20,  1.53it/s]Extractor Predicting: 110it [01:20,  1.51it/s]Extractor Predicting: 111it [01:21,  1.55it/s]Extractor Predicting: 112it [01:21,  1.55it/s]Extractor Predicting: 113it [01:22,  1.59it/s]Extractor Predicting: 114it [01:23,  1.58it/s]Extractor Predicting: 115it [01:23,  1.58it/s]Extractor Predicting: 116it [01:24,  1.57it/s]Extractor Predicting: 117it [01:25,  1.54it/s]Extractor Predicting: 118it [01:25,  1.52it/s]Extractor Predicting: 119it [01:26,  1.50it/s]Extractor Predicting: 120it [01:27,  1.48it/s]Extractor Predicting: 121it [01:27,  1.51it/s]Extractor Predicting: 122it [01:28,  1.51it/s]Extractor Predicting: 123it [01:29,  1.49it/s]Extractor Predicting: 124it [01:29,  1.47it/s]Extractor Predicting: 125it [01:30,  1.46it/s]Extractor Predicting: 126it [01:31,  1.45it/s]Extractor Predicting: 127it [01:31,  1.45it/s]Extractor Predicting: 128it [01:32,  1.51it/s]Extractor Predicting: 129it [01:33,  1.48it/s]Extractor Predicting: 130it [01:33,  1.51it/s]Extractor Predicting: 131it [01:34,  1.50it/s]Extractor Predicting: 132it [01:35,  1.50it/s]Extractor Predicting: 133it [01:35,  1.48it/s]Extractor Predicting: 134it [01:36,  1.45it/s]Extractor Predicting: 135it [01:37,  1.46it/s]Extractor Predicting: 136it [01:37,  1.47it/s]Extractor Predicting: 137it [01:38,  1.48it/s]Extractor Predicting: 138it [01:39,  1.48it/s]Extractor Predicting: 139it [01:40,  1.46it/s]Extractor Predicting: 140it [01:40,  1.49it/s]Extractor Predicting: 141it [01:41,  1.47it/s]Extractor Predicting: 142it [01:42,  1.44it/s]Extractor Predicting: 143it [01:42,  1.47it/s]Extractor Predicting: 144it [01:43,  1.76it/s]Extractor Predicting: 144it [01:43,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:21,  1.55it/s]Extractor Predicting: 33it [00:22,  1.58it/s]Extractor Predicting: 34it [00:22,  1.60it/s]Extractor Predicting: 35it [00:23,  1.61it/s]Extractor Predicting: 36it [00:23,  1.62it/s]Extractor Predicting: 37it [00:24,  1.62it/s]Extractor Predicting: 38it [00:25,  1.65it/s]Extractor Predicting: 39it [00:25,  1.68it/s]Extractor Predicting: 40it [00:26,  1.70it/s]Extractor Predicting: 41it [00:26,  1.68it/s]Extractor Predicting: 42it [00:27,  1.69it/s]Extractor Predicting: 43it [00:28,  1.67it/s]Extractor Predicting: 44it [00:28,  1.69it/s]Extractor Predicting: 45it [00:29,  1.68it/s]Extractor Predicting: 46it [00:29,  1.63it/s]Extractor Predicting: 47it [00:30,  1.66it/s]Extractor Predicting: 48it [00:31,  1.70it/s]Extractor Predicting: 49it [00:31,  1.72it/s]Extractor Predicting: 50it [00:32,  1.70it/s]Extractor Predicting: 51it [00:32,  1.71it/s]Extractor Predicting: 52it [00:33,  1.68it/s]Extractor Predicting: 53it [00:34,  1.67it/s]Extractor Predicting: 54it [00:34,  1.66it/s]Extractor Predicting: 55it [00:35,  1.63it/s]Extractor Predicting: 56it [00:35,  1.65it/s]Extractor Predicting: 57it [00:36,  1.63it/s]Extractor Predicting: 58it [00:37,  1.61it/s]Extractor Predicting: 59it [00:37,  1.56it/s]Extractor Predicting: 60it [00:38,  1.51it/s]Extractor Predicting: 61it [00:39,  1.45it/s]Extractor Predicting: 62it [00:40,  1.41it/s]Extractor Predicting: 63it [00:40,  1.42it/s]Extractor Predicting: 64it [00:41,  1.40it/s]Extractor Predicting: 65it [00:42,  1.43it/s]Extractor Predicting: 66it [00:42,  1.43it/s]Extractor Predicting: 67it [00:43,  1.43it/s]Extractor Predicting: 68it [00:44,  1.41it/s]Extractor Predicting: 69it [00:45,  1.39it/s]Extractor Predicting: 70it [00:45,  1.38it/s]Extractor Predicting: 71it [00:46,  1.36it/s]Extractor Predicting: 72it [00:47,  1.39it/s]Extractor Predicting: 73it [00:47,  1.39it/s]Extractor Predicting: 74it [00:48,  1.41it/s]Extractor Predicting: 75it [00:49,  1.42it/s]Extractor Predicting: 76it [00:49,  1.42it/s]Extractor Predicting: 77it [00:50,  1.41it/s]Extractor Predicting: 78it [00:51,  1.39it/s]Extractor Predicting: 79it [00:52,  1.38it/s]Extractor Predicting: 80it [00:52,  1.38it/s]Extractor Predicting: 81it [00:53,  1.38it/s]Extractor Predicting: 82it [00:54,  1.38it/s]Extractor Predicting: 83it [00:55,  1.38it/s]Extractor Predicting: 84it [00:55,  1.41it/s]Extractor Predicting: 85it [00:56,  1.40it/s]Extractor Predicting: 86it [00:57,  1.37it/s]Extractor Predicting: 87it [00:57,  1.37it/s]Extractor Predicting: 88it [00:58,  1.39it/s]Extractor Predicting: 89it [00:59,  1.40it/s]Extractor Predicting: 90it [01:00,  1.32it/s]Extractor Predicting: 91it [01:00,  1.33it/s]Extractor Predicting: 92it [01:01,  1.33it/s]Extractor Predicting: 93it [01:02,  1.39it/s]Extractor Predicting: 94it [01:03,  1.40it/s]Extractor Predicting: 95it [01:03,  1.44it/s]Extractor Predicting: 96it [01:04,  1.48it/s]Extractor Predicting: 97it [01:05,  1.48it/s]Extractor Predicting: 98it [01:05,  1.51it/s]Extractor Predicting: 99it [01:06,  1.49it/s]Extractor Predicting: 100it [01:07,  1.50it/s]Extractor Predicting: 101it [01:07,  1.55it/s]Extractor Predicting: 102it [01:08,  1.55it/s]Extractor Predicting: 103it [01:08,  1.50it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:10,  1.47it/s]Extractor Predicting: 106it [01:11,  1.46it/s]Extractor Predicting: 107it [01:11,  1.48it/s]Extractor Predicting: 108it [01:12,  1.47it/s]Extractor Predicting: 109it [01:13,  1.45it/s]Extractor Predicting: 110it [01:13,  1.44it/s]Extractor Predicting: 111it [01:14,  1.46it/s]Extractor Predicting: 112it [01:15,  1.43it/s]Extractor Predicting: 113it [01:15,  1.46it/s]Extractor Predicting: 114it [01:16,  1.43it/s]Extractor Predicting: 115it [01:17,  1.44it/s]Extractor Predicting: 116it [01:17,  1.50it/s]Extractor Predicting: 117it [01:18,  1.49it/s]Extractor Predicting: 118it [01:19,  1.54it/s]Extractor Predicting: 119it [01:19,  1.57it/s]Extractor Predicting: 120it [01:20,  1.57it/s]Extractor Predicting: 121it [01:21,  1.61it/s]Extractor Predicting: 122it [01:21,  1.60it/s]Extractor Predicting: 123it [01:22,  1.61it/s]Extractor Predicting: 124it [01:22,  1.62it/s]Extractor Predicting: 125it [01:23,  1.64it/s]Extractor Predicting: 126it [01:24,  1.61it/s]Extractor Predicting: 127it [01:24,  1.61it/s]Extractor Predicting: 128it [01:25,  1.65it/s]Extractor Predicting: 129it [01:25,  1.63it/s]Extractor Predicting: 130it [01:26,  1.71it/s]Extractor Predicting: 131it [01:26,  1.73it/s]Extractor Predicting: 132it [01:27,  1.68it/s]Extractor Predicting: 133it [01:28,  1.63it/s]Extractor Predicting: 134it [01:28,  1.62it/s]Extractor Predicting: 135it [01:29,  1.64it/s]Extractor Predicting: 136it [01:30,  1.66it/s]Extractor Predicting: 137it [01:30,  1.65it/s]Extractor Predicting: 138it [01:31,  1.66it/s]Extractor Predicting: 139it [01:31,  1.62it/s]Extractor Predicting: 140it [01:32,  1.64it/s]Extractor Predicting: 141it [01:33,  1.62it/s]Extractor Predicting: 142it [01:33,  1.68it/s]Extractor Predicting: 143it [01:34,  1.66it/s]Extractor Predicting: 144it [01:34,  1.64it/s]Extractor Predicting: 145it [01:35,  1.62it/s]Extractor Predicting: 146it [01:36,  1.57it/s]Extractor Predicting: 147it [01:36,  1.54it/s]Extractor Predicting: 148it [01:37,  1.55it/s]Extractor Predicting: 149it [01:38,  1.52it/s]Extractor Predicting: 150it [01:38,  1.51it/s]Extractor Predicting: 151it [01:39,  1.51it/s]Extractor Predicting: 152it [01:40,  1.52it/s]Extractor Predicting: 153it [01:40,  1.48it/s]Extractor Predicting: 154it [01:41,  1.51it/s]Extractor Predicting: 155it [01:42,  1.53it/s]Extractor Predicting: 156it [01:42,  1.50it/s]Extractor Predicting: 157it [01:43,  1.51it/s]Extractor Predicting: 158it [01:44,  1.49it/s]Extractor Predicting: 159it [01:44,  1.48it/s]Extractor Predicting: 160it [01:45,  1.47it/s]Extractor Predicting: 161it [01:46,  1.47it/s]Extractor Predicting: 162it [01:47,  1.46it/s]Extractor Predicting: 163it [01:47,  1.49it/s]Extractor Predicting: 164it [01:48,  1.49it/s]Extractor Predicting: 165it [01:49,  1.47it/s]Extractor Predicting: 166it [01:49,  1.44it/s]Extractor Predicting: 167it [01:50,  1.46it/s]Extractor Predicting: 168it [01:51,  1.45it/s]Extractor Predicting: 169it [01:51,  1.48it/s]Extractor Predicting: 170it [01:52,  1.47it/s]Extractor Predicting: 171it [01:53,  1.47it/s]Extractor Predicting: 172it [01:53,  1.48it/s]Extractor Predicting: 173it [01:54,  1.46it/s]Extractor Predicting: 174it [01:55,  1.46it/s]Extractor Predicting: 175it [01:55,  1.47it/s]Extractor Predicting: 176it [01:56,  1.47it/s]Extractor Predicting: 177it [01:57,  1.45it/s]Extractor Predicting: 178it [01:57,  1.46it/s]Extractor Predicting: 179it [01:58,  1.46it/s]Extractor Predicting: 180it [01:59,  1.45it/s]Extractor Predicting: 181it [02:00,  1.45it/s]Extractor Predicting: 182it [02:00,  1.45it/s]Extractor Predicting: 183it [02:01,  1.45it/s]Extractor Predicting: 184it [02:02,  1.45it/s]Extractor Predicting: 185it [02:02,  1.47it/s]Extractor Predicting: 186it [02:03,  1.45it/s]Extractor Predicting: 187it [02:04,  1.47it/s]Extractor Predicting: 188it [02:04,  1.46it/s]Extractor Predicting: 189it [02:05,  1.48it/s]Extractor Predicting: 190it [02:06,  1.47it/s]Extractor Predicting: 191it [02:06,  1.50it/s]Extractor Predicting: 192it [02:07,  1.37it/s]Extractor Predicting: 193it [02:08,  1.43it/s]Extractor Predicting: 194it [02:09,  1.43it/s]Extractor Predicting: 195it [02:09,  1.44it/s]Extractor Predicting: 196it [02:10,  1.45it/s]Extractor Predicting: 197it [02:11,  1.46it/s]Extractor Predicting: 198it [02:11,  1.43it/s]Extractor Predicting: 199it [02:12,  1.45it/s]Extractor Predicting: 200it [02:13,  1.45it/s]Extractor Predicting: 201it [02:13,  1.49it/s]Extractor Predicting: 202it [02:14,  1.47it/s]Extractor Predicting: 203it [02:15,  1.50it/s]Extractor Predicting: 204it [02:15,  1.49it/s]Extractor Predicting: 205it [02:16,  1.48it/s]Extractor Predicting: 206it [02:17,  1.54it/s]Extractor Predicting: 207it [02:17,  1.52it/s]Extractor Predicting: 208it [02:18,  1.55it/s]Extractor Predicting: 209it [02:19,  1.52it/s]Extractor Predicting: 210it [02:19,  1.52it/s]Extractor Predicting: 211it [02:20,  1.50it/s]Extractor Predicting: 212it [02:21,  1.47it/s]Extractor Predicting: 213it [02:21,  1.47it/s]Extractor Predicting: 214it [02:22,  1.47it/s]Extractor Predicting: 215it [02:23,  1.45it/s]Extractor Predicting: 216it [02:23,  1.45it/s]Extractor Predicting: 217it [02:24,  1.41it/s]Extractor Predicting: 218it [02:25,  1.44it/s]Extractor Predicting: 219it [02:25,  1.44it/s]Extractor Predicting: 220it [02:26,  1.48it/s]Extractor Predicting: 221it [02:27,  1.47it/s]Extractor Predicting: 222it [02:27,  1.47it/s]Extractor Predicting: 223it [02:28,  1.48it/s]Extractor Predicting: 224it [02:29,  1.47it/s]Extractor Predicting: 225it [02:30,  1.46it/s]Extractor Predicting: 226it [02:30,  1.47it/s]Extractor Predicting: 227it [02:31,  1.46it/s]Extractor Predicting: 228it [02:32,  1.50it/s]Extractor Predicting: 229it [02:32,  1.47it/s]Extractor Predicting: 230it [02:33,  1.49it/s]Extractor Predicting: 231it [02:34,  1.45it/s]Extractor Predicting: 232it [02:34,  1.44it/s]Extractor Predicting: 233it [02:35,  1.44it/s]Extractor Predicting: 234it [02:36,  1.46it/s]Extractor Predicting: 235it [02:36,  1.47it/s]Extractor Predicting: 236it [02:37,  1.48it/s]Extractor Predicting: 237it [02:38,  1.42it/s]Extractor Predicting: 238it [02:39,  1.41it/s]Extractor Predicting: 239it [02:39,  1.41it/s]Extractor Predicting: 240it [02:40,  1.42it/s]Extractor Predicting: 241it [02:41,  1.42it/s]Extractor Predicting: 242it [02:41,  1.44it/s]Extractor Predicting: 243it [02:42,  1.44it/s]Extractor Predicting: 244it [02:43,  1.45it/s]Extractor Predicting: 245it [02:43,  1.44it/s]Extractor Predicting: 246it [02:44,  1.42it/s]Extractor Predicting: 247it [02:45,  1.38it/s]Extractor Predicting: 248it [02:46,  1.40it/s]Extractor Predicting: 249it [02:46,  1.45it/s]Extractor Predicting: 250it [02:47,  1.47it/s]Extractor Predicting: 251it [02:48,  1.45it/s]Extractor Predicting: 252it [02:48,  1.48it/s]Extractor Predicting: 253it [02:49,  1.48it/s]Extractor Predicting: 254it [02:50,  1.45it/s]Extractor Predicting: 255it [02:50,  1.45it/s]Extractor Predicting: 256it [02:51,  1.43it/s]Extractor Predicting: 257it [02:52,  1.44it/s]Extractor Predicting: 258it [02:52,  1.43it/s]Extractor Predicting: 259it [02:53,  1.46it/s]Extractor Predicting: 260it [02:54,  1.47it/s]Extractor Predicting: 261it [02:54,  1.50it/s]Extractor Predicting: 262it [02:55,  1.49it/s]Extractor Predicting: 263it [02:56,  1.48it/s]Extractor Predicting: 264it [02:56,  1.50it/s]Extractor Predicting: 265it [02:57,  1.50it/s]Extractor Predicting: 266it [02:58,  1.49it/s]Extractor Predicting: 267it [02:58,  1.52it/s]Extractor Predicting: 268it [02:59,  1.51it/s]Extractor Predicting: 269it [03:00,  1.50it/s]Extractor Predicting: 270it [03:00,  1.48it/s]Extractor Predicting: 271it [03:01,  1.51it/s]Extractor Predicting: 272it [03:02,  1.50it/s]Extractor Predicting: 273it [03:02,  1.52it/s]Extractor Predicting: 274it [03:03,  1.53it/s]Extractor Predicting: 275it [03:04,  1.54it/s]Extractor Predicting: 276it [03:04,  1.51it/s]Extractor Predicting: 277it [03:05,  1.51it/s]Extractor Predicting: 278it [03:06,  1.52it/s]Extractor Predicting: 279it [03:06,  1.51it/s]Extractor Predicting: 280it [03:07,  1.50it/s]Extractor Predicting: 281it [03:08,  1.53it/s]Extractor Predicting: 282it [03:08,  1.52it/s]Extractor Predicting: 283it [03:09,  1.35it/s]Extractor Predicting: 284it [03:10,  1.57it/s]Extractor Predicting: 284it [03:10,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.32it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.38it/s]Extractor Predicting: 5it [00:03,  1.84it/s]Extractor Predicting: 5it [00:03,  1.59it/s]
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_10_seed_3/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_3', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl'}
train vocab size: 86326
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 86426, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/model', pretrained_wv='outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=86426, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.302, loss:51059.1524
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.058, loss:2751.7272
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.034, loss:2402.5352
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.035, loss:2437.9964
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.024, loss:2292.2241
>> valid entity prec:0.3884, rec:0.6451, f1:0.4848
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 4.128, loss:2218.4623
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.031, loss:2132.5502
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 1.018, loss:1993.2576
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 1.017, loss:1963.0380
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 1.037, loss:1808.6831
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4103, rec:0.3507, f1:0.3782
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 1100, avg_time 3.997, loss:1743.4318
g_step 1200, step 1200, avg_time 1.030, loss:1737.0235
g_step 1300, step 1300, avg_time 1.033, loss:1570.2668
g_step 1400, step 1400, avg_time 1.026, loss:1499.6708
g_step 1500, step 1500, avg_time 1.051, loss:1556.2204
>> valid entity prec:0.4255, rec:0.7179, f1:0.5343
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 1600, avg_time 4.087, loss:1490.4976
g_step 1700, step 1700, avg_time 1.023, loss:1451.3411
g_step 1800, step 1800, avg_time 1.024, loss:1415.9959
g_step 1900, step 1900, avg_time 1.034, loss:1418.8433
g_step 2000, step 2000, avg_time 1.024, loss:1410.7617
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4681, rec:0.5101, f1:0.4882
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 2100, avg_time 4.017, loss:1366.5836
g_step 2200, step 2200, avg_time 1.018, loss:1352.3071
g_step 2300, step 2300, avg_time 1.028, loss:1322.0166
g_step 2400, step 2400, avg_time 1.036, loss:1346.2146
g_step 2500, step 2500, avg_time 1.040, loss:1341.5288
>> valid entity prec:0.4652, rec:0.5447, f1:0.5018
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 2600, avg_time 4.021, loss:1319.6702
g_step 2700, step 2700, avg_time 1.031, loss:1323.5725
g_step 2800, step 2800, avg_time 1.032, loss:1242.7276
g_step 2900, step 2900, avg_time 1.023, loss:1261.1551
g_step 3000, step 3000, avg_time 1.038, loss:1265.8638
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4478, rec:0.4640, f1:0.4558
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 3100, avg_time 4.010, loss:1225.7585
g_step 3200, step 3200, avg_time 1.033, loss:1221.4859
g_step 3300, step 72, avg_time 1.028, loss:1193.3302
g_step 3400, step 172, avg_time 1.033, loss:1188.5974
g_step 3500, step 272, avg_time 1.031, loss:1159.3427
>> valid entity prec:0.4568, rec:0.4890, f1:0.4723
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 372, avg_time 4.005, loss:1186.5076
g_step 3700, step 472, avg_time 1.017, loss:1199.4341
g_step 3800, step 572, avg_time 1.022, loss:1177.3670
g_step 3900, step 672, avg_time 1.020, loss:1157.6318
g_step 4000, step 772, avg_time 1.038, loss:1182.2248
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4547, rec:0.4639, f1:0.4593
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 872, avg_time 4.003, loss:1160.5726
g_step 4200, step 972, avg_time 1.035, loss:1186.1651
g_step 4300, step 1072, avg_time 1.032, loss:1215.8442
g_step 4400, step 1172, avg_time 1.032, loss:1175.1678
g_step 4500, step 1272, avg_time 1.029, loss:1138.9991
>> valid entity prec:0.4583, rec:0.4772, f1:0.4676
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 1372, avg_time 3.995, loss:1119.8046
g_step 4700, step 1472, avg_time 1.037, loss:1122.7230
g_step 4800, step 1572, avg_time 1.041, loss:1139.2304
g_step 4900, step 1672, avg_time 1.028, loss:1141.8290
g_step 5000, step 1772, avg_time 1.031, loss:1161.6139
learning rate was adjusted to 0.0008
>> valid entity prec:0.4551, rec:0.4780, f1:0.4663
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 1872, avg_time 4.009, loss:1172.0911
g_step 5200, step 1972, avg_time 1.017, loss:1090.5834
g_step 5300, step 2072, avg_time 1.037, loss:1080.9381
g_step 5400, step 2172, avg_time 1.029, loss:1082.0920
g_step 5500, step 2272, avg_time 1.024, loss:1093.0966
>> valid entity prec:0.4463, rec:0.7023, f1:0.5458
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 2372, avg_time 4.063, loss:1107.1533
g_step 5700, step 2472, avg_time 1.026, loss:1155.9699
g_step 5800, step 2572, avg_time 1.027, loss:1090.4805
g_step 5900, step 2672, avg_time 1.028, loss:1066.2015
g_step 6000, step 2772, avg_time 1.038, loss:1163.9100
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4776, rec:0.5073, f1:0.4920
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 2872, avg_time 4.021, loss:1048.6380
g_step 6200, step 2972, avg_time 1.021, loss:1031.9410
g_step 6300, step 3072, avg_time 1.029, loss:1091.6080
g_step 6400, step 3172, avg_time 1.035, loss:1079.0879
g_step 6500, step 44, avg_time 1.029, loss:1075.0343
>> valid entity prec:0.4870, rec:0.5728, f1:0.5265
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 144, avg_time 4.028, loss:1046.5100
g_step 6700, step 244, avg_time 1.019, loss:999.9093
g_step 6800, step 344, avg_time 1.032, loss:1086.6593
g_step 6900, step 444, avg_time 1.035, loss:1095.3306
g_step 7000, step 544, avg_time 1.033, loss:1058.3753
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4548, rec:0.5695, f1:0.5058
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 644, avg_time 4.016, loss:1064.3218
g_step 7200, step 744, avg_time 1.018, loss:963.7506
g_step 7300, step 844, avg_time 1.029, loss:1069.6896
g_step 7400, step 944, avg_time 1.027, loss:1088.0215
g_step 7500, step 1044, avg_time 1.045, loss:1033.2713
>> valid entity prec:0.4721, rec:0.6057, f1:0.5306
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 1144, avg_time 4.039, loss:1024.6729
g_step 7700, step 1244, avg_time 1.024, loss:1034.3009
g_step 7800, step 1344, avg_time 1.044, loss:1080.2362
g_step 7900, step 1444, avg_time 1.016, loss:1070.8078
g_step 8000, step 1544, avg_time 1.021, loss:1036.9623
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4733, rec:0.5648, f1:0.5150
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 1644, avg_time 4.041, loss:1052.1259
g_step 8200, step 1744, avg_time 1.024, loss:1001.5820
g_step 8300, step 1844, avg_time 1.026, loss:990.8408
g_step 8400, step 1944, avg_time 1.035, loss:1056.2749
g_step 8500, step 2044, avg_time 1.030, loss:991.6161
>> valid entity prec:0.4739, rec:0.6052, f1:0.5315
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 2144, avg_time 4.035, loss:1019.5887
g_step 8700, step 2244, avg_time 1.027, loss:974.8002
g_step 8800, step 2344, avg_time 1.033, loss:981.4947
g_step 8900, step 2444, avg_time 1.030, loss:998.1355
g_step 9000, step 2544, avg_time 1.013, loss:950.1375
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.4804, rec:0.5491, f1:0.5124
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 2644, avg_time 4.005, loss:963.1274
g_step 9200, step 2744, avg_time 1.018, loss:1010.5366
g_step 9300, step 2844, avg_time 1.021, loss:1028.6725
g_step 9400, step 2944, avg_time 1.032, loss:972.7175
g_step 9500, step 3044, avg_time 1.028, loss:974.3069
>> valid entity prec:0.4668, rec:0.4487, f1:0.4575
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 3144, avg_time 3.988, loss:984.7728
g_step 9700, step 16, avg_time 1.024, loss:1040.2100
g_step 9800, step 116, avg_time 1.016, loss:979.3461
g_step 9900, step 216, avg_time 1.020, loss:966.2059
g_step 10000, step 316, avg_time 1.026, loss:945.5145
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.4844, rec:0.5413, f1:0.5112
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'labels': ['country', 'place of death', 'production company', 'screenwriter', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 21439
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21539, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:06,  6.45s/it]Extractor Predicting: 2it [00:07,  3.02s/it]Extractor Predicting: 3it [00:07,  1.94s/it]Extractor Predicting: 4it [00:08,  1.43s/it]Extractor Predicting: 5it [00:09,  1.16s/it]Extractor Predicting: 6it [00:09,  1.01it/s]Extractor Predicting: 7it [00:10,  1.03s/it]Extractor Predicting: 8it [00:11,  1.07it/s]Extractor Predicting: 9it [00:12,  1.14it/s]Extractor Predicting: 10it [00:13,  1.22it/s]Extractor Predicting: 11it [00:13,  1.28it/s]Extractor Predicting: 12it [00:14,  1.30it/s]Extractor Predicting: 13it [00:15,  1.38it/s]Extractor Predicting: 14it [00:15,  1.40it/s]Extractor Predicting: 15it [00:16,  1.18it/s]Extractor Predicting: 16it [00:17,  1.27it/s]Extractor Predicting: 17it [00:18,  1.32it/s]Extractor Predicting: 18it [00:18,  1.39it/s]Extractor Predicting: 19it [00:19,  1.42it/s]Extractor Predicting: 20it [00:20,  1.46it/s]Extractor Predicting: 21it [00:20,  1.47it/s]Extractor Predicting: 22it [00:21,  1.51it/s]Extractor Predicting: 23it [00:22,  1.54it/s]Extractor Predicting: 24it [00:22,  1.49it/s]Extractor Predicting: 25it [00:26,  1.45s/it]Extractor Predicting: 26it [00:26,  1.22s/it]Extractor Predicting: 27it [00:27,  1.06s/it]Extractor Predicting: 28it [00:28,  1.07it/s]Extractor Predicting: 29it [00:28,  1.11it/s]Extractor Predicting: 30it [00:29,  1.17it/s]Extractor Predicting: 31it [00:30,  1.25it/s]Extractor Predicting: 32it [00:30,  1.33it/s]Extractor Predicting: 33it [00:31,  1.38it/s]Extractor Predicting: 34it [00:32,  1.45it/s]Extractor Predicting: 35it [00:32,  1.51it/s]Extractor Predicting: 36it [00:33,  1.50it/s]Extractor Predicting: 37it [00:34,  1.57it/s]Extractor Predicting: 38it [00:34,  1.56it/s]Extractor Predicting: 39it [00:35,  1.53it/s]Extractor Predicting: 40it [00:36,  1.54it/s]Extractor Predicting: 41it [00:36,  1.56it/s]Extractor Predicting: 42it [00:37,  1.57it/s]Extractor Predicting: 43it [00:37,  1.55it/s]Extractor Predicting: 44it [00:38,  1.53it/s]Extractor Predicting: 45it [00:39,  1.56it/s]Extractor Predicting: 46it [00:39,  1.53it/s]Extractor Predicting: 47it [00:40,  1.52it/s]Extractor Predicting: 48it [00:41,  1.54it/s]Extractor Predicting: 49it [00:41,  1.57it/s]Extractor Predicting: 50it [00:42,  1.60it/s]Extractor Predicting: 51it [00:43,  1.61it/s]Extractor Predicting: 52it [00:43,  1.59it/s]Extractor Predicting: 53it [00:44,  1.62it/s]Extractor Predicting: 54it [00:44,  1.61it/s]Extractor Predicting: 55it [00:45,  1.61it/s]Extractor Predicting: 56it [00:46,  1.61it/s]Extractor Predicting: 57it [00:46,  1.64it/s]Extractor Predicting: 58it [00:47,  1.66it/s]Extractor Predicting: 59it [00:48,  1.57it/s]Extractor Predicting: 60it [00:48,  1.57it/s]Extractor Predicting: 61it [00:49,  1.55it/s]Extractor Predicting: 62it [00:49,  1.56it/s]Extractor Predicting: 63it [00:50,  1.54it/s]Extractor Predicting: 64it [00:51,  1.60it/s]Extractor Predicting: 65it [00:51,  1.64it/s]Extractor Predicting: 66it [00:52,  1.62it/s]Extractor Predicting: 67it [00:53,  1.59it/s]Extractor Predicting: 68it [00:53,  1.59it/s]Extractor Predicting: 69it [00:54,  1.60it/s]Extractor Predicting: 70it [00:55,  1.55it/s]Extractor Predicting: 71it [00:55,  1.59it/s]Extractor Predicting: 72it [00:56,  1.55it/s]Extractor Predicting: 73it [00:56,  1.52it/s]Extractor Predicting: 74it [00:57,  1.50it/s]Extractor Predicting: 75it [00:58,  1.51it/s]Extractor Predicting: 76it [00:59,  1.48it/s]Extractor Predicting: 77it [00:59,  1.45it/s]Extractor Predicting: 78it [01:00,  1.47it/s]Extractor Predicting: 79it [01:01,  1.43it/s]Extractor Predicting: 80it [01:01,  1.41it/s]Extractor Predicting: 81it [01:02,  1.39it/s]Extractor Predicting: 82it [01:03,  1.38it/s]Extractor Predicting: 83it [01:04,  1.38it/s]Extractor Predicting: 84it [01:04,  1.38it/s]Extractor Predicting: 85it [01:05,  1.38it/s]Extractor Predicting: 86it [01:06,  1.39it/s]Extractor Predicting: 87it [01:06,  1.39it/s]Extractor Predicting: 88it [01:07,  1.39it/s]Extractor Predicting: 89it [01:08,  1.41it/s]Extractor Predicting: 90it [01:09,  1.41it/s]Extractor Predicting: 91it [01:09,  1.39it/s]Extractor Predicting: 92it [01:10,  1.39it/s]Extractor Predicting: 93it [01:11,  1.38it/s]Extractor Predicting: 94it [01:12,  1.37it/s]Extractor Predicting: 95it [01:12,  1.39it/s]Extractor Predicting: 96it [01:13,  1.39it/s]Extractor Predicting: 97it [01:14,  1.41it/s]Extractor Predicting: 98it [01:14,  1.42it/s]Extractor Predicting: 99it [01:15,  1.42it/s]Extractor Predicting: 100it [01:16,  1.41it/s]Extractor Predicting: 101it [01:16,  1.41it/s]Extractor Predicting: 102it [01:17,  1.37it/s]Extractor Predicting: 103it [01:18,  1.38it/s]Extractor Predicting: 104it [01:19,  1.37it/s]Extractor Predicting: 105it [01:19,  1.38it/s]Extractor Predicting: 106it [01:20,  1.38it/s]Extractor Predicting: 107it [01:21,  1.38it/s]Extractor Predicting: 108it [01:22,  1.40it/s]Extractor Predicting: 109it [01:22,  1.39it/s]Extractor Predicting: 110it [01:23,  1.42it/s]Extractor Predicting: 111it [01:24,  1.42it/s]Extractor Predicting: 112it [01:24,  1.44it/s]Extractor Predicting: 113it [01:25,  1.44it/s]Extractor Predicting: 114it [01:26,  1.43it/s]Extractor Predicting: 115it [01:27,  1.37it/s]Extractor Predicting: 116it [01:27,  1.38it/s]Extractor Predicting: 117it [01:28,  1.39it/s]Extractor Predicting: 118it [01:29,  1.39it/s]Extractor Predicting: 119it [01:29,  1.37it/s]Extractor Predicting: 120it [01:30,  1.39it/s]Extractor Predicting: 121it [01:31,  1.41it/s]Extractor Predicting: 122it [01:32,  1.38it/s]Extractor Predicting: 123it [01:32,  1.39it/s]Extractor Predicting: 124it [01:33,  1.38it/s]Extractor Predicting: 125it [01:34,  1.39it/s]Extractor Predicting: 126it [01:34,  1.42it/s]Extractor Predicting: 127it [01:35,  1.47it/s]Extractor Predicting: 128it [01:36,  1.35it/s]Extractor Predicting: 129it [01:37,  1.40it/s]Extractor Predicting: 130it [01:37,  1.39it/s]Extractor Predicting: 131it [01:38,  1.42it/s]Extractor Predicting: 132it [01:39,  1.41it/s]Extractor Predicting: 133it [01:39,  1.41it/s]Extractor Predicting: 134it [01:40,  1.42it/s]Extractor Predicting: 135it [01:41,  1.44it/s]Extractor Predicting: 136it [01:41,  1.47it/s]Extractor Predicting: 137it [01:42,  1.47it/s]Extractor Predicting: 138it [01:43,  1.49it/s]Extractor Predicting: 139it [01:43,  1.49it/s]Extractor Predicting: 140it [01:44,  1.50it/s]Extractor Predicting: 141it [01:45,  1.51it/s]Extractor Predicting: 142it [01:45,  1.49it/s]Extractor Predicting: 143it [01:46,  1.45it/s]Extractor Predicting: 144it [01:47,  1.46it/s]Extractor Predicting: 145it [01:47,  1.50it/s]Extractor Predicting: 146it [01:48,  1.52it/s]Extractor Predicting: 147it [01:49,  1.51it/s]Extractor Predicting: 148it [01:49,  1.52it/s]Extractor Predicting: 149it [01:50,  1.49it/s]Extractor Predicting: 150it [01:51,  1.51it/s]Extractor Predicting: 151it [01:51,  1.48it/s]Extractor Predicting: 152it [01:52,  1.48it/s]Extractor Predicting: 153it [01:53,  1.48it/s]Extractor Predicting: 154it [01:54,  1.43it/s]Extractor Predicting: 155it [01:54,  1.43it/s]Extractor Predicting: 156it [01:55,  1.43it/s]Extractor Predicting: 157it [01:56,  1.44it/s]Extractor Predicting: 158it [01:56,  1.43it/s]Extractor Predicting: 159it [01:57,  1.43it/s]Extractor Predicting: 160it [01:58,  1.45it/s]Extractor Predicting: 161it [01:58,  1.44it/s]Extractor Predicting: 162it [01:59,  1.49it/s]Extractor Predicting: 163it [02:00,  1.60it/s]Extractor Predicting: 164it [02:00,  1.70it/s]Extractor Predicting: 165it [02:01,  1.72it/s]Extractor Predicting: 166it [02:01,  1.68it/s]Extractor Predicting: 167it [02:02,  1.61it/s]Extractor Predicting: 168it [02:03,  1.54it/s]Extractor Predicting: 169it [02:03,  1.49it/s]Extractor Predicting: 170it [02:04,  1.50it/s]Extractor Predicting: 171it [02:05,  1.50it/s]Extractor Predicting: 172it [02:05,  1.49it/s]Extractor Predicting: 173it [02:06,  1.49it/s]Extractor Predicting: 174it [02:07,  1.49it/s]Extractor Predicting: 175it [02:07,  1.46it/s]Extractor Predicting: 176it [02:08,  1.46it/s]Extractor Predicting: 177it [02:09,  1.46it/s]Extractor Predicting: 178it [02:09,  1.48it/s]Extractor Predicting: 179it [02:10,  1.44it/s]Extractor Predicting: 180it [02:11,  1.45it/s]Extractor Predicting: 181it [02:11,  1.48it/s]Extractor Predicting: 182it [02:12,  1.49it/s]Extractor Predicting: 183it [02:13,  1.46it/s]Extractor Predicting: 184it [02:14,  1.46it/s]Extractor Predicting: 185it [02:14,  1.42it/s]Extractor Predicting: 186it [02:15,  1.45it/s]Extractor Predicting: 187it [02:16,  1.45it/s]Extractor Predicting: 188it [02:16,  1.46it/s]Extractor Predicting: 189it [02:17,  1.46it/s]Extractor Predicting: 190it [02:18,  1.45it/s]Extractor Predicting: 191it [02:18,  1.47it/s]Extractor Predicting: 192it [02:19,  1.49it/s]Extractor Predicting: 193it [02:20,  1.48it/s]Extractor Predicting: 194it [02:20,  1.49it/s]Extractor Predicting: 195it [02:21,  1.45it/s]Extractor Predicting: 196it [02:22,  1.47it/s]Extractor Predicting: 197it [02:22,  1.45it/s]Extractor Predicting: 198it [02:23,  1.46it/s]Extractor Predicting: 199it [02:24,  1.45it/s]Extractor Predicting: 200it [02:25,  1.45it/s]Extractor Predicting: 201it [02:25,  1.42it/s]Extractor Predicting: 202it [02:26,  1.43it/s]Extractor Predicting: 203it [02:27,  1.43it/s]Extractor Predicting: 204it [02:27,  1.43it/s]Extractor Predicting: 205it [02:28,  1.43it/s]Extractor Predicting: 206it [02:29,  1.43it/s]Extractor Predicting: 207it [02:29,  1.47it/s]Extractor Predicting: 208it [02:30,  1.49it/s]Extractor Predicting: 209it [02:31,  1.47it/s]Extractor Predicting: 210it [02:31,  1.46it/s]Extractor Predicting: 211it [02:32,  1.46it/s]Extractor Predicting: 212it [02:33,  1.44it/s]Extractor Predicting: 213it [02:34,  1.41it/s]Extractor Predicting: 214it [02:34,  1.46it/s]Extractor Predicting: 215it [02:35,  1.47it/s]Extractor Predicting: 216it [02:36,  1.44it/s]Extractor Predicting: 217it [02:36,  1.41it/s]Extractor Predicting: 218it [02:37,  1.39it/s]Extractor Predicting: 219it [02:38,  1.39it/s]Extractor Predicting: 220it [02:39,  1.39it/s]Extractor Predicting: 221it [02:39,  1.43it/s]Extractor Predicting: 222it [02:40,  1.43it/s]Extractor Predicting: 223it [02:41,  1.44it/s]Extractor Predicting: 224it [02:41,  1.47it/s]Extractor Predicting: 225it [02:42,  1.51it/s]Extractor Predicting: 226it [02:43,  1.49it/s]Extractor Predicting: 227it [02:43,  1.48it/s]Extractor Predicting: 228it [02:44,  1.48it/s]Extractor Predicting: 229it [02:45,  1.41it/s]Extractor Predicting: 230it [02:45,  1.46it/s]Extractor Predicting: 231it [02:46,  1.46it/s]Extractor Predicting: 232it [02:47,  1.43it/s]Extractor Predicting: 233it [02:47,  1.40it/s]Extractor Predicting: 234it [02:48,  1.35it/s]Extractor Predicting: 235it [02:49,  1.37it/s]Extractor Predicting: 236it [02:50,  1.35it/s]Extractor Predicting: 237it [02:51,  1.33it/s]Extractor Predicting: 238it [02:51,  1.32it/s]Extractor Predicting: 239it [02:52,  1.30it/s]Extractor Predicting: 240it [02:53,  1.32it/s]Extractor Predicting: 241it [02:54,  1.36it/s]Extractor Predicting: 242it [02:55,  1.23it/s]Extractor Predicting: 243it [02:55,  1.30it/s]Extractor Predicting: 244it [02:56,  1.33it/s]Extractor Predicting: 245it [02:57,  1.39it/s]Extractor Predicting: 246it [02:57,  1.41it/s]Extractor Predicting: 247it [02:58,  1.44it/s]Extractor Predicting: 248it [02:59,  1.47it/s]Extractor Predicting: 249it [02:59,  1.51it/s]Extractor Predicting: 250it [03:00,  1.48it/s]Extractor Predicting: 251it [03:00,  1.52it/s]Extractor Predicting: 252it [03:01,  1.49it/s]Extractor Predicting: 253it [03:02,  1.48it/s]Extractor Predicting: 254it [03:03,  1.49it/s]Extractor Predicting: 255it [03:03,  1.50it/s]Extractor Predicting: 256it [03:04,  1.48it/s]Extractor Predicting: 257it [03:05,  1.49it/s]Extractor Predicting: 258it [03:05,  1.50it/s]Extractor Predicting: 259it [03:06,  1.48it/s]Extractor Predicting: 260it [03:07,  1.48it/s]Extractor Predicting: 261it [03:07,  1.46it/s]Extractor Predicting: 262it [03:08,  1.45it/s]Extractor Predicting: 263it [03:09,  1.45it/s]Extractor Predicting: 264it [03:09,  1.46it/s]Extractor Predicting: 265it [03:10,  1.46it/s]Extractor Predicting: 266it [03:11,  1.45it/s]Extractor Predicting: 267it [03:11,  1.45it/s]Extractor Predicting: 268it [03:12,  1.44it/s]Extractor Predicting: 269it [03:13,  1.42it/s]Extractor Predicting: 270it [03:13,  1.45it/s]Extractor Predicting: 271it [03:14,  1.46it/s]Extractor Predicting: 272it [03:15,  1.52it/s]Extractor Predicting: 273it [03:15,  1.51it/s]Extractor Predicting: 274it [03:16,  1.52it/s]Extractor Predicting: 275it [03:17,  1.47it/s]Extractor Predicting: 276it [03:17,  1.46it/s]Extractor Predicting: 277it [03:18,  1.48it/s]Extractor Predicting: 278it [03:19,  1.46it/s]Extractor Predicting: 279it [03:20,  1.48it/s]Extractor Predicting: 280it [03:20,  1.53it/s]Extractor Predicting: 281it [03:21,  1.52it/s]Extractor Predicting: 282it [03:21,  1.49it/s]Extractor Predicting: 283it [03:22,  1.46it/s]Extractor Predicting: 284it [03:23,  1.43it/s]Extractor Predicting: 285it [03:24,  1.39it/s]Extractor Predicting: 286it [03:24,  1.40it/s]Extractor Predicting: 287it [03:25,  1.39it/s]Extractor Predicting: 288it [03:26,  1.38it/s]Extractor Predicting: 289it [03:27,  1.38it/s]Extractor Predicting: 290it [03:27,  1.39it/s]Extractor Predicting: 291it [03:28,  1.42it/s]Extractor Predicting: 292it [03:29,  1.44it/s]Extractor Predicting: 293it [03:29,  1.44it/s]Extractor Predicting: 294it [03:30,  1.40it/s]Extractor Predicting: 295it [03:31,  1.40it/s]Extractor Predicting: 296it [03:32,  1.39it/s]Extractor Predicting: 297it [03:32,  1.37it/s]Extractor Predicting: 298it [03:33,  1.36it/s]Extractor Predicting: 299it [03:34,  1.35it/s]Extractor Predicting: 300it [03:35,  1.36it/s]Extractor Predicting: 301it [03:35,  1.34it/s]Extractor Predicting: 302it [03:36,  1.33it/s]Extractor Predicting: 303it [03:37,  1.34it/s]Extractor Predicting: 303it [03:37,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:04,  1.49it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.53it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.47it/s]Extractor Predicting: 22it [00:14,  1.45it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:16,  1.47it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.48it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.49it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:24,  1.47it/s]Extractor Predicting: 38it [00:25,  1.48it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:28,  1.48it/s]Extractor Predicting: 44it [00:29,  1.44it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:31,  1.41it/s]Extractor Predicting: 47it [00:31,  1.42it/s]Extractor Predicting: 48it [00:32,  1.43it/s]Extractor Predicting: 49it [00:33,  1.43it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:36,  1.45it/s]Extractor Predicting: 55it [00:37,  1.49it/s]Extractor Predicting: 56it [00:37,  1.48it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.48it/s]Extractor Predicting: 59it [00:39,  1.48it/s]Extractor Predicting: 60it [00:40,  1.48it/s]Extractor Predicting: 61it [00:41,  1.47it/s]Extractor Predicting: 62it [00:41,  1.48it/s]Extractor Predicting: 63it [00:42,  1.50it/s]Extractor Predicting: 64it [00:43,  1.51it/s]Extractor Predicting: 65it [00:44,  1.41it/s]Extractor Predicting: 66it [00:44,  1.44it/s]Extractor Predicting: 67it [00:45,  1.47it/s]Extractor Predicting: 68it [00:46,  1.44it/s]Extractor Predicting: 69it [00:46,  1.44it/s]Extractor Predicting: 70it [00:47,  1.45it/s]Extractor Predicting: 71it [00:48,  1.46it/s]Extractor Predicting: 72it [00:48,  1.48it/s]Extractor Predicting: 73it [00:49,  1.48it/s]Extractor Predicting: 74it [00:50,  1.48it/s]Extractor Predicting: 75it [00:50,  1.47it/s]Extractor Predicting: 76it [00:51,  1.49it/s]Extractor Predicting: 77it [00:52,  1.48it/s]Extractor Predicting: 78it [00:52,  1.48it/s]Extractor Predicting: 79it [00:53,  1.48it/s]Extractor Predicting: 80it [00:54,  1.48it/s]Extractor Predicting: 81it [00:54,  1.47it/s]Extractor Predicting: 82it [00:55,  1.36it/s]Extractor Predicting: 83it [00:56,  1.39it/s]Extractor Predicting: 84it [00:57,  1.42it/s]Extractor Predicting: 85it [00:57,  1.43it/s]Extractor Predicting: 86it [00:58,  1.39it/s]Extractor Predicting: 87it [00:59,  1.39it/s]Extractor Predicting: 88it [00:59,  1.40it/s]Extractor Predicting: 89it [01:00,  1.41it/s]Extractor Predicting: 90it [01:01,  1.43it/s]Extractor Predicting: 91it [01:01,  1.44it/s]Extractor Predicting: 92it [01:02,  1.46it/s]Extractor Predicting: 93it [01:03,  1.46it/s]Extractor Predicting: 94it [01:04,  1.45it/s]Extractor Predicting: 95it [01:04,  1.47it/s]Extractor Predicting: 96it [01:05,  1.47it/s]Extractor Predicting: 97it [01:06,  1.44it/s]Extractor Predicting: 98it [01:06,  1.47it/s]Extractor Predicting: 99it [01:07,  1.47it/s]Extractor Predicting: 100it [01:08,  1.44it/s]Extractor Predicting: 101it [01:08,  1.46it/s]Extractor Predicting: 102it [01:09,  1.45it/s]Extractor Predicting: 103it [01:10,  1.44it/s]Extractor Predicting: 104it [01:10,  1.46it/s]Extractor Predicting: 105it [01:11,  1.45it/s]Extractor Predicting: 106it [01:12,  1.44it/s]Extractor Predicting: 107it [01:12,  1.47it/s]Extractor Predicting: 108it [01:13,  1.45it/s]Extractor Predicting: 109it [01:14,  1.45it/s]Extractor Predicting: 110it [01:15,  1.47it/s]Extractor Predicting: 111it [01:15,  1.51it/s]Extractor Predicting: 112it [01:16,  1.52it/s]Extractor Predicting: 113it [01:16,  1.52it/s]Extractor Predicting: 114it [01:17,  1.49it/s]Extractor Predicting: 115it [01:18,  1.50it/s]Extractor Predicting: 116it [01:18,  1.51it/s]Extractor Predicting: 117it [01:19,  1.48it/s]Extractor Predicting: 118it [01:20,  1.49it/s]Extractor Predicting: 119it [01:20,  1.49it/s]Extractor Predicting: 120it [01:21,  1.48it/s]Extractor Predicting: 121it [01:22,  1.50it/s]Extractor Predicting: 122it [01:23,  1.48it/s]Extractor Predicting: 123it [01:23,  1.50it/s]Extractor Predicting: 124it [01:24,  1.50it/s]Extractor Predicting: 125it [01:24,  1.51it/s]Extractor Predicting: 126it [01:25,  1.48it/s]Extractor Predicting: 127it [01:26,  1.48it/s]Extractor Predicting: 128it [01:27,  1.48it/s]Extractor Predicting: 129it [01:27,  1.53it/s]Extractor Predicting: 130it [01:28,  1.55it/s]Extractor Predicting: 131it [01:28,  1.53it/s]Extractor Predicting: 132it [01:29,  1.49it/s]Extractor Predicting: 133it [01:30,  1.44it/s]Extractor Predicting: 134it [01:31,  1.43it/s]Extractor Predicting: 135it [01:31,  1.45it/s]Extractor Predicting: 136it [01:32,  1.44it/s]Extractor Predicting: 137it [01:33,  1.44it/s]Extractor Predicting: 138it [01:33,  1.42it/s]Extractor Predicting: 139it [01:34,  1.42it/s]Extractor Predicting: 140it [01:35,  1.42it/s]Extractor Predicting: 141it [01:36,  1.41it/s]Extractor Predicting: 142it [01:36,  1.41it/s]Extractor Predicting: 143it [01:37,  1.41it/s]Extractor Predicting: 144it [01:38,  1.42it/s]Extractor Predicting: 145it [01:38,  1.42it/s]Extractor Predicting: 146it [01:39,  1.43it/s]Extractor Predicting: 147it [01:40,  1.41it/s]Extractor Predicting: 148it [01:40,  1.45it/s]Extractor Predicting: 149it [01:41,  1.44it/s]Extractor Predicting: 150it [01:42,  1.44it/s]Extractor Predicting: 151it [01:43,  1.44it/s]Extractor Predicting: 152it [01:43,  1.49it/s]Extractor Predicting: 153it [01:44,  1.50it/s]Extractor Predicting: 154it [01:44,  1.49it/s]Extractor Predicting: 155it [01:45,  1.48it/s]Extractor Predicting: 156it [01:46,  1.47it/s]Extractor Predicting: 157it [01:46,  1.49it/s]Extractor Predicting: 158it [01:47,  1.47it/s]Extractor Predicting: 159it [01:48,  1.45it/s]Extractor Predicting: 160it [01:49,  1.45it/s]Extractor Predicting: 161it [01:49,  1.45it/s]Extractor Predicting: 162it [01:50,  1.45it/s]Extractor Predicting: 163it [01:51,  1.45it/s]Extractor Predicting: 164it [01:51,  1.45it/s]Extractor Predicting: 165it [01:52,  1.49it/s]Extractor Predicting: 166it [01:53,  1.50it/s]Extractor Predicting: 167it [01:53,  1.48it/s]Extractor Predicting: 168it [01:54,  1.49it/s]Extractor Predicting: 169it [01:55,  1.49it/s]Extractor Predicting: 170it [01:55,  1.47it/s]Extractor Predicting: 171it [01:56,  1.44it/s]Extractor Predicting: 172it [01:57,  1.45it/s]Extractor Predicting: 173it [01:57,  1.44it/s]Extractor Predicting: 174it [01:58,  1.48it/s]Extractor Predicting: 175it [01:59,  1.50it/s]Extractor Predicting: 176it [01:59,  1.47it/s]Extractor Predicting: 177it [02:00,  1.48it/s]Extractor Predicting: 178it [02:01,  1.46it/s]Extractor Predicting: 179it [02:02,  1.45it/s]Extractor Predicting: 180it [02:02,  1.49it/s]Extractor Predicting: 181it [02:03,  1.51it/s]Extractor Predicting: 182it [02:04,  1.48it/s]Extractor Predicting: 183it [02:04,  1.51it/s]Extractor Predicting: 184it [02:05,  1.53it/s]Extractor Predicting: 185it [02:05,  1.52it/s]Extractor Predicting: 186it [02:06,  1.52it/s]Extractor Predicting: 187it [02:07,  1.53it/s]Extractor Predicting: 188it [02:07,  1.49it/s]Extractor Predicting: 189it [02:08,  1.46it/s]Extractor Predicting: 190it [02:09,  1.44it/s]Extractor Predicting: 191it [02:10,  1.47it/s]Extractor Predicting: 192it [02:10,  1.36it/s]Extractor Predicting: 193it [02:11,  1.38it/s]Extractor Predicting: 194it [02:12,  1.40it/s]Extractor Predicting: 195it [02:12,  1.44it/s]Extractor Predicting: 196it [02:13,  1.49it/s]Extractor Predicting: 197it [02:14,  1.49it/s]Extractor Predicting: 198it [02:14,  1.52it/s]Extractor Predicting: 199it [02:15,  1.52it/s]Extractor Predicting: 200it [02:16,  1.49it/s]Extractor Predicting: 201it [02:16,  1.50it/s]Extractor Predicting: 202it [02:17,  1.50it/s]Extractor Predicting: 203it [02:18,  1.51it/s]Extractor Predicting: 204it [02:18,  1.51it/s]Extractor Predicting: 205it [02:19,  1.53it/s]Extractor Predicting: 206it [02:20,  1.51it/s]Extractor Predicting: 207it [02:20,  1.52it/s]Extractor Predicting: 208it [02:21,  1.55it/s]Extractor Predicting: 209it [02:22,  1.54it/s]Extractor Predicting: 210it [02:22,  1.52it/s]Extractor Predicting: 211it [02:23,  1.51it/s]Extractor Predicting: 212it [02:24,  1.50it/s]Extractor Predicting: 213it [02:24,  1.49it/s]Extractor Predicting: 214it [02:25,  1.48it/s]Extractor Predicting: 215it [02:26,  1.48it/s]Extractor Predicting: 216it [02:26,  1.48it/s]Extractor Predicting: 217it [02:27,  1.46it/s]Extractor Predicting: 218it [02:28,  1.52it/s]Extractor Predicting: 219it [02:28,  1.52it/s]Extractor Predicting: 220it [02:29,  1.51it/s]Extractor Predicting: 221it [02:30,  1.48it/s]Extractor Predicting: 222it [02:30,  1.49it/s]Extractor Predicting: 223it [02:31,  1.51it/s]Extractor Predicting: 224it [02:32,  1.50it/s]Extractor Predicting: 225it [02:32,  1.53it/s]Extractor Predicting: 226it [02:33,  1.52it/s]Extractor Predicting: 227it [02:34,  1.53it/s]Extractor Predicting: 228it [02:34,  1.51it/s]Extractor Predicting: 229it [02:35,  1.51it/s]Extractor Predicting: 230it [02:36,  1.49it/s]Extractor Predicting: 231it [02:36,  1.51it/s]Extractor Predicting: 232it [02:37,  1.49it/s]Extractor Predicting: 233it [02:38,  1.48it/s]Extractor Predicting: 234it [02:38,  1.48it/s]Extractor Predicting: 235it [02:39,  1.49it/s]Extractor Predicting: 236it [02:40,  1.48it/s]Extractor Predicting: 237it [02:40,  1.50it/s]Extractor Predicting: 238it [02:41,  1.48it/s]Extractor Predicting: 239it [02:42,  1.53it/s]Extractor Predicting: 240it [02:42,  1.51it/s]Extractor Predicting: 241it [02:43,  1.48it/s]Extractor Predicting: 242it [02:44,  1.49it/s]Extractor Predicting: 243it [02:44,  1.52it/s]Extractor Predicting: 244it [02:45,  1.54it/s]Extractor Predicting: 245it [02:46,  1.55it/s]Extractor Predicting: 246it [02:46,  1.59it/s]Extractor Predicting: 247it [02:47,  1.55it/s]Extractor Predicting: 248it [02:47,  1.56it/s]Extractor Predicting: 249it [02:48,  1.55it/s]Extractor Predicting: 250it [02:49,  1.54it/s]Extractor Predicting: 251it [02:49,  1.54it/s]Extractor Predicting: 252it [02:50,  1.55it/s]Extractor Predicting: 253it [02:51,  1.54it/s]Extractor Predicting: 254it [02:51,  1.54it/s]Extractor Predicting: 255it [02:52,  1.57it/s]Extractor Predicting: 256it [02:53,  1.54it/s]Extractor Predicting: 257it [02:53,  1.54it/s]Extractor Predicting: 258it [02:54,  1.53it/s]Extractor Predicting: 259it [02:55,  1.53it/s]Extractor Predicting: 260it [02:55,  1.52it/s]Extractor Predicting: 261it [02:56,  1.54it/s]Extractor Predicting: 262it [02:57,  1.55it/s]Extractor Predicting: 263it [02:57,  1.58it/s]Extractor Predicting: 264it [02:58,  1.52it/s]Extractor Predicting: 265it [02:59,  1.49it/s]Extractor Predicting: 266it [02:59,  1.48it/s]Extractor Predicting: 267it [03:00,  1.47it/s]Extractor Predicting: 268it [03:01,  1.50it/s]Extractor Predicting: 269it [03:01,  1.53it/s]Extractor Predicting: 270it [03:02,  1.52it/s]Extractor Predicting: 271it [03:03,  1.52it/s]Extractor Predicting: 272it [03:03,  1.50it/s]Extractor Predicting: 273it [03:04,  1.54it/s]Extractor Predicting: 274it [03:05,  1.53it/s]Extractor Predicting: 275it [03:05,  1.55it/s]Extractor Predicting: 276it [03:06,  1.53it/s]Extractor Predicting: 277it [03:07,  1.47it/s]Extractor Predicting: 278it [03:07,  1.50it/s]Extractor Predicting: 279it [03:08,  1.54it/s]Extractor Predicting: 280it [03:08,  1.55it/s]Extractor Predicting: 281it [03:09,  1.57it/s]Extractor Predicting: 282it [03:10,  1.57it/s]Extractor Predicting: 283it [03:10,  1.57it/s]Extractor Predicting: 284it [03:11,  1.57it/s]Extractor Predicting: 285it [03:12,  1.53it/s]Extractor Predicting: 286it [03:12,  1.58it/s]Extractor Predicting: 287it [03:13,  1.57it/s]Extractor Predicting: 288it [03:14,  1.56it/s]Extractor Predicting: 289it [03:14,  1.56it/s]Extractor Predicting: 290it [03:15,  1.58it/s]Extractor Predicting: 291it [03:15,  1.58it/s]Extractor Predicting: 292it [03:16,  1.54it/s]Extractor Predicting: 293it [03:17,  1.54it/s]Extractor Predicting: 294it [03:17,  1.54it/s]Extractor Predicting: 295it [03:18,  1.54it/s]Extractor Predicting: 296it [03:19,  1.55it/s]Extractor Predicting: 297it [03:19,  1.51it/s]Extractor Predicting: 298it [03:20,  1.54it/s]Extractor Predicting: 299it [03:21,  1.36it/s]Extractor Predicting: 300it [03:22,  1.37it/s]Extractor Predicting: 301it [03:22,  1.34it/s]Extractor Predicting: 302it [03:23,  1.37it/s]Extractor Predicting: 303it [03:24,  1.37it/s]Extractor Predicting: 304it [03:25,  1.39it/s]Extractor Predicting: 305it [03:25,  1.39it/s]Extractor Predicting: 306it [03:26,  1.38it/s]Extractor Predicting: 306it [03:26,  1.48it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6322
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6422, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:04,  1.39it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:06,  1.42it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.42it/s]Extractor Predicting: 12it [00:08,  1.43it/s]Extractor Predicting: 13it [00:09,  1.43it/s]Extractor Predicting: 14it [00:09,  1.41it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.40it/s]Extractor Predicting: 17it [00:11,  1.38it/s]Extractor Predicting: 18it [00:12,  1.40it/s]Extractor Predicting: 19it [00:13,  1.42it/s]Extractor Predicting: 20it [00:14,  1.41it/s]Extractor Predicting: 21it [00:14,  1.41it/s]Extractor Predicting: 22it [00:15,  1.42it/s]Extractor Predicting: 23it [00:16,  1.46it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:17,  1.50it/s]Extractor Predicting: 26it [00:18,  1.51it/s]Extractor Predicting: 27it [00:18,  1.51it/s]Extractor Predicting: 28it [00:19,  1.51it/s]Extractor Predicting: 29it [00:20,  1.48it/s]Extractor Predicting: 30it [00:20,  1.50it/s]Extractor Predicting: 31it [00:21,  1.50it/s]Extractor Predicting: 32it [00:22,  1.50it/s]Extractor Predicting: 33it [00:22,  1.49it/s]Extractor Predicting: 34it [00:23,  1.51it/s]Extractor Predicting: 35it [00:24,  1.51it/s]Extractor Predicting: 36it [00:24,  1.55it/s]Extractor Predicting: 37it [00:25,  1.53it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.49it/s]Extractor Predicting: 41it [00:28,  1.47it/s]Extractor Predicting: 42it [00:28,  1.46it/s]Extractor Predicting: 43it [00:29,  1.43it/s]Extractor Predicting: 44it [00:30,  1.44it/s]Extractor Predicting: 45it [00:30,  1.45it/s]Extractor Predicting: 46it [00:31,  1.39it/s]Extractor Predicting: 47it [00:32,  1.42it/s]Extractor Predicting: 48it [00:32,  1.44it/s]Extractor Predicting: 49it [00:33,  1.43it/s]Extractor Predicting: 50it [00:34,  1.43it/s]Extractor Predicting: 51it [00:35,  1.44it/s]Extractor Predicting: 52it [00:35,  1.42it/s]Extractor Predicting: 53it [00:36,  1.43it/s]Extractor Predicting: 54it [00:37,  1.43it/s]Extractor Predicting: 55it [00:37,  1.43it/s]Extractor Predicting: 56it [00:38,  1.42it/s]Extractor Predicting: 57it [00:39,  1.38it/s]Extractor Predicting: 58it [00:40,  1.39it/s]Extractor Predicting: 59it [00:40,  1.38it/s]Extractor Predicting: 60it [00:41,  1.25it/s]Extractor Predicting: 60it [00:41,  1.44it/s]
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_10_seed_3/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_3', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/fewrel/unseen_10_seed_3/generator/synthetic.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 21358
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21458, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/model', pretrained_wv='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21458, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.623, loss:63582.5143
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.273, loss:2329.7096
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.273, loss:1898.0337
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 1.239, loss:1773.2713
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 1.255, loss:1731.2836
>> valid entity prec:0.4265, rec:0.4246, f1:0.4256
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 225, avg_time 2.698, loss:1602.4884
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 325, avg_time 1.258, loss:1540.2091
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.241, loss:1459.9458
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 1.252, loss:1400.1996
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 1.255, loss:1445.6144
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5519, rec:0.3650, f1:0.4394
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 350, avg_time 2.681, loss:1342.1991
g_step 1200, step 75, avg_time 1.252, loss:1332.4342
g_step 1300, step 175, avg_time 1.279, loss:1311.9378
g_step 1400, step 275, avg_time 1.253, loss:1301.6289
g_step 1500, step 375, avg_time 1.241, loss:1242.8164
>> valid entity prec:0.6462, rec:0.3247, f1:0.4322
>> valid relation prec:0.1429, rec:0.0003, f1:0.0006
>> valid relation with NER prec:0.1429, rec:0.0003, f1:0.0006
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 100, avg_time 1.245, loss:1238.8994
g_step 1700, step 200, avg_time 1.282, loss:1207.0700
g_step 1800, step 300, avg_time 1.232, loss:1236.0950
g_step 1900, step 25, avg_time 1.246, loss:1240.5581
g_step 2000, step 125, avg_time 1.287, loss:1211.0638
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4690, rec:0.6131, f1:0.5314
>> valid relation prec:0.3627, rec:0.0307, f1:0.0566
>> valid relation with NER prec:0.3627, rec:0.0307, f1:0.0566
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 225, avg_time 2.652, loss:1216.6852
g_step 2200, step 325, avg_time 1.232, loss:1175.5346
g_step 2300, step 50, avg_time 1.272, loss:1112.7378
g_step 2400, step 150, avg_time 1.250, loss:1149.8727
g_step 2500, step 250, avg_time 1.264, loss:1173.7008
>> valid entity prec:0.5263, rec:0.5116, f1:0.5188
>> valid relation prec:0.2768, rec:0.0623, f1:0.1017
>> valid relation with NER prec:0.2768, rec:0.0623, f1:0.1017
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 350, avg_time 2.679, loss:1166.7470
g_step 2700, step 75, avg_time 1.257, loss:1095.4498
g_step 2800, step 175, avg_time 1.243, loss:1149.7941
g_step 2900, step 275, avg_time 1.279, loss:1146.1321
g_step 3000, step 375, avg_time 1.244, loss:1111.0053
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5874, rec:0.4432, f1:0.5052
>> valid relation prec:0.2839, rec:0.0258, f1:0.0474
>> valid relation with NER prec:0.2839, rec:0.0258, f1:0.0474
g_step 3100, step 100, avg_time 1.254, loss:1069.4404
g_step 3200, step 200, avg_time 1.299, loss:1096.7889
g_step 3300, step 300, avg_time 1.261, loss:1096.9861
g_step 3400, step 25, avg_time 1.234, loss:1092.9841
g_step 3500, step 125, avg_time 1.258, loss:1039.4843
>> valid entity prec:0.5140, rec:0.6037, f1:0.5553
>> valid relation prec:0.2620, rec:0.0706, f1:0.1112
>> valid relation with NER prec:0.2620, rec:0.0706, f1:0.1112
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 225, avg_time 2.692, loss:1057.2508
g_step 3700, step 325, avg_time 1.288, loss:1078.7391
g_step 3800, step 50, avg_time 1.219, loss:1053.1934
g_step 3900, step 150, avg_time 1.253, loss:1049.4367
g_step 4000, step 250, avg_time 1.258, loss:1050.2815
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4894, rec:0.5988, f1:0.5386
>> valid relation prec:0.2258, rec:0.0597, f1:0.0944
>> valid relation with NER prec:0.2258, rec:0.0597, f1:0.0944
g_step 4100, step 350, avg_time 2.690, loss:1075.1401
g_step 4200, step 75, avg_time 1.278, loss:998.8395
g_step 4300, step 175, avg_time 1.248, loss:1001.4695
g_step 4400, step 275, avg_time 1.270, loss:1041.6942
g_step 4500, step 375, avg_time 1.273, loss:1018.2937
>> valid entity prec:0.4987, rec:0.5605, f1:0.5278
>> valid relation prec:0.2386, rec:0.0675, f1:0.1052
>> valid relation with NER prec:0.2386, rec:0.0675, f1:0.1052
g_step 4600, step 100, avg_time 1.268, loss:987.3272
g_step 4700, step 200, avg_time 1.254, loss:970.4329
g_step 4800, step 300, avg_time 1.256, loss:1002.6928
g_step 4900, step 25, avg_time 1.285, loss:1006.2476
g_step 5000, step 125, avg_time 1.252, loss:975.1128
learning rate was adjusted to 0.0008
>> valid entity prec:0.5709, rec:0.5030, f1:0.5348
>> valid relation prec:0.2996, rec:0.0663, f1:0.1086
>> valid relation with NER prec:0.2996, rec:0.0663, f1:0.1086
g_step 5100, step 225, avg_time 2.672, loss:960.5589
g_step 5200, step 325, avg_time 1.267, loss:971.5532
g_step 5300, step 50, avg_time 1.247, loss:943.0788
g_step 5400, step 150, avg_time 1.283, loss:951.3877
g_step 5500, step 250, avg_time 1.249, loss:925.6237
>> valid entity prec:0.4953, rec:0.5409, f1:0.5171
>> valid relation prec:0.2378, rec:0.0588, f1:0.0943
>> valid relation with NER prec:0.2378, rec:0.0588, f1:0.0943
g_step 5600, step 350, avg_time 2.681, loss:985.0744
g_step 5700, step 75, avg_time 1.268, loss:906.6560
g_step 5800, step 175, avg_time 1.244, loss:926.2652
g_step 5900, step 275, avg_time 1.262, loss:932.6540
g_step 6000, step 375, avg_time 1.282, loss:938.2396
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5576, rec:0.5976, f1:0.5769
>> valid relation prec:0.3300, rec:0.0749, f1:0.1221
>> valid relation with NER prec:0.3300, rec:0.0749, f1:0.1221
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6100, step 100, avg_time 1.259, loss:919.6627
g_step 6200, step 200, avg_time 1.225, loss:886.4896
g_step 6300, step 300, avg_time 1.284, loss:904.1342
g_step 6400, step 25, avg_time 1.279, loss:888.0601
g_step 6500, step 125, avg_time 1.291, loss:872.5512
>> valid entity prec:0.4963, rec:0.5968, f1:0.5419
>> valid relation prec:0.2530, rec:0.1392, f1:0.1796
>> valid relation with NER prec:0.2530, rec:0.1392, f1:0.1796
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6600, step 225, avg_time 2.685, loss:855.6567
g_step 6700, step 325, avg_time 1.223, loss:893.8815
g_step 6800, step 50, avg_time 1.286, loss:855.3634
g_step 6900, step 150, avg_time 1.276, loss:865.1613
g_step 7000, step 250, avg_time 1.248, loss:845.9773
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5016, rec:0.5711, f1:0.5341
>> valid relation prec:0.2164, rec:0.0809, f1:0.1178
>> valid relation with NER prec:0.2164, rec:0.0809, f1:0.1178
g_step 7100, step 350, avg_time 2.673, loss:870.0161
g_step 7200, step 75, avg_time 1.270, loss:815.5396
g_step 7300, step 175, avg_time 1.282, loss:822.1943
g_step 7400, step 275, avg_time 1.278, loss:799.4365
g_step 7500, step 375, avg_time 1.236, loss:856.4875
>> valid entity prec:0.5218, rec:0.5272, f1:0.5245
>> valid relation prec:0.2290, rec:0.0789, f1:0.1174
>> valid relation with NER prec:0.2290, rec:0.0789, f1:0.1174
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.39s/it]Extractor Predicting: 2it [00:08,  3.47s/it]Extractor Predicting: 3it [00:08,  2.20s/it]Extractor Predicting: 4it [00:09,  1.59s/it]Extractor Predicting: 5it [00:10,  1.24s/it]Extractor Predicting: 6it [00:10,  1.04s/it]Extractor Predicting: 7it [00:11,  1.10it/s]Extractor Predicting: 8it [00:11,  1.24it/s]Extractor Predicting: 9it [00:12,  1.30it/s]Extractor Predicting: 10it [00:13,  1.37it/s]Extractor Predicting: 11it [00:13,  1.42it/s]Extractor Predicting: 12it [00:14,  1.44it/s]Extractor Predicting: 13it [00:15,  1.46it/s]Extractor Predicting: 14it [00:15,  1.47it/s]Extractor Predicting: 15it [00:16,  1.50it/s]Extractor Predicting: 16it [00:17,  1.49it/s]Extractor Predicting: 17it [00:17,  1.50it/s]Extractor Predicting: 18it [00:18,  1.52it/s]Extractor Predicting: 19it [00:19,  1.46it/s]Extractor Predicting: 20it [00:19,  1.49it/s]Extractor Predicting: 21it [00:20,  1.47it/s]Extractor Predicting: 22it [00:21,  1.50it/s]Extractor Predicting: 23it [00:21,  1.51it/s]Extractor Predicting: 24it [00:22,  1.53it/s]Extractor Predicting: 25it [00:23,  1.50it/s]Extractor Predicting: 26it [00:23,  1.51it/s]Extractor Predicting: 27it [00:24,  1.51it/s]Extractor Predicting: 28it [00:25,  1.52it/s]Extractor Predicting: 29it [00:25,  1.50it/s]Extractor Predicting: 30it [00:26,  1.49it/s]Extractor Predicting: 31it [00:27,  1.49it/s]Extractor Predicting: 32it [00:27,  1.51it/s]Extractor Predicting: 33it [00:28,  1.49it/s]Extractor Predicting: 34it [00:29,  1.48it/s]Extractor Predicting: 35it [00:29,  1.45it/s]Extractor Predicting: 36it [00:30,  1.44it/s]Extractor Predicting: 37it [00:31,  1.45it/s]Extractor Predicting: 38it [00:32,  1.45it/s]Extractor Predicting: 39it [00:32,  1.46it/s]Extractor Predicting: 40it [00:33,  1.45it/s]Extractor Predicting: 41it [00:34,  1.46it/s]Extractor Predicting: 42it [00:34,  1.45it/s]Extractor Predicting: 43it [00:35,  1.44it/s]Extractor Predicting: 44it [00:36,  1.43it/s]Extractor Predicting: 45it [00:36,  1.43it/s]Extractor Predicting: 46it [00:37,  1.42it/s]Extractor Predicting: 47it [00:38,  1.47it/s]Extractor Predicting: 48it [00:38,  1.46it/s]Extractor Predicting: 49it [00:39,  1.49it/s]Extractor Predicting: 50it [00:40,  1.45it/s]Extractor Predicting: 51it [00:40,  1.48it/s]Extractor Predicting: 52it [00:41,  1.49it/s]Extractor Predicting: 53it [00:42,  1.48it/s]Extractor Predicting: 54it [00:43,  1.47it/s]Extractor Predicting: 55it [00:43,  1.47it/s]Extractor Predicting: 56it [00:44,  1.47it/s]Extractor Predicting: 57it [00:45,  1.50it/s]Extractor Predicting: 58it [00:45,  1.49it/s]Extractor Predicting: 59it [00:46,  1.46it/s]Extractor Predicting: 60it [00:47,  1.45it/s]Extractor Predicting: 61it [00:47,  1.43it/s]Extractor Predicting: 62it [00:48,  1.43it/s]Extractor Predicting: 63it [00:49,  1.45it/s]Extractor Predicting: 64it [00:49,  1.43it/s]Extractor Predicting: 65it [00:50,  1.46it/s]Extractor Predicting: 66it [00:51,  1.47it/s]Extractor Predicting: 67it [00:51,  1.47it/s]Extractor Predicting: 68it [00:52,  1.49it/s]Extractor Predicting: 69it [00:53,  1.47it/s]Extractor Predicting: 70it [00:53,  1.47it/s]Extractor Predicting: 71it [00:54,  1.43it/s]Extractor Predicting: 72it [00:55,  1.46it/s]Extractor Predicting: 73it [00:56,  1.45it/s]Extractor Predicting: 74it [00:56,  1.46it/s]Extractor Predicting: 75it [00:57,  1.47it/s]Extractor Predicting: 76it [00:58,  1.48it/s]Extractor Predicting: 77it [00:58,  1.50it/s]Extractor Predicting: 78it [00:59,  1.43it/s]Extractor Predicting: 79it [01:00,  1.44it/s]Extractor Predicting: 80it [01:00,  1.44it/s]Extractor Predicting: 81it [01:01,  1.44it/s]Extractor Predicting: 82it [01:02,  1.44it/s]Extractor Predicting: 83it [01:02,  1.48it/s]Extractor Predicting: 84it [01:03,  1.46it/s]Extractor Predicting: 85it [01:04,  1.45it/s]Extractor Predicting: 86it [01:04,  1.46it/s]Extractor Predicting: 87it [01:05,  1.47it/s]Extractor Predicting: 88it [01:06,  1.49it/s]Extractor Predicting: 89it [01:06,  1.48it/s]Extractor Predicting: 90it [01:07,  1.51it/s]Extractor Predicting: 91it [01:08,  1.55it/s]Extractor Predicting: 92it [01:08,  1.56it/s]Extractor Predicting: 93it [01:09,  1.53it/s]Extractor Predicting: 94it [01:10,  1.55it/s]Extractor Predicting: 95it [01:10,  1.42it/s]Extractor Predicting: 96it [01:11,  1.44it/s]Extractor Predicting: 97it [01:12,  1.46it/s]Extractor Predicting: 98it [01:13,  1.45it/s]Extractor Predicting: 99it [01:13,  1.43it/s]Extractor Predicting: 100it [01:14,  1.43it/s]Extractor Predicting: 101it [01:15,  1.48it/s]Extractor Predicting: 102it [01:15,  1.53it/s]Extractor Predicting: 103it [01:16,  1.51it/s]Extractor Predicting: 104it [01:16,  1.53it/s]Extractor Predicting: 105it [01:17,  1.52it/s]Extractor Predicting: 106it [01:18,  1.53it/s]Extractor Predicting: 107it [01:18,  1.51it/s]Extractor Predicting: 108it [01:19,  1.51it/s]Extractor Predicting: 109it [01:20,  1.51it/s]Extractor Predicting: 110it [01:20,  1.49it/s]Extractor Predicting: 111it [01:21,  1.52it/s]Extractor Predicting: 112it [01:22,  1.53it/s]Extractor Predicting: 113it [01:22,  1.56it/s]Extractor Predicting: 114it [01:23,  1.55it/s]Extractor Predicting: 115it [01:24,  1.56it/s]Extractor Predicting: 116it [01:24,  1.56it/s]Extractor Predicting: 117it [01:25,  1.53it/s]Extractor Predicting: 118it [01:26,  1.50it/s]Extractor Predicting: 119it [01:26,  1.48it/s]Extractor Predicting: 120it [01:27,  1.47it/s]Extractor Predicting: 121it [01:28,  1.50it/s]Extractor Predicting: 122it [01:28,  1.50it/s]Extractor Predicting: 123it [01:29,  1.49it/s]Extractor Predicting: 124it [01:30,  1.47it/s]Extractor Predicting: 125it [01:30,  1.46it/s]Extractor Predicting: 126it [01:31,  1.44it/s]Extractor Predicting: 127it [01:32,  1.45it/s]Extractor Predicting: 128it [01:32,  1.50it/s]Extractor Predicting: 129it [01:33,  1.47it/s]Extractor Predicting: 130it [01:34,  1.50it/s]Extractor Predicting: 131it [01:34,  1.49it/s]Extractor Predicting: 132it [01:35,  1.48it/s]Extractor Predicting: 133it [01:36,  1.47it/s]Extractor Predicting: 134it [01:37,  1.44it/s]Extractor Predicting: 135it [01:37,  1.45it/s]Extractor Predicting: 136it [01:38,  1.47it/s]Extractor Predicting: 137it [01:39,  1.48it/s]Extractor Predicting: 138it [01:39,  1.48it/s]Extractor Predicting: 139it [01:40,  1.45it/s]Extractor Predicting: 140it [01:41,  1.48it/s]Extractor Predicting: 141it [01:41,  1.46it/s]Extractor Predicting: 142it [01:42,  1.44it/s]Extractor Predicting: 143it [01:43,  1.46it/s]Extractor Predicting: 144it [01:43,  1.75it/s]Extractor Predicting: 144it [01:43,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.29620710415412405,
  "recall": 0.14105504587155962,
  "score": 0.19110506894542628,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.44it/s]Extractor Predicting: 11it [00:07,  1.43it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.42it/s]Extractor Predicting: 18it [00:12,  1.45it/s]Extractor Predicting: 19it [00:13,  1.47it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.47it/s]Extractor Predicting: 22it [00:15,  1.51it/s]Extractor Predicting: 23it [00:15,  1.52it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:17,  1.46it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:19,  1.46it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:21,  1.50it/s]Extractor Predicting: 32it [00:21,  1.56it/s]Extractor Predicting: 33it [00:22,  1.59it/s]Extractor Predicting: 34it [00:22,  1.60it/s]Extractor Predicting: 35it [00:23,  1.63it/s]Extractor Predicting: 36it [00:24,  1.63it/s]Extractor Predicting: 37it [00:24,  1.63it/s]Extractor Predicting: 38it [00:25,  1.65it/s]Extractor Predicting: 39it [00:25,  1.68it/s]Extractor Predicting: 40it [00:26,  1.71it/s]Extractor Predicting: 41it [00:27,  1.69it/s]Extractor Predicting: 42it [00:27,  1.70it/s]Extractor Predicting: 43it [00:28,  1.68it/s]Extractor Predicting: 44it [00:28,  1.70it/s]Extractor Predicting: 45it [00:29,  1.69it/s]Extractor Predicting: 46it [00:30,  1.63it/s]Extractor Predicting: 47it [00:30,  1.67it/s]Extractor Predicting: 48it [00:31,  1.71it/s]Extractor Predicting: 49it [00:31,  1.73it/s]Extractor Predicting: 50it [00:32,  1.71it/s]Extractor Predicting: 51it [00:32,  1.72it/s]Extractor Predicting: 52it [00:33,  1.69it/s]Extractor Predicting: 53it [00:34,  1.68it/s]Extractor Predicting: 54it [00:34,  1.67it/s]Extractor Predicting: 55it [00:35,  1.64it/s]Extractor Predicting: 56it [00:35,  1.65it/s]Extractor Predicting: 57it [00:36,  1.64it/s]Extractor Predicting: 58it [00:37,  1.61it/s]Extractor Predicting: 59it [00:37,  1.57it/s]Extractor Predicting: 60it [00:38,  1.52it/s]Extractor Predicting: 61it [00:39,  1.47it/s]Extractor Predicting: 62it [00:40,  1.43it/s]Extractor Predicting: 63it [00:40,  1.43it/s]Extractor Predicting: 64it [00:41,  1.41it/s]Extractor Predicting: 65it [00:42,  1.45it/s]Extractor Predicting: 66it [00:42,  1.45it/s]Extractor Predicting: 67it [00:43,  1.45it/s]Extractor Predicting: 68it [00:44,  1.42it/s]Extractor Predicting: 69it [00:45,  1.40it/s]Extractor Predicting: 70it [00:45,  1.39it/s]Extractor Predicting: 71it [00:46,  1.36it/s]Extractor Predicting: 72it [00:47,  1.40it/s]Extractor Predicting: 73it [00:47,  1.39it/s]Extractor Predicting: 74it [00:48,  1.42it/s]Extractor Predicting: 75it [00:49,  1.43it/s]Extractor Predicting: 76it [00:50,  1.43it/s]Extractor Predicting: 77it [00:50,  1.42it/s]Extractor Predicting: 78it [00:51,  1.40it/s]Extractor Predicting: 79it [00:52,  1.39it/s]Extractor Predicting: 80it [00:52,  1.39it/s]Extractor Predicting: 81it [00:53,  1.27it/s]Extractor Predicting: 82it [00:54,  1.30it/s]Extractor Predicting: 83it [00:55,  1.32it/s]Extractor Predicting: 84it [00:55,  1.36it/s]Extractor Predicting: 85it [00:56,  1.36it/s]Extractor Predicting: 86it [00:57,  1.35it/s]Extractor Predicting: 87it [00:58,  1.36it/s]Extractor Predicting: 88it [00:58,  1.38it/s]Extractor Predicting: 89it [00:59,  1.38it/s]Extractor Predicting: 90it [01:00,  1.42it/s]Extractor Predicting: 91it [01:01,  1.39it/s]Extractor Predicting: 92it [01:01,  1.37it/s]Extractor Predicting: 93it [01:02,  1.41it/s]Extractor Predicting: 94it [01:03,  1.41it/s]Extractor Predicting: 95it [01:03,  1.44it/s]Extractor Predicting: 96it [01:04,  1.48it/s]Extractor Predicting: 97it [01:05,  1.47it/s]Extractor Predicting: 98it [01:05,  1.50it/s]Extractor Predicting: 99it [01:06,  1.48it/s]Extractor Predicting: 100it [01:07,  1.48it/s]Extractor Predicting: 101it [01:07,  1.54it/s]Extractor Predicting: 102it [01:08,  1.53it/s]Extractor Predicting: 103it [01:09,  1.49it/s]Extractor Predicting: 104it [01:09,  1.44it/s]Extractor Predicting: 105it [01:10,  1.45it/s]Extractor Predicting: 106it [01:11,  1.45it/s]Extractor Predicting: 107it [01:11,  1.46it/s]Extractor Predicting: 108it [01:12,  1.45it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:14,  1.43it/s]Extractor Predicting: 111it [01:14,  1.46it/s]Extractor Predicting: 112it [01:15,  1.42it/s]Extractor Predicting: 113it [01:16,  1.46it/s]Extractor Predicting: 114it [01:16,  1.42it/s]Extractor Predicting: 115it [01:17,  1.43it/s]Extractor Predicting: 116it [01:18,  1.49it/s]Extractor Predicting: 117it [01:18,  1.49it/s]Extractor Predicting: 118it [01:19,  1.54it/s]Extractor Predicting: 119it [01:19,  1.57it/s]Extractor Predicting: 120it [01:20,  1.58it/s]Extractor Predicting: 121it [01:21,  1.60it/s]Extractor Predicting: 122it [01:21,  1.60it/s]Extractor Predicting: 123it [01:22,  1.61it/s]Extractor Predicting: 124it [01:23,  1.62it/s]Extractor Predicting: 125it [01:23,  1.63it/s]Extractor Predicting: 126it [01:24,  1.61it/s]Extractor Predicting: 127it [01:24,  1.61it/s]Extractor Predicting: 128it [01:25,  1.65it/s]Extractor Predicting: 129it [01:26,  1.64it/s]Extractor Predicting: 130it [01:26,  1.71it/s]Extractor Predicting: 131it [01:27,  1.73it/s]Extractor Predicting: 132it [01:27,  1.68it/s]Extractor Predicting: 133it [01:28,  1.63it/s]Extractor Predicting: 134it [01:29,  1.62it/s]Extractor Predicting: 135it [01:29,  1.63it/s]Extractor Predicting: 136it [01:30,  1.66it/s]Extractor Predicting: 137it [01:30,  1.66it/s]Extractor Predicting: 138it [01:31,  1.66it/s]Extractor Predicting: 139it [01:32,  1.63it/s]Extractor Predicting: 140it [01:32,  1.66it/s]Extractor Predicting: 141it [01:33,  1.64it/s]Extractor Predicting: 142it [01:33,  1.69it/s]Extractor Predicting: 143it [01:34,  1.67it/s]Extractor Predicting: 144it [01:35,  1.65it/s]Extractor Predicting: 145it [01:35,  1.63it/s]Extractor Predicting: 146it [01:36,  1.58it/s]Extractor Predicting: 147it [01:37,  1.55it/s]Extractor Predicting: 148it [01:37,  1.56it/s]Extractor Predicting: 149it [01:38,  1.53it/s]Extractor Predicting: 150it [01:39,  1.52it/s]Extractor Predicting: 151it [01:39,  1.52it/s]Extractor Predicting: 152it [01:40,  1.52it/s]Extractor Predicting: 153it [01:41,  1.48it/s]Extractor Predicting: 154it [01:41,  1.51it/s]Extractor Predicting: 155it [01:42,  1.53it/s]Extractor Predicting: 156it [01:43,  1.50it/s]Extractor Predicting: 157it [01:43,  1.51it/s]Extractor Predicting: 158it [01:44,  1.49it/s]Extractor Predicting: 159it [01:45,  1.48it/s]Extractor Predicting: 160it [01:45,  1.47it/s]Extractor Predicting: 161it [01:46,  1.47it/s]Extractor Predicting: 162it [01:47,  1.46it/s]Extractor Predicting: 163it [01:47,  1.49it/s]Extractor Predicting: 164it [01:48,  1.49it/s]Extractor Predicting: 165it [01:49,  1.47it/s]Extractor Predicting: 166it [01:49,  1.43it/s]Extractor Predicting: 167it [01:50,  1.46it/s]Extractor Predicting: 168it [01:51,  1.45it/s]Extractor Predicting: 169it [01:51,  1.48it/s]Extractor Predicting: 170it [01:52,  1.47it/s]Extractor Predicting: 171it [01:53,  1.47it/s]Extractor Predicting: 172it [01:53,  1.48it/s]Extractor Predicting: 173it [01:54,  1.46it/s]Extractor Predicting: 174it [01:55,  1.46it/s]Extractor Predicting: 175it [01:56,  1.48it/s]Extractor Predicting: 176it [01:56,  1.48it/s]Extractor Predicting: 177it [01:57,  1.46it/s]Extractor Predicting: 178it [01:58,  1.47it/s]Extractor Predicting: 179it [01:58,  1.47it/s]Extractor Predicting: 180it [01:59,  1.46it/s]Extractor Predicting: 181it [02:00,  1.45it/s]Extractor Predicting: 182it [02:01,  1.32it/s]Extractor Predicting: 183it [02:01,  1.36it/s]Extractor Predicting: 184it [02:02,  1.38it/s]Extractor Predicting: 185it [02:03,  1.42it/s]Extractor Predicting: 186it [02:03,  1.41it/s]Extractor Predicting: 187it [02:04,  1.43it/s]Extractor Predicting: 188it [02:05,  1.43it/s]Extractor Predicting: 189it [02:05,  1.46it/s]Extractor Predicting: 190it [02:06,  1.39it/s]Extractor Predicting: 191it [02:07,  1.44it/s]Extractor Predicting: 192it [02:07,  1.45it/s]Extractor Predicting: 193it [02:08,  1.48it/s]Extractor Predicting: 194it [02:09,  1.45it/s]Extractor Predicting: 195it [02:10,  1.46it/s]Extractor Predicting: 196it [02:10,  1.46it/s]Extractor Predicting: 197it [02:11,  1.46it/s]Extractor Predicting: 198it [02:12,  1.43it/s]Extractor Predicting: 199it [02:12,  1.44it/s]Extractor Predicting: 200it [02:13,  1.45it/s]Extractor Predicting: 201it [02:14,  1.48it/s]Extractor Predicting: 202it [02:14,  1.47it/s]Extractor Predicting: 203it [02:15,  1.50it/s]Extractor Predicting: 204it [02:16,  1.48it/s]Extractor Predicting: 205it [02:16,  1.47it/s]Extractor Predicting: 206it [02:17,  1.53it/s]Extractor Predicting: 207it [02:18,  1.53it/s]Extractor Predicting: 208it [02:18,  1.54it/s]Extractor Predicting: 209it [02:19,  1.52it/s]Extractor Predicting: 210it [02:20,  1.51it/s]Extractor Predicting: 211it [02:20,  1.50it/s]Extractor Predicting: 212it [02:21,  1.46it/s]Extractor Predicting: 213it [02:22,  1.47it/s]Extractor Predicting: 214it [02:22,  1.47it/s]Extractor Predicting: 215it [02:23,  1.45it/s]Extractor Predicting: 216it [02:24,  1.45it/s]Extractor Predicting: 217it [02:24,  1.41it/s]Extractor Predicting: 218it [02:25,  1.44it/s]Extractor Predicting: 219it [02:26,  1.45it/s]Extractor Predicting: 220it [02:26,  1.48it/s]Extractor Predicting: 221it [02:27,  1.47it/s]Extractor Predicting: 222it [02:28,  1.47it/s]Extractor Predicting: 223it [02:28,  1.49it/s]Extractor Predicting: 224it [02:29,  1.47it/s]Extractor Predicting: 225it [02:30,  1.46it/s]Extractor Predicting: 226it [02:31,  1.47it/s]Extractor Predicting: 227it [02:31,  1.46it/s]Extractor Predicting: 228it [02:32,  1.50it/s]Extractor Predicting: 229it [02:33,  1.47it/s]Extractor Predicting: 230it [02:33,  1.48it/s]Extractor Predicting: 231it [02:34,  1.45it/s]Extractor Predicting: 232it [02:35,  1.44it/s]Extractor Predicting: 233it [02:35,  1.45it/s]Extractor Predicting: 234it [02:36,  1.47it/s]Extractor Predicting: 235it [02:37,  1.47it/s]Extractor Predicting: 236it [02:37,  1.48it/s]Extractor Predicting: 237it [02:38,  1.43it/s]Extractor Predicting: 238it [02:39,  1.41it/s]Extractor Predicting: 239it [02:40,  1.41it/s]Extractor Predicting: 240it [02:40,  1.42it/s]Extractor Predicting: 241it [02:41,  1.42it/s]Extractor Predicting: 242it [02:42,  1.44it/s]Extractor Predicting: 243it [02:42,  1.44it/s]Extractor Predicting: 244it [02:43,  1.44it/s]Extractor Predicting: 245it [02:44,  1.44it/s]Extractor Predicting: 246it [02:44,  1.41it/s]Extractor Predicting: 247it [02:45,  1.38it/s]Extractor Predicting: 248it [02:46,  1.40it/s]Extractor Predicting: 249it [02:47,  1.45it/s]Extractor Predicting: 250it [02:47,  1.46it/s]Extractor Predicting: 251it [02:48,  1.45it/s]Extractor Predicting: 252it [02:49,  1.47it/s]Extractor Predicting: 253it [02:49,  1.48it/s]Extractor Predicting: 254it [02:50,  1.45it/s]Extractor Predicting: 255it [02:51,  1.45it/s]Extractor Predicting: 256it [02:51,  1.43it/s]Extractor Predicting: 257it [02:52,  1.44it/s]Extractor Predicting: 258it [02:53,  1.43it/s]Extractor Predicting: 259it [02:53,  1.46it/s]Extractor Predicting: 260it [02:54,  1.47it/s]Extractor Predicting: 261it [02:55,  1.50it/s]Extractor Predicting: 262it [02:55,  1.49it/s]Extractor Predicting: 263it [02:56,  1.48it/s]Extractor Predicting: 264it [02:57,  1.49it/s]Extractor Predicting: 265it [02:57,  1.49it/s]Extractor Predicting: 266it [02:58,  1.49it/s]Extractor Predicting: 267it [02:59,  1.51it/s]Extractor Predicting: 268it [02:59,  1.51it/s]Extractor Predicting: 269it [03:00,  1.50it/s]Extractor Predicting: 270it [03:01,  1.47it/s]Extractor Predicting: 271it [03:01,  1.51it/s]Extractor Predicting: 272it [03:02,  1.36it/s]Extractor Predicting: 273it [03:03,  1.42it/s]Extractor Predicting: 274it [03:04,  1.45it/s]Extractor Predicting: 275it [03:04,  1.48it/s]Extractor Predicting: 276it [03:05,  1.46it/s]Extractor Predicting: 277it [03:06,  1.47it/s]Extractor Predicting: 278it [03:06,  1.48it/s]Extractor Predicting: 279it [03:07,  1.49it/s]Extractor Predicting: 280it [03:08,  1.47it/s]Extractor Predicting: 281it [03:08,  1.51it/s]Extractor Predicting: 282it [03:09,  1.49it/s]Extractor Predicting: 283it [03:10,  1.45it/s]Extractor Predicting: 284it [03:10,  1.67it/s]Extractor Predicting: 284it [03:10,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.37549407114624506,
  "recall": 0.11173184357541899,
  "score": 0.17221844550192614,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.31it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:02,  1.37it/s]Extractor Predicting: 5it [00:03,  1.83it/s]Extractor Predicting: 5it [00:03,  1.58it/s]
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.35,
  "recall": 0.03535353535353535,
  "score": 0.06422018348623852,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_3/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_3', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/wiki/unseen_10_seed_3/generator/synthetic.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl'}
train vocab size: 28251
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28351, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/model', pretrained_wv='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28351, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.510, loss:61144.5843
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.146, loss:2292.3479
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.143, loss:1938.9772
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 25, avg_time 1.148, loss:1803.6526
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 125, avg_time 1.146, loss:1727.3945
>> valid entity prec:0.3250, rec:0.3142, f1:0.3195
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 225, avg_time 4.122, loss:1607.5557
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 325, avg_time 1.151, loss:1592.2662
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 50, avg_time 1.133, loss:1512.3759
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 150, avg_time 1.155, loss:1514.1908
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 250, avg_time 1.142, loss:1407.1021
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.3938, rec:0.3217, f1:0.3541
>> valid relation prec:0.5000, rec:0.0001, f1:0.0002
>> valid relation with NER prec:0.5000, rec:0.0001, f1:0.0002
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 350, avg_time 4.087, loss:1474.6619
g_step 1200, step 75, avg_time 1.162, loss:1385.1615
g_step 1300, step 175, avg_time 1.137, loss:1387.6601
g_step 1400, step 275, avg_time 1.118, loss:1357.7266
g_step 1500, step 375, avg_time 1.151, loss:1413.3106
>> valid entity prec:0.4818, rec:0.3623, f1:0.4136
>> valid relation prec:0.6190, rec:0.0167, f1:0.0326
>> valid relation with NER prec:0.6190, rec:0.0167, f1:0.0326
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 100, avg_time 1.164, loss:1342.5723
g_step 1700, step 200, avg_time 1.135, loss:1358.3804
g_step 1800, step 300, avg_time 1.140, loss:1337.1814
g_step 1900, step 25, avg_time 1.130, loss:1313.6473
g_step 2000, step 125, avg_time 1.145, loss:1316.9176
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4602, rec:0.2904, f1:0.3561
>> valid relation prec:0.1250, rec:0.0007, f1:0.0014
>> valid relation with NER prec:0.1250, rec:0.0007, f1:0.0014
g_step 2100, step 225, avg_time 4.023, loss:1293.0751
g_step 2200, step 325, avg_time 1.149, loss:1260.5475
g_step 2300, step 50, avg_time 1.142, loss:1263.2396
g_step 2400, step 150, avg_time 1.131, loss:1259.2203
g_step 2500, step 250, avg_time 1.164, loss:1316.3718
>> valid entity prec:0.4856, rec:0.3866, f1:0.4305
>> valid relation prec:0.3862, rec:0.0236, f1:0.0445
>> valid relation with NER prec:0.3862, rec:0.0236, f1:0.0445
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 350, avg_time 4.059, loss:1252.9548
g_step 2700, step 75, avg_time 1.160, loss:1228.1512
g_step 2800, step 175, avg_time 1.134, loss:1260.0100
g_step 2900, step 275, avg_time 1.153, loss:1247.9364
g_step 3000, step 375, avg_time 1.139, loss:1204.9347
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5405, rec:0.4020, f1:0.4611
>> valid relation prec:0.3621, rec:0.0123, f1:0.0238
>> valid relation with NER prec:0.3621, rec:0.0123, f1:0.0238
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 100, avg_time 1.149, loss:1192.3221
g_step 3200, step 200, avg_time 1.136, loss:1200.2760
g_step 3300, step 300, avg_time 1.160, loss:1235.7578
g_step 3400, step 25, avg_time 1.144, loss:1197.6725
g_step 3500, step 125, avg_time 1.148, loss:1194.0236
>> valid entity prec:0.5165, rec:0.3932, f1:0.4465
>> valid relation prec:0.3469, rec:0.0209, f1:0.0395
>> valid relation with NER prec:0.3469, rec:0.0209, f1:0.0395
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 225, avg_time 4.060, loss:1188.3354
g_step 3700, step 325, avg_time 1.160, loss:1176.4180
g_step 3800, step 50, avg_time 1.117, loss:1171.5446
g_step 3900, step 150, avg_time 1.149, loss:1181.7216
g_step 4000, step 250, avg_time 1.154, loss:1141.3675
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5494, rec:0.4617, f1:0.5017
>> valid relation prec:0.4089, rec:0.0226, f1:0.0428
>> valid relation with NER prec:0.4089, rec:0.0226, f1:0.0428
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 350, avg_time 4.068, loss:1167.5067
g_step 4200, step 75, avg_time 1.158, loss:1116.9374
g_step 4300, step 175, avg_time 1.145, loss:1132.6024
g_step 4400, step 275, avg_time 1.147, loss:1162.1237
g_step 4500, step 375, avg_time 1.127, loss:1148.1653
>> valid entity prec:0.5689, rec:0.3284, f1:0.4165
>> valid relation prec:0.3195, rec:0.0225, f1:0.0420
>> valid relation with NER prec:0.3195, rec:0.0225, f1:0.0420
g_step 4600, step 100, avg_time 1.153, loss:1075.9565
g_step 4700, step 200, avg_time 1.149, loss:1132.9966
g_step 4800, step 300, avg_time 1.148, loss:1127.5635
g_step 4900, step 25, avg_time 1.133, loss:1110.5381
g_step 5000, step 125, avg_time 1.140, loss:1065.8437
learning rate was adjusted to 0.0008
>> valid entity prec:0.5229, rec:0.2975, f1:0.3792
>> valid relation prec:0.4405, rec:0.0277, f1:0.0521
>> valid relation with NER prec:0.4405, rec:0.0277, f1:0.0521
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 225, avg_time 4.025, loss:1094.6753
g_step 5200, step 325, avg_time 1.158, loss:1100.6768
g_step 5300, step 50, avg_time 1.128, loss:1090.7962
g_step 5400, step 150, avg_time 1.140, loss:1070.6419
g_step 5500, step 250, avg_time 1.155, loss:1042.9877
>> valid entity prec:0.5222, rec:0.5793, f1:0.5493
>> valid relation prec:0.3053, rec:0.0636, f1:0.1053
>> valid relation with NER prec:0.3053, rec:0.0636, f1:0.1053
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 350, avg_time 4.082, loss:1067.2325
g_step 5700, step 75, avg_time 1.141, loss:1023.0056
g_step 5800, step 175, avg_time 1.133, loss:1030.4536
g_step 5900, step 275, avg_time 1.133, loss:1039.8291
g_step 6000, step 375, avg_time 1.159, loss:1062.7317
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4782, rec:0.4126, f1:0.4430
>> valid relation prec:0.3545, rec:0.0410, f1:0.0736
>> valid relation with NER prec:0.3545, rec:0.0410, f1:0.0736
g_step 6100, step 100, avg_time 1.148, loss:1009.4519
g_step 6200, step 200, avg_time 1.138, loss:1007.3268
g_step 6300, step 300, avg_time 1.155, loss:1014.0197
g_step 6400, step 25, avg_time 1.157, loss:986.3197
g_step 6500, step 125, avg_time 1.156, loss:973.6644
>> valid entity prec:0.5436, rec:0.4326, f1:0.4818
>> valid relation prec:0.2956, rec:0.0479, f1:0.0825
>> valid relation with NER prec:0.2956, rec:0.0479, f1:0.0825
g_step 6600, step 225, avg_time 4.060, loss:1006.2023
g_step 6700, step 325, avg_time 1.134, loss:1007.3841
g_step 6800, step 50, avg_time 1.141, loss:959.8937
g_step 6900, step 150, avg_time 1.147, loss:956.5461
g_step 7000, step 250, avg_time 1.151, loss:960.1159
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5141, rec:0.4723, f1:0.4923
>> valid relation prec:0.2664, rec:0.0371, f1:0.0651
>> valid relation with NER prec:0.2664, rec:0.0371, f1:0.0651
g_step 7100, step 350, avg_time 4.069, loss:980.7184
g_step 7200, step 75, avg_time 1.142, loss:932.2648
g_step 7300, step 175, avg_time 1.150, loss:931.3788
g_step 7400, step 275, avg_time 1.150, loss:932.7398
g_step 7500, step 375, avg_time 1.138, loss:944.5255
>> valid entity prec:0.5249, rec:0.4854, f1:0.5044
>> valid relation prec:0.2876, rec:0.0554, f1:0.0929
>> valid relation with NER prec:0.2876, rec:0.0554, f1:0.0929
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'labels': ['country', 'place of death', 'production company', 'screenwriter', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 21439
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21539, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:06,  6.35s/it]Extractor Predicting: 2it [00:06,  2.97s/it]Extractor Predicting: 3it [00:07,  1.92s/it]Extractor Predicting: 4it [00:08,  1.41s/it]Extractor Predicting: 5it [00:08,  1.13s/it]Extractor Predicting: 6it [00:09,  1.03it/s]Extractor Predicting: 7it [00:10,  1.01s/it]Extractor Predicting: 8it [00:11,  1.10it/s]Extractor Predicting: 9it [00:12,  1.19it/s]Extractor Predicting: 10it [00:12,  1.27it/s]Extractor Predicting: 11it [00:13,  1.34it/s]Extractor Predicting: 12it [00:14,  1.34it/s]Extractor Predicting: 13it [00:14,  1.42it/s]Extractor Predicting: 14it [00:15,  1.45it/s]Extractor Predicting: 15it [00:16,  1.21it/s]Extractor Predicting: 16it [00:17,  1.31it/s]Extractor Predicting: 17it [00:17,  1.36it/s]Extractor Predicting: 18it [00:18,  1.43it/s]Extractor Predicting: 19it [00:19,  1.46it/s]Extractor Predicting: 20it [00:19,  1.50it/s]Extractor Predicting: 21it [00:20,  1.51it/s]Extractor Predicting: 22it [00:20,  1.55it/s]Extractor Predicting: 23it [00:21,  1.59it/s]Extractor Predicting: 24it [00:22,  1.54it/s]Extractor Predicting: 25it [00:25,  1.41s/it]Extractor Predicting: 26it [00:26,  1.19s/it]Extractor Predicting: 27it [00:26,  1.03s/it]Extractor Predicting: 28it [00:27,  1.10it/s]Extractor Predicting: 29it [00:27,  1.22it/s]Extractor Predicting: 30it [00:28,  1.25it/s]Extractor Predicting: 31it [00:29,  1.33it/s]Extractor Predicting: 32it [00:29,  1.40it/s]Extractor Predicting: 33it [00:30,  1.45it/s]Extractor Predicting: 34it [00:31,  1.51it/s]Extractor Predicting: 35it [00:31,  1.57it/s]Extractor Predicting: 36it [00:32,  1.57it/s]Extractor Predicting: 37it [00:33,  1.62it/s]Extractor Predicting: 38it [00:33,  1.50it/s]Extractor Predicting: 39it [00:34,  1.52it/s]Extractor Predicting: 40it [00:35,  1.55it/s]Extractor Predicting: 41it [00:35,  1.57it/s]Extractor Predicting: 42it [00:36,  1.60it/s]Extractor Predicting: 43it [00:36,  1.60it/s]Extractor Predicting: 44it [00:37,  1.57it/s]Extractor Predicting: 45it [00:38,  1.60it/s]Extractor Predicting: 46it [00:38,  1.57it/s]Extractor Predicting: 47it [00:39,  1.56it/s]Extractor Predicting: 48it [00:40,  1.60it/s]Extractor Predicting: 49it [00:40,  1.61it/s]Extractor Predicting: 50it [00:41,  1.65it/s]Extractor Predicting: 51it [00:41,  1.66it/s]Extractor Predicting: 52it [00:42,  1.64it/s]Extractor Predicting: 53it [00:43,  1.67it/s]Extractor Predicting: 54it [00:43,  1.66it/s]Extractor Predicting: 55it [00:44,  1.66it/s]Extractor Predicting: 56it [00:44,  1.65it/s]Extractor Predicting: 57it [00:45,  1.68it/s]Extractor Predicting: 58it [00:45,  1.71it/s]Extractor Predicting: 59it [00:46,  1.62it/s]Extractor Predicting: 60it [00:47,  1.62it/s]Extractor Predicting: 61it [00:47,  1.60it/s]Extractor Predicting: 62it [00:48,  1.61it/s]Extractor Predicting: 63it [00:49,  1.59it/s]Extractor Predicting: 64it [00:49,  1.64it/s]Extractor Predicting: 65it [00:50,  1.68it/s]Extractor Predicting: 66it [00:50,  1.66it/s]Extractor Predicting: 67it [00:51,  1.64it/s]Extractor Predicting: 68it [00:52,  1.64it/s]Extractor Predicting: 69it [00:52,  1.65it/s]Extractor Predicting: 70it [00:53,  1.60it/s]Extractor Predicting: 71it [00:54,  1.63it/s]Extractor Predicting: 72it [00:54,  1.60it/s]Extractor Predicting: 73it [00:55,  1.58it/s]Extractor Predicting: 74it [00:55,  1.56it/s]Extractor Predicting: 75it [00:56,  1.56it/s]Extractor Predicting: 76it [00:57,  1.54it/s]Extractor Predicting: 77it [00:57,  1.51it/s]Extractor Predicting: 78it [00:58,  1.53it/s]Extractor Predicting: 79it [00:59,  1.49it/s]Extractor Predicting: 80it [01:00,  1.46it/s]Extractor Predicting: 81it [01:00,  1.44it/s]Extractor Predicting: 82it [01:01,  1.43it/s]Extractor Predicting: 83it [01:02,  1.43it/s]Extractor Predicting: 84it [01:02,  1.42it/s]Extractor Predicting: 85it [01:03,  1.42it/s]Extractor Predicting: 86it [01:04,  1.43it/s]Extractor Predicting: 87it [01:04,  1.43it/s]Extractor Predicting: 88it [01:05,  1.43it/s]Extractor Predicting: 89it [01:06,  1.44it/s]Extractor Predicting: 90it [01:07,  1.44it/s]Extractor Predicting: 91it [01:07,  1.42it/s]Extractor Predicting: 92it [01:08,  1.43it/s]Extractor Predicting: 93it [01:09,  1.41it/s]Extractor Predicting: 94it [01:09,  1.41it/s]Extractor Predicting: 95it [01:10,  1.42it/s]Extractor Predicting: 96it [01:11,  1.43it/s]Extractor Predicting: 97it [01:11,  1.45it/s]Extractor Predicting: 98it [01:12,  1.45it/s]Extractor Predicting: 99it [01:13,  1.45it/s]Extractor Predicting: 100it [01:14,  1.44it/s]Extractor Predicting: 101it [01:14,  1.45it/s]Extractor Predicting: 102it [01:15,  1.40it/s]Extractor Predicting: 103it [01:16,  1.42it/s]Extractor Predicting: 104it [01:16,  1.41it/s]Extractor Predicting: 105it [01:17,  1.41it/s]Extractor Predicting: 106it [01:18,  1.41it/s]Extractor Predicting: 107it [01:19,  1.42it/s]Extractor Predicting: 108it [01:19,  1.43it/s]Extractor Predicting: 109it [01:20,  1.43it/s]Extractor Predicting: 110it [01:21,  1.46it/s]Extractor Predicting: 111it [01:21,  1.46it/s]Extractor Predicting: 112it [01:22,  1.48it/s]Extractor Predicting: 113it [01:23,  1.47it/s]Extractor Predicting: 114it [01:23,  1.47it/s]Extractor Predicting: 115it [01:24,  1.41it/s]Extractor Predicting: 116it [01:25,  1.43it/s]Extractor Predicting: 117it [01:25,  1.43it/s]Extractor Predicting: 118it [01:26,  1.43it/s]Extractor Predicting: 119it [01:27,  1.41it/s]Extractor Predicting: 120it [01:28,  1.43it/s]Extractor Predicting: 121it [01:28,  1.44it/s]Extractor Predicting: 122it [01:29,  1.40it/s]Extractor Predicting: 123it [01:30,  1.42it/s]Extractor Predicting: 124it [01:30,  1.41it/s]Extractor Predicting: 125it [01:31,  1.43it/s]Extractor Predicting: 126it [01:32,  1.45it/s]Extractor Predicting: 127it [01:32,  1.51it/s]Extractor Predicting: 128it [01:33,  1.52it/s]Extractor Predicting: 129it [01:34,  1.53it/s]Extractor Predicting: 130it [01:34,  1.49it/s]Extractor Predicting: 131it [01:35,  1.51it/s]Extractor Predicting: 132it [01:36,  1.48it/s]Extractor Predicting: 133it [01:36,  1.48it/s]Extractor Predicting: 134it [01:37,  1.48it/s]Extractor Predicting: 135it [01:38,  1.50it/s]Extractor Predicting: 136it [01:38,  1.53it/s]Extractor Predicting: 137it [01:39,  1.54it/s]Extractor Predicting: 138it [01:40,  1.55it/s]Extractor Predicting: 139it [01:40,  1.55it/s]Extractor Predicting: 140it [01:41,  1.56it/s]Extractor Predicting: 141it [01:41,  1.57it/s]Extractor Predicting: 142it [01:42,  1.55it/s]Extractor Predicting: 143it [01:43,  1.51it/s]Extractor Predicting: 144it [01:43,  1.51it/s]Extractor Predicting: 145it [01:44,  1.56it/s]Extractor Predicting: 146it [01:45,  1.43it/s]Extractor Predicting: 147it [01:46,  1.47it/s]Extractor Predicting: 148it [01:46,  1.51it/s]Extractor Predicting: 149it [01:47,  1.51it/s]Extractor Predicting: 150it [01:47,  1.53it/s]Extractor Predicting: 151it [01:48,  1.51it/s]Extractor Predicting: 152it [01:49,  1.52it/s]Extractor Predicting: 153it [01:49,  1.52it/s]Extractor Predicting: 154it [01:50,  1.48it/s]Extractor Predicting: 155it [01:51,  1.48it/s]Extractor Predicting: 156it [01:52,  1.49it/s]Extractor Predicting: 157it [01:52,  1.49it/s]Extractor Predicting: 158it [01:53,  1.48it/s]Extractor Predicting: 159it [01:54,  1.48it/s]Extractor Predicting: 160it [01:54,  1.50it/s]Extractor Predicting: 161it [01:55,  1.49it/s]Extractor Predicting: 162it [01:55,  1.53it/s]Extractor Predicting: 163it [01:56,  1.66it/s]Extractor Predicting: 164it [01:56,  1.75it/s]Extractor Predicting: 165it [01:57,  1.76it/s]Extractor Predicting: 166it [01:58,  1.72it/s]Extractor Predicting: 167it [01:58,  1.65it/s]Extractor Predicting: 168it [01:59,  1.58it/s]Extractor Predicting: 169it [02:00,  1.53it/s]Extractor Predicting: 170it [02:00,  1.54it/s]Extractor Predicting: 171it [02:01,  1.54it/s]Extractor Predicting: 172it [02:02,  1.51it/s]Extractor Predicting: 173it [02:02,  1.51it/s]Extractor Predicting: 174it [02:03,  1.52it/s]Extractor Predicting: 175it [02:04,  1.49it/s]Extractor Predicting: 176it [02:04,  1.49it/s]Extractor Predicting: 177it [02:05,  1.49it/s]Extractor Predicting: 178it [02:06,  1.51it/s]Extractor Predicting: 179it [02:06,  1.47it/s]Extractor Predicting: 180it [02:07,  1.48it/s]Extractor Predicting: 181it [02:08,  1.51it/s]Extractor Predicting: 182it [02:08,  1.52it/s]Extractor Predicting: 183it [02:09,  1.49it/s]Extractor Predicting: 184it [02:10,  1.49it/s]Extractor Predicting: 185it [02:10,  1.46it/s]Extractor Predicting: 186it [02:11,  1.49it/s]Extractor Predicting: 187it [02:12,  1.49it/s]Extractor Predicting: 188it [02:12,  1.49it/s]Extractor Predicting: 189it [02:13,  1.49it/s]Extractor Predicting: 190it [02:14,  1.49it/s]Extractor Predicting: 191it [02:14,  1.51it/s]Extractor Predicting: 192it [02:15,  1.53it/s]Extractor Predicting: 193it [02:16,  1.52it/s]Extractor Predicting: 194it [02:16,  1.54it/s]Extractor Predicting: 195it [02:17,  1.49it/s]Extractor Predicting: 196it [02:18,  1.51it/s]Extractor Predicting: 197it [02:18,  1.49it/s]Extractor Predicting: 198it [02:19,  1.49it/s]Extractor Predicting: 199it [02:20,  1.49it/s]Extractor Predicting: 200it [02:20,  1.48it/s]Extractor Predicting: 201it [02:21,  1.45it/s]Extractor Predicting: 202it [02:22,  1.46it/s]Extractor Predicting: 203it [02:23,  1.45it/s]Extractor Predicting: 204it [02:23,  1.45it/s]Extractor Predicting: 205it [02:24,  1.46it/s]Extractor Predicting: 206it [02:25,  1.47it/s]Extractor Predicting: 207it [02:25,  1.51it/s]Extractor Predicting: 208it [02:26,  1.53it/s]Extractor Predicting: 209it [02:26,  1.51it/s]Extractor Predicting: 210it [02:27,  1.51it/s]Extractor Predicting: 211it [02:28,  1.50it/s]Extractor Predicting: 212it [02:29,  1.48it/s]Extractor Predicting: 213it [02:29,  1.45it/s]Extractor Predicting: 214it [02:30,  1.49it/s]Extractor Predicting: 215it [02:31,  1.51it/s]Extractor Predicting: 216it [02:31,  1.50it/s]Extractor Predicting: 217it [02:32,  1.48it/s]Extractor Predicting: 218it [02:33,  1.47it/s]Extractor Predicting: 219it [02:33,  1.48it/s]Extractor Predicting: 220it [02:34,  1.48it/s]Extractor Predicting: 221it [02:35,  1.50it/s]Extractor Predicting: 222it [02:35,  1.49it/s]Extractor Predicting: 223it [02:36,  1.50it/s]Extractor Predicting: 224it [02:37,  1.52it/s]Extractor Predicting: 225it [02:37,  1.56it/s]Extractor Predicting: 226it [02:38,  1.54it/s]Extractor Predicting: 227it [02:38,  1.52it/s]Extractor Predicting: 228it [02:39,  1.52it/s]Extractor Predicting: 229it [02:40,  1.45it/s]Extractor Predicting: 230it [02:40,  1.50it/s]Extractor Predicting: 231it [02:41,  1.50it/s]Extractor Predicting: 232it [02:42,  1.49it/s]Extractor Predicting: 233it [02:43,  1.47it/s]Extractor Predicting: 234it [02:43,  1.44it/s]Extractor Predicting: 235it [02:44,  1.45it/s]Extractor Predicting: 236it [02:45,  1.43it/s]Extractor Predicting: 237it [02:45,  1.39it/s]Extractor Predicting: 238it [02:46,  1.38it/s]Extractor Predicting: 239it [02:47,  1.35it/s]Extractor Predicting: 240it [02:48,  1.37it/s]Extractor Predicting: 241it [02:48,  1.40it/s]Extractor Predicting: 242it [02:49,  1.40it/s]Extractor Predicting: 243it [02:50,  1.44it/s]Extractor Predicting: 244it [02:50,  1.44it/s]Extractor Predicting: 245it [02:51,  1.48it/s]Extractor Predicting: 246it [02:52,  1.50it/s]Extractor Predicting: 247it [02:52,  1.52it/s]Extractor Predicting: 248it [02:53,  1.54it/s]Extractor Predicting: 249it [02:54,  1.57it/s]Extractor Predicting: 250it [02:54,  1.54it/s]Extractor Predicting: 251it [02:55,  1.57it/s]Extractor Predicting: 252it [02:56,  1.54it/s]Extractor Predicting: 253it [02:56,  1.53it/s]Extractor Predicting: 254it [02:57,  1.54it/s]Extractor Predicting: 255it [02:57,  1.54it/s]Extractor Predicting: 256it [02:58,  1.53it/s]Extractor Predicting: 257it [02:59,  1.54it/s]Extractor Predicting: 258it [02:59,  1.55it/s]Extractor Predicting: 259it [03:00,  1.53it/s]Extractor Predicting: 260it [03:01,  1.52it/s]Extractor Predicting: 261it [03:01,  1.50it/s]Extractor Predicting: 262it [03:02,  1.50it/s]Extractor Predicting: 263it [03:03,  1.36it/s]Extractor Predicting: 264it [03:04,  1.41it/s]Extractor Predicting: 265it [03:04,  1.44it/s]Extractor Predicting: 266it [03:05,  1.45it/s]Extractor Predicting: 267it [03:06,  1.47it/s]Extractor Predicting: 268it [03:06,  1.47it/s]Extractor Predicting: 269it [03:07,  1.46it/s]Extractor Predicting: 270it [03:08,  1.50it/s]Extractor Predicting: 271it [03:08,  1.51it/s]Extractor Predicting: 272it [03:09,  1.57it/s]Extractor Predicting: 273it [03:10,  1.57it/s]Extractor Predicting: 274it [03:10,  1.57it/s]Extractor Predicting: 275it [03:11,  1.54it/s]Extractor Predicting: 276it [03:12,  1.48it/s]Extractor Predicting: 277it [03:12,  1.51it/s]Extractor Predicting: 278it [03:13,  1.49it/s]Extractor Predicting: 279it [03:14,  1.52it/s]Extractor Predicting: 280it [03:14,  1.57it/s]Extractor Predicting: 281it [03:15,  1.57it/s]Extractor Predicting: 282it [03:15,  1.53it/s]Extractor Predicting: 283it [03:16,  1.51it/s]Extractor Predicting: 284it [03:17,  1.49it/s]Extractor Predicting: 285it [03:18,  1.46it/s]Extractor Predicting: 286it [03:18,  1.48it/s]Extractor Predicting: 287it [03:19,  1.46it/s]Extractor Predicting: 288it [03:20,  1.46it/s]Extractor Predicting: 289it [03:20,  1.45it/s]Extractor Predicting: 290it [03:21,  1.44it/s]Extractor Predicting: 291it [03:22,  1.47it/s]Extractor Predicting: 292it [03:22,  1.49it/s]Extractor Predicting: 293it [03:23,  1.49it/s]Extractor Predicting: 294it [03:24,  1.44it/s]Extractor Predicting: 295it [03:24,  1.45it/s]Extractor Predicting: 296it [03:25,  1.43it/s]Extractor Predicting: 297it [03:26,  1.41it/s]Extractor Predicting: 298it [03:27,  1.40it/s]Extractor Predicting: 299it [03:27,  1.39it/s]Extractor Predicting: 300it [03:28,  1.39it/s]Extractor Predicting: 301it [03:29,  1.37it/s]Extractor Predicting: 302it [03:29,  1.37it/s]Extractor Predicting: 303it [03:30,  1.38it/s]Extractor Predicting: 303it [03:30,  1.44it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.36909871244635195,
  "recall": 0.07026143790849673,
  "score": 0.11805078929306795,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.51it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:22,  1.48it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.54it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:28,  1.47it/s]Extractor Predicting: 45it [00:29,  1.46it/s]Extractor Predicting: 46it [00:30,  1.45it/s]Extractor Predicting: 47it [00:31,  1.46it/s]Extractor Predicting: 48it [00:31,  1.47it/s]Extractor Predicting: 49it [00:32,  1.48it/s]Extractor Predicting: 50it [00:32,  1.52it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:35,  1.50it/s]Extractor Predicting: 55it [00:36,  1.53it/s]Extractor Predicting: 56it [00:36,  1.53it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:38,  1.51it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:39,  1.51it/s]Extractor Predicting: 61it [00:40,  1.51it/s]Extractor Predicting: 62it [00:40,  1.51it/s]Extractor Predicting: 63it [00:41,  1.53it/s]Extractor Predicting: 64it [00:42,  1.54it/s]Extractor Predicting: 65it [00:42,  1.45it/s]Extractor Predicting: 66it [00:43,  1.48it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:44,  1.49it/s]Extractor Predicting: 69it [00:45,  1.49it/s]Extractor Predicting: 70it [00:46,  1.50it/s]Extractor Predicting: 71it [00:46,  1.51it/s]Extractor Predicting: 72it [00:47,  1.39it/s]Extractor Predicting: 73it [00:48,  1.44it/s]Extractor Predicting: 74it [00:49,  1.46it/s]Extractor Predicting: 75it [00:49,  1.46it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.52it/s]Extractor Predicting: 79it [00:52,  1.52it/s]Extractor Predicting: 80it [00:52,  1.52it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.55it/s]Extractor Predicting: 83it [00:54,  1.54it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:56,  1.53it/s]Extractor Predicting: 86it [00:56,  1.55it/s]Extractor Predicting: 87it [00:57,  1.52it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [01:00,  1.51it/s]Extractor Predicting: 92it [01:00,  1.52it/s]Extractor Predicting: 93it [01:01,  1.51it/s]Extractor Predicting: 94it [01:02,  1.51it/s]Extractor Predicting: 95it [01:02,  1.53it/s]Extractor Predicting: 96it [01:03,  1.52it/s]Extractor Predicting: 97it [01:04,  1.50it/s]Extractor Predicting: 98it [01:04,  1.53it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:06,  1.50it/s]Extractor Predicting: 101it [01:06,  1.52it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:08,  1.51it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:12,  1.50it/s]Extractor Predicting: 111it [01:13,  1.54it/s]Extractor Predicting: 112it [01:14,  1.54it/s]Extractor Predicting: 113it [01:14,  1.56it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:15,  1.53it/s]Extractor Predicting: 116it [01:16,  1.54it/s]Extractor Predicting: 117it [01:17,  1.52it/s]Extractor Predicting: 118it [01:17,  1.51it/s]Extractor Predicting: 119it [01:18,  1.50it/s]Extractor Predicting: 120it [01:19,  1.50it/s]Extractor Predicting: 121it [01:19,  1.52it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:21,  1.53it/s]Extractor Predicting: 124it [01:21,  1.52it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:23,  1.51it/s]Extractor Predicting: 127it [01:23,  1.50it/s]Extractor Predicting: 128it [01:24,  1.51it/s]Extractor Predicting: 129it [01:25,  1.56it/s]Extractor Predicting: 130it [01:25,  1.58it/s]Extractor Predicting: 131it [01:26,  1.56it/s]Extractor Predicting: 132it [01:27,  1.52it/s]Extractor Predicting: 133it [01:27,  1.47it/s]Extractor Predicting: 134it [01:28,  1.46it/s]Extractor Predicting: 135it [01:29,  1.48it/s]Extractor Predicting: 136it [01:29,  1.47it/s]Extractor Predicting: 137it [01:30,  1.47it/s]Extractor Predicting: 138it [01:31,  1.46it/s]Extractor Predicting: 139it [01:31,  1.46it/s]Extractor Predicting: 140it [01:32,  1.47it/s]Extractor Predicting: 141it [01:33,  1.46it/s]Extractor Predicting: 142it [01:34,  1.45it/s]Extractor Predicting: 143it [01:34,  1.46it/s]Extractor Predicting: 144it [01:35,  1.47it/s]Extractor Predicting: 145it [01:36,  1.47it/s]Extractor Predicting: 146it [01:36,  1.48it/s]Extractor Predicting: 147it [01:37,  1.45it/s]Extractor Predicting: 148it [01:38,  1.49it/s]Extractor Predicting: 149it [01:38,  1.48it/s]Extractor Predicting: 150it [01:39,  1.48it/s]Extractor Predicting: 151it [01:40,  1.48it/s]Extractor Predicting: 152it [01:40,  1.52it/s]Extractor Predicting: 153it [01:41,  1.54it/s]Extractor Predicting: 154it [01:42,  1.53it/s]Extractor Predicting: 155it [01:42,  1.52it/s]Extractor Predicting: 156it [01:43,  1.52it/s]Extractor Predicting: 157it [01:43,  1.53it/s]Extractor Predicting: 158it [01:44,  1.51it/s]Extractor Predicting: 159it [01:45,  1.51it/s]Extractor Predicting: 160it [01:46,  1.52it/s]Extractor Predicting: 161it [01:46,  1.52it/s]Extractor Predicting: 162it [01:47,  1.51it/s]Extractor Predicting: 163it [01:47,  1.52it/s]Extractor Predicting: 164it [01:48,  1.51it/s]Extractor Predicting: 165it [01:49,  1.54it/s]Extractor Predicting: 166it [01:49,  1.55it/s]Extractor Predicting: 167it [01:50,  1.53it/s]Extractor Predicting: 168it [01:51,  1.54it/s]Extractor Predicting: 169it [01:51,  1.53it/s]Extractor Predicting: 170it [01:52,  1.51it/s]Extractor Predicting: 171it [01:53,  1.48it/s]Extractor Predicting: 172it [01:53,  1.48it/s]Extractor Predicting: 173it [01:54,  1.49it/s]Extractor Predicting: 174it [01:55,  1.52it/s]Extractor Predicting: 175it [01:55,  1.54it/s]Extractor Predicting: 176it [01:56,  1.50it/s]Extractor Predicting: 177it [01:57,  1.52it/s]Extractor Predicting: 178it [01:57,  1.50it/s]Extractor Predicting: 179it [01:58,  1.36it/s]Extractor Predicting: 180it [01:59,  1.44it/s]Extractor Predicting: 181it [02:00,  1.47it/s]Extractor Predicting: 182it [02:00,  1.47it/s]Extractor Predicting: 183it [02:01,  1.51it/s]Extractor Predicting: 184it [02:01,  1.54it/s]Extractor Predicting: 185it [02:02,  1.54it/s]Extractor Predicting: 186it [02:03,  1.55it/s]Extractor Predicting: 187it [02:03,  1.55it/s]Extractor Predicting: 188it [02:04,  1.52it/s]Extractor Predicting: 189it [02:05,  1.50it/s]Extractor Predicting: 190it [02:05,  1.47it/s]Extractor Predicting: 191it [02:06,  1.49it/s]Extractor Predicting: 192it [02:07,  1.52it/s]Extractor Predicting: 193it [02:07,  1.51it/s]Extractor Predicting: 194it [02:08,  1.49it/s]Extractor Predicting: 195it [02:09,  1.52it/s]Extractor Predicting: 196it [02:09,  1.55it/s]Extractor Predicting: 197it [02:10,  1.54it/s]Extractor Predicting: 198it [02:11,  1.56it/s]Extractor Predicting: 199it [02:11,  1.55it/s]Extractor Predicting: 200it [02:12,  1.52it/s]Extractor Predicting: 201it [02:13,  1.52it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:14,  1.53it/s]Extractor Predicting: 204it [02:15,  1.54it/s]Extractor Predicting: 205it [02:15,  1.57it/s]Extractor Predicting: 206it [02:16,  1.55it/s]Extractor Predicting: 207it [02:16,  1.56it/s]Extractor Predicting: 208it [02:17,  1.59it/s]Extractor Predicting: 209it [02:18,  1.58it/s]Extractor Predicting: 210it [02:18,  1.56it/s]Extractor Predicting: 211it [02:19,  1.55it/s]Extractor Predicting: 212it [02:20,  1.54it/s]Extractor Predicting: 213it [02:20,  1.52it/s]Extractor Predicting: 214it [02:21,  1.52it/s]Extractor Predicting: 215it [02:22,  1.51it/s]Extractor Predicting: 216it [02:22,  1.52it/s]Extractor Predicting: 217it [02:23,  1.49it/s]Extractor Predicting: 218it [02:24,  1.55it/s]Extractor Predicting: 219it [02:24,  1.55it/s]Extractor Predicting: 220it [02:25,  1.53it/s]Extractor Predicting: 221it [02:26,  1.50it/s]Extractor Predicting: 222it [02:26,  1.52it/s]Extractor Predicting: 223it [02:27,  1.53it/s]Extractor Predicting: 224it [02:28,  1.53it/s]Extractor Predicting: 225it [02:28,  1.56it/s]Extractor Predicting: 226it [02:29,  1.56it/s]Extractor Predicting: 227it [02:29,  1.56it/s]Extractor Predicting: 228it [02:30,  1.53it/s]Extractor Predicting: 229it [02:31,  1.55it/s]Extractor Predicting: 230it [02:31,  1.52it/s]Extractor Predicting: 231it [02:32,  1.55it/s]Extractor Predicting: 232it [02:33,  1.53it/s]Extractor Predicting: 233it [02:33,  1.52it/s]Extractor Predicting: 234it [02:34,  1.51it/s]Extractor Predicting: 235it [02:35,  1.52it/s]Extractor Predicting: 236it [02:35,  1.50it/s]Extractor Predicting: 237it [02:36,  1.52it/s]Extractor Predicting: 238it [02:37,  1.50it/s]Extractor Predicting: 239it [02:37,  1.55it/s]Extractor Predicting: 240it [02:38,  1.53it/s]Extractor Predicting: 241it [02:39,  1.51it/s]Extractor Predicting: 242it [02:39,  1.52it/s]Extractor Predicting: 243it [02:40,  1.55it/s]Extractor Predicting: 244it [02:41,  1.57it/s]Extractor Predicting: 245it [02:41,  1.59it/s]Extractor Predicting: 246it [02:42,  1.63it/s]Extractor Predicting: 247it [02:42,  1.60it/s]Extractor Predicting: 248it [02:43,  1.60it/s]Extractor Predicting: 249it [02:44,  1.59it/s]Extractor Predicting: 250it [02:44,  1.59it/s]Extractor Predicting: 251it [02:45,  1.58it/s]Extractor Predicting: 252it [02:46,  1.59it/s]Extractor Predicting: 253it [02:46,  1.59it/s]Extractor Predicting: 254it [02:47,  1.59it/s]Extractor Predicting: 255it [02:47,  1.61it/s]Extractor Predicting: 256it [02:48,  1.59it/s]Extractor Predicting: 257it [02:49,  1.58it/s]Extractor Predicting: 258it [02:49,  1.58it/s]Extractor Predicting: 259it [02:50,  1.56it/s]Extractor Predicting: 260it [02:51,  1.56it/s]Extractor Predicting: 261it [02:51,  1.57it/s]Extractor Predicting: 262it [02:52,  1.59it/s]Extractor Predicting: 263it [02:52,  1.62it/s]Extractor Predicting: 264it [02:53,  1.57it/s]Extractor Predicting: 265it [02:54,  1.53it/s]Extractor Predicting: 266it [02:55,  1.52it/s]Extractor Predicting: 267it [02:55,  1.51it/s]Extractor Predicting: 268it [02:56,  1.53it/s]Extractor Predicting: 269it [02:56,  1.56it/s]Extractor Predicting: 270it [02:57,  1.55it/s]Extractor Predicting: 271it [02:58,  1.55it/s]Extractor Predicting: 272it [02:58,  1.55it/s]Extractor Predicting: 273it [02:59,  1.58it/s]Extractor Predicting: 274it [03:00,  1.58it/s]Extractor Predicting: 275it [03:00,  1.60it/s]Extractor Predicting: 276it [03:01,  1.58it/s]Extractor Predicting: 277it [03:02,  1.52it/s]Extractor Predicting: 278it [03:02,  1.55it/s]Extractor Predicting: 279it [03:03,  1.59it/s]Extractor Predicting: 280it [03:04,  1.44it/s]Extractor Predicting: 281it [03:04,  1.51it/s]Extractor Predicting: 282it [03:05,  1.54it/s]Extractor Predicting: 283it [03:05,  1.57it/s]Extractor Predicting: 284it [03:06,  1.58it/s]Extractor Predicting: 285it [03:07,  1.54it/s]Extractor Predicting: 286it [03:07,  1.60it/s]Extractor Predicting: 287it [03:08,  1.59it/s]Extractor Predicting: 288it [03:09,  1.58it/s]Extractor Predicting: 289it [03:09,  1.58it/s]Extractor Predicting: 290it [03:10,  1.61it/s]Extractor Predicting: 291it [03:10,  1.61it/s]Extractor Predicting: 292it [03:11,  1.57it/s]Extractor Predicting: 293it [03:12,  1.57it/s]Extractor Predicting: 294it [03:12,  1.57it/s]Extractor Predicting: 295it [03:13,  1.57it/s]Extractor Predicting: 296it [03:14,  1.58it/s]Extractor Predicting: 297it [03:14,  1.56it/s]Extractor Predicting: 298it [03:15,  1.59it/s]Extractor Predicting: 299it [03:16,  1.54it/s]Extractor Predicting: 300it [03:16,  1.51it/s]Extractor Predicting: 301it [03:17,  1.45it/s]Extractor Predicting: 302it [03:18,  1.46it/s]Extractor Predicting: 303it [03:18,  1.45it/s]Extractor Predicting: 304it [03:19,  1.45it/s]Extractor Predicting: 305it [03:20,  1.43it/s]Extractor Predicting: 306it [03:21,  1.42it/s]Extractor Predicting: 306it [03:21,  1.52it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35545023696682465,
  "recall": 0.030637254901960783,
  "score": 0.056412185031966905,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6322
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6422, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:18,  1.53it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.53it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:22,  1.52it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:23,  1.54it/s]Extractor Predicting: 36it [00:24,  1.58it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.59it/s]Extractor Predicting: 39it [00:26,  1.56it/s]Extractor Predicting: 40it [00:26,  1.44it/s]Extractor Predicting: 41it [00:27,  1.46it/s]Extractor Predicting: 42it [00:28,  1.46it/s]Extractor Predicting: 43it [00:30,  1.04s/it]Extractor Predicting: 44it [00:30,  1.08it/s]Extractor Predicting: 45it [00:31,  1.18it/s]Extractor Predicting: 46it [00:32,  1.22it/s]Extractor Predicting: 47it [00:32,  1.29it/s]Extractor Predicting: 48it [00:33,  1.36it/s]Extractor Predicting: 49it [00:34,  1.40it/s]Extractor Predicting: 50it [00:34,  1.42it/s]Extractor Predicting: 51it [00:35,  1.45it/s]Extractor Predicting: 52it [00:36,  1.45it/s]Extractor Predicting: 53it [00:36,  1.46it/s]Extractor Predicting: 54it [00:37,  1.47it/s]Extractor Predicting: 55it [00:38,  1.48it/s]Extractor Predicting: 56it [00:38,  1.49it/s]Extractor Predicting: 57it [00:39,  1.44it/s]Extractor Predicting: 58it [00:40,  1.45it/s]Extractor Predicting: 59it [00:40,  1.44it/s]Extractor Predicting: 60it [00:41,  1.28it/s]Extractor Predicting: 60it [00:41,  1.43it/s]
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.46226415094339623,
  "recall": 0.015269554378311,
  "score": 0.02956259426847662,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_10_seed_3/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_10_seed_3', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/fewrel/unseen_10_seed_3/generator/synthetic.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
{'num_pseudo': 750, 'num_train': 3000, 'num_pseudo_per_label': 50, 'num_train_per_label': 46}
num of filtered data: 3608 mean pseudo reward: 1.0
fit {'path_train': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/filtered.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl'}
train vocab size: 20505
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20605, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/model', pretrained_wv='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20605, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.400, loss:55529.3445
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 49, avg_time 1.108, loss:2561.8726
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 149, avg_time 1.080, loss:2342.7596
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 98, avg_time 1.076, loss:2186.0477
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 47, avg_time 1.086, loss:2174.5927
>> valid entity prec:0.4310, rec:0.4883, f1:0.4579
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 147, avg_time 2.539, loss:2076.9773
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 96, avg_time 1.087, loss:1924.3744
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 45, avg_time 1.076, loss:1848.6105
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 145, avg_time 1.074, loss:1740.7680
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 94, avg_time 1.079, loss:1597.8216
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5074, rec:0.4362, f1:0.4691
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 43, avg_time 2.489, loss:1580.9662
g_step 1200, step 143, avg_time 1.083, loss:1529.1691
g_step 1300, step 92, avg_time 1.073, loss:1424.9659
g_step 1400, step 41, avg_time 1.078, loss:1368.8369
g_step 1500, step 141, avg_time 1.081, loss:1349.5743
>> valid entity prec:0.5512, rec:0.5745, f1:0.5626
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 90, avg_time 2.482, loss:1258.2934
g_step 1700, step 39, avg_time 1.084, loss:1255.4755
g_step 1800, step 139, avg_time 1.079, loss:1261.6753
g_step 1900, step 88, avg_time 1.074, loss:1152.5841
g_step 2000, step 37, avg_time 1.076, loss:1140.1547
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5298, rec:0.5754, f1:0.5516
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 137, avg_time 2.498, loss:1133.4247
g_step 2200, step 86, avg_time 1.074, loss:1066.1247
g_step 2300, step 35, avg_time 1.065, loss:1057.4515
g_step 2400, step 135, avg_time 1.089, loss:1020.1039
g_step 2500, step 84, avg_time 1.081, loss:980.3318
>> valid entity prec:0.5212, rec:0.5569, f1:0.5385
>> valid relation prec:0.0145, rec:0.0026, f1:0.0044
>> valid relation with NER prec:0.0145, rec:0.0026, f1:0.0044
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 33, avg_time 2.491, loss:962.7982
g_step 2700, step 133, avg_time 1.075, loss:949.6500
g_step 2800, step 82, avg_time 1.074, loss:871.6853
g_step 2900, step 31, avg_time 1.075, loss:897.2771
g_step 3000, step 131, avg_time 1.075, loss:864.1924
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5378, rec:0.4926, f1:0.5142
>> valid relation prec:0.0234, rec:0.0057, f1:0.0092
>> valid relation with NER prec:0.0234, rec:0.0057, f1:0.0092
new max relation f1 on valid!
new max relation f1 with NER on valid!
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/dev.jsonl', 'labels': ['genre', 'located in or next to body of water', 'manufacturer', 'participant in', 'participating team'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11910
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12010, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.36s/it]Extractor Predicting: 2it [00:08,  3.44s/it]Extractor Predicting: 3it [00:08,  2.18s/it]Extractor Predicting: 4it [00:09,  1.57s/it]Extractor Predicting: 5it [00:09,  1.23s/it]Extractor Predicting: 6it [00:10,  1.03s/it]Extractor Predicting: 7it [00:11,  1.12it/s]Extractor Predicting: 8it [00:11,  1.26it/s]Extractor Predicting: 9it [00:12,  1.32it/s]Extractor Predicting: 10it [00:13,  1.39it/s]Extractor Predicting: 11it [00:13,  1.44it/s]Extractor Predicting: 12it [00:14,  1.46it/s]Extractor Predicting: 13it [00:15,  1.48it/s]Extractor Predicting: 14it [00:15,  1.48it/s]Extractor Predicting: 15it [00:16,  1.52it/s]Extractor Predicting: 16it [00:17,  1.51it/s]Extractor Predicting: 17it [00:17,  1.51it/s]Extractor Predicting: 18it [00:18,  1.53it/s]Extractor Predicting: 19it [00:18,  1.56it/s]Extractor Predicting: 20it [00:19,  1.56it/s]Extractor Predicting: 21it [00:20,  1.54it/s]Extractor Predicting: 22it [00:20,  1.55it/s]Extractor Predicting: 23it [00:21,  1.56it/s]Extractor Predicting: 24it [00:22,  1.57it/s]Extractor Predicting: 25it [00:22,  1.54it/s]Extractor Predicting: 26it [00:23,  1.54it/s]Extractor Predicting: 27it [00:24,  1.54it/s]Extractor Predicting: 28it [00:24,  1.55it/s]Extractor Predicting: 29it [00:25,  1.53it/s]Extractor Predicting: 30it [00:26,  1.51it/s]Extractor Predicting: 31it [00:26,  1.51it/s]Extractor Predicting: 32it [00:27,  1.52it/s]Extractor Predicting: 33it [00:28,  1.51it/s]Extractor Predicting: 34it [00:28,  1.50it/s]Extractor Predicting: 35it [00:29,  1.47it/s]Extractor Predicting: 36it [00:30,  1.46it/s]Extractor Predicting: 37it [00:30,  1.47it/s]Extractor Predicting: 38it [00:31,  1.47it/s]Extractor Predicting: 39it [00:32,  1.48it/s]Extractor Predicting: 40it [00:32,  1.47it/s]Extractor Predicting: 41it [00:33,  1.47it/s]Extractor Predicting: 42it [00:34,  1.46it/s]Extractor Predicting: 43it [00:34,  1.44it/s]Extractor Predicting: 44it [00:35,  1.44it/s]Extractor Predicting: 45it [00:36,  1.43it/s]Extractor Predicting: 46it [00:37,  1.43it/s]Extractor Predicting: 47it [00:37,  1.47it/s]Extractor Predicting: 48it [00:38,  1.47it/s]Extractor Predicting: 49it [00:39,  1.50it/s]Extractor Predicting: 50it [00:39,  1.46it/s]Extractor Predicting: 51it [00:40,  1.48it/s]Extractor Predicting: 52it [00:41,  1.49it/s]Extractor Predicting: 53it [00:41,  1.48it/s]Extractor Predicting: 54it [00:42,  1.48it/s]Extractor Predicting: 55it [00:43,  1.48it/s]Extractor Predicting: 56it [00:43,  1.48it/s]Extractor Predicting: 57it [00:44,  1.51it/s]Extractor Predicting: 58it [00:45,  1.50it/s]Extractor Predicting: 59it [00:45,  1.47it/s]Extractor Predicting: 60it [00:46,  1.45it/s]Extractor Predicting: 61it [00:47,  1.43it/s]Extractor Predicting: 62it [00:47,  1.44it/s]Extractor Predicting: 63it [00:48,  1.45it/s]Extractor Predicting: 64it [00:49,  1.44it/s]Extractor Predicting: 65it [00:49,  1.47it/s]Extractor Predicting: 66it [00:50,  1.48it/s]Extractor Predicting: 67it [00:51,  1.47it/s]Extractor Predicting: 68it [00:52,  1.48it/s]Extractor Predicting: 69it [00:52,  1.46it/s]Extractor Predicting: 70it [00:53,  1.46it/s]Extractor Predicting: 71it [00:54,  1.43it/s]Extractor Predicting: 72it [00:54,  1.46it/s]Extractor Predicting: 73it [00:55,  1.45it/s]Extractor Predicting: 74it [00:56,  1.46it/s]Extractor Predicting: 75it [00:56,  1.47it/s]Extractor Predicting: 76it [00:57,  1.47it/s]Extractor Predicting: 77it [00:58,  1.49it/s]Extractor Predicting: 78it [00:58,  1.46it/s]Extractor Predicting: 79it [00:59,  1.46it/s]Extractor Predicting: 80it [01:00,  1.45it/s]Extractor Predicting: 81it [01:00,  1.45it/s]Extractor Predicting: 82it [01:01,  1.35it/s]Extractor Predicting: 83it [01:02,  1.41it/s]Extractor Predicting: 84it [01:03,  1.41it/s]Extractor Predicting: 85it [01:03,  1.41it/s]Extractor Predicting: 86it [01:04,  1.43it/s]Extractor Predicting: 87it [01:05,  1.45it/s]Extractor Predicting: 88it [01:05,  1.48it/s]Extractor Predicting: 89it [01:06,  1.48it/s]Extractor Predicting: 90it [01:07,  1.51it/s]Extractor Predicting: 91it [01:07,  1.55it/s]Extractor Predicting: 92it [01:08,  1.56it/s]Extractor Predicting: 93it [01:09,  1.53it/s]Extractor Predicting: 94it [01:09,  1.55it/s]Extractor Predicting: 95it [01:10,  1.55it/s]Extractor Predicting: 96it [01:10,  1.54it/s]Extractor Predicting: 97it [01:11,  1.54it/s]Extractor Predicting: 98it [01:12,  1.51it/s]Extractor Predicting: 99it [01:13,  1.49it/s]Extractor Predicting: 100it [01:13,  1.47it/s]Extractor Predicting: 101it [01:14,  1.52it/s]Extractor Predicting: 102it [01:14,  1.57it/s]Extractor Predicting: 103it [01:15,  1.55it/s]Extractor Predicting: 104it [01:16,  1.56it/s]Extractor Predicting: 105it [01:16,  1.55it/s]Extractor Predicting: 106it [01:17,  1.56it/s]Extractor Predicting: 107it [01:18,  1.54it/s]Extractor Predicting: 108it [01:18,  1.53it/s]Extractor Predicting: 109it [01:19,  1.53it/s]Extractor Predicting: 110it [01:20,  1.52it/s]Extractor Predicting: 111it [01:20,  1.55it/s]Extractor Predicting: 112it [01:21,  1.55it/s]Extractor Predicting: 113it [01:21,  1.60it/s]Extractor Predicting: 114it [01:22,  1.58it/s]Extractor Predicting: 115it [01:23,  1.59it/s]Extractor Predicting: 116it [01:23,  1.58it/s]Extractor Predicting: 117it [01:24,  1.55it/s]Extractor Predicting: 118it [01:25,  1.52it/s]Extractor Predicting: 119it [01:25,  1.50it/s]Extractor Predicting: 120it [01:26,  1.48it/s]Extractor Predicting: 121it [01:27,  1.51it/s]Extractor Predicting: 122it [01:27,  1.51it/s]Extractor Predicting: 123it [01:28,  1.50it/s]Extractor Predicting: 124it [01:29,  1.48it/s]Extractor Predicting: 125it [01:30,  1.47it/s]Extractor Predicting: 126it [01:30,  1.45it/s]Extractor Predicting: 127it [01:31,  1.46it/s]Extractor Predicting: 128it [01:31,  1.51it/s]Extractor Predicting: 129it [01:32,  1.48it/s]Extractor Predicting: 130it [01:33,  1.51it/s]Extractor Predicting: 131it [01:34,  1.50it/s]Extractor Predicting: 132it [01:34,  1.49it/s]Extractor Predicting: 133it [01:35,  1.48it/s]Extractor Predicting: 134it [01:36,  1.46it/s]Extractor Predicting: 135it [01:36,  1.46it/s]Extractor Predicting: 136it [01:37,  1.47it/s]Extractor Predicting: 137it [01:38,  1.48it/s]Extractor Predicting: 138it [01:38,  1.48it/s]Extractor Predicting: 139it [01:39,  1.46it/s]Extractor Predicting: 140it [01:40,  1.49it/s]Extractor Predicting: 141it [01:40,  1.47it/s]Extractor Predicting: 142it [01:41,  1.44it/s]Extractor Predicting: 143it [01:42,  1.47it/s]Extractor Predicting: 144it [01:42,  1.75it/s]Extractor Predicting: 144it [01:42,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19834
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19934, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.45it/s]Extractor Predicting: 18it [00:12,  1.47it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:16,  1.51it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:18,  1.50it/s]Extractor Predicting: 28it [00:18,  1.48it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:20,  1.45it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.56it/s]Extractor Predicting: 33it [00:21,  1.59it/s]Extractor Predicting: 34it [00:22,  1.61it/s]Extractor Predicting: 35it [00:23,  1.63it/s]Extractor Predicting: 36it [00:23,  1.63it/s]Extractor Predicting: 37it [00:24,  1.63it/s]Extractor Predicting: 38it [00:24,  1.66it/s]Extractor Predicting: 39it [00:25,  1.69it/s]Extractor Predicting: 40it [00:26,  1.71it/s]Extractor Predicting: 41it [00:26,  1.69it/s]Extractor Predicting: 42it [00:27,  1.70it/s]Extractor Predicting: 43it [00:27,  1.68it/s]Extractor Predicting: 44it [00:28,  1.69it/s]Extractor Predicting: 45it [00:29,  1.68it/s]Extractor Predicting: 46it [00:29,  1.63it/s]Extractor Predicting: 47it [00:30,  1.67it/s]Extractor Predicting: 48it [00:30,  1.71it/s]Extractor Predicting: 49it [00:31,  1.72it/s]Extractor Predicting: 50it [00:32,  1.71it/s]Extractor Predicting: 51it [00:32,  1.72it/s]Extractor Predicting: 52it [00:33,  1.69it/s]Extractor Predicting: 53it [00:33,  1.68it/s]Extractor Predicting: 54it [00:34,  1.67it/s]Extractor Predicting: 55it [00:35,  1.64it/s]Extractor Predicting: 56it [00:35,  1.64it/s]Extractor Predicting: 57it [00:36,  1.63it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:37,  1.57it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:39,  1.47it/s]Extractor Predicting: 62it [00:39,  1.43it/s]Extractor Predicting: 63it [00:40,  1.44it/s]Extractor Predicting: 64it [00:41,  1.42it/s]Extractor Predicting: 65it [00:41,  1.45it/s]Extractor Predicting: 66it [00:42,  1.45it/s]Extractor Predicting: 67it [00:43,  1.45it/s]Extractor Predicting: 68it [00:43,  1.43it/s]Extractor Predicting: 69it [00:44,  1.41it/s]Extractor Predicting: 70it [00:45,  1.39it/s]Extractor Predicting: 71it [00:46,  1.37it/s]Extractor Predicting: 72it [00:46,  1.41it/s]Extractor Predicting: 73it [00:47,  1.40it/s]Extractor Predicting: 74it [00:48,  1.42it/s]Extractor Predicting: 75it [00:48,  1.43it/s]Extractor Predicting: 76it [00:49,  1.44it/s]Extractor Predicting: 77it [00:50,  1.43it/s]Extractor Predicting: 78it [00:51,  1.41it/s]Extractor Predicting: 79it [00:51,  1.39it/s]Extractor Predicting: 80it [00:52,  1.40it/s]Extractor Predicting: 81it [00:53,  1.39it/s]Extractor Predicting: 82it [00:53,  1.39it/s]Extractor Predicting: 83it [00:54,  1.39it/s]Extractor Predicting: 84it [00:55,  1.42it/s]Extractor Predicting: 85it [00:56,  1.41it/s]Extractor Predicting: 86it [00:56,  1.38it/s]Extractor Predicting: 87it [00:57,  1.39it/s]Extractor Predicting: 88it [00:58,  1.40it/s]Extractor Predicting: 89it [00:59,  1.30it/s]Extractor Predicting: 90it [00:59,  1.36it/s]Extractor Predicting: 91it [01:00,  1.36it/s]Extractor Predicting: 92it [01:01,  1.36it/s]Extractor Predicting: 93it [01:01,  1.40it/s]Extractor Predicting: 94it [01:02,  1.41it/s]Extractor Predicting: 95it [01:03,  1.45it/s]Extractor Predicting: 96it [01:03,  1.48it/s]Extractor Predicting: 97it [01:04,  1.48it/s]Extractor Predicting: 98it [01:05,  1.51it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:06,  1.50it/s]Extractor Predicting: 101it [01:07,  1.56it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:08,  1.51it/s]Extractor Predicting: 104it [01:09,  1.47it/s]Extractor Predicting: 105it [01:09,  1.48it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:11,  1.48it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.44it/s]Extractor Predicting: 111it [01:14,  1.34it/s]Extractor Predicting: 112it [01:15,  1.17it/s]Extractor Predicting: 113it [01:15,  1.26it/s]Extractor Predicting: 114it [01:16,  1.29it/s]Extractor Predicting: 115it [01:17,  1.34it/s]Extractor Predicting: 116it [01:17,  1.42it/s]Extractor Predicting: 117it [01:18,  1.44it/s]Extractor Predicting: 118it [01:19,  1.51it/s]Extractor Predicting: 119it [01:19,  1.55it/s]Extractor Predicting: 120it [01:20,  1.57it/s]Extractor Predicting: 121it [01:21,  1.60it/s]Extractor Predicting: 122it [01:21,  1.59it/s]Extractor Predicting: 123it [01:22,  1.61it/s]Extractor Predicting: 124it [01:22,  1.62it/s]Extractor Predicting: 125it [01:23,  1.64it/s]Extractor Predicting: 126it [01:24,  1.62it/s]Extractor Predicting: 127it [01:24,  1.61it/s]Extractor Predicting: 128it [01:25,  1.66it/s]Extractor Predicting: 129it [01:25,  1.64it/s]Extractor Predicting: 130it [01:26,  1.72it/s]Extractor Predicting: 131it [01:27,  1.73it/s]Extractor Predicting: 132it [01:27,  1.69it/s]Extractor Predicting: 133it [01:28,  1.64it/s]Extractor Predicting: 134it [01:28,  1.62it/s]Extractor Predicting: 135it [01:29,  1.65it/s]Extractor Predicting: 136it [01:30,  1.67it/s]Extractor Predicting: 137it [01:30,  1.67it/s]Extractor Predicting: 138it [01:31,  1.67it/s]Extractor Predicting: 139it [01:31,  1.63it/s]Extractor Predicting: 140it [01:32,  1.66it/s]Extractor Predicting: 141it [01:33,  1.64it/s]Extractor Predicting: 142it [01:33,  1.69it/s]Extractor Predicting: 143it [01:34,  1.68it/s]Extractor Predicting: 144it [01:34,  1.66it/s]Extractor Predicting: 145it [01:35,  1.64it/s]Extractor Predicting: 146it [01:36,  1.59it/s]Extractor Predicting: 147it [01:36,  1.56it/s]Extractor Predicting: 148it [01:37,  1.56it/s]Extractor Predicting: 149it [01:38,  1.53it/s]Extractor Predicting: 150it [01:38,  1.52it/s]Extractor Predicting: 151it [01:39,  1.52it/s]Extractor Predicting: 152it [01:40,  1.52it/s]Extractor Predicting: 153it [01:40,  1.49it/s]Extractor Predicting: 154it [01:41,  1.52it/s]Extractor Predicting: 155it [01:42,  1.54it/s]Extractor Predicting: 156it [01:42,  1.51it/s]Extractor Predicting: 157it [01:43,  1.51it/s]Extractor Predicting: 158it [01:44,  1.50it/s]Extractor Predicting: 159it [01:44,  1.49it/s]Extractor Predicting: 160it [01:45,  1.48it/s]Extractor Predicting: 161it [01:46,  1.48it/s]Extractor Predicting: 162it [01:46,  1.47it/s]Extractor Predicting: 163it [01:47,  1.49it/s]Extractor Predicting: 164it [01:48,  1.49it/s]Extractor Predicting: 165it [01:48,  1.47it/s]Extractor Predicting: 166it [01:49,  1.44it/s]Extractor Predicting: 167it [01:50,  1.47it/s]Extractor Predicting: 168it [01:51,  1.46it/s]Extractor Predicting: 169it [01:51,  1.48it/s]Extractor Predicting: 170it [01:52,  1.47it/s]Extractor Predicting: 171it [01:53,  1.47it/s]Extractor Predicting: 172it [01:53,  1.49it/s]Extractor Predicting: 173it [01:54,  1.47it/s]Extractor Predicting: 174it [01:55,  1.47it/s]Extractor Predicting: 175it [01:55,  1.36it/s]Extractor Predicting: 176it [01:56,  1.39it/s]Extractor Predicting: 177it [01:57,  1.40it/s]Extractor Predicting: 178it [01:58,  1.43it/s]Extractor Predicting: 179it [01:58,  1.44it/s]Extractor Predicting: 180it [01:59,  1.45it/s]Extractor Predicting: 181it [02:00,  1.45it/s]Extractor Predicting: 182it [02:00,  1.46it/s]Extractor Predicting: 183it [02:01,  1.46it/s]Extractor Predicting: 184it [02:02,  1.46it/s]Extractor Predicting: 185it [02:02,  1.47it/s]Extractor Predicting: 186it [02:03,  1.46it/s]Extractor Predicting: 187it [02:04,  1.48it/s]Extractor Predicting: 188it [02:04,  1.47it/s]Extractor Predicting: 189it [02:05,  1.49it/s]Extractor Predicting: 190it [02:06,  1.48it/s]Extractor Predicting: 191it [02:06,  1.51it/s]Extractor Predicting: 192it [02:07,  1.51it/s]Extractor Predicting: 193it [02:08,  1.53it/s]Extractor Predicting: 194it [02:08,  1.50it/s]Extractor Predicting: 195it [02:09,  1.50it/s]Extractor Predicting: 196it [02:10,  1.50it/s]Extractor Predicting: 197it [02:10,  1.50it/s]Extractor Predicting: 198it [02:11,  1.46it/s]Extractor Predicting: 199it [02:12,  1.47it/s]Extractor Predicting: 200it [02:12,  1.48it/s]Extractor Predicting: 201it [02:13,  1.51it/s]Extractor Predicting: 202it [02:14,  1.49it/s]Extractor Predicting: 203it [02:14,  1.51it/s]Extractor Predicting: 204it [02:15,  1.50it/s]Extractor Predicting: 205it [02:16,  1.50it/s]Extractor Predicting: 206it [02:16,  1.55it/s]Extractor Predicting: 207it [02:17,  1.54it/s]Extractor Predicting: 208it [02:18,  1.56it/s]Extractor Predicting: 209it [02:18,  1.54it/s]Extractor Predicting: 210it [02:19,  1.53it/s]Extractor Predicting: 211it [02:20,  1.52it/s]Extractor Predicting: 212it [02:20,  1.48it/s]Extractor Predicting: 213it [02:21,  1.48it/s]Extractor Predicting: 214it [02:22,  1.49it/s]Extractor Predicting: 215it [02:22,  1.47it/s]Extractor Predicting: 216it [02:23,  1.46it/s]Extractor Predicting: 217it [02:24,  1.43it/s]Extractor Predicting: 218it [02:24,  1.45it/s]Extractor Predicting: 219it [02:25,  1.46it/s]Extractor Predicting: 220it [02:26,  1.49it/s]Extractor Predicting: 221it [02:26,  1.49it/s]Extractor Predicting: 222it [02:27,  1.48it/s]Extractor Predicting: 223it [02:28,  1.50it/s]Extractor Predicting: 224it [02:28,  1.48it/s]Extractor Predicting: 225it [02:29,  1.47it/s]Extractor Predicting: 226it [02:30,  1.48it/s]Extractor Predicting: 227it [02:30,  1.47it/s]Extractor Predicting: 228it [02:31,  1.51it/s]Extractor Predicting: 229it [02:32,  1.48it/s]Extractor Predicting: 230it [02:32,  1.49it/s]Extractor Predicting: 231it [02:33,  1.45it/s]Extractor Predicting: 232it [02:34,  1.44it/s]Extractor Predicting: 233it [02:35,  1.45it/s]Extractor Predicting: 234it [02:35,  1.47it/s]Extractor Predicting: 235it [02:36,  1.48it/s]Extractor Predicting: 236it [02:37,  1.49it/s]Extractor Predicting: 237it [02:37,  1.43it/s]Extractor Predicting: 238it [02:38,  1.42it/s]Extractor Predicting: 239it [02:39,  1.42it/s]Extractor Predicting: 240it [02:39,  1.43it/s]Extractor Predicting: 241it [02:40,  1.43it/s]Extractor Predicting: 242it [02:41,  1.45it/s]Extractor Predicting: 243it [02:41,  1.45it/s]Extractor Predicting: 244it [02:42,  1.45it/s]Extractor Predicting: 245it [02:43,  1.45it/s]Extractor Predicting: 246it [02:44,  1.42it/s]Extractor Predicting: 247it [02:44,  1.39it/s]Extractor Predicting: 248it [02:45,  1.41it/s]Extractor Predicting: 249it [02:46,  1.46it/s]Extractor Predicting: 250it [02:46,  1.48it/s]Extractor Predicting: 251it [02:47,  1.46it/s]Extractor Predicting: 252it [02:48,  1.49it/s]Extractor Predicting: 253it [02:48,  1.49it/s]Extractor Predicting: 254it [02:49,  1.46it/s]Extractor Predicting: 255it [02:50,  1.45it/s]Extractor Predicting: 256it [02:50,  1.44it/s]Extractor Predicting: 257it [02:51,  1.45it/s]Extractor Predicting: 258it [02:52,  1.44it/s]Extractor Predicting: 259it [02:52,  1.47it/s]Extractor Predicting: 260it [02:53,  1.48it/s]Extractor Predicting: 261it [02:54,  1.50it/s]Extractor Predicting: 262it [02:55,  1.37it/s]Extractor Predicting: 263it [02:55,  1.39it/s]Extractor Predicting: 264it [02:56,  1.43it/s]Extractor Predicting: 265it [02:57,  1.46it/s]Extractor Predicting: 266it [02:57,  1.47it/s]Extractor Predicting: 267it [02:58,  1.50it/s]Extractor Predicting: 268it [02:59,  1.50it/s]Extractor Predicting: 269it [02:59,  1.49it/s]Extractor Predicting: 270it [03:00,  1.47it/s]Extractor Predicting: 271it [03:01,  1.50it/s]Extractor Predicting: 272it [03:01,  1.50it/s]Extractor Predicting: 273it [03:02,  1.53it/s]Extractor Predicting: 274it [03:03,  1.53it/s]Extractor Predicting: 275it [03:03,  1.54it/s]Extractor Predicting: 276it [03:04,  1.51it/s]Extractor Predicting: 277it [03:05,  1.51it/s]Extractor Predicting: 278it [03:05,  1.52it/s]Extractor Predicting: 279it [03:06,  1.52it/s]Extractor Predicting: 280it [03:07,  1.50it/s]Extractor Predicting: 281it [03:07,  1.54it/s]Extractor Predicting: 282it [03:08,  1.52it/s]Extractor Predicting: 283it [03:09,  1.48it/s]Extractor Predicting: 284it [03:09,  1.69it/s]Extractor Predicting: 284it [03:09,  1.50it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/fewrel/unseen_10_seed_3/test.jsonl', 'labels': ['competition class', 'country of citizenship', 'father', 'field of work', 'heritage designation', 'licensed to broadcast to', 'located in the administrative territorial entity', 'occupant', 'occupation', 'record label'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1045
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1145, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.86it/s]Extractor Predicting: 5it [00:03,  1.61it/s]
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_10_seed_3/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_10_seed_3', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_3/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_3/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/wiki/unseen_10_seed_3/generator/synthetic.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_3/train.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
{'num_pseudo': 750, 'num_train': 3000, 'num_pseudo_per_label': 50, 'num_train_per_label': 30}
num of filtered data: 3368 mean pseudo reward: 1.0
fit {'path_train': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/filtered.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl'}
train vocab size: 27223
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27323, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/model', pretrained_wv='outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27323, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.395, loss:54377.4561
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 59, avg_time 1.093, loss:2417.1170
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 18, avg_time 1.051, loss:2226.6545
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 118, avg_time 1.066, loss:2175.1397
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 77, avg_time 1.057, loss:2112.4241
>> valid entity prec:0.3322, rec:0.2932, f1:0.3115
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 36, avg_time 4.016, loss:2056.3811
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 136, avg_time 1.061, loss:2008.4029
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 95, avg_time 1.051, loss:1963.0405
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 54, avg_time 1.065, loss:1859.4335
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.060, loss:1788.7585
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4253, rec:0.5215, f1:0.4685
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 113, avg_time 3.986, loss:1717.1880
g_step 1200, step 72, avg_time 1.061, loss:1603.4086
g_step 1300, step 31, avg_time 1.056, loss:1550.1766
g_step 1400, step 131, avg_time 1.055, loss:1515.1076
g_step 1500, step 90, avg_time 1.060, loss:1458.5618
>> valid entity prec:0.4476, rec:0.3117, f1:0.3675
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 49, avg_time 3.952, loss:1383.0812
g_step 1700, step 8, avg_time 1.055, loss:1343.3818
g_step 1800, step 108, avg_time 1.067, loss:1332.8566
g_step 1900, step 67, avg_time 1.053, loss:1270.7019
g_step 2000, step 26, avg_time 1.070, loss:1267.4927
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4045, rec:0.4156, f1:0.4100
>> valid relation prec:0.0778, rec:0.0033, f1:0.0063
>> valid relation with NER prec:0.0778, rec:0.0033, f1:0.0063
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 126, avg_time 3.964, loss:1193.8620
g_step 2200, step 85, avg_time 1.061, loss:1187.5838
g_step 2300, step 44, avg_time 1.053, loss:1135.9753
g_step 2400, step 3, avg_time 1.063, loss:1139.1737
g_step 2500, step 103, avg_time 1.064, loss:1090.6985
>> valid entity prec:0.4623, rec:0.4953, f1:0.4782
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 62, avg_time 3.979, loss:1064.0362
g_step 2700, step 21, avg_time 1.049, loss:1043.5087
g_step 2800, step 121, avg_time 1.067, loss:1021.4676
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/dev.jsonl', 'labels': ['country', 'place of death', 'production company', 'screenwriter', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 21439
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21539, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:06,  6.22s/it]Extractor Predicting: 2it [00:06,  2.92s/it]Extractor Predicting: 3it [00:07,  1.88s/it]Extractor Predicting: 4it [00:08,  1.38s/it]Extractor Predicting: 5it [00:08,  1.12s/it]Extractor Predicting: 6it [00:09,  1.04it/s]Extractor Predicting: 7it [00:10,  1.00s/it]Extractor Predicting: 8it [00:11,  1.11it/s]Extractor Predicting: 9it [00:11,  1.19it/s]Extractor Predicting: 10it [00:12,  1.28it/s]Extractor Predicting: 11it [00:13,  1.34it/s]Extractor Predicting: 12it [00:13,  1.34it/s]Extractor Predicting: 13it [00:14,  1.43it/s]Extractor Predicting: 14it [00:15,  1.46it/s]Extractor Predicting: 15it [00:16,  1.22it/s]Extractor Predicting: 16it [00:16,  1.32it/s]Extractor Predicting: 17it [00:17,  1.37it/s]Extractor Predicting: 18it [00:18,  1.45it/s]Extractor Predicting: 19it [00:18,  1.47it/s]Extractor Predicting: 20it [00:19,  1.51it/s]Extractor Predicting: 21it [00:20,  1.52it/s]Extractor Predicting: 22it [00:20,  1.56it/s]Extractor Predicting: 23it [00:21,  1.60it/s]Extractor Predicting: 24it [00:22,  1.54it/s]Extractor Predicting: 25it [00:25,  1.40s/it]Extractor Predicting: 26it [00:25,  1.19s/it]Extractor Predicting: 27it [00:26,  1.02s/it]Extractor Predicting: 28it [00:27,  1.10it/s]Extractor Predicting: 29it [00:27,  1.22it/s]Extractor Predicting: 30it [00:28,  1.26it/s]Extractor Predicting: 31it [00:29,  1.33it/s]Extractor Predicting: 32it [00:29,  1.41it/s]Extractor Predicting: 33it [00:30,  1.46it/s]Extractor Predicting: 34it [00:30,  1.51it/s]Extractor Predicting: 35it [00:31,  1.58it/s]Extractor Predicting: 36it [00:32,  1.57it/s]Extractor Predicting: 37it [00:32,  1.63it/s]Extractor Predicting: 38it [00:33,  1.62it/s]Extractor Predicting: 39it [00:34,  1.59it/s]Extractor Predicting: 40it [00:34,  1.60it/s]Extractor Predicting: 41it [00:35,  1.61it/s]Extractor Predicting: 42it [00:35,  1.63it/s]Extractor Predicting: 43it [00:36,  1.62it/s]Extractor Predicting: 44it [00:37,  1.58it/s]Extractor Predicting: 45it [00:37,  1.61it/s]Extractor Predicting: 46it [00:38,  1.58it/s]Extractor Predicting: 47it [00:39,  1.57it/s]Extractor Predicting: 48it [00:39,  1.61it/s]Extractor Predicting: 49it [00:40,  1.62it/s]Extractor Predicting: 50it [00:40,  1.65it/s]Extractor Predicting: 51it [00:41,  1.66it/s]Extractor Predicting: 52it [00:42,  1.65it/s]Extractor Predicting: 53it [00:42,  1.68it/s]Extractor Predicting: 54it [00:43,  1.67it/s]Extractor Predicting: 55it [00:43,  1.67it/s]Extractor Predicting: 56it [00:44,  1.66it/s]Extractor Predicting: 57it [00:44,  1.69it/s]Extractor Predicting: 58it [00:45,  1.72it/s]Extractor Predicting: 59it [00:46,  1.62it/s]Extractor Predicting: 60it [00:46,  1.62it/s]Extractor Predicting: 61it [00:47,  1.61it/s]Extractor Predicting: 62it [00:48,  1.62it/s]Extractor Predicting: 63it [00:48,  1.60it/s]Extractor Predicting: 64it [00:49,  1.65it/s]Extractor Predicting: 65it [00:49,  1.69it/s]Extractor Predicting: 66it [00:50,  1.67it/s]Extractor Predicting: 67it [00:51,  1.65it/s]Extractor Predicting: 68it [00:51,  1.64it/s]Extractor Predicting: 69it [00:52,  1.66it/s]Extractor Predicting: 70it [00:52,  1.61it/s]Extractor Predicting: 71it [00:53,  1.64it/s]Extractor Predicting: 72it [00:54,  1.61it/s]Extractor Predicting: 73it [00:54,  1.59it/s]Extractor Predicting: 74it [00:55,  1.57it/s]Extractor Predicting: 75it [00:56,  1.57it/s]Extractor Predicting: 76it [00:56,  1.54it/s]Extractor Predicting: 77it [00:57,  1.51it/s]Extractor Predicting: 78it [00:58,  1.54it/s]Extractor Predicting: 79it [00:58,  1.50it/s]Extractor Predicting: 80it [00:59,  1.47it/s]Extractor Predicting: 81it [01:00,  1.44it/s]Extractor Predicting: 82it [01:00,  1.43it/s]Extractor Predicting: 83it [01:01,  1.43it/s]Extractor Predicting: 84it [01:02,  1.42it/s]Extractor Predicting: 85it [01:03,  1.42it/s]Extractor Predicting: 86it [01:04,  1.32it/s]Extractor Predicting: 87it [01:04,  1.35it/s]Extractor Predicting: 88it [01:05,  1.37it/s]Extractor Predicting: 89it [01:06,  1.40it/s]Extractor Predicting: 90it [01:06,  1.42it/s]Extractor Predicting: 91it [01:07,  1.40it/s]Extractor Predicting: 92it [01:08,  1.42it/s]Extractor Predicting: 93it [01:08,  1.41it/s]Extractor Predicting: 94it [01:09,  1.41it/s]Extractor Predicting: 95it [01:10,  1.42it/s]Extractor Predicting: 96it [01:10,  1.44it/s]Extractor Predicting: 97it [01:11,  1.45it/s]Extractor Predicting: 98it [01:12,  1.46it/s]Extractor Predicting: 99it [01:13,  1.47it/s]Extractor Predicting: 100it [01:13,  1.45it/s]Extractor Predicting: 101it [01:14,  1.45it/s]Extractor Predicting: 102it [01:15,  1.41it/s]Extractor Predicting: 103it [01:15,  1.43it/s]Extractor Predicting: 104it [01:16,  1.42it/s]Extractor Predicting: 105it [01:17,  1.42it/s]Extractor Predicting: 106it [01:17,  1.42it/s]Extractor Predicting: 107it [01:18,  1.43it/s]Extractor Predicting: 108it [01:19,  1.44it/s]Extractor Predicting: 109it [01:20,  1.43it/s]Extractor Predicting: 110it [01:20,  1.47it/s]Extractor Predicting: 111it [01:21,  1.47it/s]Extractor Predicting: 112it [01:21,  1.49it/s]Extractor Predicting: 113it [01:22,  1.49it/s]Extractor Predicting: 114it [01:23,  1.48it/s]Extractor Predicting: 115it [01:24,  1.42it/s]Extractor Predicting: 116it [01:24,  1.43it/s]Extractor Predicting: 117it [01:25,  1.43it/s]Extractor Predicting: 118it [01:26,  1.43it/s]Extractor Predicting: 119it [01:26,  1.42it/s]Extractor Predicting: 120it [01:27,  1.43it/s]Extractor Predicting: 121it [01:28,  1.45it/s]Extractor Predicting: 122it [01:29,  1.41it/s]Extractor Predicting: 123it [01:29,  1.43it/s]Extractor Predicting: 124it [01:30,  1.42it/s]Extractor Predicting: 125it [01:31,  1.42it/s]Extractor Predicting: 126it [01:31,  1.45it/s]Extractor Predicting: 127it [01:32,  1.51it/s]Extractor Predicting: 128it [01:33,  1.53it/s]Extractor Predicting: 129it [01:33,  1.54it/s]Extractor Predicting: 130it [01:34,  1.50it/s]Extractor Predicting: 131it [01:35,  1.52it/s]Extractor Predicting: 132it [01:35,  1.50it/s]Extractor Predicting: 133it [01:36,  1.49it/s]Extractor Predicting: 134it [01:37,  1.49it/s]Extractor Predicting: 135it [01:37,  1.51it/s]Extractor Predicting: 136it [01:38,  1.55it/s]Extractor Predicting: 137it [01:38,  1.55it/s]Extractor Predicting: 138it [01:39,  1.56it/s]Extractor Predicting: 139it [01:40,  1.56it/s]Extractor Predicting: 140it [01:40,  1.57it/s]Extractor Predicting: 141it [01:41,  1.58it/s]Extractor Predicting: 142it [01:42,  1.55it/s]Extractor Predicting: 143it [01:42,  1.51it/s]Extractor Predicting: 144it [01:43,  1.51it/s]Extractor Predicting: 145it [01:44,  1.56it/s]Extractor Predicting: 146it [01:44,  1.58it/s]Extractor Predicting: 147it [01:45,  1.58it/s]Extractor Predicting: 148it [01:45,  1.59it/s]Extractor Predicting: 149it [01:46,  1.56it/s]Extractor Predicting: 150it [01:47,  1.57it/s]Extractor Predicting: 151it [01:47,  1.54it/s]Extractor Predicting: 152it [01:48,  1.54it/s]Extractor Predicting: 153it [01:49,  1.54it/s]Extractor Predicting: 154it [01:49,  1.49it/s]Extractor Predicting: 155it [01:50,  1.49it/s]Extractor Predicting: 156it [01:51,  1.49it/s]Extractor Predicting: 157it [01:51,  1.50it/s]Extractor Predicting: 158it [01:52,  1.48it/s]Extractor Predicting: 159it [01:53,  1.49it/s]Extractor Predicting: 160it [01:53,  1.50it/s]Extractor Predicting: 161it [01:54,  1.50it/s]Extractor Predicting: 162it [01:55,  1.54it/s]Extractor Predicting: 163it [01:55,  1.67it/s]Extractor Predicting: 164it [01:56,  1.77it/s]Extractor Predicting: 165it [01:56,  1.78it/s]Extractor Predicting: 166it [01:57,  1.74it/s]Extractor Predicting: 167it [01:58,  1.66it/s]Extractor Predicting: 168it [01:58,  1.59it/s]Extractor Predicting: 169it [01:59,  1.54it/s]Extractor Predicting: 170it [02:00,  1.55it/s]Extractor Predicting: 171it [02:00,  1.55it/s]Extractor Predicting: 172it [02:01,  1.53it/s]Extractor Predicting: 173it [02:02,  1.53it/s]Extractor Predicting: 174it [02:02,  1.53it/s]Extractor Predicting: 175it [02:03,  1.51it/s]Extractor Predicting: 176it [02:04,  1.51it/s]Extractor Predicting: 177it [02:04,  1.51it/s]Extractor Predicting: 178it [02:05,  1.53it/s]Extractor Predicting: 179it [02:06,  1.49it/s]Extractor Predicting: 180it [02:06,  1.50it/s]Extractor Predicting: 181it [02:07,  1.53it/s]Extractor Predicting: 182it [02:07,  1.54it/s]Extractor Predicting: 183it [02:08,  1.51it/s]Extractor Predicting: 184it [02:09,  1.50it/s]Extractor Predicting: 185it [02:10,  1.48it/s]Extractor Predicting: 186it [02:10,  1.50it/s]Extractor Predicting: 187it [02:11,  1.51it/s]Extractor Predicting: 188it [02:11,  1.51it/s]Extractor Predicting: 189it [02:12,  1.51it/s]Extractor Predicting: 190it [02:13,  1.50it/s]Extractor Predicting: 191it [02:13,  1.52it/s]Extractor Predicting: 192it [02:14,  1.54it/s]Extractor Predicting: 193it [02:15,  1.54it/s]Extractor Predicting: 194it [02:15,  1.55it/s]Extractor Predicting: 195it [02:16,  1.51it/s]Extractor Predicting: 196it [02:17,  1.52it/s]Extractor Predicting: 197it [02:17,  1.51it/s]Extractor Predicting: 198it [02:18,  1.51it/s]Extractor Predicting: 199it [02:19,  1.50it/s]Extractor Predicting: 200it [02:19,  1.49it/s]Extractor Predicting: 201it [02:20,  1.46it/s]Extractor Predicting: 202it [02:21,  1.47it/s]Extractor Predicting: 203it [02:21,  1.47it/s]Extractor Predicting: 204it [02:22,  1.46it/s]Extractor Predicting: 205it [02:23,  1.47it/s]Extractor Predicting: 206it [02:24,  1.47it/s]Extractor Predicting: 207it [02:24,  1.52it/s]Extractor Predicting: 208it [02:25,  1.52it/s]Extractor Predicting: 209it [02:25,  1.51it/s]Extractor Predicting: 210it [02:26,  1.51it/s]Extractor Predicting: 211it [02:27,  1.51it/s]Extractor Predicting: 212it [02:28,  1.49it/s]Extractor Predicting: 213it [02:28,  1.33it/s]Extractor Predicting: 214it [02:29,  1.40it/s]Extractor Predicting: 215it [02:30,  1.44it/s]Extractor Predicting: 216it [02:30,  1.45it/s]Extractor Predicting: 217it [02:31,  1.45it/s]Extractor Predicting: 218it [02:32,  1.46it/s]Extractor Predicting: 219it [02:32,  1.47it/s]Extractor Predicting: 220it [02:33,  1.48it/s]Extractor Predicting: 221it [02:34,  1.51it/s]Extractor Predicting: 222it [02:34,  1.50it/s]Extractor Predicting: 223it [02:35,  1.51it/s]Extractor Predicting: 224it [02:36,  1.54it/s]Extractor Predicting: 225it [02:36,  1.58it/s]Extractor Predicting: 226it [02:37,  1.55it/s]Extractor Predicting: 227it [02:38,  1.54it/s]Extractor Predicting: 228it [02:38,  1.54it/s]Extractor Predicting: 229it [02:39,  1.47it/s]Extractor Predicting: 230it [02:40,  1.52it/s]Extractor Predicting: 231it [02:40,  1.52it/s]Extractor Predicting: 232it [02:41,  1.51it/s]Extractor Predicting: 233it [02:42,  1.49it/s]Extractor Predicting: 234it [02:42,  1.45it/s]Extractor Predicting: 235it [02:43,  1.47it/s]Extractor Predicting: 236it [02:44,  1.44it/s]Extractor Predicting: 237it [02:45,  1.40it/s]Extractor Predicting: 238it [02:45,  1.39it/s]Extractor Predicting: 239it [02:46,  1.37it/s]Extractor Predicting: 240it [02:47,  1.38it/s]Extractor Predicting: 241it [02:47,  1.41it/s]Extractor Predicting: 242it [02:48,  1.41it/s]Extractor Predicting: 243it [02:49,  1.45it/s]Extractor Predicting: 244it [02:49,  1.45it/s]Extractor Predicting: 245it [02:50,  1.50it/s]Extractor Predicting: 246it [02:51,  1.52it/s]Extractor Predicting: 247it [02:51,  1.54it/s]Extractor Predicting: 248it [02:52,  1.56it/s]Extractor Predicting: 249it [02:53,  1.59it/s]Extractor Predicting: 250it [02:53,  1.55it/s]Extractor Predicting: 251it [02:54,  1.58it/s]Extractor Predicting: 252it [02:54,  1.56it/s]Extractor Predicting: 253it [02:55,  1.55it/s]Extractor Predicting: 254it [02:56,  1.56it/s]Extractor Predicting: 255it [02:56,  1.57it/s]Extractor Predicting: 256it [02:57,  1.55it/s]Extractor Predicting: 257it [02:58,  1.56it/s]Extractor Predicting: 258it [02:58,  1.57it/s]Extractor Predicting: 259it [02:59,  1.54it/s]Extractor Predicting: 260it [03:00,  1.54it/s]Extractor Predicting: 261it [03:00,  1.52it/s]Extractor Predicting: 262it [03:01,  1.52it/s]Extractor Predicting: 263it [03:02,  1.52it/s]Extractor Predicting: 264it [03:02,  1.52it/s]Extractor Predicting: 265it [03:03,  1.53it/s]Extractor Predicting: 266it [03:04,  1.51it/s]Extractor Predicting: 267it [03:04,  1.51it/s]Extractor Predicting: 268it [03:05,  1.50it/s]Extractor Predicting: 269it [03:06,  1.48it/s]Extractor Predicting: 270it [03:06,  1.51it/s]Extractor Predicting: 271it [03:07,  1.53it/s]Extractor Predicting: 272it [03:08,  1.58it/s]Extractor Predicting: 273it [03:08,  1.58it/s]Extractor Predicting: 274it [03:09,  1.58it/s]Extractor Predicting: 275it [03:09,  1.53it/s]Extractor Predicting: 276it [03:10,  1.53it/s]Extractor Predicting: 277it [03:11,  1.55it/s]Extractor Predicting: 278it [03:11,  1.52it/s]Extractor Predicting: 279it [03:12,  1.54it/s]Extractor Predicting: 280it [03:13,  1.59it/s]Extractor Predicting: 281it [03:13,  1.58it/s]Extractor Predicting: 282it [03:14,  1.55it/s]Extractor Predicting: 283it [03:15,  1.52it/s]Extractor Predicting: 284it [03:15,  1.50it/s]Extractor Predicting: 285it [03:16,  1.47it/s]Extractor Predicting: 286it [03:17,  1.48it/s]Extractor Predicting: 287it [03:17,  1.47it/s]Extractor Predicting: 288it [03:18,  1.45it/s]Extractor Predicting: 289it [03:19,  1.44it/s]Extractor Predicting: 290it [03:20,  1.36it/s]Extractor Predicting: 291it [03:20,  1.41it/s]Extractor Predicting: 292it [03:21,  1.45it/s]Extractor Predicting: 293it [03:22,  1.46it/s]Extractor Predicting: 294it [03:22,  1.42it/s]Extractor Predicting: 295it [03:23,  1.44it/s]Extractor Predicting: 296it [03:24,  1.43it/s]Extractor Predicting: 297it [03:24,  1.41it/s]Extractor Predicting: 298it [03:25,  1.39it/s]Extractor Predicting: 299it [03:26,  1.39it/s]Extractor Predicting: 300it [03:27,  1.40it/s]Extractor Predicting: 301it [03:27,  1.37it/s]Extractor Predicting: 302it [03:28,  1.37it/s]Extractor Predicting: 303it [03:29,  1.38it/s]Extractor Predicting: 303it [03:29,  1.45it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6463414634146342,
  "recall": 0.006185807656395891,
  "score": 0.012254335260115608,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 20815
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20915, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:12,  1.51it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:14,  1.51it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.54it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:28,  1.48it/s]Extractor Predicting: 45it [00:29,  1.47it/s]Extractor Predicting: 46it [00:30,  1.46it/s]Extractor Predicting: 47it [00:30,  1.47it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:32,  1.53it/s]Extractor Predicting: 51it [00:33,  1.54it/s]Extractor Predicting: 52it [00:34,  1.55it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:36,  1.54it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:37,  1.55it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:38,  1.53it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:39,  1.52it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.53it/s]Extractor Predicting: 64it [00:42,  1.42it/s]Extractor Predicting: 65it [00:42,  1.38it/s]Extractor Predicting: 66it [00:43,  1.43it/s]Extractor Predicting: 67it [00:44,  1.48it/s]Extractor Predicting: 68it [00:44,  1.46it/s]Extractor Predicting: 69it [00:45,  1.48it/s]Extractor Predicting: 70it [00:46,  1.49it/s]Extractor Predicting: 71it [00:46,  1.50it/s]Extractor Predicting: 72it [00:47,  1.52it/s]Extractor Predicting: 73it [00:48,  1.53it/s]Extractor Predicting: 74it [00:48,  1.53it/s]Extractor Predicting: 75it [00:49,  1.52it/s]Extractor Predicting: 76it [00:50,  1.54it/s]Extractor Predicting: 77it [00:50,  1.53it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:52,  1.52it/s]Extractor Predicting: 80it [00:52,  1.53it/s]Extractor Predicting: 81it [00:53,  1.54it/s]Extractor Predicting: 82it [00:53,  1.55it/s]Extractor Predicting: 83it [00:54,  1.55it/s]Extractor Predicting: 84it [00:55,  1.54it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:56,  1.55it/s]Extractor Predicting: 87it [00:57,  1.52it/s]Extractor Predicting: 88it [00:57,  1.50it/s]Extractor Predicting: 89it [00:58,  1.50it/s]Extractor Predicting: 90it [00:59,  1.50it/s]Extractor Predicting: 91it [00:59,  1.51it/s]Extractor Predicting: 92it [01:00,  1.52it/s]Extractor Predicting: 93it [01:01,  1.51it/s]Extractor Predicting: 94it [01:01,  1.51it/s]Extractor Predicting: 95it [01:02,  1.52it/s]Extractor Predicting: 96it [01:03,  1.52it/s]Extractor Predicting: 97it [01:03,  1.49it/s]Extractor Predicting: 98it [01:04,  1.53it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:05,  1.49it/s]Extractor Predicting: 101it [01:06,  1.52it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:07,  1.50it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:09,  1.49it/s]Extractor Predicting: 107it [01:10,  1.52it/s]Extractor Predicting: 108it [01:11,  1.50it/s]Extractor Predicting: 109it [01:11,  1.49it/s]Extractor Predicting: 110it [01:12,  1.51it/s]Extractor Predicting: 111it [01:13,  1.55it/s]Extractor Predicting: 112it [01:13,  1.56it/s]Extractor Predicting: 113it [01:14,  1.57it/s]Extractor Predicting: 114it [01:15,  1.53it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:16,  1.55it/s]Extractor Predicting: 117it [01:16,  1.52it/s]Extractor Predicting: 118it [01:17,  1.52it/s]Extractor Predicting: 119it [01:18,  1.51it/s]Extractor Predicting: 120it [01:18,  1.51it/s]Extractor Predicting: 121it [01:19,  1.53it/s]Extractor Predicting: 122it [01:20,  1.52it/s]Extractor Predicting: 123it [01:20,  1.54it/s]Extractor Predicting: 124it [01:21,  1.54it/s]Extractor Predicting: 125it [01:22,  1.55it/s]Extractor Predicting: 126it [01:22,  1.52it/s]Extractor Predicting: 127it [01:23,  1.51it/s]Extractor Predicting: 128it [01:24,  1.52it/s]Extractor Predicting: 129it [01:24,  1.56it/s]Extractor Predicting: 130it [01:25,  1.59it/s]Extractor Predicting: 131it [01:26,  1.56it/s]Extractor Predicting: 132it [01:26,  1.52it/s]Extractor Predicting: 133it [01:27,  1.47it/s]Extractor Predicting: 134it [01:28,  1.47it/s]Extractor Predicting: 135it [01:28,  1.48it/s]Extractor Predicting: 136it [01:29,  1.47it/s]Extractor Predicting: 137it [01:30,  1.47it/s]Extractor Predicting: 138it [01:30,  1.47it/s]Extractor Predicting: 139it [01:31,  1.47it/s]Extractor Predicting: 140it [01:32,  1.47it/s]Extractor Predicting: 141it [01:32,  1.46it/s]Extractor Predicting: 142it [01:33,  1.33it/s]Extractor Predicting: 143it [01:34,  1.37it/s]Extractor Predicting: 144it [01:35,  1.40it/s]Extractor Predicting: 145it [01:35,  1.43it/s]Extractor Predicting: 146it [01:36,  1.44it/s]Extractor Predicting: 147it [01:37,  1.44it/s]Extractor Predicting: 148it [01:37,  1.48it/s]Extractor Predicting: 149it [01:38,  1.47it/s]Extractor Predicting: 150it [01:39,  1.48it/s]Extractor Predicting: 151it [01:39,  1.48it/s]Extractor Predicting: 152it [01:40,  1.53it/s]Extractor Predicting: 153it [01:41,  1.54it/s]Extractor Predicting: 154it [01:41,  1.53it/s]Extractor Predicting: 155it [01:42,  1.53it/s]Extractor Predicting: 156it [01:43,  1.53it/s]Extractor Predicting: 157it [01:43,  1.54it/s]Extractor Predicting: 158it [01:44,  1.52it/s]Extractor Predicting: 159it [01:45,  1.51it/s]Extractor Predicting: 160it [01:45,  1.52it/s]Extractor Predicting: 161it [01:46,  1.52it/s]Extractor Predicting: 162it [01:47,  1.51it/s]Extractor Predicting: 163it [01:47,  1.52it/s]Extractor Predicting: 164it [01:48,  1.52it/s]Extractor Predicting: 165it [01:49,  1.56it/s]Extractor Predicting: 166it [01:49,  1.56it/s]Extractor Predicting: 167it [01:50,  1.54it/s]Extractor Predicting: 168it [01:50,  1.55it/s]Extractor Predicting: 169it [01:51,  1.55it/s]Extractor Predicting: 170it [01:52,  1.53it/s]Extractor Predicting: 171it [01:52,  1.49it/s]Extractor Predicting: 172it [01:53,  1.50it/s]Extractor Predicting: 173it [01:54,  1.50it/s]Extractor Predicting: 174it [01:54,  1.54it/s]Extractor Predicting: 175it [01:55,  1.56it/s]Extractor Predicting: 176it [01:56,  1.51it/s]Extractor Predicting: 177it [01:56,  1.53it/s]Extractor Predicting: 178it [01:57,  1.50it/s]Extractor Predicting: 179it [01:58,  1.49it/s]Extractor Predicting: 180it [01:58,  1.54it/s]Extractor Predicting: 181it [01:59,  1.55it/s]Extractor Predicting: 182it [02:00,  1.53it/s]Extractor Predicting: 183it [02:00,  1.55it/s]Extractor Predicting: 184it [02:01,  1.57it/s]Extractor Predicting: 185it [02:02,  1.55it/s]Extractor Predicting: 186it [02:02,  1.57it/s]Extractor Predicting: 187it [02:03,  1.56it/s]Extractor Predicting: 188it [02:04,  1.53it/s]Extractor Predicting: 189it [02:04,  1.50it/s]Extractor Predicting: 190it [02:05,  1.48it/s]Extractor Predicting: 191it [02:06,  1.50it/s]Extractor Predicting: 192it [02:06,  1.53it/s]Extractor Predicting: 193it [02:07,  1.52it/s]Extractor Predicting: 194it [02:08,  1.51it/s]Extractor Predicting: 195it [02:08,  1.53it/s]Extractor Predicting: 196it [02:09,  1.57it/s]Extractor Predicting: 197it [02:09,  1.55it/s]Extractor Predicting: 198it [02:10,  1.57it/s]Extractor Predicting: 199it [02:11,  1.56it/s]Extractor Predicting: 200it [02:11,  1.54it/s]Extractor Predicting: 201it [02:12,  1.53it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:13,  1.54it/s]Extractor Predicting: 204it [02:14,  1.55it/s]Extractor Predicting: 205it [02:15,  1.58it/s]Extractor Predicting: 206it [02:15,  1.55it/s]Extractor Predicting: 207it [02:16,  1.56it/s]Extractor Predicting: 208it [02:16,  1.59it/s]Extractor Predicting: 209it [02:17,  1.59it/s]Extractor Predicting: 210it [02:18,  1.55it/s]Extractor Predicting: 211it [02:18,  1.55it/s]Extractor Predicting: 212it [02:19,  1.54it/s]Extractor Predicting: 213it [02:20,  1.53it/s]Extractor Predicting: 214it [02:20,  1.52it/s]Extractor Predicting: 215it [02:21,  1.51it/s]Extractor Predicting: 216it [02:22,  1.52it/s]Extractor Predicting: 217it [02:22,  1.50it/s]Extractor Predicting: 218it [02:23,  1.55it/s]Extractor Predicting: 219it [02:24,  1.56it/s]Extractor Predicting: 220it [02:24,  1.54it/s]Extractor Predicting: 221it [02:25,  1.51it/s]Extractor Predicting: 222it [02:26,  1.53it/s]Extractor Predicting: 223it [02:26,  1.54it/s]Extractor Predicting: 224it [02:27,  1.53it/s]Extractor Predicting: 225it [02:28,  1.56it/s]Extractor Predicting: 226it [02:28,  1.56it/s]Extractor Predicting: 227it [02:29,  1.56it/s]Extractor Predicting: 228it [02:30,  1.40it/s]Extractor Predicting: 229it [02:30,  1.45it/s]Extractor Predicting: 230it [02:31,  1.46it/s]Extractor Predicting: 231it [02:32,  1.51it/s]Extractor Predicting: 232it [02:32,  1.50it/s]Extractor Predicting: 233it [02:33,  1.50it/s]Extractor Predicting: 234it [02:34,  1.50it/s]Extractor Predicting: 235it [02:34,  1.52it/s]Extractor Predicting: 236it [02:35,  1.51it/s]Extractor Predicting: 237it [02:36,  1.53it/s]Extractor Predicting: 238it [02:36,  1.51it/s]Extractor Predicting: 239it [02:37,  1.56it/s]Extractor Predicting: 240it [02:38,  1.54it/s]Extractor Predicting: 241it [02:38,  1.52it/s]Extractor Predicting: 242it [02:39,  1.53it/s]Extractor Predicting: 243it [02:39,  1.56it/s]Extractor Predicting: 244it [02:40,  1.59it/s]Extractor Predicting: 245it [02:41,  1.60it/s]Extractor Predicting: 246it [02:41,  1.64it/s]Extractor Predicting: 247it [02:42,  1.61it/s]Extractor Predicting: 248it [02:43,  1.62it/s]Extractor Predicting: 249it [02:43,  1.60it/s]Extractor Predicting: 250it [02:44,  1.60it/s]Extractor Predicting: 251it [02:44,  1.60it/s]Extractor Predicting: 252it [02:45,  1.61it/s]Extractor Predicting: 253it [02:46,  1.60it/s]Extractor Predicting: 254it [02:46,  1.60it/s]Extractor Predicting: 255it [02:47,  1.63it/s]Extractor Predicting: 256it [02:48,  1.61it/s]Extractor Predicting: 257it [02:48,  1.60it/s]Extractor Predicting: 258it [02:49,  1.59it/s]Extractor Predicting: 259it [02:49,  1.58it/s]Extractor Predicting: 260it [02:50,  1.58it/s]Extractor Predicting: 261it [02:51,  1.59it/s]Extractor Predicting: 262it [02:51,  1.61it/s]Extractor Predicting: 263it [02:52,  1.63it/s]Extractor Predicting: 264it [02:53,  1.58it/s]Extractor Predicting: 265it [02:53,  1.54it/s]Extractor Predicting: 266it [02:54,  1.53it/s]Extractor Predicting: 267it [02:55,  1.51it/s]Extractor Predicting: 268it [02:55,  1.54it/s]Extractor Predicting: 269it [02:56,  1.56it/s]Extractor Predicting: 270it [02:56,  1.56it/s]Extractor Predicting: 271it [02:57,  1.55it/s]Extractor Predicting: 272it [02:58,  1.55it/s]Extractor Predicting: 273it [02:58,  1.57it/s]Extractor Predicting: 274it [02:59,  1.57it/s]Extractor Predicting: 275it [03:00,  1.60it/s]Extractor Predicting: 276it [03:00,  1.58it/s]Extractor Predicting: 277it [03:01,  1.52it/s]Extractor Predicting: 278it [03:02,  1.56it/s]Extractor Predicting: 279it [03:02,  1.59it/s]Extractor Predicting: 280it [03:03,  1.61it/s]Extractor Predicting: 281it [03:03,  1.63it/s]Extractor Predicting: 282it [03:04,  1.63it/s]Extractor Predicting: 283it [03:05,  1.63it/s]Extractor Predicting: 284it [03:05,  1.62it/s]Extractor Predicting: 285it [03:06,  1.58it/s]Extractor Predicting: 286it [03:06,  1.62it/s]Extractor Predicting: 287it [03:07,  1.60it/s]Extractor Predicting: 288it [03:08,  1.59it/s]Extractor Predicting: 289it [03:08,  1.59it/s]Extractor Predicting: 290it [03:09,  1.61it/s]Extractor Predicting: 291it [03:10,  1.62it/s]Extractor Predicting: 292it [03:10,  1.57it/s]Extractor Predicting: 293it [03:11,  1.57it/s]Extractor Predicting: 294it [03:12,  1.57it/s]Extractor Predicting: 295it [03:12,  1.57it/s]Extractor Predicting: 296it [03:13,  1.58it/s]Extractor Predicting: 297it [03:13,  1.56it/s]Extractor Predicting: 298it [03:14,  1.59it/s]Extractor Predicting: 299it [03:15,  1.54it/s]Extractor Predicting: 300it [03:15,  1.51it/s]Extractor Predicting: 301it [03:16,  1.45it/s]Extractor Predicting: 302it [03:17,  1.47it/s]Extractor Predicting: 303it [03:18,  1.46it/s]Extractor Predicting: 304it [03:18,  1.47it/s]Extractor Predicting: 305it [03:19,  1.45it/s]Extractor Predicting: 306it [03:20,  1.44it/s]Extractor Predicting: 306it [03:20,  1.53it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27741935483870966,
  "recall": 0.005855119825708061,
  "score": 0.01146819575943459,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor', 'path_test': 'zero_rte/wiki/unseen_10_seed_3/test.jsonl', 'labels': ['continent', 'field of this occupation', 'field of work', 'founded by', 'movement', 'owned by', 'performer', 'producer', 'record label', 'replaces'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 6322
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 6422, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.42it/s]Extractor Predicting: 14it [00:09,  1.42it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.42it/s]Extractor Predicting: 18it [00:12,  1.44it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.54it/s]Extractor Predicting: 27it [00:18,  1.54it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.54it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:22,  1.52it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:23,  1.54it/s]Extractor Predicting: 36it [00:24,  1.57it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.58it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.49it/s]Extractor Predicting: 43it [00:28,  1.47it/s]Extractor Predicting: 44it [00:29,  1.47it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:30,  1.42it/s]Extractor Predicting: 47it [00:31,  1.44it/s]Extractor Predicting: 48it [00:32,  1.46it/s]Extractor Predicting: 49it [00:32,  1.47it/s]Extractor Predicting: 50it [00:33,  1.47it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:34,  1.46it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:36,  1.48it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:37,  1.48it/s]Extractor Predicting: 57it [00:38,  1.44it/s]Extractor Predicting: 58it [00:39,  1.45it/s]Extractor Predicting: 59it [00:39,  1.44it/s]Extractor Predicting: 60it [00:40,  1.28it/s]Extractor Predicting: 60it [00:40,  1.47it/s]
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6666666666666666,
  "recall": 0.001869741352446245,
  "score": 0.0037290242386575517,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_10_seed_3/extractor/results_multi_is_eval_False.json"
}
