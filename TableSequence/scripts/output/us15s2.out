/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:12, 12.83s/it]Extractor Predicting: 2it [00:15,  6.62s/it]Extractor Predicting: 3it [00:16,  4.12s/it]Extractor Predicting: 4it [00:16,  2.74s/it]Extractor Predicting: 5it [00:17,  1.98s/it]Extractor Predicting: 6it [00:18,  1.53s/it]Extractor Predicting: 7it [00:18,  1.23s/it]Extractor Predicting: 8it [00:19,  1.04s/it]Extractor Predicting: 9it [00:20,  1.04it/s]Extractor Predicting: 10it [00:20,  1.17it/s]Extractor Predicting: 11it [00:21,  1.24it/s]Extractor Predicting: 12it [00:22,  1.32it/s]Extractor Predicting: 13it [00:22,  1.39it/s]Extractor Predicting: 14it [00:23,  1.42it/s]Extractor Predicting: 15it [00:24,  1.47it/s]Extractor Predicting: 16it [00:24,  1.49it/s]Extractor Predicting: 17it [00:25,  1.55it/s]Extractor Predicting: 18it [00:25,  1.55it/s]Extractor Predicting: 19it [00:26,  1.58it/s]Extractor Predicting: 20it [00:27,  1.55it/s]Extractor Predicting: 21it [00:27,  1.57it/s]Extractor Predicting: 22it [00:28,  1.59it/s]Extractor Predicting: 23it [00:29,  1.62it/s]Extractor Predicting: 24it [00:29,  1.59it/s]Extractor Predicting: 25it [00:30,  1.55it/s]Extractor Predicting: 26it [00:31,  1.55it/s]Extractor Predicting: 27it [00:31,  1.54it/s]Extractor Predicting: 28it [00:32,  1.51it/s]Extractor Predicting: 29it [00:33,  1.53it/s]Extractor Predicting: 30it [00:33,  1.53it/s]Extractor Predicting: 31it [00:34,  1.54it/s]Extractor Predicting: 32it [00:34,  1.53it/s]Extractor Predicting: 33it [00:35,  1.52it/s]Extractor Predicting: 34it [00:36,  1.48it/s]Extractor Predicting: 35it [00:36,  1.52it/s]Extractor Predicting: 36it [00:37,  1.55it/s]Extractor Predicting: 37it [00:38,  1.53it/s]Extractor Predicting: 38it [00:38,  1.52it/s]Extractor Predicting: 39it [00:39,  1.52it/s]Extractor Predicting: 40it [00:40,  1.52it/s]Extractor Predicting: 41it [00:40,  1.53it/s]Extractor Predicting: 42it [00:41,  1.54it/s]Extractor Predicting: 43it [00:42,  1.53it/s]Extractor Predicting: 44it [00:42,  1.51it/s]Extractor Predicting: 45it [00:43,  1.54it/s]Extractor Predicting: 46it [00:44,  1.55it/s]Extractor Predicting: 47it [00:44,  1.51it/s]Extractor Predicting: 48it [00:45,  1.51it/s]Extractor Predicting: 49it [00:46,  1.48it/s]Extractor Predicting: 50it [00:46,  1.47it/s]Extractor Predicting: 51it [00:47,  1.47it/s]Extractor Predicting: 52it [00:48,  1.49it/s]Extractor Predicting: 53it [00:48,  1.49it/s]Extractor Predicting: 54it [00:49,  1.46it/s]Extractor Predicting: 55it [00:50,  1.48it/s]Extractor Predicting: 56it [00:50,  1.48it/s]Extractor Predicting: 57it [00:51,  1.51it/s]Extractor Predicting: 58it [00:52,  1.50it/s]Extractor Predicting: 59it [00:52,  1.49it/s]Extractor Predicting: 60it [00:53,  1.50it/s]Extractor Predicting: 61it [00:54,  1.52it/s]Extractor Predicting: 62it [00:54,  1.54it/s]Extractor Predicting: 63it [00:55,  1.50it/s]Extractor Predicting: 64it [00:56,  1.49it/s]Extractor Predicting: 65it [00:56,  1.54it/s]Extractor Predicting: 66it [00:57,  1.52it/s]Extractor Predicting: 67it [00:58,  1.55it/s]Extractor Predicting: 68it [00:58,  1.55it/s]Extractor Predicting: 69it [00:59,  1.55it/s]Extractor Predicting: 70it [01:00,  1.59it/s]Extractor Predicting: 71it [01:00,  1.58it/s]Extractor Predicting: 72it [01:01,  1.59it/s]Extractor Predicting: 73it [01:01,  1.63it/s]Extractor Predicting: 74it [01:02,  1.63it/s]Extractor Predicting: 75it [01:03,  1.59it/s]Extractor Predicting: 76it [01:03,  1.55it/s]Extractor Predicting: 77it [01:04,  1.55it/s]Extractor Predicting: 78it [01:05,  1.55it/s]Extractor Predicting: 79it [01:05,  1.54it/s]Extractor Predicting: 80it [01:06,  1.54it/s]Extractor Predicting: 81it [01:07,  1.54it/s]Extractor Predicting: 82it [01:07,  1.53it/s]Extractor Predicting: 83it [01:08,  1.55it/s]Extractor Predicting: 84it [01:09,  1.51it/s]Extractor Predicting: 85it [01:09,  1.56it/s]Extractor Predicting: 86it [01:10,  1.52it/s]Extractor Predicting: 87it [01:10,  1.55it/s]Extractor Predicting: 88it [01:11,  1.52it/s]Extractor Predicting: 89it [01:12,  1.49it/s]Extractor Predicting: 90it [01:13,  1.48it/s]Extractor Predicting: 91it [01:13,  1.46it/s]Extractor Predicting: 92it [01:14,  1.48it/s]Extractor Predicting: 93it [01:15,  1.51it/s]Extractor Predicting: 94it [01:15,  1.48it/s]Extractor Predicting: 95it [01:16,  1.52it/s]Extractor Predicting: 96it [01:16,  1.53it/s]Extractor Predicting: 97it [01:17,  1.53it/s]Extractor Predicting: 98it [01:18,  1.38it/s]Extractor Predicting: 99it [01:19,  1.38it/s]Extractor Predicting: 100it [01:19,  1.39it/s]Extractor Predicting: 101it [01:20,  1.44it/s]Extractor Predicting: 102it [01:21,  1.43it/s]Extractor Predicting: 103it [01:22,  1.44it/s]Extractor Predicting: 104it [01:22,  1.48it/s]Extractor Predicting: 105it [01:23,  1.49it/s]Extractor Predicting: 106it [01:24,  1.45it/s]Extractor Predicting: 107it [01:24,  1.46it/s]Extractor Predicting: 108it [01:25,  1.49it/s]Extractor Predicting: 109it [01:25,  1.53it/s]Extractor Predicting: 110it [01:26,  1.53it/s]Extractor Predicting: 111it [01:27,  1.58it/s]Extractor Predicting: 112it [01:27,  1.56it/s]Extractor Predicting: 113it [01:28,  1.54it/s]Extractor Predicting: 114it [01:29,  1.52it/s]Extractor Predicting: 115it [01:29,  1.50it/s]Extractor Predicting: 116it [01:30,  1.50it/s]Extractor Predicting: 117it [01:31,  1.49it/s]Extractor Predicting: 118it [01:31,  1.48it/s]Extractor Predicting: 119it [01:32,  1.49it/s]Extractor Predicting: 120it [01:33,  1.53it/s]Extractor Predicting: 121it [01:33,  1.50it/s]Extractor Predicting: 122it [01:34,  1.52it/s]Extractor Predicting: 123it [01:35,  1.48it/s]Extractor Predicting: 124it [01:35,  1.49it/s]Extractor Predicting: 125it [01:36,  1.50it/s]Extractor Predicting: 126it [01:37,  1.52it/s]Extractor Predicting: 127it [01:37,  1.51it/s]Extractor Predicting: 128it [01:38,  1.49it/s]Extractor Predicting: 129it [01:39,  1.50it/s]Extractor Predicting: 130it [01:39,  1.47it/s]Extractor Predicting: 131it [01:40,  1.49it/s]Extractor Predicting: 132it [01:41,  1.51it/s]Extractor Predicting: 133it [01:41,  1.48it/s]Extractor Predicting: 134it [01:42,  1.44it/s]Extractor Predicting: 135it [01:43,  1.46it/s]Extractor Predicting: 136it [01:43,  1.47it/s]Extractor Predicting: 137it [01:44,  1.47it/s]Extractor Predicting: 138it [01:45,  1.48it/s]Extractor Predicting: 139it [01:46,  1.46it/s]Extractor Predicting: 140it [01:46,  1.45it/s]Extractor Predicting: 141it [01:47,  1.44it/s]Extractor Predicting: 142it [01:48,  1.49it/s]Extractor Predicting: 143it [01:48,  1.52it/s]Extractor Predicting: 143it [01:48,  1.32it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.39226519337016574,
  "recall": 0.08156232050545663,
  "score": 0.1350451735615787,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.78it/s]Extractor Predicting: 3it [00:01,  1.72it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.68it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.66it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.58it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.71it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:16,  1.69it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:18,  1.58it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:20,  1.53it/s]Extractor Predicting: 35it [00:21,  1.52it/s]Extractor Predicting: 36it [00:22,  1.51it/s]Extractor Predicting: 37it [00:22,  1.52it/s]Extractor Predicting: 38it [00:23,  1.50it/s]Extractor Predicting: 39it [00:24,  1.52it/s]Extractor Predicting: 40it [00:24,  1.50it/s]Extractor Predicting: 41it [00:25,  1.52it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:26,  1.48it/s]Extractor Predicting: 44it [00:27,  1.50it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:28,  1.51it/s]Extractor Predicting: 47it [00:29,  1.51it/s]Extractor Predicting: 48it [00:30,  1.50it/s]Extractor Predicting: 49it [00:30,  1.50it/s]Extractor Predicting: 50it [00:31,  1.52it/s]Extractor Predicting: 51it [00:32,  1.52it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:33,  1.53it/s]Extractor Predicting: 54it [00:34,  1.53it/s]Extractor Predicting: 55it [00:34,  1.51it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:36,  1.55it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.54it/s]Extractor Predicting: 61it [00:38,  1.51it/s]Extractor Predicting: 62it [00:39,  1.54it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:41,  1.52it/s]Extractor Predicting: 67it [00:42,  1.55it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:43,  1.57it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:45,  1.54it/s]Extractor Predicting: 72it [00:45,  1.52it/s]Extractor Predicting: 73it [00:46,  1.51it/s]Extractor Predicting: 74it [00:47,  1.54it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:49,  1.62it/s]Extractor Predicting: 79it [00:50,  1.61it/s]Extractor Predicting: 80it [00:51,  1.42it/s]Extractor Predicting: 81it [00:51,  1.45it/s]Extractor Predicting: 82it [00:52,  1.51it/s]Extractor Predicting: 83it [00:52,  1.50it/s]Extractor Predicting: 84it [00:53,  1.50it/s]Extractor Predicting: 85it [00:54,  1.49it/s]Extractor Predicting: 86it [00:54,  1.52it/s]Extractor Predicting: 87it [00:55,  1.54it/s]Extractor Predicting: 88it [00:56,  1.53it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:57,  1.59it/s]Extractor Predicting: 91it [00:58,  1.59it/s]Extractor Predicting: 92it [00:58,  1.54it/s]Extractor Predicting: 93it [00:59,  1.55it/s]Extractor Predicting: 94it [01:00,  1.52it/s]Extractor Predicting: 95it [01:00,  1.51it/s]Extractor Predicting: 96it [01:01,  1.50it/s]Extractor Predicting: 97it [01:02,  1.52it/s]Extractor Predicting: 98it [01:02,  1.50it/s]Extractor Predicting: 99it [01:03,  1.52it/s]Extractor Predicting: 100it [01:04,  1.52it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.53it/s]Extractor Predicting: 103it [01:06,  1.53it/s]Extractor Predicting: 104it [01:06,  1.53it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:08,  1.51it/s]Extractor Predicting: 108it [01:09,  1.53it/s]Extractor Predicting: 109it [01:09,  1.51it/s]Extractor Predicting: 110it [01:10,  1.50it/s]Extractor Predicting: 111it [01:11,  1.50it/s]Extractor Predicting: 112it [01:12,  1.48it/s]Extractor Predicting: 113it [01:12,  1.51it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:13,  1.51it/s]Extractor Predicting: 116it [01:14,  1.57it/s]Extractor Predicting: 117it [01:15,  1.65it/s]Extractor Predicting: 118it [01:15,  1.68it/s]Extractor Predicting: 119it [01:16,  1.67it/s]Extractor Predicting: 120it [01:16,  1.66it/s]Extractor Predicting: 121it [01:17,  1.64it/s]Extractor Predicting: 122it [01:18,  1.62it/s]Extractor Predicting: 123it [01:18,  1.60it/s]Extractor Predicting: 124it [01:19,  1.64it/s]Extractor Predicting: 125it [01:19,  1.70it/s]Extractor Predicting: 126it [01:20,  1.68it/s]Extractor Predicting: 127it [01:21,  1.69it/s]Extractor Predicting: 128it [01:21,  1.71it/s]Extractor Predicting: 129it [01:22,  1.68it/s]Extractor Predicting: 130it [01:22,  1.67it/s]Extractor Predicting: 131it [01:23,  1.69it/s]Extractor Predicting: 132it [01:24,  1.69it/s]Extractor Predicting: 133it [01:24,  1.68it/s]Extractor Predicting: 134it [01:25,  1.69it/s]Extractor Predicting: 135it [01:25,  1.71it/s]Extractor Predicting: 136it [01:26,  1.74it/s]Extractor Predicting: 137it [01:26,  1.74it/s]Extractor Predicting: 138it [01:27,  1.71it/s]Extractor Predicting: 139it [01:28,  1.67it/s]Extractor Predicting: 140it [01:28,  1.69it/s]Extractor Predicting: 141it [01:29,  1.65it/s]Extractor Predicting: 142it [01:30,  1.65it/s]Extractor Predicting: 143it [01:30,  1.67it/s]Extractor Predicting: 144it [01:31,  1.63it/s]Extractor Predicting: 145it [01:31,  1.62it/s]Extractor Predicting: 146it [01:32,  1.64it/s]Extractor Predicting: 147it [01:33,  1.64it/s]Extractor Predicting: 148it [01:33,  1.62it/s]Extractor Predicting: 149it [01:34,  1.68it/s]Extractor Predicting: 150it [01:34,  1.65it/s]Extractor Predicting: 151it [01:35,  1.63it/s]Extractor Predicting: 152it [01:36,  1.66it/s]Extractor Predicting: 153it [01:36,  1.68it/s]Extractor Predicting: 154it [01:37,  1.69it/s]Extractor Predicting: 155it [01:37,  1.69it/s]Extractor Predicting: 156it [01:38,  1.66it/s]Extractor Predicting: 157it [01:39,  1.65it/s]Extractor Predicting: 158it [01:39,  1.67it/s]Extractor Predicting: 159it [01:40,  1.63it/s]Extractor Predicting: 160it [01:40,  1.63it/s]Extractor Predicting: 161it [01:41,  1.64it/s]Extractor Predicting: 162it [01:42,  1.63it/s]Extractor Predicting: 163it [01:42,  1.62it/s]Extractor Predicting: 164it [01:43,  1.63it/s]Extractor Predicting: 165it [01:43,  1.64it/s]Extractor Predicting: 166it [01:44,  1.65it/s]Extractor Predicting: 167it [01:45,  1.65it/s]Extractor Predicting: 168it [01:45,  1.61it/s]Extractor Predicting: 169it [01:46,  1.61it/s]Extractor Predicting: 170it [01:47,  1.58it/s]Extractor Predicting: 171it [01:47,  1.56it/s]Extractor Predicting: 172it [01:48,  1.63it/s]Extractor Predicting: 173it [01:49,  1.57it/s]Extractor Predicting: 174it [01:49,  1.56it/s]Extractor Predicting: 175it [01:50,  1.54it/s]Extractor Predicting: 176it [01:50,  1.54it/s]Extractor Predicting: 177it [01:51,  1.52it/s]Extractor Predicting: 178it [01:52,  1.50it/s]Extractor Predicting: 179it [01:53,  1.50it/s]Extractor Predicting: 180it [01:53,  1.54it/s]Extractor Predicting: 181it [01:54,  1.54it/s]Extractor Predicting: 182it [01:54,  1.51it/s]Extractor Predicting: 183it [01:55,  1.51it/s]Extractor Predicting: 184it [01:56,  1.50it/s]Extractor Predicting: 185it [01:56,  1.50it/s]Extractor Predicting: 186it [01:57,  1.49it/s]Extractor Predicting: 187it [01:58,  1.48it/s]Extractor Predicting: 188it [01:58,  1.52it/s]Extractor Predicting: 189it [01:59,  1.52it/s]Extractor Predicting: 190it [02:00,  1.38it/s]Extractor Predicting: 191it [02:01,  1.39it/s]Extractor Predicting: 192it [02:01,  1.43it/s]Extractor Predicting: 193it [02:02,  1.43it/s]Extractor Predicting: 194it [02:03,  1.44it/s]Extractor Predicting: 195it [02:03,  1.46it/s]Extractor Predicting: 196it [02:04,  1.48it/s]Extractor Predicting: 197it [02:05,  1.48it/s]Extractor Predicting: 198it [02:05,  1.49it/s]Extractor Predicting: 199it [02:06,  1.49it/s]Extractor Predicting: 200it [02:07,  1.48it/s]Extractor Predicting: 201it [02:07,  1.50it/s]Extractor Predicting: 202it [02:08,  1.50it/s]Extractor Predicting: 203it [02:09,  1.52it/s]Extractor Predicting: 204it [02:09,  1.54it/s]Extractor Predicting: 205it [02:10,  1.59it/s]Extractor Predicting: 206it [02:11,  1.58it/s]Extractor Predicting: 207it [02:11,  1.57it/s]Extractor Predicting: 208it [02:12,  1.57it/s]Extractor Predicting: 209it [02:12,  1.56it/s]Extractor Predicting: 210it [02:13,  1.58it/s]Extractor Predicting: 211it [02:14,  1.62it/s]Extractor Predicting: 212it [02:14,  1.62it/s]Extractor Predicting: 213it [02:15,  1.64it/s]Extractor Predicting: 214it [02:16,  1.62it/s]Extractor Predicting: 215it [02:16,  1.57it/s]Extractor Predicting: 216it [02:17,  1.58it/s]Extractor Predicting: 217it [02:17,  1.59it/s]Extractor Predicting: 218it [02:18,  1.62it/s]Extractor Predicting: 219it [02:19,  1.64it/s]Extractor Predicting: 220it [02:19,  1.61it/s]Extractor Predicting: 221it [02:20,  1.64it/s]Extractor Predicting: 222it [02:20,  1.66it/s]Extractor Predicting: 223it [02:21,  1.63it/s]Extractor Predicting: 224it [02:22,  1.63it/s]Extractor Predicting: 225it [02:22,  1.62it/s]Extractor Predicting: 226it [02:23,  1.57it/s]Extractor Predicting: 227it [02:24,  1.56it/s]Extractor Predicting: 228it [02:24,  1.57it/s]Extractor Predicting: 229it [02:25,  1.57it/s]Extractor Predicting: 230it [02:26,  1.56it/s]Extractor Predicting: 231it [02:26,  1.57it/s]Extractor Predicting: 232it [02:27,  1.60it/s]Extractor Predicting: 233it [02:27,  1.66it/s]Extractor Predicting: 234it [02:28,  1.67it/s]Extractor Predicting: 235it [02:29,  1.67it/s]Extractor Predicting: 236it [02:29,  1.72it/s]Extractor Predicting: 237it [02:30,  1.72it/s]Extractor Predicting: 238it [02:30,  1.72it/s]Extractor Predicting: 239it [02:31,  1.71it/s]Extractor Predicting: 240it [02:31,  1.73it/s]Extractor Predicting: 241it [02:32,  1.72it/s]Extractor Predicting: 242it [02:33,  1.72it/s]Extractor Predicting: 243it [02:33,  1.77it/s]Extractor Predicting: 244it [02:34,  1.75it/s]Extractor Predicting: 245it [02:34,  1.81it/s]Extractor Predicting: 246it [02:35,  1.84it/s]Extractor Predicting: 247it [02:35,  1.78it/s]Extractor Predicting: 248it [02:36,  1.73it/s]Extractor Predicting: 249it [02:37,  1.73it/s]Extractor Predicting: 250it [02:37,  1.73it/s]Extractor Predicting: 251it [02:38,  1.76it/s]Extractor Predicting: 252it [02:38,  1.77it/s]Extractor Predicting: 253it [02:39,  1.75it/s]Extractor Predicting: 254it [02:39,  1.73it/s]Extractor Predicting: 255it [02:40,  1.75it/s]Extractor Predicting: 256it [02:41,  1.74it/s]Extractor Predicting: 257it [02:41,  1.77it/s]Extractor Predicting: 258it [02:42,  1.77it/s]Extractor Predicting: 259it [02:42,  1.79it/s]Extractor Predicting: 260it [02:43,  1.77it/s]Extractor Predicting: 261it [02:43,  1.67it/s]Extractor Predicting: 262it [02:44,  1.67it/s]Extractor Predicting: 263it [02:45,  1.62it/s]Extractor Predicting: 264it [02:45,  1.57it/s]Extractor Predicting: 265it [02:46,  1.56it/s]Extractor Predicting: 266it [02:47,  1.57it/s]Extractor Predicting: 267it [02:47,  1.60it/s]Extractor Predicting: 268it [02:48,  1.59it/s]Extractor Predicting: 269it [02:49,  1.55it/s]Extractor Predicting: 270it [02:49,  1.53it/s]Extractor Predicting: 271it [02:50,  1.52it/s]Extractor Predicting: 272it [02:51,  1.55it/s]Extractor Predicting: 273it [02:51,  1.54it/s]Extractor Predicting: 274it [02:52,  1.53it/s]Extractor Predicting: 275it [02:52,  1.53it/s]Extractor Predicting: 276it [02:53,  1.54it/s]Extractor Predicting: 277it [02:54,  1.52it/s]Extractor Predicting: 278it [02:54,  1.52it/s]Extractor Predicting: 279it [02:55,  1.55it/s]Extractor Predicting: 280it [02:56,  1.53it/s]Extractor Predicting: 281it [02:56,  1.52it/s]Extractor Predicting: 282it [02:57,  1.51it/s]Extractor Predicting: 283it [02:58,  1.52it/s]Extractor Predicting: 284it [02:58,  1.51it/s]Extractor Predicting: 285it [02:59,  1.52it/s]Extractor Predicting: 286it [03:00,  1.52it/s]Extractor Predicting: 287it [03:00,  1.51it/s]Extractor Predicting: 288it [03:01,  1.55it/s]Extractor Predicting: 289it [03:02,  1.55it/s]Extractor Predicting: 290it [03:02,  1.57it/s]Extractor Predicting: 291it [03:03,  1.58it/s]Extractor Predicting: 292it [03:04,  1.57it/s]Extractor Predicting: 293it [03:04,  1.57it/s]Extractor Predicting: 294it [03:05,  1.59it/s]Extractor Predicting: 295it [03:05,  1.59it/s]Extractor Predicting: 296it [03:06,  1.57it/s]Extractor Predicting: 297it [03:07,  1.59it/s]Extractor Predicting: 298it [03:07,  1.63it/s]Extractor Predicting: 299it [03:08,  1.59it/s]Extractor Predicting: 300it [03:09,  1.60it/s]Extractor Predicting: 301it [03:09,  1.58it/s]Extractor Predicting: 302it [03:10,  1.56it/s]Extractor Predicting: 303it [03:10,  1.56it/s]Extractor Predicting: 304it [03:11,  1.55it/s]Extractor Predicting: 305it [03:12,  1.55it/s]Extractor Predicting: 306it [03:12,  1.57it/s]Extractor Predicting: 307it [03:13,  1.57it/s]Extractor Predicting: 308it [03:14,  1.56it/s]Extractor Predicting: 309it [03:14,  1.53it/s]Extractor Predicting: 310it [03:15,  1.54it/s]Extractor Predicting: 311it [03:16,  1.55it/s]Extractor Predicting: 312it [03:17,  1.39it/s]Extractor Predicting: 313it [03:17,  1.44it/s]Extractor Predicting: 314it [03:18,  1.47it/s]Extractor Predicting: 315it [03:18,  1.51it/s]Extractor Predicting: 316it [03:19,  1.52it/s]Extractor Predicting: 317it [03:20,  1.56it/s]Extractor Predicting: 318it [03:20,  1.60it/s]Extractor Predicting: 319it [03:21,  1.60it/s]Extractor Predicting: 320it [03:22,  1.58it/s]Extractor Predicting: 321it [03:22,  1.57it/s]Extractor Predicting: 322it [03:23,  1.57it/s]Extractor Predicting: 323it [03:23,  1.59it/s]Extractor Predicting: 324it [03:24,  1.60it/s]Extractor Predicting: 325it [03:25,  1.59it/s]Extractor Predicting: 326it [03:25,  1.60it/s]Extractor Predicting: 327it [03:26,  1.60it/s]Extractor Predicting: 328it [03:27,  1.59it/s]Extractor Predicting: 329it [03:27,  1.59it/s]Extractor Predicting: 330it [03:28,  1.58it/s]Extractor Predicting: 331it [03:29,  1.57it/s]Extractor Predicting: 332it [03:29,  1.59it/s]Extractor Predicting: 333it [03:30,  1.56it/s]Extractor Predicting: 334it [03:30,  1.59it/s]Extractor Predicting: 335it [03:31,  1.58it/s]Extractor Predicting: 336it [03:32,  1.59it/s]Extractor Predicting: 337it [03:32,  1.57it/s]Extractor Predicting: 338it [03:33,  1.58it/s]Extractor Predicting: 339it [03:34,  1.59it/s]Extractor Predicting: 340it [03:34,  1.57it/s]Extractor Predicting: 341it [03:35,  1.59it/s]Extractor Predicting: 342it [03:35,  1.61it/s]Extractor Predicting: 343it [03:36,  1.60it/s]Extractor Predicting: 344it [03:37,  1.63it/s]Extractor Predicting: 345it [03:37,  1.60it/s]Extractor Predicting: 346it [03:38,  1.59it/s]Extractor Predicting: 347it [03:39,  1.60it/s]Extractor Predicting: 348it [03:39,  1.56it/s]Extractor Predicting: 349it [03:40,  1.58it/s]Extractor Predicting: 350it [03:40,  1.56it/s]Extractor Predicting: 351it [03:41,  1.57it/s]Extractor Predicting: 352it [03:42,  1.56it/s]Extractor Predicting: 353it [03:42,  1.56it/s]Extractor Predicting: 354it [03:43,  1.56it/s]Extractor Predicting: 355it [03:44,  1.55it/s]Extractor Predicting: 356it [03:44,  1.57it/s]Extractor Predicting: 357it [03:45,  1.54it/s]Extractor Predicting: 358it [03:46,  1.56it/s]Extractor Predicting: 359it [03:46,  1.57it/s]Extractor Predicting: 360it [03:47,  1.57it/s]Extractor Predicting: 361it [03:48,  1.57it/s]Extractor Predicting: 362it [03:48,  1.60it/s]Extractor Predicting: 363it [03:49,  1.59it/s]Extractor Predicting: 364it [03:49,  1.62it/s]Extractor Predicting: 365it [03:50,  1.60it/s]Extractor Predicting: 366it [03:51,  1.60it/s]Extractor Predicting: 367it [03:51,  1.60it/s]Extractor Predicting: 368it [03:52,  1.60it/s]Extractor Predicting: 369it [03:53,  1.56it/s]Extractor Predicting: 370it [03:53,  1.56it/s]Extractor Predicting: 371it [03:54,  1.56it/s]Extractor Predicting: 372it [03:54,  1.57it/s]Extractor Predicting: 373it [03:55,  1.57it/s]Extractor Predicting: 374it [03:56,  1.60it/s]Extractor Predicting: 375it [03:56,  1.59it/s]Extractor Predicting: 376it [03:57,  1.56it/s]Extractor Predicting: 377it [03:58,  1.62it/s]Extractor Predicting: 378it [03:58,  1.62it/s]Extractor Predicting: 379it [03:59,  1.64it/s]Extractor Predicting: 380it [03:59,  1.60it/s]Extractor Predicting: 381it [04:00,  1.60it/s]Extractor Predicting: 382it [04:01,  1.59it/s]Extractor Predicting: 383it [04:01,  1.57it/s]Extractor Predicting: 384it [04:02,  1.56it/s]Extractor Predicting: 385it [04:03,  1.57it/s]Extractor Predicting: 386it [04:03,  1.55it/s]Extractor Predicting: 387it [04:04,  1.55it/s]Extractor Predicting: 388it [04:05,  1.50it/s]Extractor Predicting: 389it [04:05,  1.52it/s]Extractor Predicting: 390it [04:06,  1.53it/s]Extractor Predicting: 391it [04:07,  1.56it/s]Extractor Predicting: 392it [04:07,  1.58it/s]Extractor Predicting: 393it [04:08,  1.56it/s]Extractor Predicting: 394it [04:08,  1.57it/s]Extractor Predicting: 395it [04:09,  1.57it/s]Extractor Predicting: 396it [04:10,  1.55it/s]Extractor Predicting: 397it [04:10,  1.56it/s]Extractor Predicting: 398it [04:11,  1.55it/s]Extractor Predicting: 399it [04:12,  1.58it/s]Extractor Predicting: 400it [04:12,  1.56it/s]Extractor Predicting: 401it [04:13,  1.57it/s]Extractor Predicting: 402it [04:14,  1.56it/s]Extractor Predicting: 403it [04:14,  1.54it/s]Extractor Predicting: 404it [04:15,  1.56it/s]Extractor Predicting: 405it [04:15,  1.57it/s]Extractor Predicting: 406it [04:16,  1.60it/s]Extractor Predicting: 407it [04:17,  1.59it/s]Extractor Predicting: 408it [04:17,  1.57it/s]Extractor Predicting: 409it [04:18,  1.59it/s]Extractor Predicting: 410it [04:19,  1.60it/s]Extractor Predicting: 411it [04:19,  1.59it/s]Extractor Predicting: 412it [04:20,  1.61it/s]Extractor Predicting: 413it [04:20,  1.60it/s]Extractor Predicting: 414it [04:21,  1.59it/s]Extractor Predicting: 415it [04:22,  1.56it/s]Extractor Predicting: 416it [04:22,  1.60it/s]Extractor Predicting: 417it [04:23,  1.60it/s]Extractor Predicting: 418it [04:24,  1.61it/s]Extractor Predicting: 419it [04:24,  1.62it/s]Extractor Predicting: 420it [04:25,  1.63it/s]Extractor Predicting: 421it [04:25,  1.60it/s]Extractor Predicting: 422it [04:26,  1.60it/s]Extractor Predicting: 423it [04:27,  1.60it/s]Extractor Predicting: 424it [04:27,  1.60it/s]Extractor Predicting: 425it [04:28,  1.42it/s]Extractor Predicting: 426it [04:29,  1.49it/s]Extractor Predicting: 427it [04:29,  1.51it/s]Extractor Predicting: 428it [04:30,  1.50it/s]Extractor Predicting: 429it [04:31,  1.53it/s]Extractor Predicting: 429it [04:31,  1.58it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35363668935865283,
  "recall": 0.09599299747130909,
  "score": 0.15099824064866518,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.73it/s]Extractor Predicting: 5it [00:03,  1.62it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6666666666666666,
  "recall": 0.03669724770642202,
  "score": 0.06956521739130435,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:17<05:35, 17.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:32<04:42, 15.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:54<05:18, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:10<04:45, 17.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:29<04:29, 17.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:45<04:04, 17.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:04<03:53, 17.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:22<03:34, 17.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:42<03:24, 18.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:59<03:03, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:17<02:42, 18.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:37<02:28, 18.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:53<02:05, 17.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:10<01:46, 17.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:28<01:29, 17.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:48<01:13, 18.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [05:06<00:54, 18.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:22<00:35, 17.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:39<00:17, 17.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:58<00:00, 17.83s/it]Generating: 100%|██████████| 20/20 [05:58<00:00, 17.91s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : head of government .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in life , the children became members of the Church of England and were elected at Westminster Abbey in 1415 . Head Entity : churches , Tail Entity : Sarah .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 259, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 380, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 469, 'raw': 640}
{'target': 600, 'success': 497, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 590, 'raw': 800}
{'target': 600, 'success': 611, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.734375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : performer .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 473, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 525, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 576, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : competition class .', 'success_rate': 0.75375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 415, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 561, 'raw': 736}
{'target': 600, 'success': 585, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8046875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : has part .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 363, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.7877604166666666, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : occupation . Context : Following World War II , the University of New Brunswick 's faculty of philosophy began at the end of 1943 , primarily teaching philosophy . Head Entity : Professor , Tail Entity : philosophy .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.7864583333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : record label .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : winner .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 14948
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15048, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:02,  2.82s/it]Extractor Estimating: 2it [00:03,  1.57s/it]Extractor Estimating: 3it [00:04,  1.09s/it]Extractor Estimating: 4it [00:04,  1.11it/s]Extractor Estimating: 5it [00:05,  1.27it/s]Extractor Estimating: 6it [00:05,  1.38it/s]Extractor Estimating: 7it [00:07,  1.13it/s]Extractor Estimating: 8it [00:11,  2.09s/it]Extractor Estimating: 9it [00:12,  1.63s/it]Extractor Estimating: 10it [00:12,  1.32s/it]Extractor Estimating: 11it [00:13,  1.09s/it]Extractor Estimating: 12it [00:14,  1.05it/s]Extractor Estimating: 13it [00:14,  1.10it/s]Extractor Estimating: 14it [00:15,  1.25it/s]Extractor Estimating: 15it [00:16,  1.36it/s]Extractor Estimating: 16it [00:16,  1.43it/s]Extractor Estimating: 17it [00:17,  1.48it/s]Extractor Estimating: 18it [00:17,  1.57it/s]Extractor Estimating: 19it [00:18,  1.56it/s]Extractor Estimating: 20it [00:19,  1.53it/s]Extractor Estimating: 21it [00:19,  1.59it/s]Extractor Estimating: 22it [00:20,  1.61it/s]Extractor Estimating: 23it [00:21,  1.58it/s]Extractor Estimating: 24it [00:21,  1.61it/s]Extractor Estimating: 25it [00:22,  1.59it/s]Extractor Estimating: 26it [00:22,  1.62it/s]Extractor Estimating: 27it [00:23,  1.61it/s]Extractor Estimating: 28it [00:24,  1.55it/s]Extractor Estimating: 29it [00:24,  1.58it/s]Extractor Estimating: 30it [00:25,  1.65it/s]Extractor Estimating: 31it [00:25,  1.65it/s]Extractor Estimating: 32it [00:26,  1.65it/s]Extractor Estimating: 33it [00:27,  1.59it/s]Extractor Estimating: 34it [00:27,  1.66it/s]Extractor Estimating: 35it [00:28,  1.67it/s]Extractor Estimating: 36it [00:29,  1.60it/s]Extractor Estimating: 37it [00:29,  1.63it/s]Extractor Estimating: 38it [00:30,  1.64it/s]Extractor Estimating: 39it [00:30,  1.68it/s]Extractor Estimating: 40it [00:31,  1.63it/s]Extractor Estimating: 41it [00:32,  1.59it/s]Extractor Estimating: 42it [00:32,  1.61it/s]Extractor Estimating: 43it [00:33,  1.60it/s]Extractor Estimating: 44it [00:33,  1.69it/s]Extractor Estimating: 45it [00:34,  1.65it/s]Extractor Estimating: 46it [00:35,  1.54it/s]Extractor Estimating: 47it [00:35,  1.56it/s]Extractor Estimating: 48it [00:36,  1.55it/s]Extractor Estimating: 49it [00:37,  1.63it/s]Extractor Estimating: 50it [00:37,  1.61it/s]Extractor Estimating: 51it [00:38,  1.54it/s]Extractor Estimating: 52it [00:39,  1.48it/s]Extractor Estimating: 53it [00:39,  1.56it/s]Extractor Estimating: 54it [00:40,  1.54it/s]Extractor Estimating: 55it [00:41,  1.47it/s]Extractor Estimating: 56it [00:41,  1.45it/s]Extractor Estimating: 57it [00:42,  1.48it/s]Extractor Estimating: 58it [00:43,  1.52it/s]Extractor Estimating: 59it [00:43,  1.53it/s]Extractor Estimating: 60it [00:44,  1.45it/s]Extractor Estimating: 61it [00:45,  1.47it/s]Extractor Estimating: 62it [00:45,  1.50it/s]Extractor Estimating: 63it [00:46,  1.43it/s]Extractor Estimating: 64it [00:47,  1.43it/s]Extractor Estimating: 65it [00:48,  1.40it/s]Extractor Estimating: 66it [00:48,  1.46it/s]Extractor Estimating: 67it [00:49,  1.42it/s]Extractor Estimating: 68it [00:50,  1.46it/s]Extractor Estimating: 69it [00:50,  1.49it/s]Extractor Estimating: 70it [00:51,  1.46it/s]Extractor Estimating: 71it [00:52,  1.48it/s]Extractor Estimating: 72it [00:52,  1.46it/s]Extractor Estimating: 73it [00:53,  1.48it/s]Extractor Estimating: 74it [00:54,  1.53it/s]Extractor Estimating: 75it [00:54,  1.51it/s]Extractor Estimating: 76it [00:55,  1.50it/s]Extractor Estimating: 77it [00:56,  1.52it/s]Extractor Estimating: 78it [00:56,  1.50it/s]Extractor Estimating: 79it [00:57,  1.47it/s]Extractor Estimating: 80it [00:58,  1.49it/s]Extractor Estimating: 81it [00:58,  1.49it/s]Extractor Estimating: 82it [00:59,  1.44it/s]Extractor Estimating: 83it [01:00,  1.46it/s]Extractor Estimating: 84it [01:01,  1.31it/s]Extractor Estimating: 85it [01:01,  1.36it/s]Extractor Estimating: 86it [01:02,  1.41it/s]Extractor Estimating: 87it [01:03,  1.44it/s]Extractor Estimating: 88it [01:03,  1.42it/s]Extractor Estimating: 89it [01:04,  1.44it/s]Extractor Estimating: 90it [01:05,  1.44it/s]Extractor Estimating: 91it [01:05,  1.45it/s]Extractor Estimating: 92it [01:06,  1.44it/s]Extractor Estimating: 93it [01:07,  1.48it/s]Extractor Estimating: 94it [01:07,  1.47it/s]Extractor Estimating: 95it [01:08,  1.43it/s]Extractor Estimating: 96it [01:09,  1.49it/s]Extractor Estimating: 97it [01:09,  1.49it/s]Extractor Estimating: 98it [01:10,  1.49it/s]Extractor Estimating: 99it [01:11,  1.50it/s]Extractor Estimating: 100it [01:11,  1.48it/s]Extractor Estimating: 101it [01:12,  1.54it/s]Extractor Estimating: 102it [01:13,  1.56it/s]Extractor Estimating: 103it [01:13,  1.56it/s]Extractor Estimating: 104it [01:14,  1.60it/s]Extractor Estimating: 105it [01:14,  1.64it/s]Extractor Estimating: 106it [01:15,  1.64it/s]Extractor Estimating: 107it [01:16,  1.59it/s]Extractor Estimating: 108it [01:16,  1.56it/s]Extractor Estimating: 109it [01:17,  1.60it/s]Extractor Estimating: 110it [01:18,  1.60it/s]Extractor Estimating: 111it [01:18,  1.64it/s]Extractor Estimating: 112it [01:19,  1.42it/s]Extractor Estimating: 113it [01:20,  1.49it/s]Extractor Estimating: 114it [01:20,  1.56it/s]Extractor Estimating: 115it [01:21,  1.55it/s]Extractor Estimating: 116it [01:22,  1.60it/s]Extractor Estimating: 117it [01:22,  1.59it/s]Extractor Estimating: 118it [01:23,  1.58it/s]Extractor Estimating: 119it [01:23,  1.54it/s]Extractor Estimating: 120it [01:24,  1.52it/s]Extractor Estimating: 121it [01:25,  1.48it/s]Extractor Estimating: 122it [01:26,  1.50it/s]Extractor Estimating: 123it [01:26,  1.51it/s]Extractor Estimating: 124it [01:27,  1.55it/s]Extractor Estimating: 125it [01:27,  1.55it/s]Extractor Estimating: 126it [01:28,  1.56it/s]Extractor Estimating: 127it [01:29,  1.50it/s]Extractor Estimating: 128it [01:29,  1.50it/s]Extractor Estimating: 129it [01:30,  1.51it/s]Extractor Estimating: 130it [01:31,  1.48it/s]Extractor Estimating: 131it [01:32,  1.44it/s]Extractor Estimating: 132it [01:32,  1.44it/s]Extractor Estimating: 133it [01:33,  1.43it/s]Extractor Estimating: 134it [01:34,  1.46it/s]Extractor Estimating: 135it [01:34,  1.48it/s]Extractor Estimating: 136it [01:35,  1.49it/s]Extractor Estimating: 137it [01:36,  1.51it/s]Extractor Estimating: 138it [01:36,  1.50it/s]Extractor Estimating: 139it [01:37,  1.51it/s]Extractor Estimating: 140it [01:38,  1.45it/s]Extractor Estimating: 141it [01:38,  1.45it/s]Extractor Estimating: 142it [01:39,  1.47it/s]Extractor Estimating: 143it [01:40,  1.52it/s]Extractor Estimating: 144it [01:40,  1.53it/s]Extractor Estimating: 145it [01:41,  1.54it/s]Extractor Estimating: 146it [01:42,  1.47it/s]Extractor Estimating: 147it [01:42,  1.41it/s]Extractor Estimating: 148it [01:43,  1.42it/s]Extractor Estimating: 149it [01:44,  1.40it/s]Extractor Estimating: 150it [01:45,  1.41it/s]Extractor Estimating: 151it [01:45,  1.43it/s]Extractor Estimating: 152it [01:46,  1.43it/s]Extractor Estimating: 153it [01:47,  1.48it/s]Extractor Estimating: 154it [01:47,  1.55it/s]Extractor Estimating: 155it [01:48,  1.46it/s]Extractor Estimating: 156it [01:48,  1.52it/s]Extractor Estimating: 157it [01:49,  1.55it/s]Extractor Estimating: 158it [01:50,  1.58it/s]Extractor Estimating: 159it [01:50,  1.59it/s]Extractor Estimating: 160it [01:51,  1.57it/s]Extractor Estimating: 161it [01:52,  1.54it/s]Extractor Estimating: 162it [01:52,  1.59it/s]Extractor Estimating: 163it [01:53,  1.62it/s]Extractor Estimating: 164it [01:53,  1.63it/s]Extractor Estimating: 165it [01:54,  1.65it/s]Extractor Estimating: 166it [01:55,  1.68it/s]Extractor Estimating: 167it [01:55,  1.62it/s]Extractor Estimating: 168it [01:56,  1.63it/s]Extractor Estimating: 169it [01:56,  1.66it/s]Extractor Estimating: 170it [01:57,  1.61it/s]Extractor Estimating: 171it [01:58,  1.60it/s]Extractor Estimating: 172it [01:58,  1.57it/s]Extractor Estimating: 173it [01:59,  1.57it/s]Extractor Estimating: 174it [02:00,  1.62it/s]Extractor Estimating: 175it [02:00,  1.61it/s]Extractor Estimating: 176it [02:01,  1.64it/s]Extractor Estimating: 177it [02:01,  1.61it/s]Extractor Estimating: 178it [02:02,  1.61it/s]Extractor Estimating: 179it [02:03,  1.60it/s]Extractor Estimating: 180it [02:03,  1.65it/s]Extractor Estimating: 181it [02:04,  1.69it/s]Extractor Estimating: 182it [02:04,  1.68it/s]Extractor Estimating: 183it [02:05,  1.69it/s]Extractor Estimating: 184it [02:06,  1.66it/s]Extractor Estimating: 185it [02:06,  1.69it/s]Extractor Estimating: 186it [02:07,  1.64it/s]Extractor Estimating: 187it [02:07,  1.67it/s]Extractor Estimating: 188it [02:08,  1.63it/s]Extractor Estimating: 189it [02:09,  1.57it/s]Extractor Estimating: 190it [02:09,  1.58it/s]Extractor Estimating: 191it [02:10,  1.60it/s]Extractor Estimating: 192it [02:11,  1.61it/s]Extractor Estimating: 193it [02:11,  1.62it/s]Extractor Estimating: 194it [02:12,  1.61it/s]Extractor Estimating: 195it [02:12,  1.63it/s]Extractor Estimating: 196it [02:13,  1.65it/s]Extractor Estimating: 197it [02:14,  1.67it/s]Extractor Estimating: 198it [02:14,  1.70it/s]Extractor Estimating: 199it [02:15,  1.66it/s]Extractor Estimating: 200it [02:15,  1.69it/s]Extractor Estimating: 201it [02:16,  1.56it/s]Extractor Estimating: 202it [02:17,  1.51it/s]Extractor Estimating: 203it [02:18,  1.51it/s]Extractor Estimating: 204it [02:18,  1.53it/s]Extractor Estimating: 205it [02:19,  1.55it/s]Extractor Estimating: 206it [02:19,  1.56it/s]Extractor Estimating: 207it [02:20,  1.58it/s]Extractor Estimating: 208it [02:21,  1.52it/s]Extractor Estimating: 209it [02:21,  1.49it/s]Extractor Estimating: 210it [02:22,  1.41it/s]Extractor Estimating: 211it [02:23,  1.36it/s]Extractor Estimating: 212it [02:24,  1.38it/s]Extractor Estimating: 213it [02:25,  1.37it/s]Extractor Estimating: 214it [02:25,  1.41it/s]Extractor Estimating: 215it [02:26,  1.42it/s]Extractor Estimating: 216it [02:27,  1.44it/s]Extractor Estimating: 217it [02:27,  1.44it/s]Extractor Estimating: 218it [02:28,  1.49it/s]Extractor Estimating: 219it [02:29,  1.45it/s]Extractor Estimating: 220it [02:29,  1.41it/s]Extractor Estimating: 221it [02:30,  1.39it/s]Extractor Estimating: 222it [02:31,  1.41it/s]Extractor Estimating: 223it [02:31,  1.42it/s]Extractor Estimating: 224it [02:32,  1.32it/s]Extractor Estimating: 225it [02:33,  1.32it/s]Extractor Estimating: 226it [02:34,  1.36it/s]Extractor Estimating: 227it [02:34,  1.37it/s]Extractor Estimating: 228it [02:35,  1.37it/s]Extractor Estimating: 229it [02:36,  1.34it/s]Extractor Estimating: 230it [02:37,  1.40it/s]Extractor Estimating: 231it [02:37,  1.41it/s]Extractor Estimating: 232it [02:38,  1.43it/s]Extractor Estimating: 233it [02:39,  1.42it/s]Extractor Estimating: 234it [02:39,  1.42it/s]Extractor Estimating: 235it [02:40,  1.48it/s]Extractor Estimating: 236it [02:41,  1.42it/s]Extractor Estimating: 237it [02:41,  1.45it/s]Extractor Estimating: 238it [02:42,  1.46it/s]Extractor Estimating: 239it [02:43,  1.39it/s]Extractor Estimating: 240it [02:44,  1.43it/s]Extractor Estimating: 241it [02:44,  1.40it/s]Extractor Estimating: 242it [02:45,  1.42it/s]Extractor Estimating: 243it [02:46,  1.43it/s]Extractor Estimating: 244it [02:46,  1.46it/s]Extractor Estimating: 245it [02:47,  1.46it/s]Extractor Estimating: 246it [02:48,  1.44it/s]Extractor Estimating: 247it [02:48,  1.42it/s]Extractor Estimating: 248it [02:49,  1.44it/s]Extractor Estimating: 249it [02:50,  1.42it/s]Extractor Estimating: 250it [02:51,  1.43it/s]Extractor Estimating: 251it [02:51,  1.48it/s]Extractor Estimating: 252it [02:52,  1.42it/s]Extractor Estimating: 253it [02:53,  1.47it/s]Extractor Estimating: 254it [02:53,  1.48it/s]Extractor Estimating: 255it [02:54,  1.48it/s]Extractor Estimating: 256it [02:55,  1.53it/s]Extractor Estimating: 257it [02:55,  1.54it/s]Extractor Estimating: 258it [02:56,  1.54it/s]Extractor Estimating: 259it [02:57,  1.46it/s]Extractor Estimating: 260it [02:57,  1.46it/s]Extractor Estimating: 261it [02:58,  1.49it/s]Extractor Estimating: 262it [02:59,  1.45it/s]Extractor Estimating: 263it [02:59,  1.48it/s]Extractor Estimating: 264it [03:00,  1.47it/s]Extractor Estimating: 265it [03:01,  1.47it/s]Extractor Estimating: 266it [03:01,  1.50it/s]Extractor Estimating: 267it [03:02,  1.53it/s]Extractor Estimating: 268it [03:03,  1.53it/s]Extractor Estimating: 269it [03:03,  1.55it/s]Extractor Estimating: 270it [03:04,  1.55it/s]Extractor Estimating: 271it [03:04,  1.58it/s]Extractor Estimating: 272it [03:05,  1.57it/s]Extractor Estimating: 273it [03:06,  1.51it/s]Extractor Estimating: 274it [03:06,  1.53it/s]Extractor Estimating: 275it [03:07,  1.52it/s]Extractor Estimating: 276it [03:08,  1.47it/s]Extractor Estimating: 277it [03:08,  1.49it/s]Extractor Estimating: 278it [03:09,  1.47it/s]Extractor Estimating: 279it [03:10,  1.50it/s]Extractor Estimating: 280it [03:10,  1.52it/s]Extractor Estimating: 281it [03:11,  1.51it/s]Extractor Estimating: 282it [03:12,  1.57it/s]Extractor Estimating: 283it [03:12,  1.55it/s]Extractor Estimating: 284it [03:13,  1.34it/s]Extractor Estimating: 285it [03:14,  1.39it/s]Extractor Estimating: 286it [03:15,  1.44it/s]Extractor Estimating: 287it [03:15,  1.47it/s]Extractor Estimating: 288it [03:16,  1.53it/s]Extractor Estimating: 289it [03:17,  1.55it/s]Extractor Estimating: 290it [03:17,  1.54it/s]Extractor Estimating: 291it [03:18,  1.56it/s]Extractor Estimating: 292it [03:18,  1.54it/s]Extractor Estimating: 293it [03:19,  1.49it/s]Extractor Estimating: 294it [03:20,  1.50it/s]Extractor Estimating: 295it [03:21,  1.50it/s]Extractor Estimating: 296it [03:21,  1.49it/s]Extractor Estimating: 297it [03:22,  1.49it/s]Extractor Estimating: 298it [03:23,  1.51it/s]Extractor Estimating: 299it [03:23,  1.46it/s]Extractor Estimating: 300it [03:24,  1.42it/s]Extractor Estimating: 301it [03:25,  1.47it/s]Extractor Estimating: 302it [03:25,  1.51it/s]Extractor Estimating: 303it [03:26,  1.38it/s]Extractor Estimating: 304it [03:27,  1.47it/s]Extractor Estimating: 305it [03:27,  1.53it/s]Extractor Estimating: 306it [03:28,  1.53it/s]Extractor Estimating: 307it [03:29,  1.56it/s]Extractor Estimating: 308it [03:29,  1.60it/s]Extractor Estimating: 309it [03:30,  1.63it/s]Extractor Estimating: 310it [03:30,  1.63it/s]Extractor Estimating: 311it [03:31,  1.65it/s]Extractor Estimating: 312it [03:32,  1.63it/s]Extractor Estimating: 313it [03:32,  1.65it/s]Extractor Estimating: 314it [03:33,  1.58it/s]Extractor Estimating: 315it [03:33,  1.57it/s]Extractor Estimating: 316it [03:34,  1.58it/s]Extractor Estimating: 317it [03:35,  1.66it/s]Extractor Estimating: 318it [03:35,  1.71it/s]Extractor Estimating: 319it [03:36,  1.61it/s]Extractor Estimating: 320it [03:36,  1.64it/s]Extractor Estimating: 321it [03:37,  1.69it/s]Extractor Estimating: 322it [03:38,  1.62it/s]Extractor Estimating: 323it [03:38,  1.66it/s]Extractor Estimating: 324it [03:39,  1.63it/s]Extractor Estimating: 325it [03:40,  1.59it/s]Extractor Estimating: 326it [03:40,  1.63it/s]Extractor Estimating: 327it [03:41,  1.64it/s]Extractor Estimating: 328it [03:41,  1.69it/s]Extractor Estimating: 329it [03:42,  1.64it/s]Extractor Estimating: 330it [03:43,  1.61it/s]Extractor Estimating: 331it [03:43,  1.63it/s]Extractor Estimating: 332it [03:44,  1.57it/s]Extractor Estimating: 333it [03:45,  1.56it/s]Extractor Estimating: 334it [03:45,  1.61it/s]Extractor Estimating: 335it [03:46,  1.59it/s]Extractor Estimating: 336it [03:46,  1.54it/s]Extractor Estimating: 337it [03:47,  1.58it/s]Extractor Estimating: 338it [03:48,  1.56it/s]Extractor Estimating: 339it [03:48,  1.47it/s]Extractor Estimating: 340it [03:49,  1.55it/s]Extractor Estimating: 341it [03:50,  1.59it/s]Extractor Estimating: 342it [03:50,  1.57it/s]Extractor Estimating: 343it [03:51,  1.58it/s]Extractor Estimating: 344it [03:52,  1.59it/s]Extractor Estimating: 345it [03:52,  1.59it/s]Extractor Estimating: 346it [03:53,  1.52it/s]Extractor Estimating: 347it [03:53,  1.56it/s]Extractor Estimating: 348it [03:54,  1.56it/s]Extractor Estimating: 349it [03:55,  1.60it/s]Extractor Estimating: 350it [03:55,  1.58it/s]Extractor Estimating: 351it [03:56,  1.47it/s]Extractor Estimating: 352it [03:57,  1.47it/s]Extractor Estimating: 353it [03:57,  1.52it/s]Extractor Estimating: 354it [03:58,  1.55it/s]Extractor Estimating: 355it [03:59,  1.49it/s]Extractor Estimating: 356it [04:00,  1.47it/s]Extractor Estimating: 357it [04:00,  1.44it/s]Extractor Estimating: 358it [04:01,  1.42it/s]Extractor Estimating: 359it [04:02,  1.48it/s]Extractor Estimating: 360it [04:02,  1.37it/s]Extractor Estimating: 361it [04:03,  1.37it/s]Extractor Estimating: 362it [04:04,  1.42it/s]Extractor Estimating: 363it [04:04,  1.44it/s]Extractor Estimating: 364it [04:05,  1.48it/s]Extractor Estimating: 365it [04:06,  1.50it/s]Extractor Estimating: 366it [04:06,  1.50it/s]Extractor Estimating: 367it [04:07,  1.53it/s]Extractor Estimating: 368it [04:08,  1.46it/s]Extractor Estimating: 369it [04:08,  1.52it/s]Extractor Estimating: 370it [04:09,  1.47it/s]Extractor Estimating: 371it [04:10,  1.54it/s]Extractor Estimating: 372it [04:10,  1.53it/s]Extractor Estimating: 373it [04:11,  1.53it/s]Extractor Estimating: 374it [04:12,  1.55it/s]Extractor Estimating: 375it [04:12,  1.55it/s]Extractor Estimating: 376it [04:13,  1.49it/s]Extractor Estimating: 377it [04:14,  1.53it/s]Extractor Estimating: 378it [04:14,  1.57it/s]Extractor Estimating: 379it [04:15,  1.41it/s]Extractor Estimating: 380it [04:16,  1.42it/s]Extractor Estimating: 381it [04:17,  1.41it/s]Extractor Estimating: 382it [04:17,  1.32it/s]Extractor Estimating: 383it [04:18,  1.40it/s]Extractor Estimating: 384it [04:19,  1.38it/s]Extractor Estimating: 385it [04:19,  1.41it/s]Extractor Estimating: 386it [04:20,  1.36it/s]Extractor Estimating: 387it [04:21,  1.37it/s]Extractor Estimating: 388it [04:22,  1.40it/s]Extractor Estimating: 389it [04:22,  1.42it/s]Extractor Estimating: 390it [04:23,  1.41it/s]Extractor Estimating: 391it [04:24,  1.43it/s]Extractor Estimating: 392it [04:24,  1.43it/s]Extractor Estimating: 393it [04:25,  1.45it/s]Extractor Estimating: 394it [04:26,  1.45it/s]Extractor Estimating: 395it [04:26,  1.49it/s]Extractor Estimating: 396it [04:27,  1.49it/s]Extractor Estimating: 397it [04:28,  1.47it/s]Extractor Estimating: 398it [04:28,  1.49it/s]Extractor Estimating: 399it [04:29,  1.43it/s]Extractor Estimating: 400it [04:30,  1.44it/s]Extractor Estimating: 401it [04:30,  1.48it/s]Extractor Estimating: 402it [04:31,  1.46it/s]Extractor Estimating: 403it [04:32,  1.47it/s]Extractor Estimating: 404it [04:32,  1.54it/s]Extractor Estimating: 405it [04:33,  1.56it/s]Extractor Estimating: 406it [04:34,  1.57it/s]Extractor Estimating: 407it [04:34,  1.62it/s]Extractor Estimating: 408it [04:35,  1.61it/s]Extractor Estimating: 409it [04:36,  1.56it/s]Extractor Estimating: 410it [04:36,  1.55it/s]Extractor Estimating: 411it [04:37,  1.53it/s]Extractor Estimating: 412it [04:38,  1.54it/s]Extractor Estimating: 413it [04:38,  1.57it/s]Extractor Estimating: 414it [04:39,  1.56it/s]Extractor Estimating: 415it [04:39,  1.53it/s]Extractor Estimating: 416it [04:40,  1.53it/s]Extractor Estimating: 417it [04:41,  1.53it/s]Extractor Estimating: 418it [04:41,  1.52it/s]Extractor Estimating: 419it [04:42,  1.53it/s]Extractor Estimating: 420it [04:43,  1.55it/s]Extractor Estimating: 421it [04:43,  1.60it/s]Extractor Estimating: 422it [04:44,  1.64it/s]Extractor Estimating: 423it [04:44,  1.65it/s]Extractor Estimating: 424it [04:45,  1.59it/s]Extractor Estimating: 425it [04:46,  1.58it/s]Extractor Estimating: 426it [04:47,  1.46it/s]Extractor Estimating: 427it [04:47,  1.42it/s]Extractor Estimating: 428it [04:48,  1.45it/s]Extractor Estimating: 429it [04:49,  1.46it/s]Extractor Estimating: 430it [04:49,  1.49it/s]Extractor Estimating: 431it [04:50,  1.50it/s]Extractor Estimating: 432it [04:51,  1.48it/s]Extractor Estimating: 433it [04:51,  1.49it/s]Extractor Estimating: 434it [04:52,  1.51it/s]Extractor Estimating: 435it [04:53,  1.46it/s]Extractor Estimating: 436it [04:53,  1.42it/s]Extractor Estimating: 437it [04:54,  1.42it/s]Extractor Estimating: 438it [04:55,  1.41it/s]Extractor Estimating: 439it [04:56,  1.40it/s]Extractor Estimating: 440it [04:56,  1.43it/s]Extractor Estimating: 441it [04:57,  1.44it/s]Extractor Estimating: 442it [04:58,  1.46it/s]Extractor Estimating: 443it [04:58,  1.43it/s]Extractor Estimating: 444it [04:59,  1.44it/s]Extractor Estimating: 445it [05:00,  1.47it/s]Extractor Estimating: 446it [05:00,  1.40it/s]Extractor Estimating: 447it [05:01,  1.42it/s]Extractor Estimating: 448it [05:02,  1.43it/s]Extractor Estimating: 449it [05:03,  1.44it/s]Extractor Estimating: 450it [05:03,  1.46it/s]Extractor Estimating: 451it [05:04,  1.50it/s]Extractor Estimating: 452it [05:04,  1.51it/s]Extractor Estimating: 453it [05:05,  1.54it/s]Extractor Estimating: 454it [05:06,  1.58it/s]Extractor Estimating: 455it [05:06,  1.61it/s]Extractor Estimating: 456it [05:07,  1.62it/s]Extractor Estimating: 457it [05:08,  1.60it/s]Extractor Estimating: 458it [05:08,  1.57it/s]Extractor Estimating: 459it [05:09,  1.57it/s]Extractor Estimating: 460it [05:09,  1.60it/s]Extractor Estimating: 461it [05:10,  1.66it/s]Extractor Estimating: 462it [05:11,  1.65it/s]Extractor Estimating: 463it [05:11,  1.66it/s]Extractor Estimating: 464it [05:12,  1.57it/s]Extractor Estimating: 465it [05:13,  1.60it/s]Extractor Estimating: 466it [05:13,  1.42it/s]Extractor Estimating: 467it [05:14,  1.51it/s]Extractor Estimating: 468it [05:15,  1.53it/s]Extractor Estimating: 469it [05:15,  1.52it/s]Extractor Estimating: 470it [05:16,  1.58it/s]Extractor Estimating: 471it [05:16,  1.57it/s]Extractor Estimating: 472it [05:17,  1.52it/s]Extractor Estimating: 473it [05:18,  1.54it/s]Extractor Estimating: 474it [05:19,  1.49it/s]Extractor Estimating: 475it [05:19,  1.52it/s]Extractor Estimating: 476it [05:20,  1.49it/s]Extractor Estimating: 477it [05:21,  1.52it/s]Extractor Estimating: 478it [05:21,  1.48it/s]Extractor Estimating: 479it [05:22,  1.42it/s]Extractor Estimating: 480it [05:23,  1.43it/s]Extractor Estimating: 481it [05:23,  1.46it/s]Extractor Estimating: 482it [05:24,  1.45it/s]Extractor Estimating: 483it [05:25,  1.50it/s]Extractor Estimating: 484it [05:25,  1.55it/s]Extractor Estimating: 485it [05:26,  1.53it/s]Extractor Estimating: 486it [05:27,  1.49it/s]Extractor Estimating: 487it [05:27,  1.52it/s]Extractor Estimating: 488it [05:28,  1.52it/s]Extractor Estimating: 489it [05:29,  1.49it/s]Extractor Estimating: 490it [05:29,  1.50it/s]Extractor Estimating: 491it [05:30,  1.52it/s]Extractor Estimating: 492it [05:31,  1.45it/s]Extractor Estimating: 493it [05:31,  1.44it/s]Extractor Estimating: 494it [05:32,  1.43it/s]Extractor Estimating: 495it [05:33,  1.43it/s]Extractor Estimating: 496it [05:34,  1.41it/s]Extractor Estimating: 497it [05:34,  1.43it/s]Extractor Estimating: 498it [05:35,  1.45it/s]Extractor Estimating: 499it [05:36,  1.45it/s]Extractor Estimating: 500it [05:36,  1.50it/s]Extractor Estimating: 500it [05:36,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 10027 mean pseudo reward: 0.9453153954799749
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 28543
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28643, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28643, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.363, loss:981.7613
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.075, loss:912.7320
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.099, loss:908.3484
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.083, loss:890.1583
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.075, loss:866.0174
>> valid entity prec:0.5814, rec:0.5579, f1:0.5694
>> valid relation prec:0.1277, rec:0.0241, f1:0.0406
>> valid relation with NER prec:0.1277, rec:0.0241, f1:0.0406
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.472, loss:863.2481
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 282, avg_time 1.074, loss:867.2281
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 382, avg_time 1.090, loss:890.6183
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.063, loss:847.7184
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.077, loss:859.7516
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5763, rec:0.5239, f1:0.5488
>> valid relation prec:0.1092, rec:0.0164, f1:0.0285
>> valid relation with NER prec:0.1092, rec:0.0164, f1:0.0285
g_step 1100, step 264, avg_time 2.489, loss:857.4356
g_step 1200, step 364, avg_time 1.085, loss:873.6498
g_step 1300, step 46, avg_time 1.075, loss:853.0118
g_step 1400, step 146, avg_time 1.073, loss:800.0054
g_step 1500, step 246, avg_time 1.095, loss:819.8184
>> valid entity prec:0.5409, rec:0.6100, f1:0.5734
>> valid relation prec:0.0840, rec:0.0121, f1:0.0211
>> valid relation with NER prec:0.0840, rec:0.0121, f1:0.0211
new max entity f1 on valid!
g_step 1600, step 346, avg_time 2.459, loss:811.5715
g_step 1700, step 28, avg_time 1.083, loss:790.7377
g_step 1800, step 128, avg_time 1.084, loss:760.9512
g_step 1900, step 228, avg_time 1.073, loss:770.9064
g_step 2000, step 328, avg_time 1.090, loss:793.6091
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5261, rec:0.5624, f1:0.5436
>> valid relation prec:0.0876, rec:0.0126, f1:0.0221
>> valid relation with NER prec:0.0876, rec:0.0126, f1:0.0221
g_step 2100, step 10, avg_time 2.469, loss:766.7685
g_step 2200, step 110, avg_time 1.071, loss:700.4710
g_step 2300, step 210, avg_time 1.078, loss:740.4891
g_step 2400, step 310, avg_time 1.088, loss:716.3693
g_step 2500, step 410, avg_time 1.087, loss:752.9956
>> valid entity prec:0.5590, rec:0.5656, f1:0.5623
>> valid relation prec:0.1179, rec:0.0238, f1:0.0397
>> valid relation with NER prec:0.1179, rec:0.0238, f1:0.0397
g_step 2600, step 92, avg_time 2.467, loss:684.2475
g_step 2700, step 192, avg_time 1.089, loss:691.2698
g_step 2800, step 292, avg_time 1.087, loss:688.2459
g_step 2900, step 392, avg_time 1.075, loss:709.7037
g_step 3000, step 74, avg_time 1.083, loss:681.7437
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5811, rec:0.5346, f1:0.5569
>> valid relation prec:0.0700, rec:0.0167, f1:0.0269
>> valid relation with NER prec:0.0700, rec:0.0167, f1:0.0269
g_step 3100, step 174, avg_time 2.456, loss:670.2581
g_step 3200, step 274, avg_time 1.084, loss:654.0720
g_step 3300, step 374, avg_time 1.082, loss:680.8044
g_step 3400, step 56, avg_time 1.085, loss:643.8459
g_step 3500, step 156, avg_time 1.076, loss:637.9872
>> valid entity prec:0.5694, rec:0.6102, f1:0.5891
>> valid relation prec:0.1003, rec:0.0227, f1:0.0370
>> valid relation with NER prec:0.1003, rec:0.0227, f1:0.0370
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 256, avg_time 2.472, loss:648.4006
g_step 3700, step 356, avg_time 1.077, loss:626.6956
g_step 3800, step 38, avg_time 1.089, loss:628.5706
g_step 3900, step 138, avg_time 1.098, loss:591.9285
g_step 4000, step 238, avg_time 1.080, loss:618.5958
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5798, rec:0.5598, f1:0.5696
>> valid relation prec:0.0692, rec:0.0178, f1:0.0283
>> valid relation with NER prec:0.0692, rec:0.0178, f1:0.0283
g_step 4100, step 338, avg_time 2.460, loss:605.0308
g_step 4200, step 20, avg_time 1.069, loss:624.8909
g_step 4300, step 120, avg_time 1.094, loss:581.5774
g_step 4400, step 220, avg_time 1.076, loss:585.3459
g_step 4500, step 320, avg_time 1.084, loss:617.9740
>> valid entity prec:0.5389, rec:0.5779, f1:0.5577
>> valid relation prec:0.0620, rec:0.0158, f1:0.0252
>> valid relation with NER prec:0.0620, rec:0.0158, f1:0.0252
g_step 4600, step 2, avg_time 2.470, loss:593.3594
g_step 4700, step 102, avg_time 1.089, loss:537.3901
g_step 4800, step 202, avg_time 1.088, loss:569.7618
g_step 4900, step 302, avg_time 1.069, loss:580.2249
g_step 5000, step 402, avg_time 1.093, loss:582.0193
learning rate was adjusted to 0.0008
>> valid entity prec:0.5886, rec:0.5158, f1:0.5498
>> valid relation prec:0.1149, rec:0.0273, f1:0.0441
>> valid relation with NER prec:0.1149, rec:0.0273, f1:0.0441
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 84, avg_time 2.472, loss:526.1179
g_step 5200, step 184, avg_time 1.074, loss:548.7839
g_step 5300, step 284, avg_time 1.068, loss:523.6980
g_step 5400, step 384, avg_time 1.094, loss:575.1265
g_step 5500, step 66, avg_time 1.072, loss:497.8353
>> valid entity prec:0.5387, rec:0.6058, f1:0.5703
>> valid relation prec:0.0660, rec:0.0221, f1:0.0331
>> valid relation with NER prec:0.0660, rec:0.0221, f1:0.0331
g_step 5600, step 166, avg_time 2.482, loss:523.5351
g_step 5700, step 266, avg_time 1.085, loss:522.8398
g_step 5800, step 366, avg_time 1.080, loss:537.7297
g_step 5900, step 48, avg_time 1.087, loss:509.2106
g_step 6000, step 148, avg_time 1.081, loss:501.4768
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5682, rec:0.5556, f1:0.5618
>> valid relation prec:0.0865, rec:0.0204, f1:0.0330
>> valid relation with NER prec:0.0865, rec:0.0204, f1:0.0330
g_step 6100, step 248, avg_time 2.470, loss:520.7520
g_step 6200, step 348, avg_time 1.076, loss:511.7136
g_step 6300, step 30, avg_time 1.076, loss:489.0134
g_step 6400, step 130, avg_time 1.078, loss:478.3540
g_step 6500, step 230, avg_time 1.084, loss:481.5861
>> valid entity prec:0.5705, rec:0.5699, f1:0.5702
>> valid relation prec:0.0664, rec:0.0175, f1:0.0277
>> valid relation with NER prec:0.0664, rec:0.0175, f1:0.0277
g_step 6600, step 330, avg_time 2.469, loss:476.9459
g_step 6700, step 12, avg_time 1.085, loss:502.7093
g_step 6800, step 112, avg_time 1.090, loss:452.9229
g_step 6900, step 212, avg_time 1.082, loss:469.0397
g_step 7000, step 312, avg_time 1.080, loss:471.5765
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5630, rec:0.5391, f1:0.5508
>> valid relation prec:0.0827, rec:0.0238, f1:0.0370
>> valid relation with NER prec:0.0827, rec:0.0238, f1:0.0370
g_step 7100, step 412, avg_time 2.466, loss:461.7180
g_step 7200, step 94, avg_time 1.080, loss:427.9053
g_step 7300, step 194, avg_time 1.084, loss:444.3358
g_step 7400, step 294, avg_time 1.096, loss:457.5878
g_step 7500, step 394, avg_time 1.073, loss:451.1756
>> valid entity prec:0.5562, rec:0.5227, f1:0.5390
>> valid relation prec:0.0780, rec:0.0210, f1:0.0330
>> valid relation with NER prec:0.0780, rec:0.0210, f1:0.0330
g_step 7600, step 76, avg_time 2.471, loss:433.0569
g_step 7700, step 176, avg_time 1.074, loss:414.9997
g_step 7800, step 276, avg_time 1.084, loss:447.0738
g_step 7900, step 376, avg_time 1.093, loss:437.9216
g_step 8000, step 58, avg_time 1.074, loss:435.8292
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5593, rec:0.5363, f1:0.5476
>> valid relation prec:0.0731, rec:0.0192, f1:0.0305
>> valid relation with NER prec:0.0731, rec:0.0192, f1:0.0305
g_step 8100, step 158, avg_time 2.477, loss:390.1713
g_step 8200, step 258, avg_time 1.076, loss:404.7793
g_step 8300, step 358, avg_time 1.094, loss:429.8633
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:16:49 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:16:49 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-16-49_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:16:50 - WARNING - datasets.builder -   Using custom data configuration default-545e329d14374a56
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-545e329d14374a56/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:16:50,750 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:16:50,751 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:16:50,752 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:16:50,753 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:16:50,761 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:50,764 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:50,764 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:50,764 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:50,764 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:50,764 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:16:50,764 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:16:50,887 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:16:54,003 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:16:54,006 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-545e329d14374a56/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 14:16:54 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1480f4612440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.93ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.76ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.10ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.27ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.38ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.47ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.47ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.50ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.54ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.54ba/s]100%|██████████| 11/11 [00:02<00:00,  4.74ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.89ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.15ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.30ba/s]100%|██████████| 4/4 [00:00<00:00,  4.37ba/s]100%|██████████| 4/4 [00:00<00:00,  4.08ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  7.80ba/s] 18%|█▊        | 2/11 [00:00<00:01,  8.88ba/s] 36%|███▋      | 4/11 [00:00<00:00,  9.67ba/s] 55%|█████▍    | 6/11 [00:00<00:00,  9.94ba/s] 73%|███████▎  | 8/11 [00:00<00:00, 10.02ba/s] 82%|████████▏ | 9/11 [00:00<00:00,  9.93ba/s]100%|██████████| 11/11 [00:01<00:00, 12.23ba/s]100%|██████████| 11/11 [00:01<00:00, 10.72ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.26ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.60ba/s]100%|██████████| 4/4 [00:00<00:00, 10.92ba/s]
[INFO|trainer.py:414] 2023-08-28 14:16:59,104 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:16:59,119 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:16:59,119 >>   Num examples = 10054
[INFO|trainer.py:1149] 2023-08-28 14:16:59,119 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:16:59,119 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:16:59,119 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:16:59,119 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:16:59,119 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:59,  3.27it/s]  0%|          | 2/785 [00:00<03:51,  3.38it/s]  0%|          | 3/785 [00:00<03:49,  3.41it/s]  1%|          | 4/785 [00:01<03:50,  3.39it/s]  1%|          | 5/785 [00:01<03:48,  3.41it/s]  1%|          | 6/785 [00:01<03:47,  3.43it/s]  1%|          | 7/785 [00:02<03:46,  3.44it/s]  1%|          | 8/785 [00:02<03:45,  3.44it/s]  1%|          | 9/785 [00:02<03:45,  3.44it/s]  1%|▏         | 10/785 [00:02<03:44,  3.45it/s]  1%|▏         | 11/785 [00:03<03:44,  3.45it/s]  2%|▏         | 12/785 [00:03<03:43,  3.45it/s]  2%|▏         | 13/785 [00:03<03:43,  3.45it/s]  2%|▏         | 14/785 [00:04<03:43,  3.45it/s]  2%|▏         | 15/785 [00:04<03:43,  3.45it/s]  2%|▏         | 16/785 [00:04<03:42,  3.45it/s]  2%|▏         | 17/785 [00:04<03:42,  3.45it/s]  2%|▏         | 18/785 [00:05<03:42,  3.45it/s]  2%|▏         | 19/785 [00:05<03:42,  3.45it/s]  3%|▎         | 20/785 [00:05<03:41,  3.45it/s]  3%|▎         | 21/785 [00:06<03:41,  3.45it/s]  3%|▎         | 22/785 [00:06<03:41,  3.45it/s]  3%|▎         | 23/785 [00:06<03:40,  3.45it/s]  3%|▎         | 24/785 [00:06<03:40,  3.45it/s]  3%|▎         | 25/785 [00:07<03:40,  3.45it/s]  3%|▎         | 26/785 [00:07<03:40,  3.44it/s]  3%|▎         | 27/785 [00:07<03:40,  3.44it/s]  4%|▎         | 28/785 [00:08<03:39,  3.45it/s]  4%|▎         | 29/785 [00:08<03:39,  3.45it/s]  4%|▍         | 30/785 [00:08<03:38,  3.45it/s]  4%|▍         | 31/785 [00:09<03:38,  3.45it/s]  4%|▍         | 32/785 [00:09<03:38,  3.45it/s]  4%|▍         | 33/785 [00:09<03:38,  3.45it/s]  4%|▍         | 34/785 [00:09<03:37,  3.45it/s]  4%|▍         | 35/785 [00:10<03:37,  3.45it/s]  5%|▍         | 36/785 [00:10<03:37,  3.45it/s]  5%|▍         | 37/785 [00:10<03:36,  3.45it/s]  5%|▍         | 38/785 [00:11<03:36,  3.45it/s]  5%|▍         | 39/785 [00:11<03:36,  3.45it/s]  5%|▌         | 40/785 [00:11<03:36,  3.45it/s]  5%|▌         | 41/785 [00:11<03:35,  3.45it/s]  5%|▌         | 42/785 [00:12<03:35,  3.44it/s]  5%|▌         | 43/785 [00:12<03:35,  3.44it/s]  6%|▌         | 44/785 [00:12<03:35,  3.45it/s]  6%|▌         | 45/785 [00:13<03:34,  3.45it/s]  6%|▌         | 46/785 [00:13<03:35,  3.43it/s]  6%|▌         | 47/785 [00:13<03:35,  3.43it/s]  6%|▌         | 48/785 [00:13<03:34,  3.44it/s]  6%|▌         | 49/785 [00:14<03:33,  3.44it/s]  6%|▋         | 50/785 [00:14<03:33,  3.44it/s]  6%|▋         | 51/785 [00:14<03:33,  3.44it/s]  7%|▋         | 52/785 [00:15<03:32,  3.44it/s]  7%|▋         | 53/785 [00:15<03:32,  3.44it/s]  7%|▋         | 54/785 [00:15<03:32,  3.44it/s]  7%|▋         | 55/785 [00:15<03:32,  3.44it/s]  7%|▋         | 56/785 [00:16<03:31,  3.44it/s]  7%|▋         | 57/785 [00:16<03:31,  3.44it/s]  7%|▋         | 58/785 [00:16<03:31,  3.44it/s]  8%|▊         | 59/785 [00:17<03:30,  3.44it/s]  8%|▊         | 60/785 [00:17<03:30,  3.44it/s]  8%|▊         | 61/785 [00:17<03:30,  3.45it/s]  8%|▊         | 62/785 [00:18<03:30,  3.44it/s]  8%|▊         | 63/785 [00:18<03:29,  3.44it/s]  8%|▊         | 64/785 [00:18<03:29,  3.44it/s]  8%|▊         | 65/785 [00:18<03:29,  3.44it/s]  8%|▊         | 66/785 [00:19<03:28,  3.44it/s]  9%|▊         | 67/785 [00:19<03:28,  3.44it/s]  9%|▊         | 68/785 [00:19<03:28,  3.44it/s]  9%|▉         | 69/785 [00:20<03:28,  3.44it/s]  9%|▉         | 70/785 [00:20<03:27,  3.44it/s]  9%|▉         | 71/785 [00:20<03:27,  3.44it/s]  9%|▉         | 72/785 [00:20<03:27,  3.44it/s]  9%|▉         | 73/785 [00:21<03:26,  3.44it/s]  9%|▉         | 74/785 [00:21<03:26,  3.44it/s] 10%|▉         | 75/785 [00:21<03:26,  3.44it/s] 10%|▉         | 76/785 [00:22<03:26,  3.44it/s] 10%|▉         | 77/785 [00:22<03:25,  3.44it/s] 10%|▉         | 78/785 [00:22<03:25,  3.44it/s] 10%|█         | 79/785 [00:22<03:25,  3.44it/s] 10%|█         | 80/785 [00:23<03:24,  3.44it/s] 10%|█         | 81/785 [00:23<03:24,  3.44it/s] 10%|█         | 82/785 [00:23<03:24,  3.44it/s] 11%|█         | 83/785 [00:24<03:24,  3.44it/s] 11%|█         | 84/785 [00:24<03:23,  3.44it/s] 11%|█         | 85/785 [00:24<03:23,  3.44it/s] 11%|█         | 86/785 [00:24<03:23,  3.44it/s] 11%|█         | 87/785 [00:25<03:23,  3.44it/s] 11%|█         | 88/785 [00:25<03:22,  3.44it/s] 11%|█▏        | 89/785 [00:25<03:22,  3.44it/s] 11%|█▏        | 90/785 [00:26<03:22,  3.44it/s] 12%|█▏        | 91/785 [00:26<03:21,  3.44it/s] 12%|█▏        | 92/785 [00:26<03:21,  3.44it/s] 12%|█▏        | 93/785 [00:27<03:21,  3.44it/s] 12%|█▏        | 94/785 [00:27<03:20,  3.44it/s] 12%|█▏        | 95/785 [00:27<03:20,  3.44it/s] 12%|█▏        | 96/785 [00:27<03:20,  3.44it/s] 12%|█▏        | 97/785 [00:28<03:20,  3.43it/s] 12%|█▏        | 98/785 [00:28<03:19,  3.44it/s] 13%|█▎        | 99/785 [00:28<03:19,  3.44it/s] 13%|█▎        | 100/785 [00:29<03:19,  3.44it/s] 13%|█▎        | 101/785 [00:29<03:19,  3.44it/s] 13%|█▎        | 102/785 [00:29<03:18,  3.44it/s] 13%|█▎        | 103/785 [00:29<03:18,  3.44it/s] 13%|█▎        | 104/785 [00:30<03:18,  3.44it/s] 13%|█▎        | 105/785 [00:30<03:17,  3.44it/s] 14%|█▎        | 106/785 [00:30<03:17,  3.44it/s] 14%|█▎        | 107/785 [00:31<03:17,  3.44it/s] 14%|█▍        | 108/785 [00:31<03:16,  3.44it/s] 14%|█▍        | 109/785 [00:31<03:16,  3.44it/s] 14%|█▍        | 110/785 [00:31<03:16,  3.44it/s] 14%|█▍        | 111/785 [00:32<03:15,  3.44it/s] 14%|█▍        | 112/785 [00:32<03:15,  3.44it/s] 14%|█▍        | 113/785 [00:32<03:15,  3.44it/s] 15%|█▍        | 114/785 [00:33<03:15,  3.44it/s] 15%|█▍        | 115/785 [00:33<03:14,  3.44it/s] 15%|█▍        | 116/785 [00:33<03:14,  3.44it/s] 15%|█▍        | 117/785 [00:34<03:14,  3.43it/s] 15%|█▌        | 118/785 [00:34<03:14,  3.44it/s] 15%|█▌        | 119/785 [00:34<03:13,  3.44it/s] 15%|█▌        | 120/785 [00:34<03:13,  3.44it/s] 15%|█▌        | 121/785 [00:35<03:13,  3.44it/s] 16%|█▌        | 122/785 [00:35<03:12,  3.44it/s] 16%|█▌        | 123/785 [00:35<03:12,  3.43it/s] 16%|█▌        | 124/785 [00:36<03:12,  3.43it/s] 16%|█▌        | 125/785 [00:36<03:12,  3.43it/s] 16%|█▌        | 126/785 [00:36<03:11,  3.43it/s] 16%|█▌        | 127/785 [00:36<03:11,  3.43it/s] 16%|█▋        | 128/785 [00:37<03:11,  3.43it/s] 16%|█▋        | 129/785 [00:37<03:11,  3.43it/s] 17%|█▋        | 130/785 [00:37<03:10,  3.43it/s] 17%|█▋        | 131/785 [00:38<03:10,  3.43it/s] 17%|█▋        | 132/785 [00:38<03:10,  3.43it/s] 17%|█▋        | 133/785 [00:38<03:09,  3.43it/s] 17%|█▋        | 134/785 [00:38<03:09,  3.43it/s] 17%|█▋        | 135/785 [00:39<03:09,  3.43it/s] 17%|█▋        | 136/785 [00:39<03:09,  3.43it/s] 17%|█▋        | 137/785 [00:39<03:08,  3.43it/s] 18%|█▊        | 138/785 [00:40<03:08,  3.43it/s] 18%|█▊        | 139/785 [00:40<03:08,  3.43it/s] 18%|█▊        | 140/785 [00:40<03:07,  3.43it/s] 18%|█▊        | 141/785 [00:40<03:07,  3.43it/s] 18%|█▊        | 142/785 [00:41<03:07,  3.43it/s] 18%|█▊        | 143/785 [00:41<03:07,  3.43it/s] 18%|█▊        | 144/785 [00:41<03:06,  3.43it/s] 18%|█▊        | 145/785 [00:42<03:06,  3.43it/s] 19%|█▊        | 146/785 [00:42<03:06,  3.44it/s] 19%|█▊        | 147/785 [00:42<03:06,  3.43it/s] 19%|█▉        | 148/785 [00:43<03:05,  3.43it/s] 19%|█▉        | 149/785 [00:43<03:05,  3.43it/s] 19%|█▉        | 150/785 [00:43<03:05,  3.43it/s] 19%|█▉        | 151/785 [00:43<03:04,  3.43it/s] 19%|█▉        | 152/785 [00:44<03:04,  3.43it/s] 19%|█▉        | 153/785 [00:44<03:04,  3.43it/s] 20%|█▉        | 154/785 [00:44<03:03,  3.43it/s] 20%|█▉        | 155/785 [00:45<03:03,  3.43it/s] 20%|█▉        | 156/785 [00:45<03:03,  3.43it/s] 20%|██        | 157/785 [00:45<03:03,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 14:17:44,838 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:17:44,838 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:17:44,838 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.16it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.90it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.07it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.44it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.05it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.78it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.56it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.03it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.92it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.97it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.99it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.04it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.03it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.07it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.13it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.07it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.13it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.99it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.98it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.03it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.06it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.13it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.13it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.04it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.11it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.13it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.08it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.04it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.99it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.94it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.05it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.09it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.14it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.11it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.16it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.01it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.06it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.07it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.07it/s][A
 48%|████▊     | 208/436 [00:04<00:05, 43.11it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 44.63it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.15it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.37it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.61it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.67it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.79it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.92it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.98it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.81it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.81it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.95it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.01it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.06it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.06it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.99it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.12it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.13it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.02it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.99it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.02it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.03it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.07it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 46.13it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.98it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.05it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.07it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.99it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.97it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.91it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.98it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.99it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.05it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.01it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.01it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.05it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.95it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.97it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.93it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.99it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.04it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.08it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.01it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.03it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.97it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.93it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:55<03:03,  3.43it/s]
100%|██████████| 436/436 [00:09<00:00, 45.93it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:17:54,349 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 14:17:54,377 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:17:57,313 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:17:57,352 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:17:57,364 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:06<1:06:31,  6.37s/it] 20%|██        | 159/785 [01:06<47:25,  4.54s/it]   20%|██        | 160/785 [01:06<34:03,  3.27s/it] 21%|██        | 161/785 [01:07<24:42,  2.38s/it] 21%|██        | 162/785 [01:07<18:10,  1.75s/it] 21%|██        | 163/785 [01:07<13:36,  1.31s/it] 21%|██        | 164/785 [01:07<10:24,  1.01s/it] 21%|██        | 165/785 [01:08<08:10,  1.26it/s] 21%|██        | 166/785 [01:08<06:36,  1.56it/s] 21%|██▏       | 167/785 [01:08<05:31,  1.87it/s] 21%|██▏       | 168/785 [01:09<04:45,  2.16it/s] 22%|██▏       | 169/785 [01:09<04:13,  2.43it/s] 22%|██▏       | 170/785 [01:09<04:02,  2.54it/s] 22%|██▏       | 171/785 [01:10<03:43,  2.75it/s] 22%|██▏       | 172/785 [01:10<03:29,  2.93it/s] 22%|██▏       | 173/785 [01:10<03:19,  3.07it/s] 22%|██▏       | 174/785 [01:10<03:12,  3.17it/s] 22%|██▏       | 175/785 [01:11<03:08,  3.24it/s] 22%|██▏       | 176/785 [01:11<03:04,  3.30it/s] 23%|██▎       | 177/785 [01:11<03:02,  3.34it/s] 23%|██▎       | 178/785 [01:12<03:00,  3.37it/s] 23%|██▎       | 179/785 [01:12<02:58,  3.39it/s] 23%|██▎       | 180/785 [01:12<02:57,  3.40it/s] 23%|██▎       | 181/785 [01:12<03:03,  3.30it/s] 23%|██▎       | 182/785 [01:13<03:00,  3.34it/s] 23%|██▎       | 183/785 [01:13<02:58,  3.37it/s] 23%|██▎       | 184/785 [01:13<02:57,  3.39it/s] 24%|██▎       | 185/785 [01:14<02:56,  3.40it/s] 24%|██▎       | 186/785 [01:14<02:55,  3.41it/s] 24%|██▍       | 187/785 [01:14<02:55,  3.42it/s] 24%|██▍       | 188/785 [01:15<02:54,  3.42it/s] 24%|██▍       | 189/785 [01:15<02:53,  3.43it/s] 24%|██▍       | 190/785 [01:15<02:53,  3.43it/s] 24%|██▍       | 191/785 [01:15<02:53,  3.43it/s] 24%|██▍       | 192/785 [01:16<02:53,  3.42it/s] 25%|██▍       | 193/785 [01:16<02:52,  3.43it/s] 25%|██▍       | 194/785 [01:16<02:52,  3.43it/s] 25%|██▍       | 195/785 [01:17<02:51,  3.43it/s] 25%|██▍       | 196/785 [01:17<02:51,  3.43it/s] 25%|██▌       | 197/785 [01:17<02:51,  3.44it/s] 25%|██▌       | 198/785 [01:17<02:50,  3.44it/s] 25%|██▌       | 199/785 [01:18<02:50,  3.44it/s] 25%|██▌       | 200/785 [01:18<02:50,  3.44it/s] 26%|██▌       | 201/785 [01:18<02:49,  3.44it/s] 26%|██▌       | 202/785 [01:19<02:49,  3.43it/s] 26%|██▌       | 203/785 [01:19<02:50,  3.42it/s] 26%|██▌       | 204/785 [01:19<02:49,  3.42it/s] 26%|██▌       | 205/785 [01:19<02:49,  3.43it/s] 26%|██▌       | 206/785 [01:20<02:48,  3.43it/s] 26%|██▋       | 207/785 [01:20<02:48,  3.43it/s] 26%|██▋       | 208/785 [01:20<02:47,  3.43it/s] 27%|██▋       | 209/785 [01:21<02:47,  3.44it/s] 27%|██▋       | 210/785 [01:21<02:47,  3.44it/s] 27%|██▋       | 211/785 [01:21<02:46,  3.44it/s] 27%|██▋       | 212/785 [01:22<02:46,  3.44it/s] 27%|██▋       | 213/785 [01:22<02:46,  3.43it/s] 27%|██▋       | 214/785 [01:22<02:47,  3.41it/s] 27%|██▋       | 215/785 [01:22<02:46,  3.42it/s] 28%|██▊       | 216/785 [01:23<02:46,  3.43it/s] 28%|██▊       | 217/785 [01:23<02:45,  3.43it/s] 28%|██▊       | 218/785 [01:23<02:45,  3.43it/s] 28%|██▊       | 219/785 [01:24<02:44,  3.44it/s] 28%|██▊       | 220/785 [01:24<02:44,  3.44it/s] 28%|██▊       | 221/785 [01:24<02:44,  3.44it/s] 28%|██▊       | 222/785 [01:24<02:43,  3.44it/s] 28%|██▊       | 223/785 [01:25<02:43,  3.44it/s] 29%|██▊       | 224/785 [01:25<02:43,  3.44it/s] 29%|██▊       | 225/785 [01:25<02:42,  3.44it/s] 29%|██▉       | 226/785 [01:26<02:42,  3.44it/s] 29%|██▉       | 227/785 [01:26<02:42,  3.43it/s] 29%|██▉       | 228/785 [01:26<02:43,  3.42it/s] 29%|██▉       | 229/785 [01:26<02:42,  3.42it/s] 29%|██▉       | 230/785 [01:27<02:41,  3.43it/s] 29%|██▉       | 231/785 [01:27<02:41,  3.43it/s] 30%|██▉       | 232/785 [01:27<02:41,  3.43it/s] 30%|██▉       | 233/785 [01:28<02:40,  3.43it/s] 30%|██▉       | 234/785 [01:28<02:40,  3.44it/s] 30%|██▉       | 235/785 [01:28<02:40,  3.43it/s] 30%|███       | 236/785 [01:29<02:39,  3.44it/s] 30%|███       | 237/785 [01:29<02:39,  3.43it/s] 30%|███       | 238/785 [01:29<02:40,  3.40it/s] 30%|███       | 239/785 [01:29<02:40,  3.41it/s] 31%|███       | 240/785 [01:30<02:39,  3.42it/s] 31%|███       | 241/785 [01:30<02:38,  3.43it/s] 31%|███       | 242/785 [01:30<02:38,  3.43it/s] 31%|███       | 243/785 [01:31<02:38,  3.43it/s] 31%|███       | 244/785 [01:31<02:37,  3.43it/s] 31%|███       | 245/785 [01:31<02:37,  3.43it/s] 31%|███▏      | 246/785 [01:31<02:36,  3.44it/s] 31%|███▏      | 247/785 [01:32<02:36,  3.43it/s] 32%|███▏      | 248/785 [01:32<02:36,  3.44it/s] 32%|███▏      | 249/785 [01:32<02:36,  3.42it/s] 32%|███▏      | 250/785 [01:33<02:36,  3.43it/s] 32%|███▏      | 251/785 [01:33<02:35,  3.43it/s] 32%|███▏      | 252/785 [01:33<02:35,  3.43it/s] 32%|███▏      | 253/785 [01:33<02:35,  3.43it/s] 32%|███▏      | 254/785 [01:34<02:34,  3.43it/s] 32%|███▏      | 255/785 [01:34<02:34,  3.43it/s] 33%|███▎      | 256/785 [01:34<02:34,  3.43it/s] 33%|███▎      | 257/785 [01:35<02:33,  3.43it/s] 33%|███▎      | 258/785 [01:35<02:33,  3.43it/s] 33%|███▎      | 259/785 [01:35<02:33,  3.43it/s] 33%|███▎      | 260/785 [01:36<02:34,  3.41it/s] 33%|███▎      | 261/785 [01:36<02:33,  3.42it/s] 33%|███▎      | 262/785 [01:36<02:33,  3.42it/s] 34%|███▎      | 263/785 [01:36<02:32,  3.42it/s] 34%|███▎      | 264/785 [01:37<02:32,  3.42it/s] 34%|███▍      | 265/785 [01:37<02:31,  3.42it/s] 34%|███▍      | 266/785 [01:37<02:31,  3.43it/s] 34%|███▍      | 267/785 [01:38<02:31,  3.42it/s] 34%|███▍      | 268/785 [01:38<02:30,  3.43it/s] 34%|███▍      | 269/785 [01:38<02:30,  3.43it/s] 34%|███▍      | 270/785 [01:38<02:30,  3.43it/s] 35%|███▍      | 271/785 [01:39<02:31,  3.39it/s] 35%|███▍      | 272/785 [01:39<02:30,  3.40it/s] 35%|███▍      | 273/785 [01:39<02:30,  3.41it/s] 35%|███▍      | 274/785 [01:40<02:29,  3.41it/s] 35%|███▌      | 275/785 [01:40<02:29,  3.42it/s] 35%|███▌      | 276/785 [01:40<02:28,  3.42it/s] 35%|███▌      | 277/785 [01:40<02:28,  3.42it/s] 35%|███▌      | 278/785 [01:41<02:28,  3.43it/s] 36%|███▌      | 279/785 [01:41<02:27,  3.43it/s] 36%|███▌      | 280/785 [01:41<02:27,  3.43it/s] 36%|███▌      | 281/785 [01:42<02:27,  3.43it/s] 36%|███▌      | 282/785 [01:42<02:27,  3.42it/s] 36%|███▌      | 283/785 [01:42<02:26,  3.42it/s] 36%|███▌      | 284/785 [01:43<02:26,  3.43it/s] 36%|███▋      | 285/785 [01:43<02:25,  3.43it/s] 36%|███▋      | 286/785 [01:43<02:25,  3.43it/s] 37%|███▋      | 287/785 [01:43<02:25,  3.43it/s] 37%|███▋      | 288/785 [01:44<02:24,  3.43it/s] 37%|███▋      | 289/785 [01:44<02:24,  3.43it/s] 37%|███▋      | 290/785 [01:44<02:24,  3.43it/s] 37%|███▋      | 291/785 [01:45<02:23,  3.43it/s] 37%|███▋      | 292/785 [01:45<02:23,  3.43it/s] 37%|███▋      | 293/785 [01:45<02:23,  3.42it/s] 37%|███▋      | 294/785 [01:45<02:23,  3.43it/s] 38%|███▊      | 295/785 [01:46<02:22,  3.43it/s] 38%|███▊      | 296/785 [01:46<02:22,  3.43it/s] 38%|███▊      | 297/785 [01:46<02:22,  3.43it/s] 38%|███▊      | 298/785 [01:47<02:22,  3.43it/s] 38%|███▊      | 299/785 [01:47<02:21,  3.43it/s] 38%|███▊      | 300/785 [01:47<02:21,  3.43it/s] 38%|███▊      | 301/785 [01:47<02:21,  3.43it/s] 38%|███▊      | 302/785 [01:48<02:20,  3.43it/s] 39%|███▊      | 303/785 [01:48<02:20,  3.43it/s] 39%|███▊      | 304/785 [01:48<02:20,  3.42it/s] 39%|███▉      | 305/785 [01:49<02:20,  3.42it/s] 39%|███▉      | 306/785 [01:49<02:19,  3.42it/s] 39%|███▉      | 307/785 [01:49<02:19,  3.43it/s] 39%|███▉      | 308/785 [01:50<02:19,  3.43it/s] 39%|███▉      | 309/785 [01:50<02:18,  3.43it/s] 39%|███▉      | 310/785 [01:50<02:18,  3.43it/s] 40%|███▉      | 311/785 [01:50<02:18,  3.43it/s] 40%|███▉      | 312/785 [01:51<02:17,  3.43it/s] 40%|███▉      | 313/785 [01:51<02:17,  3.43it/s] 40%|████      | 314/785 [01:51<02:17,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 14:18:50,953 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:18:50,953 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:18:50,954 >>   Batch size = 8
{'eval_loss': 1.0355836153030396, 'eval_runtime': 9.4832, 'eval_samples_per_second': 367.175, 'eval_steps_per_second': 45.976, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.54it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.81it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.01it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.37it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.97it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.75it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.53it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.16it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.08it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.11it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.20it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.12it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.18it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.13it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.13it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.15it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.94it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.96it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.99it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.04it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.10it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.04it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.07it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.11it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.17it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.05it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.03it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.01it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.02it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.09it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.99it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.04it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.10it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.06it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.03it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.95it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.00it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.08it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.02it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.92it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.02it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.06it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.02it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.89it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.94it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.03it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.06it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.99it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.96it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.02it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.06it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.05it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.95it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.82it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.82it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.78it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.76it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.75it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.91it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.91it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.96it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.88it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 45.99it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.92it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.01it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.97it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.98it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.99it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.94it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.05it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.03it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 42.68it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 43.69it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 44.40it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 44.92it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.23it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.43it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.59it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.70it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.45it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.44it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.50it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.54it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.61it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.63it/s][A                                                 
                                                 [A 40%|████      | 314/785 [02:01<02:17,  3.43it/s]
100%|██████████| 436/436 [00:09<00:00, 45.63it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:19:00,477 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 14:19:00,492 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:19:04,628 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:19:04,837 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:19:04,940 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:13<51:44,  6.61s/it] 40%|████      | 316/785 [02:13<36:57,  4.73s/it] 40%|████      | 317/785 [02:13<26:29,  3.40s/it] 41%|████      | 318/785 [02:14<19:10,  2.46s/it] 41%|████      | 319/785 [02:14<14:04,  1.81s/it] 41%|████      | 320/785 [02:14<10:30,  1.36s/it] 41%|████      | 321/785 [02:14<08:00,  1.04s/it] 41%|████      | 322/785 [02:15<06:16,  1.23it/s] 41%|████      | 323/785 [02:15<05:03,  1.52it/s] 41%|████▏     | 324/785 [02:15<04:12,  1.83it/s] 41%|████▏     | 325/785 [02:16<03:36,  2.13it/s] 42%|████▏     | 326/785 [02:16<03:11,  2.40it/s] 42%|████▏     | 327/785 [02:16<02:59,  2.56it/s] 42%|████▏     | 328/785 [02:17<02:45,  2.77it/s] 42%|████▏     | 329/785 [02:17<02:35,  2.94it/s] 42%|████▏     | 330/785 [02:17<02:27,  3.08it/s] 42%|████▏     | 331/785 [02:17<02:22,  3.18it/s] 42%|████▏     | 332/785 [02:18<02:19,  3.25it/s] 42%|████▏     | 333/785 [02:18<02:16,  3.31it/s] 43%|████▎     | 334/785 [02:18<02:14,  3.35it/s] 43%|████▎     | 335/785 [02:19<02:13,  3.38it/s] 43%|████▎     | 336/785 [02:19<02:12,  3.40it/s] 43%|████▎     | 337/785 [02:19<02:11,  3.41it/s] 43%|████▎     | 338/785 [02:19<02:11,  3.41it/s] 43%|████▎     | 339/785 [02:20<02:10,  3.42it/s] 43%|████▎     | 340/785 [02:20<02:09,  3.43it/s] 43%|████▎     | 341/785 [02:20<02:09,  3.43it/s] 44%|████▎     | 342/785 [02:21<02:08,  3.43it/s] 44%|████▎     | 343/785 [02:21<02:08,  3.44it/s] 44%|████▍     | 344/785 [02:21<02:08,  3.44it/s] 44%|████▍     | 345/785 [02:21<02:07,  3.44it/s] 44%|████▍     | 346/785 [02:22<02:07,  3.44it/s] 44%|████▍     | 347/785 [02:22<02:07,  3.44it/s] 44%|████▍     | 348/785 [02:22<02:06,  3.44it/s] 44%|████▍     | 349/785 [02:23<02:07,  3.43it/s] 45%|████▍     | 350/785 [02:23<02:06,  3.43it/s] 45%|████▍     | 351/785 [02:23<02:06,  3.43it/s] 45%|████▍     | 352/785 [02:23<02:05,  3.44it/s] 45%|████▍     | 353/785 [02:24<02:05,  3.44it/s] 45%|████▌     | 354/785 [02:24<02:05,  3.44it/s] 45%|████▌     | 355/785 [02:24<02:05,  3.44it/s] 45%|████▌     | 356/785 [02:25<02:04,  3.44it/s] 45%|████▌     | 357/785 [02:25<02:04,  3.44it/s] 46%|████▌     | 358/785 [02:25<02:04,  3.44it/s] 46%|████▌     | 359/785 [02:26<02:03,  3.44it/s] 46%|████▌     | 360/785 [02:26<02:03,  3.44it/s] 46%|████▌     | 361/785 [02:26<02:03,  3.44it/s] 46%|████▌     | 362/785 [02:26<02:02,  3.44it/s] 46%|████▌     | 363/785 [02:27<02:02,  3.44it/s] 46%|████▋     | 364/785 [02:27<02:02,  3.44it/s] 46%|████▋     | 365/785 [02:27<02:02,  3.44it/s] 47%|████▋     | 366/785 [02:28<02:01,  3.44it/s] 47%|████▋     | 367/785 [02:28<02:01,  3.44it/s] 47%|████▋     | 368/785 [02:28<02:01,  3.44it/s] 47%|████▋     | 369/785 [02:28<02:00,  3.44it/s] 47%|████▋     | 370/785 [02:29<02:00,  3.44it/s] 47%|████▋     | 371/785 [02:29<02:00,  3.44it/s] 47%|████▋     | 372/785 [02:29<02:00,  3.44it/s] 48%|████▊     | 373/785 [02:30<01:59,  3.44it/s] 48%|████▊     | 374/785 [02:30<01:59,  3.44it/s] 48%|████▊     | 375/785 [02:30<01:59,  3.44it/s] 48%|████▊     | 376/785 [02:30<01:59,  3.42it/s] 48%|████▊     | 377/785 [02:31<01:59,  3.43it/s] 48%|████▊     | 378/785 [02:31<01:58,  3.43it/s] 48%|████▊     | 379/785 [02:31<01:58,  3.43it/s] 48%|████▊     | 380/785 [02:32<01:57,  3.44it/s] 49%|████▊     | 381/785 [02:32<01:57,  3.44it/s] 49%|████▊     | 382/785 [02:32<01:57,  3.44it/s] 49%|████▉     | 383/785 [02:32<01:56,  3.44it/s] 49%|████▉     | 384/785 [02:33<01:56,  3.44it/s] 49%|████▉     | 385/785 [02:33<01:56,  3.44it/s] 49%|████▉     | 386/785 [02:33<01:55,  3.44it/s] 49%|████▉     | 387/785 [02:34<01:55,  3.43it/s] 49%|████▉     | 388/785 [02:34<01:55,  3.43it/s] 50%|████▉     | 389/785 [02:34<01:55,  3.43it/s] 50%|████▉     | 390/785 [02:35<01:55,  3.43it/s] 50%|████▉     | 391/785 [02:35<01:54,  3.43it/s] 50%|████▉     | 392/785 [02:35<01:54,  3.43it/s] 50%|█████     | 393/785 [02:35<01:54,  3.43it/s] 50%|█████     | 394/785 [02:36<01:53,  3.43it/s] 50%|█████     | 395/785 [02:36<01:53,  3.43it/s] 50%|█████     | 396/785 [02:36<01:53,  3.43it/s] 51%|█████     | 397/785 [02:37<01:52,  3.43it/s] 51%|█████     | 398/785 [02:37<01:53,  3.42it/s] 51%|█████     | 399/785 [02:37<01:52,  3.43it/s] 51%|█████     | 400/785 [02:37<01:52,  3.43it/s] 51%|█████     | 401/785 [02:38<01:52,  3.42it/s] 51%|█████     | 402/785 [02:38<01:51,  3.43it/s] 51%|█████▏    | 403/785 [02:38<01:51,  3.43it/s] 51%|█████▏    | 404/785 [02:39<01:51,  3.43it/s] 52%|█████▏    | 405/785 [02:39<01:50,  3.43it/s] 52%|█████▏    | 406/785 [02:39<01:50,  3.43it/s] 52%|█████▏    | 407/785 [02:39<01:50,  3.43it/s] 52%|█████▏    | 408/785 [02:40<01:49,  3.43it/s] 52%|█████▏    | 409/785 [02:40<01:50,  3.42it/s] 52%|█████▏    | 410/785 [02:40<01:49,  3.42it/s] 52%|█████▏    | 411/785 [02:41<01:49,  3.42it/s] 52%|█████▏    | 412/785 [02:41<01:48,  3.43it/s] 53%|█████▎    | 413/785 [02:41<01:48,  3.42it/s] 53%|█████▎    | 414/785 [02:42<01:48,  3.43it/s] 53%|█████▎    | 415/785 [02:42<01:47,  3.43it/s] 53%|█████▎    | 416/785 [02:42<01:47,  3.43it/s] 53%|█████▎    | 417/785 [02:42<01:47,  3.43it/s] 53%|█████▎    | 418/785 [02:43<01:46,  3.43it/s] 53%|█████▎    | 419/785 [02:43<01:46,  3.43it/s] 54%|█████▎    | 420/785 [02:43<01:46,  3.41it/s] 54%|█████▎    | 421/785 [02:44<01:46,  3.42it/s] 54%|█████▍    | 422/785 [02:44<01:46,  3.42it/s] 54%|█████▍    | 423/785 [02:44<01:45,  3.42it/s] 54%|█████▍    | 424/785 [02:44<01:45,  3.43it/s] 54%|█████▍    | 425/785 [02:45<01:45,  3.42it/s] 54%|█████▍    | 426/785 [02:45<01:44,  3.43it/s] 54%|█████▍    | 427/785 [02:45<01:44,  3.43it/s] 55%|█████▍    | 428/785 [02:46<01:44,  3.43it/s] 55%|█████▍    | 429/785 [02:46<01:43,  3.43it/s] 55%|█████▍    | 430/785 [02:46<01:43,  3.43it/s] 55%|█████▍    | 431/785 [02:46<01:43,  3.42it/s] 55%|█████▌    | 432/785 [02:47<01:43,  3.42it/s] 55%|█████▌    | 433/785 [02:47<01:42,  3.43it/s] 55%|█████▌    | 434/785 [02:47<01:42,  3.43it/s] 55%|█████▌    | 435/785 [02:48<01:42,  3.43it/s] 56%|█████▌    | 436/785 [02:48<01:41,  3.43it/s] 56%|█████▌    | 437/785 [02:48<01:41,  3.43it/s] 56%|█████▌    | 438/785 [02:49<01:41,  3.43it/s] 56%|█████▌    | 439/785 [02:49<01:40,  3.43it/s] 56%|█████▌    | 440/785 [02:49<01:40,  3.43it/s] 56%|█████▌    | 441/785 [02:49<01:40,  3.43it/s] 56%|█████▋    | 442/785 [02:50<01:40,  3.42it/s] 56%|█████▋    | 443/785 [02:50<01:39,  3.42it/s] 57%|█████▋    | 444/785 [02:50<01:39,  3.43it/s] 57%|█████▋    | 445/785 [02:51<01:39,  3.43it/s] 57%|█████▋    | 446/785 [02:51<01:38,  3.43it/s] 57%|█████▋    | 447/785 [02:51<01:38,  3.43it/s] 57%|█████▋    | 448/785 [02:51<01:38,  3.43it/s] 57%|█████▋    | 449/785 [02:52<01:37,  3.43it/s] 57%|█████▋    | 450/785 [02:52<01:37,  3.43it/s] 57%|█████▋    | 451/785 [02:52<01:37,  3.43it/s] 58%|█████▊    | 452/785 [02:53<01:37,  3.43it/s] 58%|█████▊    | 453/785 [02:53<01:36,  3.42it/s] 58%|█████▊    | 454/785 [02:53<01:36,  3.42it/s] 58%|█████▊    | 455/785 [02:53<01:36,  3.43it/s] 58%|█████▊    | 456/785 [02:54<01:35,  3.43it/s] 58%|█████▊    | 457/785 [02:54<01:35,  3.43it/s] 58%|█████▊    | 458/785 [02:54<01:35,  3.43it/s] 58%|█████▊    | 459/785 [02:55<01:35,  3.43it/s] 59%|█████▊    | 460/785 [02:55<01:34,  3.43it/s] 59%|█████▊    | 461/785 [02:55<01:34,  3.43it/s] 59%|█████▉    | 462/785 [02:56<01:34,  3.43it/s] 59%|█████▉    | 463/785 [02:56<01:33,  3.43it/s] 59%|█████▉    | 464/785 [02:56<01:33,  3.43it/s] 59%|█████▉    | 465/785 [02:56<01:33,  3.43it/s] 59%|█████▉    | 466/785 [02:57<01:32,  3.43it/s] 59%|█████▉    | 467/785 [02:57<01:32,  3.43it/s] 60%|█████▉    | 468/785 [02:57<01:32,  3.43it/s] 60%|█████▉    | 469/785 [02:58<01:32,  3.43it/s] 60%|█████▉    | 470/785 [02:58<01:32,  3.42it/s] 60%|██████    | 471/785 [02:58<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 14:19:57,830 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:19:57,830 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:19:57,831 >>   Batch size = 8
{'eval_loss': 1.0481760501861572, 'eval_runtime': 9.5043, 'eval_samples_per_second': 366.359, 'eval_steps_per_second': 45.874, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.56it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.94it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.21it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.38it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.84it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.58it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.30it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.84it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.88it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.90it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.95it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.05it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.01it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.09it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.99it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.95it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.73it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.81it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.78it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.94it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.01it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.00it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.00it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.09it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.00it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.90it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.91it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.77it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.80it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.73it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.77it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.76it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.82it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.87it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.86it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.85it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.87it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.94it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.86it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.99it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.07it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.04it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.97it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.89it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.94it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.91it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.98it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.92it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.97it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.05it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.98it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.96it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.82it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.74it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.73it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.67it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.65it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.67it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.66it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.66it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.63it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.66it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.72it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.85it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.80it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.86it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.95it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.88it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.97it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.93it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.97it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.81it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.95it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.93it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.93it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.99it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.85it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.94it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 41.68it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 42.91it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 43.85it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 44.55it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 44.99it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.29it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.53it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.73it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:08<01:31,  3.42it/s]
100%|██████████| 436/436 [00:09<00:00, 45.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:20:07,591 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 14:20:07,867 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:20:10,630 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:20:10,648 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:20:10,661 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:19<32:57,  6.32s/it] 60%|██████    | 473/785 [03:19<23:31,  4.53s/it] 60%|██████    | 474/785 [03:19<16:52,  3.25s/it] 61%|██████    | 475/785 [03:19<12:13,  2.37s/it] 61%|██████    | 476/785 [03:20<08:58,  1.74s/it] 61%|██████    | 477/785 [03:20<06:42,  1.31s/it] 61%|██████    | 478/785 [03:20<05:07,  1.00s/it] 61%|██████    | 479/785 [03:21<04:01,  1.27it/s] 61%|██████    | 480/785 [03:21<03:15,  1.56it/s] 61%|██████▏   | 481/785 [03:21<02:42,  1.87it/s] 61%|██████▏   | 482/785 [03:22<02:19,  2.17it/s] 62%|██████▏   | 483/785 [03:22<02:04,  2.44it/s] 62%|██████▏   | 484/785 [03:22<01:53,  2.66it/s] 62%|██████▏   | 485/785 [03:22<01:45,  2.86it/s] 62%|██████▏   | 486/785 [03:23<01:39,  3.01it/s] 62%|██████▏   | 487/785 [03:23<01:35,  3.12it/s] 62%|██████▏   | 488/785 [03:23<01:32,  3.21it/s] 62%|██████▏   | 489/785 [03:24<01:30,  3.27it/s] 62%|██████▏   | 490/785 [03:24<01:28,  3.32it/s] 63%|██████▎   | 491/785 [03:24<01:27,  3.35it/s] 63%|██████▎   | 492/785 [03:24<01:26,  3.38it/s] 63%|██████▎   | 493/785 [03:25<01:26,  3.39it/s] 63%|██████▎   | 494/785 [03:25<01:25,  3.41it/s] 63%|██████▎   | 495/785 [03:25<01:25,  3.40it/s] 63%|██████▎   | 496/785 [03:26<01:24,  3.41it/s] 63%|██████▎   | 497/785 [03:26<01:24,  3.42it/s] 63%|██████▎   | 498/785 [03:26<01:23,  3.42it/s] 64%|██████▎   | 499/785 [03:26<01:23,  3.43it/s] 64%|██████▎   | 500/785 [03:27<01:23,  3.43it/s]                                                  64%|██████▎   | 500/785 [03:27<01:23,  3.43it/s] 64%|██████▍   | 501/785 [03:27<01:22,  3.43it/s] 64%|██████▍   | 502/785 [03:27<01:22,  3.43it/s] 64%|██████▍   | 503/785 [03:28<01:22,  3.43it/s] 64%|██████▍   | 504/785 [03:28<01:21,  3.43it/s] 64%|██████▍   | 505/785 [03:28<01:21,  3.43it/s] 64%|██████▍   | 506/785 [03:29<01:21,  3.43it/s] 65%|██████▍   | 507/785 [03:29<01:21,  3.41it/s] 65%|██████▍   | 508/785 [03:29<01:21,  3.42it/s] 65%|██████▍   | 509/785 [03:29<01:20,  3.42it/s] 65%|██████▍   | 510/785 [03:30<01:20,  3.43it/s] 65%|██████▌   | 511/785 [03:30<01:19,  3.43it/s] 65%|██████▌   | 512/785 [03:30<01:19,  3.43it/s] 65%|██████▌   | 513/785 [03:31<01:19,  3.43it/s] 65%|██████▌   | 514/785 [03:31<01:18,  3.43it/s] 66%|██████▌   | 515/785 [03:31<01:18,  3.43it/s] 66%|██████▌   | 516/785 [03:31<01:18,  3.43it/s] 66%|██████▌   | 517/785 [03:32<01:18,  3.43it/s] 66%|██████▌   | 518/785 [03:32<01:18,  3.42it/s] 66%|██████▌   | 519/785 [03:32<01:17,  3.43it/s] 66%|██████▌   | 520/785 [03:33<01:17,  3.43it/s] 66%|██████▋   | 521/785 [03:33<01:16,  3.43it/s] 66%|██████▋   | 522/785 [03:33<01:16,  3.43it/s] 67%|██████▋   | 523/785 [03:33<01:16,  3.43it/s] 67%|██████▋   | 524/785 [03:34<01:16,  3.43it/s] 67%|██████▋   | 525/785 [03:34<01:15,  3.43it/s] 67%|██████▋   | 526/785 [03:34<01:15,  3.43it/s] 67%|██████▋   | 527/785 [03:35<01:15,  3.43it/s] 67%|██████▋   | 528/785 [03:35<01:14,  3.43it/s] 67%|██████▋   | 529/785 [03:35<01:14,  3.42it/s] 68%|██████▊   | 530/785 [03:36<01:14,  3.42it/s] 68%|██████▊   | 531/785 [03:36<01:14,  3.42it/s] 68%|██████▊   | 532/785 [03:36<01:13,  3.43it/s] 68%|██████▊   | 533/785 [03:36<01:13,  3.43it/s] 68%|██████▊   | 534/785 [03:37<01:13,  3.43it/s] 68%|██████▊   | 535/785 [03:37<01:12,  3.43it/s] 68%|██████▊   | 536/785 [03:37<01:12,  3.43it/s] 68%|██████▊   | 537/785 [03:38<01:12,  3.43it/s] 69%|██████▊   | 538/785 [03:38<01:11,  3.43it/s] 69%|██████▊   | 539/785 [03:38<01:11,  3.43it/s] 69%|██████▉   | 540/785 [03:38<01:11,  3.42it/s] 69%|██████▉   | 541/785 [03:39<01:11,  3.42it/s] 69%|██████▉   | 542/785 [03:39<01:10,  3.42it/s] 69%|██████▉   | 543/785 [03:39<01:10,  3.43it/s] 69%|██████▉   | 544/785 [03:40<01:10,  3.42it/s] 69%|██████▉   | 545/785 [03:40<01:10,  3.42it/s] 70%|██████▉   | 546/785 [03:40<01:09,  3.42it/s] 70%|██████▉   | 547/785 [03:40<01:09,  3.42it/s] 70%|██████▉   | 548/785 [03:41<01:09,  3.43it/s] 70%|██████▉   | 549/785 [03:41<01:08,  3.43it/s] 70%|███████   | 550/785 [03:41<01:08,  3.42it/s] 70%|███████   | 551/785 [03:42<01:09,  3.39it/s] 70%|███████   | 552/785 [03:42<01:08,  3.40it/s] 70%|███████   | 553/785 [03:42<01:08,  3.41it/s] 71%|███████   | 554/785 [03:43<01:07,  3.41it/s] 71%|███████   | 555/785 [03:43<01:07,  3.42it/s] 71%|███████   | 556/785 [03:43<01:06,  3.42it/s] 71%|███████   | 557/785 [03:43<01:06,  3.42it/s] 71%|███████   | 558/785 [03:44<01:06,  3.42it/s] 71%|███████   | 559/785 [03:44<01:05,  3.43it/s] 71%|███████▏  | 560/785 [03:44<01:05,  3.43it/s] 71%|███████▏  | 561/785 [03:45<01:05,  3.43it/s] 72%|███████▏  | 562/785 [03:45<01:05,  3.41it/s] 72%|███████▏  | 563/785 [03:45<01:04,  3.42it/s] 72%|███████▏  | 564/785 [03:45<01:04,  3.42it/s] 72%|███████▏  | 565/785 [03:46<01:04,  3.42it/s] 72%|███████▏  | 566/785 [03:46<01:04,  3.42it/s] 72%|███████▏  | 567/785 [03:46<01:03,  3.42it/s] 72%|███████▏  | 568/785 [03:47<01:03,  3.42it/s] 72%|███████▏  | 569/785 [03:47<01:03,  3.42it/s] 73%|███████▎  | 570/785 [03:47<01:02,  3.42it/s] 73%|███████▎  | 571/785 [03:47<01:02,  3.42it/s] 73%|███████▎  | 572/785 [03:48<01:02,  3.42it/s] 73%|███████▎  | 573/785 [03:48<01:02,  3.41it/s] 73%|███████▎  | 574/785 [03:48<01:01,  3.40it/s] 73%|███████▎  | 575/785 [03:49<01:01,  3.41it/s] 73%|███████▎  | 576/785 [03:49<01:01,  3.42it/s] 74%|███████▎  | 577/785 [03:49<01:00,  3.42it/s] 74%|███████▎  | 578/785 [03:50<01:00,  3.42it/s] 74%|███████▍  | 579/785 [03:50<01:02,  3.30it/s] 74%|███████▍  | 580/785 [03:50<01:01,  3.33it/s] 74%|███████▍  | 581/785 [03:50<01:00,  3.36it/s] 74%|███████▍  | 582/785 [03:51<01:00,  3.38it/s] 74%|███████▍  | 583/785 [03:51<00:59,  3.39it/s] 74%|███████▍  | 584/785 [03:51<00:59,  3.39it/s] 75%|███████▍  | 585/785 [03:52<00:58,  3.40it/s] 75%|███████▍  | 586/785 [03:52<00:58,  3.40it/s] 75%|███████▍  | 587/785 [03:52<00:58,  3.41it/s] 75%|███████▍  | 588/785 [03:53<00:57,  3.42it/s] 75%|███████▌  | 589/785 [03:53<00:57,  3.42it/s] 75%|███████▌  | 590/785 [03:53<00:57,  3.42it/s] 75%|███████▌  | 591/785 [03:53<00:56,  3.42it/s] 75%|███████▌  | 592/785 [03:54<00:56,  3.42it/s] 76%|███████▌  | 593/785 [03:54<00:56,  3.42it/s] 76%|███████▌  | 594/785 [03:54<00:55,  3.42it/s] 76%|███████▌  | 595/785 [03:55<00:55,  3.40it/s] 76%|███████▌  | 596/785 [03:55<00:55,  3.41it/s] 76%|███████▌  | 597/785 [03:55<00:55,  3.41it/s] 76%|███████▌  | 598/785 [03:55<00:54,  3.41it/s] 76%|███████▋  | 599/785 [03:56<00:54,  3.41it/s] 76%|███████▋  | 600/785 [03:56<00:54,  3.42it/s] 77%|███████▋  | 601/785 [03:56<00:53,  3.42it/s] 77%|███████▋  | 602/785 [03:57<00:53,  3.42it/s] 77%|███████▋  | 603/785 [03:57<00:53,  3.42it/s] 77%|███████▋  | 604/785 [03:57<00:52,  3.42it/s] 77%|███████▋  | 605/785 [03:57<00:52,  3.42it/s] 77%|███████▋  | 606/785 [03:58<00:52,  3.42it/s] 77%|███████▋  | 607/785 [03:58<00:52,  3.42it/s] 77%|███████▋  | 608/785 [03:58<00:51,  3.42it/s] 78%|███████▊  | 609/785 [03:59<00:51,  3.42it/s] 78%|███████▊  | 610/785 [03:59<00:51,  3.42it/s] 78%|███████▊  | 611/785 [03:59<00:50,  3.42it/s] 78%|███████▊  | 612/785 [04:00<00:50,  3.40it/s] 78%|███████▊  | 613/785 [04:00<00:50,  3.41it/s] 78%|███████▊  | 614/785 [04:00<00:50,  3.41it/s] 78%|███████▊  | 615/785 [04:00<00:49,  3.41it/s] 78%|███████▊  | 616/785 [04:01<00:49,  3.42it/s] 79%|███████▊  | 617/785 [04:01<00:49,  3.42it/s] 79%|███████▊  | 618/785 [04:01<00:48,  3.42it/s] 79%|███████▉  | 619/785 [04:02<00:48,  3.42it/s] 79%|███████▉  | 620/785 [04:02<00:48,  3.42it/s] 79%|███████▉  | 621/785 [04:02<00:47,  3.42it/s] 79%|███████▉  | 622/785 [04:02<00:47,  3.42it/s] 79%|███████▉  | 623/785 [04:03<00:47,  3.41it/s] 79%|███████▉  | 624/785 [04:03<00:47,  3.41it/s] 80%|███████▉  | 625/785 [04:03<00:46,  3.42it/s] 80%|███████▉  | 626/785 [04:04<00:46,  3.42it/s] 80%|███████▉  | 627/785 [04:04<00:46,  3.42it/s] 80%|████████  | 628/785 [04:04<00:45,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 14:21:03,872 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:21:03,872 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:21:03,872 >>   Batch size = 8
{'eval_loss': 1.0617702007293701, 'eval_runtime': 9.5286, 'eval_samples_per_second': 365.425, 'eval_steps_per_second': 45.757, 'epoch': 3.0}
{'loss': 0.7438, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.75it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.49it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.86it/s][A
  5%|▌         | 22/436 [00:00<00:08, 47.21it/s][A
  6%|▌         | 27/436 [00:00<00:08, 46.82it/s][A
  7%|▋         | 32/436 [00:00<00:08, 46.57it/s][A
  8%|▊         | 37/436 [00:00<00:08, 46.41it/s][A
 10%|▉         | 42/436 [00:00<00:08, 46.14it/s][A
 11%|█         | 47/436 [00:01<00:08, 46.05it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 45.93it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 45.94it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 42.38it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 43.42it/s][A
 17%|█▋        | 72/436 [00:01<00:08, 44.22it/s][A
 18%|█▊        | 77/436 [00:01<00:08, 44.75it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 45.21it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 45.35it/s][A
 21%|██        | 92/436 [00:02<00:07, 45.53it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 45.66it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 45.60it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 45.67it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 45.66it/s][A
 27%|██▋       | 117/436 [00:02<00:06, 45.74it/s][A
 28%|██▊       | 122/436 [00:02<00:06, 45.66it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 45.68it/s][A
 30%|███       | 132/436 [00:02<00:06, 45.63it/s][A
 31%|███▏      | 137/436 [00:02<00:06, 45.64it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 45.59it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 45.63it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 45.60it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 45.76it/s][A
 37%|███▋      | 162/436 [00:03<00:05, 45.78it/s][A
 38%|███▊      | 167/436 [00:03<00:05, 45.86it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 45.95it/s][A
 41%|████      | 177/436 [00:03<00:05, 45.94it/s][A
 42%|████▏     | 182/436 [00:03<00:05, 45.97it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 45.99it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 45.92it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 45.97it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 42.03it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.20it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 43.98it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.54it/s][A
 51%|█████     | 222/436 [00:04<00:04, 44.93it/s][A
 52%|█████▏    | 227/436 [00:04<00:04, 45.20it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 45.47it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 45.29it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 45.83it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 45.90it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 45.89it/s][A
 59%|█████▉    | 257/436 [00:05<00:03, 45.91it/s][A
 60%|██████    | 262/436 [00:05<00:03, 45.93it/s][A
 61%|██████    | 267/436 [00:05<00:03, 45.98it/s][A
 62%|██████▏   | 272/436 [00:05<00:03, 45.98it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 46.05it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 45.97it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 45.95it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 46.00it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 45.96it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 46.01it/s][A
 70%|███████   | 307/436 [00:06<00:02, 45.95it/s][A
 72%|███████▏  | 312/436 [00:06<00:02, 45.90it/s][A
 73%|███████▎  | 317/436 [00:06<00:02, 45.91it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 45.93it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 45.93it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 45.93it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 45.92it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 45.99it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 45.92it/s][A
 81%|████████  | 352/436 [00:07<00:01, 45.93it/s][A
 82%|████████▏ | 357/436 [00:07<00:01, 45.84it/s][A
 83%|████████▎ | 362/436 [00:07<00:01, 45.86it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 45.92it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 45.94it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 45.96it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 45.96it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 45.94it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 45.96it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 45.97it/s][A
 92%|█████████▏| 402/436 [00:08<00:00, 46.03it/s][A
 93%|█████████▎| 407/436 [00:08<00:00, 45.90it/s][A
 94%|█████████▍| 412/436 [00:09<00:00, 45.85it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 45.89it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 45.92it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 45.94it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 45.99it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:14<00:45,  3.42it/s]
100%|██████████| 436/436 [00:09<00:00, 45.99it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:21:13,441 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 14:21:13,463 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:21:15,954 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:21:15,981 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:21:15,991 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:23<15:11,  5.84s/it] 80%|████████  | 630/785 [04:23<10:50,  4.20s/it] 80%|████████  | 631/785 [04:24<07:45,  3.02s/it] 81%|████████  | 632/785 [04:24<05:37,  2.20s/it] 81%|████████  | 633/785 [04:24<04:07,  1.63s/it] 81%|████████  | 634/785 [04:25<03:05,  1.23s/it] 81%|████████  | 635/785 [04:25<02:22,  1.06it/s] 81%|████████  | 636/785 [04:25<01:51,  1.33it/s] 81%|████████  | 637/785 [04:25<01:30,  1.63it/s] 81%|████████▏ | 638/785 [04:26<01:15,  1.94it/s] 81%|████████▏ | 639/785 [04:26<01:05,  2.23it/s] 82%|████████▏ | 640/785 [04:26<00:58,  2.49it/s] 82%|████████▏ | 641/785 [04:27<00:56,  2.54it/s] 82%|████████▏ | 642/785 [04:27<00:51,  2.76it/s] 82%|████████▏ | 643/785 [04:27<00:48,  2.93it/s] 82%|████████▏ | 644/785 [04:28<00:45,  3.07it/s] 82%|████████▏ | 645/785 [04:28<00:44,  3.17it/s] 82%|████████▏ | 646/785 [04:28<00:42,  3.24it/s] 82%|████████▏ | 647/785 [04:28<00:41,  3.30it/s] 83%|████████▎ | 648/785 [04:29<00:41,  3.34it/s] 83%|████████▎ | 649/785 [04:29<00:40,  3.37it/s] 83%|████████▎ | 650/785 [04:29<00:39,  3.39it/s] 83%|████████▎ | 651/785 [04:30<00:39,  3.40it/s] 83%|████████▎ | 652/785 [04:30<00:39,  3.41it/s] 83%|████████▎ | 653/785 [04:30<00:38,  3.39it/s] 83%|████████▎ | 654/785 [04:30<00:38,  3.41it/s] 83%|████████▎ | 655/785 [04:31<00:38,  3.41it/s] 84%|████████▎ | 656/785 [04:31<00:37,  3.42it/s] 84%|████████▎ | 657/785 [04:31<00:37,  3.42it/s] 84%|████████▍ | 658/785 [04:32<00:37,  3.43it/s] 84%|████████▍ | 659/785 [04:32<00:36,  3.43it/s] 84%|████████▍ | 660/785 [04:32<00:36,  3.43it/s] 84%|████████▍ | 661/785 [04:32<00:36,  3.43it/s] 84%|████████▍ | 662/785 [04:33<00:35,  3.43it/s] 84%|████████▍ | 663/785 [04:33<00:35,  3.43it/s] 85%|████████▍ | 664/785 [04:33<00:35,  3.42it/s] 85%|████████▍ | 665/785 [04:34<00:35,  3.42it/s] 85%|████████▍ | 666/785 [04:34<00:34,  3.43it/s] 85%|████████▍ | 667/785 [04:34<00:34,  3.43it/s] 85%|████████▌ | 668/785 [04:35<00:34,  3.43it/s] 85%|████████▌ | 669/785 [04:35<00:33,  3.43it/s] 85%|████████▌ | 670/785 [04:35<00:33,  3.43it/s] 85%|████████▌ | 671/785 [04:35<00:33,  3.43it/s] 86%|████████▌ | 672/785 [04:36<00:32,  3.43it/s] 86%|████████▌ | 673/785 [04:36<00:32,  3.43it/s] 86%|████████▌ | 674/785 [04:36<00:32,  3.43it/s] 86%|████████▌ | 675/785 [04:37<00:32,  3.37it/s] 86%|████████▌ | 676/785 [04:37<00:32,  3.39it/s] 86%|████████▌ | 677/785 [04:37<00:31,  3.40it/s] 86%|████████▋ | 678/785 [04:37<00:31,  3.41it/s] 86%|████████▋ | 679/785 [04:38<00:30,  3.42it/s] 87%|████████▋ | 680/785 [04:38<00:30,  3.42it/s] 87%|████████▋ | 681/785 [04:38<00:30,  3.43it/s] 87%|████████▋ | 682/785 [04:39<00:30,  3.43it/s] 87%|████████▋ | 683/785 [04:39<00:29,  3.43it/s] 87%|████████▋ | 684/785 [04:39<00:29,  3.43it/s] 87%|████████▋ | 685/785 [04:39<00:29,  3.43it/s] 87%|████████▋ | 686/785 [04:40<00:29,  3.40it/s] 88%|████████▊ | 687/785 [04:40<00:28,  3.41it/s] 88%|████████▊ | 688/785 [04:40<00:28,  3.41it/s] 88%|████████▊ | 689/785 [04:41<00:28,  3.42it/s] 88%|████████▊ | 690/785 [04:41<00:27,  3.42it/s] 88%|████████▊ | 691/785 [04:41<00:27,  3.42it/s] 88%|████████▊ | 692/785 [04:42<00:27,  3.42it/s] 88%|████████▊ | 693/785 [04:42<00:26,  3.42it/s] 88%|████████▊ | 694/785 [04:42<00:26,  3.42it/s] 89%|████████▊ | 695/785 [04:42<00:26,  3.42it/s] 89%|████████▊ | 696/785 [04:43<00:25,  3.42it/s] 89%|████████▉ | 697/785 [04:43<00:25,  3.41it/s] 89%|████████▉ | 698/785 [04:43<00:25,  3.42it/s] 89%|████████▉ | 699/785 [04:44<00:25,  3.42it/s] 89%|████████▉ | 700/785 [04:44<00:24,  3.42it/s] 89%|████████▉ | 701/785 [04:44<00:24,  3.42it/s] 89%|████████▉ | 702/785 [04:44<00:24,  3.42it/s] 90%|████████▉ | 703/785 [04:45<00:23,  3.42it/s] 90%|████████▉ | 704/785 [04:45<00:23,  3.43it/s] 90%|████████▉ | 705/785 [04:45<00:23,  3.42it/s] 90%|████████▉ | 706/785 [04:46<00:23,  3.43it/s] 90%|█████████ | 707/785 [04:46<00:22,  3.43it/s] 90%|█████████ | 708/785 [04:46<00:22,  3.41it/s] 90%|█████████ | 709/785 [04:46<00:22,  3.42it/s] 90%|█████████ | 710/785 [04:47<00:21,  3.42it/s] 91%|█████████ | 711/785 [04:47<00:21,  3.42it/s] 91%|█████████ | 712/785 [04:47<00:21,  3.42it/s] 91%|█████████ | 713/785 [04:48<00:21,  3.42it/s] 91%|█████████ | 714/785 [04:48<00:20,  3.43it/s] 91%|█████████ | 715/785 [04:48<00:20,  3.42it/s] 91%|█████████ | 716/785 [04:49<00:20,  3.42it/s] 91%|█████████▏| 717/785 [04:49<00:19,  3.42it/s] 91%|█████████▏| 718/785 [04:49<00:19,  3.42it/s] 92%|█████████▏| 719/785 [04:49<00:19,  3.42it/s] 92%|█████████▏| 720/785 [04:50<00:19,  3.42it/s] 92%|█████████▏| 721/785 [04:50<00:18,  3.42it/s] 92%|█████████▏| 722/785 [04:50<00:18,  3.42it/s] 92%|█████████▏| 723/785 [04:51<00:18,  3.42it/s] 92%|█████████▏| 724/785 [04:51<00:17,  3.41it/s] 92%|█████████▏| 725/785 [04:51<00:17,  3.42it/s] 92%|█████████▏| 726/785 [04:51<00:17,  3.42it/s] 93%|█████████▎| 727/785 [04:52<00:16,  3.42it/s] 93%|█████████▎| 728/785 [04:52<00:16,  3.42it/s] 93%|█████████▎| 729/785 [04:52<00:16,  3.42it/s] 93%|█████████▎| 730/785 [04:53<00:16,  3.41it/s] 93%|█████████▎| 731/785 [04:53<00:15,  3.42it/s] 93%|█████████▎| 732/785 [04:53<00:15,  3.42it/s] 93%|█████████▎| 733/785 [04:54<00:15,  3.42it/s] 94%|█████████▎| 734/785 [04:54<00:14,  3.42it/s] 94%|█████████▎| 735/785 [04:54<00:14,  3.42it/s] 94%|█████████▍| 736/785 [04:54<00:14,  3.42it/s] 94%|█████████▍| 737/785 [04:55<00:14,  3.42it/s] 94%|█████████▍| 738/785 [04:55<00:13,  3.42it/s] 94%|█████████▍| 739/785 [04:55<00:13,  3.42it/s] 94%|█████████▍| 740/785 [04:56<00:13,  3.42it/s] 94%|█████████▍| 741/785 [04:56<00:12,  3.41it/s] 95%|█████████▍| 742/785 [04:56<00:12,  3.42it/s] 95%|█████████▍| 743/785 [04:56<00:12,  3.42it/s] 95%|█████████▍| 744/785 [04:57<00:11,  3.42it/s] 95%|█████████▍| 745/785 [04:57<00:11,  3.42it/s] 95%|█████████▌| 746/785 [04:57<00:11,  3.42it/s] 95%|█████████▌| 747/785 [04:58<00:11,  3.42it/s] 95%|█████████▌| 748/785 [04:58<00:10,  3.42it/s] 95%|█████████▌| 749/785 [04:58<00:10,  3.42it/s] 96%|█████████▌| 750/785 [04:58<00:10,  3.42it/s] 96%|█████████▌| 751/785 [04:59<00:09,  3.42it/s] 96%|█████████▌| 752/785 [04:59<00:09,  3.42it/s] 96%|█████████▌| 753/785 [04:59<00:09,  3.42it/s] 96%|█████████▌| 754/785 [05:00<00:09,  3.42it/s] 96%|█████████▌| 755/785 [05:00<00:08,  3.42it/s] 96%|█████████▋| 756/785 [05:00<00:08,  3.42it/s] 96%|█████████▋| 757/785 [05:01<00:08,  3.42it/s] 97%|█████████▋| 758/785 [05:01<00:07,  3.42it/s] 97%|█████████▋| 759/785 [05:01<00:07,  3.42it/s] 97%|█████████▋| 760/785 [05:01<00:07,  3.42it/s] 97%|█████████▋| 761/785 [05:02<00:07,  3.42it/s] 97%|█████████▋| 762/785 [05:02<00:06,  3.42it/s] 97%|█████████▋| 763/785 [05:02<00:06,  3.42it/s] 97%|█████████▋| 764/785 [05:03<00:06,  3.42it/s] 97%|█████████▋| 765/785 [05:03<00:05,  3.42it/s] 98%|█████████▊| 766/785 [05:03<00:05,  3.42it/s] 98%|█████████▊| 767/785 [05:03<00:05,  3.42it/s] 98%|█████████▊| 768/785 [05:04<00:04,  3.42it/s] 98%|█████████▊| 769/785 [05:04<00:04,  3.41it/s] 98%|█████████▊| 770/785 [05:04<00:04,  3.42it/s] 98%|█████████▊| 771/785 [05:05<00:04,  3.42it/s] 98%|█████████▊| 772/785 [05:05<00:03,  3.42it/s] 98%|█████████▊| 773/785 [05:05<00:03,  3.42it/s] 99%|█████████▊| 774/785 [05:06<00:03,  3.42it/s] 99%|█████████▊| 775/785 [05:06<00:02,  3.42it/s] 99%|█████████▉| 776/785 [05:06<00:02,  3.42it/s] 99%|█████████▉| 777/785 [05:06<00:02,  3.42it/s] 99%|█████████▉| 778/785 [05:07<00:02,  3.42it/s] 99%|█████████▉| 779/785 [05:07<00:01,  3.42it/s] 99%|█████████▉| 780/785 [05:07<00:01,  3.31it/s] 99%|█████████▉| 781/785 [05:08<00:01,  3.34it/s]100%|█████████▉| 782/785 [05:08<00:00,  3.36it/s]100%|█████████▉| 783/785 [05:08<00:00,  3.38it/s]100%|█████████▉| 784/785 [05:08<00:00,  3.40it/s]100%|██████████| 785/785 [05:09<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 14:22:08,373 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:22:08,373 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:22:08,373 >>   Batch size = 8
{'eval_loss': 1.0740032196044922, 'eval_runtime': 9.5533, 'eval_samples_per_second': 364.481, 'eval_steps_per_second': 45.639, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 55.97it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.43it/s][A
  4%|▍         | 17/436 [00:00<00:08, 47.84it/s][A
  5%|▌         | 22/436 [00:00<00:08, 47.09it/s][A
  6%|▌         | 27/436 [00:00<00:08, 46.60it/s][A
  7%|▋         | 32/436 [00:00<00:08, 46.24it/s][A
  8%|▊         | 37/436 [00:00<00:08, 46.01it/s][A
 10%|▉         | 42/436 [00:00<00:08, 45.93it/s][A
 11%|█         | 47/436 [00:01<00:08, 45.82it/s][A
 12%|█▏        | 52/436 [00:01<00:08, 45.81it/s][A
 13%|█▎        | 57/436 [00:01<00:08, 45.70it/s][A
 14%|█▍        | 62/436 [00:01<00:08, 45.69it/s][A
 15%|█▌        | 67/436 [00:01<00:08, 45.81it/s][A
 17%|█▋        | 72/436 [00:01<00:07, 45.91it/s][A
 18%|█▊        | 77/436 [00:01<00:07, 45.93it/s][A
 19%|█▉        | 82/436 [00:01<00:07, 45.93it/s][A
 20%|█▉        | 87/436 [00:01<00:07, 46.01it/s][A
 21%|██        | 92/436 [00:01<00:07, 46.03it/s][A
 22%|██▏       | 97/436 [00:02<00:07, 45.99it/s][A
 23%|██▎       | 102/436 [00:02<00:07, 46.01it/s][A
 25%|██▍       | 107/436 [00:02<00:07, 45.93it/s][A
 26%|██▌       | 112/436 [00:02<00:07, 45.94it/s][A
 27%|██▋       | 117/436 [00:02<00:06, 46.02it/s][A
 28%|██▊       | 122/436 [00:02<00:06, 46.05it/s][A
 29%|██▉       | 127/436 [00:02<00:06, 45.99it/s][A
 30%|███       | 132/436 [00:02<00:06, 45.95it/s][A
 31%|███▏      | 137/436 [00:02<00:06, 45.94it/s][A
 33%|███▎      | 142/436 [00:03<00:06, 45.95it/s][A
 34%|███▎      | 147/436 [00:03<00:06, 45.99it/s][A
 35%|███▍      | 152/436 [00:03<00:06, 45.90it/s][A
 36%|███▌      | 157/436 [00:03<00:06, 45.99it/s][A
 37%|███▋      | 162/436 [00:03<00:05, 45.95it/s][A
 38%|███▊      | 167/436 [00:03<00:05, 45.94it/s][A
 39%|███▉      | 172/436 [00:03<00:05, 45.92it/s][A
 41%|████      | 177/436 [00:03<00:05, 45.91it/s][A
 42%|████▏     | 182/436 [00:03<00:05, 45.97it/s][A
 43%|████▎     | 187/436 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 192/436 [00:04<00:05, 46.03it/s][A
 45%|████▌     | 197/436 [00:04<00:05, 45.90it/s][A
 46%|████▋     | 202/436 [00:04<00:05, 42.31it/s][A
 47%|████▋     | 207/436 [00:04<00:05, 43.38it/s][A
 49%|████▊     | 212/436 [00:04<00:05, 44.11it/s][A
 50%|████▉     | 217/436 [00:04<00:04, 44.64it/s][A
 51%|█████     | 222/436 [00:04<00:04, 45.06it/s][A
 52%|█████▏    | 227/436 [00:04<00:04, 45.27it/s][A
 53%|█████▎    | 232/436 [00:05<00:04, 45.50it/s][A
 54%|█████▍    | 237/436 [00:05<00:04, 45.58it/s][A
 56%|█████▌    | 242/436 [00:05<00:04, 45.71it/s][A
 57%|█████▋    | 247/436 [00:05<00:04, 45.82it/s][A
 58%|█████▊    | 252/436 [00:05<00:04, 45.91it/s][A
 59%|█████▉    | 257/436 [00:05<00:03, 45.91it/s][A
 60%|██████    | 262/436 [00:05<00:03, 45.89it/s][A
 61%|██████    | 267/436 [00:05<00:03, 45.92it/s][A
 62%|██████▏   | 272/436 [00:05<00:03, 45.97it/s][A
 64%|██████▎   | 277/436 [00:06<00:03, 45.95it/s][A
 65%|██████▍   | 282/436 [00:06<00:03, 45.94it/s][A
 66%|██████▌   | 287/436 [00:06<00:03, 45.90it/s][A
 67%|██████▋   | 292/436 [00:06<00:03, 45.96it/s][A
 68%|██████▊   | 297/436 [00:06<00:03, 46.02it/s][A
 69%|██████▉   | 302/436 [00:06<00:02, 45.99it/s][A
 70%|███████   | 307/436 [00:06<00:02, 45.91it/s][A
 72%|███████▏  | 312/436 [00:06<00:02, 45.79it/s][A
 73%|███████▎  | 317/436 [00:06<00:02, 45.91it/s][A
 74%|███████▍  | 322/436 [00:07<00:02, 45.94it/s][A
 75%|███████▌  | 327/436 [00:07<00:02, 45.93it/s][A
 76%|███████▌  | 332/436 [00:07<00:02, 45.93it/s][A
 77%|███████▋  | 337/436 [00:07<00:02, 45.88it/s][A
 78%|███████▊  | 342/436 [00:07<00:02, 45.88it/s][A
 80%|███████▉  | 347/436 [00:07<00:01, 45.94it/s][A
 81%|████████  | 352/436 [00:07<00:01, 46.00it/s][A
 82%|████████▏ | 357/436 [00:07<00:01, 45.94it/s][A
 83%|████████▎ | 362/436 [00:07<00:01, 45.94it/s][A
 84%|████████▍ | 367/436 [00:08<00:01, 45.95it/s][A
 85%|████████▌ | 372/436 [00:08<00:01, 45.90it/s][A
 86%|████████▋ | 377/436 [00:08<00:01, 45.95it/s][A
 88%|████████▊ | 382/436 [00:08<00:01, 45.89it/s][A
 89%|████████▉ | 387/436 [00:08<00:01, 45.92it/s][A
 90%|████████▉ | 392/436 [00:08<00:00, 45.94it/s][A
 91%|█████████ | 397/436 [00:08<00:00, 46.01it/s][A
 92%|█████████▏| 402/436 [00:08<00:00, 45.94it/s][A
 93%|█████████▎| 407/436 [00:08<00:00, 45.92it/s][A
 94%|█████████▍| 412/436 [00:08<00:00, 45.91it/s][A
 96%|█████████▌| 417/436 [00:09<00:00, 46.01it/s][A
 97%|█████████▋| 422/436 [00:09<00:00, 46.01it/s][A
 98%|█████████▊| 427/436 [00:09<00:00, 45.89it/s][A
 99%|█████████▉| 432/436 [00:09<00:00, 45.96it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:18<00:00,  3.41it/s]
100%|██████████| 436/436 [00:09<00:00, 45.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:22:17,907 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 14:22:17,927 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:22:20,899 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:22:20,933 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:22:20,948 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:22:28,141 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:22:28,141 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-157 (score: 1.0355836153030396).
                                                 100%|██████████| 785/785 [05:35<00:00,  3.41it/s]100%|██████████| 785/785 [05:35<00:00,  2.34it/s]
[INFO|trainer.py:1894] 2023-08-28 14:22:34,452 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 14:22:34,488 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:22:37,944 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:22:37,966 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:22:37,977 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:22:38,172 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:38,172 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:38,172 >>   train_loss               =     0.7316
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:38,172 >>   train_runtime            = 0:05:35.31
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:38,172 >>   train_samples            =      10054
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:38,172 >>   train_samples_per_second =    149.917
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:38,172 >>   train_steps_per_second   =      2.341
{'eval_loss': 1.0775076150894165, 'eval_runtime': 9.5187, 'eval_samples_per_second': 365.806, 'eval_steps_per_second': 45.805, 'epoch': 5.0}
{'train_runtime': 335.3195, 'train_samples_per_second': 149.917, 'train_steps_per_second': 2.341, 'train_loss': 0.7315669357396994, 'epoch': 5.0}
08/28/2023 14:22:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:22:38,215 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:22:38,215 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 14:22:38,215 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 56.94it/s]  3%|▎         | 12/436 [00:00<00:08, 50.22it/s]  4%|▍         | 18/436 [00:00<00:08, 48.36it/s]  5%|▌         | 23/436 [00:00<00:08, 47.81it/s]  6%|▋         | 28/436 [00:00<00:08, 47.46it/s]  8%|▊         | 33/436 [00:00<00:08, 47.10it/s]  9%|▊         | 38/436 [00:00<00:08, 47.01it/s] 10%|▉         | 43/436 [00:00<00:08, 46.83it/s] 11%|█         | 48/436 [00:01<00:08, 46.62it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.51it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.51it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.50it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.59it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.59it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.60it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.54it/s] 20%|██        | 88/436 [00:01<00:07, 46.55it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.48it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.39it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.38it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.42it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.40it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.43it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.52it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.52it/s] 31%|███       | 133/436 [00:02<00:06, 46.46it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.43it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.40it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.42it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.41it/s] 36%|███▌      | 158/436 [00:03<00:05, 46.44it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.47it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.45it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.44it/s] 41%|████      | 178/436 [00:03<00:05, 46.43it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.46it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.43it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.41it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.44it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.41it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.32it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.37it/s] 50%|█████     | 218/436 [00:04<00:04, 46.43it/s] 51%|█████     | 223/436 [00:04<00:04, 46.49it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.46it/s] 53%|█████▎    | 233/436 [00:04<00:04, 46.45it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.40it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.37it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.40it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.40it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.39it/s] 60%|██████    | 263/436 [00:05<00:03, 46.40it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.43it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.46it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.40it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.41it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.42it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.39it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.38it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.33it/s] 71%|███████   | 308/436 [00:06<00:02, 46.32it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.33it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.39it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.42it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.43it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.34it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.37it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.38it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.34it/s] 81%|████████  | 353/436 [00:07<00:01, 46.37it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.30it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.40it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.43it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.44it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.45it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.33it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.34it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.29it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.33it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.35it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.30it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.41it/s] 96%|█████████▌| 418/436 [00:08<00:00, 46.44it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.42it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.28it/s] 99%|█████████▉| 433/436 [00:09<00:00, 46.31it/s]100%|██████████| 436/436 [00:09<00:00, 46.53it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:22:47,608 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:47,608 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:47,608 >>   eval_loss               =     1.0356
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:47,608 >>   eval_runtime            = 0:00:09.39
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:47,608 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:47,608 >>   eval_samples_per_second =    370.734
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:47,608 >>   eval_steps_per_second   =     46.422
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:22:47,608 >>   perplexity              =     2.8167
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,289 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:54,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:22:54,921 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:22:54,922 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:22:55,473 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:22:56,508 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:22:56,508 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:59,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:59,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:59,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:59,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:22:59,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:23:00,036 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:23:00,037 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:23:00,626 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:23:00,789 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:23:00,789 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-157
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-471
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-314
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:07,  1.46it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:18,  1.43it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.46it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:22,  1.43it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:24,  1.48it/s]Extractor Predicting: 38it [00:25,  1.46it/s]Extractor Predicting: 39it [00:26,  1.46it/s]Extractor Predicting: 40it [00:26,  1.47it/s]Extractor Predicting: 41it [00:27,  1.47it/s]Extractor Predicting: 42it [00:28,  1.48it/s]Extractor Predicting: 43it [00:29,  1.47it/s]Extractor Predicting: 44it [00:29,  1.45it/s]Extractor Predicting: 45it [00:30,  1.48it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:31,  1.45it/s]Extractor Predicting: 48it [00:32,  1.44it/s]Extractor Predicting: 49it [00:33,  1.42it/s]Extractor Predicting: 50it [00:33,  1.42it/s]Extractor Predicting: 51it [00:34,  1.42it/s]Extractor Predicting: 52it [00:35,  1.44it/s]Extractor Predicting: 53it [00:36,  1.34it/s]Extractor Predicting: 54it [00:36,  1.35it/s]Extractor Predicting: 55it [00:37,  1.37it/s]Extractor Predicting: 56it [00:38,  1.38it/s]Extractor Predicting: 57it [00:38,  1.41it/s]Extractor Predicting: 58it [00:39,  1.41it/s]Extractor Predicting: 59it [00:40,  1.41it/s]Extractor Predicting: 60it [00:41,  1.42it/s]Extractor Predicting: 61it [00:41,  1.44it/s]Extractor Predicting: 62it [00:42,  1.45it/s]Extractor Predicting: 63it [00:43,  1.41it/s]Extractor Predicting: 64it [00:43,  1.41it/s]Extractor Predicting: 65it [00:44,  1.45it/s]Extractor Predicting: 66it [00:45,  1.43it/s]Extractor Predicting: 67it [00:45,  1.46it/s]Extractor Predicting: 68it [00:46,  1.47it/s]Extractor Predicting: 69it [00:47,  1.47it/s]Extractor Predicting: 70it [00:47,  1.50it/s]Extractor Predicting: 71it [00:48,  1.50it/s]Extractor Predicting: 72it [00:49,  1.50it/s]Extractor Predicting: 73it [00:49,  1.55it/s]Extractor Predicting: 74it [00:50,  1.55it/s]Extractor Predicting: 75it [00:51,  1.52it/s]Extractor Predicting: 76it [00:51,  1.47it/s]Extractor Predicting: 77it [00:52,  1.47it/s]Extractor Predicting: 78it [00:53,  1.47it/s]Extractor Predicting: 79it [00:53,  1.46it/s]Extractor Predicting: 80it [00:54,  1.46it/s]Extractor Predicting: 81it [00:55,  1.45it/s]Extractor Predicting: 82it [00:56,  1.44it/s]Extractor Predicting: 83it [00:56,  1.46it/s]Extractor Predicting: 84it [00:57,  1.43it/s]Extractor Predicting: 85it [00:58,  1.48it/s]Extractor Predicting: 86it [00:58,  1.44it/s]Extractor Predicting: 87it [00:59,  1.47it/s]Extractor Predicting: 88it [01:00,  1.45it/s]Extractor Predicting: 89it [01:00,  1.43it/s]Extractor Predicting: 90it [01:01,  1.41it/s]Extractor Predicting: 91it [01:02,  1.40it/s]Extractor Predicting: 92it [01:03,  1.41it/s]Extractor Predicting: 93it [01:03,  1.44it/s]Extractor Predicting: 94it [01:04,  1.42it/s]Extractor Predicting: 95it [01:05,  1.45it/s]Extractor Predicting: 96it [01:05,  1.46it/s]Extractor Predicting: 97it [01:06,  1.45it/s]Extractor Predicting: 98it [01:07,  1.43it/s]Extractor Predicting: 99it [01:07,  1.40it/s]Extractor Predicting: 100it [01:08,  1.38it/s]Extractor Predicting: 101it [01:09,  1.41it/s]Extractor Predicting: 102it [01:10,  1.40it/s]Extractor Predicting: 103it [01:10,  1.39it/s]Extractor Predicting: 104it [01:11,  1.43it/s]Extractor Predicting: 105it [01:12,  1.43it/s]Extractor Predicting: 106it [01:12,  1.40it/s]Extractor Predicting: 107it [01:13,  1.41it/s]Extractor Predicting: 108it [01:14,  1.43it/s]Extractor Predicting: 109it [01:14,  1.47it/s]Extractor Predicting: 110it [01:15,  1.48it/s]Extractor Predicting: 111it [01:16,  1.51it/s]Extractor Predicting: 112it [01:16,  1.50it/s]Extractor Predicting: 113it [01:17,  1.49it/s]Extractor Predicting: 114it [01:18,  1.46it/s]Extractor Predicting: 115it [01:18,  1.44it/s]Extractor Predicting: 116it [01:19,  1.44it/s]Extractor Predicting: 117it [01:20,  1.44it/s]Extractor Predicting: 118it [01:21,  1.43it/s]Extractor Predicting: 119it [01:21,  1.43it/s]Extractor Predicting: 120it [01:22,  1.47it/s]Extractor Predicting: 121it [01:23,  1.45it/s]Extractor Predicting: 122it [01:23,  1.45it/s]Extractor Predicting: 123it [01:24,  1.42it/s]Extractor Predicting: 124it [01:25,  1.44it/s]Extractor Predicting: 125it [01:25,  1.45it/s]Extractor Predicting: 126it [01:26,  1.45it/s]Extractor Predicting: 127it [01:27,  1.45it/s]Extractor Predicting: 128it [01:27,  1.43it/s]Extractor Predicting: 129it [01:28,  1.44it/s]Extractor Predicting: 130it [01:29,  1.42it/s]Extractor Predicting: 131it [01:30,  1.44it/s]Extractor Predicting: 132it [01:30,  1.34it/s]Extractor Predicting: 133it [01:31,  1.34it/s]Extractor Predicting: 134it [01:32,  1.33it/s]Extractor Predicting: 135it [01:33,  1.36it/s]Extractor Predicting: 136it [01:33,  1.39it/s]Extractor Predicting: 137it [01:34,  1.40it/s]Extractor Predicting: 138it [01:35,  1.42it/s]Extractor Predicting: 139it [01:35,  1.41it/s]Extractor Predicting: 140it [01:36,  1.39it/s]Extractor Predicting: 141it [01:37,  1.39it/s]Extractor Predicting: 142it [01:38,  1.43it/s]Extractor Predicting: 143it [01:38,  1.47it/s]Extractor Predicting: 143it [01:38,  1.45it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:48,572 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:48,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:48,579 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:48,579 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:48,579 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:24:49,179 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:24:49,180 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:24:49,745 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:24:50,764 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:24:50,764 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:53,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:53,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:53,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:53,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:53,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:24:54,248 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:24:54,250 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:24:54,832 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:24:54,982 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:24:54,982 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3932853717026379,
  "recall": 0.04709936817920735,
  "score": 0.08412413439343422,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.61it/s]Extractor Predicting: 30it [00:19,  1.43it/s]Extractor Predicting: 31it [00:20,  1.43it/s]Extractor Predicting: 32it [00:20,  1.42it/s]Extractor Predicting: 33it [00:21,  1.43it/s]Extractor Predicting: 34it [00:22,  1.43it/s]Extractor Predicting: 35it [00:22,  1.43it/s]Extractor Predicting: 36it [00:23,  1.44it/s]Extractor Predicting: 37it [00:24,  1.45it/s]Extractor Predicting: 38it [00:24,  1.44it/s]Extractor Predicting: 39it [00:25,  1.45it/s]Extractor Predicting: 40it [00:26,  1.44it/s]Extractor Predicting: 41it [00:26,  1.46it/s]Extractor Predicting: 42it [00:27,  1.48it/s]Extractor Predicting: 43it [00:28,  1.44it/s]Extractor Predicting: 44it [00:29,  1.46it/s]Extractor Predicting: 45it [00:29,  1.46it/s]Extractor Predicting: 46it [00:30,  1.46it/s]Extractor Predicting: 47it [00:31,  1.46it/s]Extractor Predicting: 48it [00:31,  1.46it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:33,  1.46it/s]Extractor Predicting: 51it [00:33,  1.46it/s]Extractor Predicting: 52it [00:34,  1.51it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:35,  1.47it/s]Extractor Predicting: 55it [00:36,  1.46it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:38,  1.49it/s]Extractor Predicting: 59it [00:39,  1.48it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.46it/s]Extractor Predicting: 62it [00:41,  1.48it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:43,  1.52it/s]Extractor Predicting: 66it [00:43,  1.47it/s]Extractor Predicting: 67it [00:44,  1.49it/s]Extractor Predicting: 68it [00:45,  1.48it/s]Extractor Predicting: 69it [00:45,  1.50it/s]Extractor Predicting: 70it [00:46,  1.50it/s]Extractor Predicting: 71it [00:47,  1.49it/s]Extractor Predicting: 72it [00:47,  1.48it/s]Extractor Predicting: 73it [00:48,  1.47it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.52it/s]Extractor Predicting: 78it [00:51,  1.55it/s]Extractor Predicting: 79it [00:52,  1.54it/s]Extractor Predicting: 80it [00:53,  1.50it/s]Extractor Predicting: 81it [00:53,  1.49it/s]Extractor Predicting: 82it [00:54,  1.51it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:55,  1.47it/s]Extractor Predicting: 85it [00:56,  1.47it/s]Extractor Predicting: 86it [00:57,  1.48it/s]Extractor Predicting: 87it [00:57,  1.49it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.53it/s]Extractor Predicting: 90it [00:59,  1.53it/s]Extractor Predicting: 91it [01:00,  1.52it/s]Extractor Predicting: 92it [01:01,  1.48it/s]Extractor Predicting: 93it [01:01,  1.48it/s]Extractor Predicting: 94it [01:02,  1.46it/s]Extractor Predicting: 95it [01:03,  1.45it/s]Extractor Predicting: 96it [01:04,  1.44it/s]Extractor Predicting: 97it [01:04,  1.46it/s]Extractor Predicting: 98it [01:05,  1.44it/s]Extractor Predicting: 99it [01:06,  1.46it/s]Extractor Predicting: 100it [01:06,  1.43it/s]Extractor Predicting: 101it [01:07,  1.43it/s]Extractor Predicting: 102it [01:08,  1.46it/s]Extractor Predicting: 103it [01:08,  1.47it/s]Extractor Predicting: 104it [01:09,  1.46it/s]Extractor Predicting: 105it [01:10,  1.46it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.45it/s]Extractor Predicting: 111it [01:14,  1.45it/s]Extractor Predicting: 112it [01:15,  1.43it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:16,  1.46it/s]Extractor Predicting: 115it [01:17,  1.44it/s]Extractor Predicting: 116it [01:17,  1.50it/s]Extractor Predicting: 117it [01:18,  1.58it/s]Extractor Predicting: 118it [01:18,  1.61it/s]Extractor Predicting: 119it [01:19,  1.60it/s]Extractor Predicting: 120it [01:20,  1.58it/s]Extractor Predicting: 121it [01:20,  1.57it/s]Extractor Predicting: 122it [01:21,  1.54it/s]Extractor Predicting: 123it [01:22,  1.53it/s]Extractor Predicting: 124it [01:22,  1.57it/s]Extractor Predicting: 125it [01:23,  1.62it/s]Extractor Predicting: 126it [01:23,  1.60it/s]Extractor Predicting: 127it [01:24,  1.61it/s]Extractor Predicting: 128it [01:25,  1.63it/s]Extractor Predicting: 129it [01:26,  1.42it/s]Extractor Predicting: 130it [01:26,  1.46it/s]Extractor Predicting: 131it [01:27,  1.51it/s]Extractor Predicting: 132it [01:27,  1.55it/s]Extractor Predicting: 133it [01:28,  1.56it/s]Extractor Predicting: 134it [01:29,  1.58it/s]Extractor Predicting: 135it [01:29,  1.61it/s]Extractor Predicting: 136it [01:30,  1.64it/s]Extractor Predicting: 137it [01:30,  1.65it/s]Extractor Predicting: 138it [01:31,  1.62it/s]Extractor Predicting: 139it [01:32,  1.59it/s]Extractor Predicting: 140it [01:32,  1.60it/s]Extractor Predicting: 141it [01:33,  1.57it/s]Extractor Predicting: 142it [01:34,  1.57it/s]Extractor Predicting: 143it [01:34,  1.59it/s]Extractor Predicting: 144it [01:35,  1.56it/s]Extractor Predicting: 145it [01:36,  1.56it/s]Extractor Predicting: 146it [01:36,  1.58it/s]Extractor Predicting: 147it [01:37,  1.58it/s]Extractor Predicting: 148it [01:37,  1.56it/s]Extractor Predicting: 149it [01:38,  1.61it/s]Extractor Predicting: 150it [01:39,  1.59it/s]Extractor Predicting: 151it [01:39,  1.57it/s]Extractor Predicting: 152it [01:40,  1.59it/s]Extractor Predicting: 153it [01:41,  1.64it/s]Extractor Predicting: 154it [01:41,  1.64it/s]Extractor Predicting: 155it [01:42,  1.64it/s]Extractor Predicting: 156it [01:42,  1.61it/s]Extractor Predicting: 157it [01:43,  1.60it/s]Extractor Predicting: 158it [01:44,  1.61it/s]Extractor Predicting: 159it [01:44,  1.56it/s]Extractor Predicting: 160it [01:45,  1.57it/s]Extractor Predicting: 161it [01:46,  1.58it/s]Extractor Predicting: 162it [01:46,  1.57it/s]Extractor Predicting: 163it [01:47,  1.57it/s]Extractor Predicting: 164it [01:47,  1.58it/s]Extractor Predicting: 165it [01:48,  1.59it/s]Extractor Predicting: 166it [01:49,  1.59it/s]Extractor Predicting: 167it [01:49,  1.59it/s]Extractor Predicting: 168it [01:50,  1.55it/s]Extractor Predicting: 169it [01:51,  1.56it/s]Extractor Predicting: 170it [01:51,  1.53it/s]Extractor Predicting: 171it [01:52,  1.51it/s]Extractor Predicting: 172it [01:53,  1.57it/s]Extractor Predicting: 173it [01:53,  1.52it/s]Extractor Predicting: 174it [01:54,  1.50it/s]Extractor Predicting: 175it [01:55,  1.47it/s]Extractor Predicting: 176it [01:55,  1.48it/s]Extractor Predicting: 177it [01:56,  1.47it/s]Extractor Predicting: 178it [01:57,  1.45it/s]Extractor Predicting: 179it [01:57,  1.46it/s]Extractor Predicting: 180it [01:58,  1.50it/s]Extractor Predicting: 181it [01:59,  1.49it/s]Extractor Predicting: 182it [01:59,  1.47it/s]Extractor Predicting: 183it [02:00,  1.47it/s]Extractor Predicting: 184it [02:01,  1.45it/s]Extractor Predicting: 185it [02:02,  1.44it/s]Extractor Predicting: 186it [02:02,  1.44it/s]Extractor Predicting: 187it [02:03,  1.43it/s]Extractor Predicting: 188it [02:04,  1.47it/s]Extractor Predicting: 189it [02:04,  1.47it/s]Extractor Predicting: 190it [02:05,  1.48it/s]Extractor Predicting: 191it [02:06,  1.44it/s]Extractor Predicting: 192it [02:06,  1.45it/s]Extractor Predicting: 193it [02:07,  1.43it/s]Extractor Predicting: 194it [02:08,  1.42it/s]Extractor Predicting: 195it [02:09,  1.42it/s]Extractor Predicting: 196it [02:09,  1.44it/s]Extractor Predicting: 197it [02:10,  1.43it/s]Extractor Predicting: 198it [02:11,  1.43it/s]Extractor Predicting: 199it [02:11,  1.43it/s]Extractor Predicting: 200it [02:12,  1.42it/s]Extractor Predicting: 201it [02:13,  1.43it/s]Extractor Predicting: 202it [02:13,  1.44it/s]Extractor Predicting: 203it [02:14,  1.45it/s]Extractor Predicting: 204it [02:15,  1.47it/s]Extractor Predicting: 205it [02:15,  1.51it/s]Extractor Predicting: 206it [02:16,  1.50it/s]Extractor Predicting: 207it [02:17,  1.49it/s]Extractor Predicting: 208it [02:17,  1.49it/s]Extractor Predicting: 209it [02:18,  1.48it/s]Extractor Predicting: 210it [02:19,  1.50it/s]Extractor Predicting: 211it [02:19,  1.53it/s]Extractor Predicting: 212it [02:20,  1.54it/s]Extractor Predicting: 213it [02:21,  1.56it/s]Extractor Predicting: 214it [02:21,  1.53it/s]Extractor Predicting: 215it [02:22,  1.49it/s]Extractor Predicting: 216it [02:23,  1.50it/s]Extractor Predicting: 217it [02:23,  1.51it/s]Extractor Predicting: 218it [02:24,  1.54it/s]Extractor Predicting: 219it [02:25,  1.56it/s]Extractor Predicting: 220it [02:25,  1.53it/s]Extractor Predicting: 221it [02:26,  1.56it/s]Extractor Predicting: 222it [02:26,  1.57it/s]Extractor Predicting: 223it [02:27,  1.55it/s]Extractor Predicting: 224it [02:28,  1.55it/s]Extractor Predicting: 225it [02:28,  1.55it/s]Extractor Predicting: 226it [02:29,  1.50it/s]Extractor Predicting: 227it [02:30,  1.49it/s]Extractor Predicting: 228it [02:30,  1.49it/s]Extractor Predicting: 229it [02:31,  1.49it/s]Extractor Predicting: 230it [02:32,  1.48it/s]Extractor Predicting: 231it [02:32,  1.49it/s]Extractor Predicting: 232it [02:33,  1.52it/s]Extractor Predicting: 233it [02:34,  1.58it/s]Extractor Predicting: 234it [02:34,  1.58it/s]Extractor Predicting: 235it [02:35,  1.58it/s]Extractor Predicting: 236it [02:36,  1.63it/s]Extractor Predicting: 237it [02:36,  1.63it/s]Extractor Predicting: 238it [02:37,  1.63it/s]Extractor Predicting: 239it [02:37,  1.62it/s]Extractor Predicting: 240it [02:38,  1.64it/s]Extractor Predicting: 241it [02:39,  1.63it/s]Extractor Predicting: 242it [02:39,  1.63it/s]Extractor Predicting: 243it [02:40,  1.68it/s]Extractor Predicting: 244it [02:40,  1.66it/s]Extractor Predicting: 245it [02:41,  1.72it/s]Extractor Predicting: 246it [02:41,  1.75it/s]Extractor Predicting: 247it [02:42,  1.49it/s]Extractor Predicting: 248it [02:43,  1.50it/s]Extractor Predicting: 249it [02:44,  1.54it/s]Extractor Predicting: 250it [02:44,  1.56it/s]Extractor Predicting: 251it [02:45,  1.61it/s]Extractor Predicting: 252it [02:45,  1.64it/s]Extractor Predicting: 253it [02:46,  1.64it/s]Extractor Predicting: 254it [02:47,  1.62it/s]Extractor Predicting: 255it [02:47,  1.65it/s]Extractor Predicting: 256it [02:48,  1.65it/s]Extractor Predicting: 257it [02:48,  1.68it/s]Extractor Predicting: 258it [02:49,  1.68it/s]Extractor Predicting: 259it [02:50,  1.70it/s]Extractor Predicting: 260it [02:50,  1.68it/s]Extractor Predicting: 261it [02:51,  1.58it/s]Extractor Predicting: 262it [02:52,  1.59it/s]Extractor Predicting: 263it [02:52,  1.54it/s]Extractor Predicting: 264it [02:53,  1.50it/s]Extractor Predicting: 265it [02:54,  1.48it/s]Extractor Predicting: 266it [02:54,  1.49it/s]Extractor Predicting: 267it [02:55,  1.51it/s]Extractor Predicting: 268it [02:56,  1.52it/s]Extractor Predicting: 269it [02:56,  1.48it/s]Extractor Predicting: 270it [02:57,  1.45it/s]Extractor Predicting: 271it [02:58,  1.45it/s]Extractor Predicting: 272it [02:58,  1.47it/s]Extractor Predicting: 273it [02:59,  1.47it/s]Extractor Predicting: 274it [03:00,  1.46it/s]Extractor Predicting: 275it [03:00,  1.46it/s]Extractor Predicting: 276it [03:01,  1.47it/s]Extractor Predicting: 277it [03:02,  1.45it/s]Extractor Predicting: 278it [03:02,  1.45it/s]Extractor Predicting: 279it [03:03,  1.48it/s]Extractor Predicting: 280it [03:04,  1.46it/s]Extractor Predicting: 281it [03:05,  1.45it/s]Extractor Predicting: 282it [03:05,  1.45it/s]Extractor Predicting: 283it [03:06,  1.45it/s]Extractor Predicting: 284it [03:07,  1.44it/s]Extractor Predicting: 285it [03:07,  1.45it/s]Extractor Predicting: 286it [03:08,  1.45it/s]Extractor Predicting: 287it [03:09,  1.44it/s]Extractor Predicting: 288it [03:09,  1.48it/s]Extractor Predicting: 289it [03:10,  1.48it/s]Extractor Predicting: 290it [03:11,  1.50it/s]Extractor Predicting: 291it [03:11,  1.51it/s]Extractor Predicting: 292it [03:12,  1.51it/s]Extractor Predicting: 293it [03:13,  1.51it/s]Extractor Predicting: 294it [03:13,  1.52it/s]Extractor Predicting: 295it [03:14,  1.52it/s]Extractor Predicting: 296it [03:15,  1.51it/s]Extractor Predicting: 297it [03:15,  1.52it/s]Extractor Predicting: 298it [03:16,  1.56it/s]Extractor Predicting: 299it [03:17,  1.52it/s]Extractor Predicting: 300it [03:17,  1.52it/s]Extractor Predicting: 301it [03:18,  1.51it/s]Extractor Predicting: 302it [03:19,  1.49it/s]Extractor Predicting: 303it [03:19,  1.49it/s]Extractor Predicting: 304it [03:20,  1.49it/s]Extractor Predicting: 305it [03:21,  1.48it/s]Extractor Predicting: 306it [03:21,  1.50it/s]Extractor Predicting: 307it [03:22,  1.50it/s]Extractor Predicting: 308it [03:23,  1.49it/s]Extractor Predicting: 309it [03:23,  1.46it/s]Extractor Predicting: 310it [03:24,  1.48it/s]Extractor Predicting: 311it [03:25,  1.48it/s]Extractor Predicting: 312it [03:25,  1.48it/s]Extractor Predicting: 313it [03:26,  1.49it/s]Extractor Predicting: 314it [03:27,  1.48it/s]Extractor Predicting: 315it [03:27,  1.50it/s]Extractor Predicting: 316it [03:28,  1.49it/s]Extractor Predicting: 317it [03:29,  1.51it/s]Extractor Predicting: 318it [03:29,  1.54it/s]Extractor Predicting: 319it [03:30,  1.54it/s]Extractor Predicting: 320it [03:31,  1.52it/s]Extractor Predicting: 321it [03:31,  1.51it/s]Extractor Predicting: 322it [03:32,  1.50it/s]Extractor Predicting: 323it [03:33,  1.52it/s]Extractor Predicting: 324it [03:33,  1.52it/s]Extractor Predicting: 325it [03:34,  1.51it/s]Extractor Predicting: 326it [03:35,  1.52it/s]Extractor Predicting: 327it [03:35,  1.52it/s]Extractor Predicting: 328it [03:36,  1.50it/s]Extractor Predicting: 329it [03:37,  1.51it/s]Extractor Predicting: 330it [03:37,  1.50it/s]Extractor Predicting: 331it [03:38,  1.49it/s]Extractor Predicting: 332it [03:39,  1.52it/s]Extractor Predicting: 333it [03:39,  1.49it/s]Extractor Predicting: 334it [03:40,  1.52it/s]Extractor Predicting: 335it [03:41,  1.51it/s]Extractor Predicting: 336it [03:41,  1.52it/s]Extractor Predicting: 337it [03:42,  1.51it/s]Extractor Predicting: 338it [03:42,  1.52it/s]Extractor Predicting: 339it [03:43,  1.52it/s]Extractor Predicting: 340it [03:44,  1.50it/s]Extractor Predicting: 341it [03:44,  1.52it/s]Extractor Predicting: 342it [03:45,  1.53it/s]Extractor Predicting: 343it [03:46,  1.53it/s]Extractor Predicting: 344it [03:46,  1.56it/s]Extractor Predicting: 345it [03:47,  1.53it/s]Extractor Predicting: 346it [03:48,  1.52it/s]Extractor Predicting: 347it [03:48,  1.53it/s]Extractor Predicting: 348it [03:49,  1.49it/s]Extractor Predicting: 349it [03:50,  1.51it/s]Extractor Predicting: 350it [03:50,  1.49it/s]Extractor Predicting: 351it [03:51,  1.50it/s]Extractor Predicting: 352it [03:52,  1.49it/s]Extractor Predicting: 353it [03:52,  1.49it/s]Extractor Predicting: 354it [03:53,  1.50it/s]Extractor Predicting: 355it [03:54,  1.48it/s]Extractor Predicting: 356it [03:54,  1.50it/s]Extractor Predicting: 357it [03:55,  1.47it/s]Extractor Predicting: 358it [03:56,  1.49it/s]Extractor Predicting: 359it [03:56,  1.50it/s]Extractor Predicting: 360it [03:57,  1.50it/s]Extractor Predicting: 361it [03:58,  1.49it/s]Extractor Predicting: 362it [03:58,  1.52it/s]Extractor Predicting: 363it [03:59,  1.35it/s]Extractor Predicting: 364it [04:00,  1.41it/s]Extractor Predicting: 365it [04:01,  1.43it/s]Extractor Predicting: 366it [04:01,  1.46it/s]Extractor Predicting: 367it [04:02,  1.47it/s]Extractor Predicting: 368it [04:03,  1.49it/s]Extractor Predicting: 369it [04:03,  1.46it/s]Extractor Predicting: 370it [04:04,  1.47it/s]Extractor Predicting: 371it [04:05,  1.48it/s]Extractor Predicting: 372it [04:05,  1.49it/s]Extractor Predicting: 373it [04:06,  1.49it/s]Extractor Predicting: 374it [04:07,  1.52it/s]Extractor Predicting: 375it [04:07,  1.51it/s]Extractor Predicting: 376it [04:08,  1.49it/s]Extractor Predicting: 377it [04:09,  1.54it/s]Extractor Predicting: 378it [04:09,  1.55it/s]Extractor Predicting: 379it [04:10,  1.57it/s]Extractor Predicting: 380it [04:11,  1.53it/s]Extractor Predicting: 381it [04:11,  1.53it/s]Extractor Predicting: 382it [04:12,  1.52it/s]Extractor Predicting: 383it [04:13,  1.50it/s]Extractor Predicting: 384it [04:13,  1.49it/s]Extractor Predicting: 385it [04:14,  1.50it/s]Extractor Predicting: 386it [04:15,  1.47it/s]Extractor Predicting: 387it [04:15,  1.47it/s]Extractor Predicting: 388it [04:16,  1.42it/s]Extractor Predicting: 389it [04:17,  1.44it/s]Extractor Predicting: 390it [04:17,  1.45it/s]Extractor Predicting: 391it [04:18,  1.48it/s]Extractor Predicting: 392it [04:19,  1.49it/s]Extractor Predicting: 393it [04:19,  1.48it/s]Extractor Predicting: 394it [04:20,  1.49it/s]Extractor Predicting: 395it [04:21,  1.50it/s]Extractor Predicting: 396it [04:21,  1.48it/s]Extractor Predicting: 397it [04:22,  1.49it/s]Extractor Predicting: 398it [04:23,  1.47it/s]Extractor Predicting: 399it [04:23,  1.51it/s]Extractor Predicting: 400it [04:24,  1.48it/s]Extractor Predicting: 401it [04:25,  1.48it/s]Extractor Predicting: 402it [04:25,  1.48it/s]Extractor Predicting: 403it [04:26,  1.47it/s]Extractor Predicting: 404it [04:27,  1.49it/s]Extractor Predicting: 405it [04:27,  1.50it/s]Extractor Predicting: 406it [04:28,  1.52it/s]Extractor Predicting: 407it [04:29,  1.51it/s]Extractor Predicting: 408it [04:29,  1.50it/s]Extractor Predicting: 409it [04:30,  1.51it/s]Extractor Predicting: 410it [04:31,  1.51it/s]Extractor Predicting: 411it [04:31,  1.51it/s]Extractor Predicting: 412it [04:32,  1.53it/s]Extractor Predicting: 413it [04:33,  1.52it/s]Extractor Predicting: 414it [04:33,  1.51it/s]Extractor Predicting: 415it [04:34,  1.48it/s]Extractor Predicting: 416it [04:35,  1.51it/s]Extractor Predicting: 417it [04:35,  1.51it/s]Extractor Predicting: 418it [04:36,  1.54it/s]Extractor Predicting: 419it [04:37,  1.54it/s]Extractor Predicting: 420it [04:37,  1.55it/s]Extractor Predicting: 421it [04:38,  1.52it/s]Extractor Predicting: 422it [04:39,  1.52it/s]Extractor Predicting: 423it [04:39,  1.53it/s]Extractor Predicting: 424it [04:40,  1.53it/s]Extractor Predicting: 425it [04:41,  1.51it/s]Extractor Predicting: 426it [04:41,  1.54it/s]Extractor Predicting: 427it [04:42,  1.53it/s]Extractor Predicting: 428it [04:43,  1.49it/s]Extractor Predicting: 429it [04:43,  1.70it/s]Extractor Predicting: 429it [04:43,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:47,659 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:47,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:47,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:47,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:47,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:29:48,286 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:29:48,287 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:29:48,889 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:29:49,960 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:29:49,960 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:52,807 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:52,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:52,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:52,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:52,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:29:53,474 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:29:53,475 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:29:54,048 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:29:54,193 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:29:54,193 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.38196782641227084,
  "recall": 0.09929974713090839,
  "score": 0.15762253956001546,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 5it [00:03,  1.55it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:29:57,820 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:29:57,821 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:29:57,826 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:29:57,827 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:29:57,828 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:30:01,002 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:30:01,002 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:30:01,016 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:30:01,017 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:30:01,022 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:01,026 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:01,026 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:01,026 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:01,026 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:01,026 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:30:01,026 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4166666666666667,
  "recall": 0.022935779816513763,
  "score": 0.043478260869565216,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:30:01,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:02,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:02,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:03,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:04,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:04,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:05,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:06,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:07,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:07,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:08,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:09,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:09,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:10,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:11,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:11,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:12,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:13,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:14,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:14,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:16,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:16,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:16<05:10, 16.34s/it][WARNING|generation_utils.py:914] 2023-08-28 14:30:17,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:18,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:19,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:19,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:20,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:21,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:21,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:22,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:23,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:23,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:24,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:25,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:25,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:26,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:27,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:27,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:28,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:28,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:29,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:30,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:31,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:30<04:31, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-28 14:30:31,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:32,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:33,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:34,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:35,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:36,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:37,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:38,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:38,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:39,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:40,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:41,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:42,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:43,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:44,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:44,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:45,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:46,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:47,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:48,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:49,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:50,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:50,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:51,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:51<04:59, 17.64s/it][WARNING|generation_utils.py:914] 2023-08-28 14:30:52,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:53,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:53,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:54,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:55,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:56,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:56,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:57,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:58,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:30:59,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:00,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:01,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:02,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:03,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:03,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:04,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:05,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:06,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:07,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:07,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:07<04:33, 17.09s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:08,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:09,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:10,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:11,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:12,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:13,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:14,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:14,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:15,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:16,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:17,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:18,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:18,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:19,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:20,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:21,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:22,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:22,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:23,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:24,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:25,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:26,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:26<04:24, 17.62s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:27,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:27,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:28,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:29,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:30,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:31,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:32,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:32,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:33,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:34,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:35,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:35,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:36,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:37,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:38,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:39,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:39,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:40,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:41,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:42,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:41<03:58, 17.04s/it][WARNING|generation_utils.py:914] 2023-08-28 14:31:43,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:43,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:44,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:45,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:46,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:47,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:47,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:48,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:49,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:49,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:50,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:51,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:52,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:53,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:53,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:54,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:55,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:56,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:56,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:57,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:58,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:59,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:31:59,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:00,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:59<03:45, 17.38s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:01,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:01,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:02,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:03,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:04,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:04,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:05,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:06,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:06,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:07,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:08,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:08,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:09,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:10,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:10,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:11,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:12,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:13,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:13,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:14,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:15,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:15,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:16,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:17,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:16<03:26, 17.18s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:18,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:18,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:19,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:20,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:21,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:22,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:22,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:23,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:24,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:25,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:26,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:27,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:27,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:28,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:29,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:30,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:31,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:31,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:32,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:33,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:34,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:34,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:34<03:10, 17.33s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:35,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:36,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:37,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:38,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:38,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:39,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:40,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:41,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:42,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:43,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:43,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:44,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:45,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:46,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:47,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:48,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:48,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:49,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:50,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:51,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:52,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:53,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:52<02:55, 17.60s/it][WARNING|generation_utils.py:914] 2023-08-28 14:32:53,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:54,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:55,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:56,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:57,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:57,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:58,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:59,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:32:59,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:00,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:01,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:02,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:03,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:03,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:04,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:05,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:06,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:06,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:07,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:08,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:09,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:09,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:10,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:09<02:37, 17.51s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:11,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:12,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:12,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:13,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:14,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:15,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:15,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:16,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:17,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:18,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:19,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:20,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:20,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:21,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:22,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:23,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:23,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:24,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:25,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:26,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:27,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:28,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:28<02:21, 17.74s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:29,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:30,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:30,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:31,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:32,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:33,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:34,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:34,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:35,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:36,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:37,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:38,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:38,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:39,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:40,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:41,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:42,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:43,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:43,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:44,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:44<02:00, 17.19s/it][WARNING|generation_utils.py:914] 2023-08-28 14:33:45,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:46,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:46,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:47,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:48,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:49,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:50,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:50,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:51,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:52,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:53,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:54,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:54,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:55,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:56,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:57,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:58,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:59,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:33:59,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:00,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:01,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:01<01:42, 17.11s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:02,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:03,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:03,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:04,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:05,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:06,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:06,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:07,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:08,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:09,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:10,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:11,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:11,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:12,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:13,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:14,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:14,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:15,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:16,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:17,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:18,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:19,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:19,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:19<01:27, 17.50s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:20,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:21,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:22,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:22,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:23,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:24,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:25,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:25,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:26,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:27,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:28,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:28,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:29,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:30,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:31,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:32,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:32,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:33,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:34,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:35,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:36,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:36,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:37,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:37<01:10, 17.57s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:38,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:39,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:40,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:40,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:41,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:42,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:43,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:43,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:44,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:45,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:46,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:46,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:47,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:48,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:49,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:50,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:51,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:51,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:52,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:53,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:54,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:54,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:54<00:52, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 14:34:55,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:56,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:57,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:57,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:58,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:59,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:34:59,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:00,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:01,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:02,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:03,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:03,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:04,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:05,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:06,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:06,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:07,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:08,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:09,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:09,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:10,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:09<00:33, 16.90s/it][WARNING|generation_utils.py:914] 2023-08-28 14:35:11,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:11,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:12,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:13,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:14,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:14,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:15,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:16,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:17,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:17,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:18,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:18,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:19,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:20,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:21,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:22,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:22,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:23,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:23,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:24,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:25,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:26,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:25<00:16, 16.48s/it][WARNING|generation_utils.py:914] 2023-08-28 14:35:26,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:27,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:28,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:29,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:29,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:30,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:31,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:32,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:33,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:33,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:34,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:35,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:36,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:36,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:37,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:38,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:39,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:40,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:40,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:41,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:35:42,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:42<00:00, 16.55s/it]Generating: 100%|██████████| 20/20 [05:42<00:00, 17.11s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:50,727 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:50,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:50,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:50,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:50,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:35:51,371 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:35:51,372 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:35:52,131 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:35:53,207 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:35:53,207 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:56,296 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:56,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:56,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:56,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:56,302 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:35:56,954 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:35:56,955 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:35:57,527 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:35:57,697 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:35:57,697 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : head of government .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.7903645833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.78515625, 'errors': {''}}
['Relation : country of citizenship . Context : On 31 March 2014 , the court announced that he would be given an automatic leave of absence pending the outcome of the trial of a civil servant accused of a separate offence , for life . Head Entity : civil servant accused of a separate offence , Tail Entity : country of citizenship .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : has part .', 'success_rate': 0.8892045454545454, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8192934782608695, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9107142857142857, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8165760869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('Government of China', 'occupation', '', 'After several years of service , he was again appointed by the Government of China in December 1858 , at the age of eighty - one .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8607954545454546, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9181547619047619, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8565340909090909, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.9077380952380952, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 13147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.53it/s]Extractor Estimating: 2it [00:01,  1.32it/s]Extractor Estimating: 3it [00:02,  1.43it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.50it/s]Extractor Estimating: 6it [00:04,  1.54it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.61it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.66it/s]Extractor Estimating: 11it [00:07,  1.62it/s]Extractor Estimating: 12it [00:07,  1.66it/s]Extractor Estimating: 13it [00:08,  1.66it/s]Extractor Estimating: 14it [00:08,  1.67it/s]Extractor Estimating: 15it [00:09,  1.70it/s]Extractor Estimating: 16it [00:09,  1.66it/s]Extractor Estimating: 17it [00:10,  1.65it/s]Extractor Estimating: 18it [00:11,  1.67it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:12,  1.63it/s]Extractor Estimating: 21it [00:13,  1.65it/s]Extractor Estimating: 22it [00:13,  1.58it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:14,  1.61it/s]Extractor Estimating: 25it [00:15,  1.66it/s]Extractor Estimating: 26it [00:16,  1.65it/s]Extractor Estimating: 27it [00:16,  1.51it/s]Extractor Estimating: 28it [00:17,  1.58it/s]Extractor Estimating: 29it [00:18,  1.59it/s]Extractor Estimating: 30it [00:18,  1.62it/s]Extractor Estimating: 31it [00:19,  1.60it/s]Extractor Estimating: 32it [00:19,  1.64it/s]Extractor Estimating: 33it [00:20,  1.65it/s]Extractor Estimating: 34it [00:21,  1.67it/s]Extractor Estimating: 35it [00:21,  1.79it/s]Extractor Estimating: 36it [00:22,  1.71it/s]Extractor Estimating: 37it [00:22,  1.76it/s]Extractor Estimating: 38it [00:23,  1.79it/s]Extractor Estimating: 39it [00:23,  1.77it/s]Extractor Estimating: 40it [00:24,  1.71it/s]Extractor Estimating: 41it [00:25,  1.71it/s]Extractor Estimating: 42it [00:25,  1.70it/s]Extractor Estimating: 43it [00:26,  1.71it/s]Extractor Estimating: 44it [00:26,  1.70it/s]Extractor Estimating: 45it [00:27,  1.71it/s]Extractor Estimating: 46it [00:27,  1.70it/s]Extractor Estimating: 47it [00:28,  1.74it/s]Extractor Estimating: 48it [00:29,  1.72it/s]Extractor Estimating: 49it [00:29,  1.65it/s]Extractor Estimating: 50it [00:30,  1.62it/s]Extractor Estimating: 51it [00:31,  1.61it/s]Extractor Estimating: 52it [00:31,  1.62it/s]Extractor Estimating: 53it [00:32,  1.55it/s]Extractor Estimating: 54it [00:32,  1.57it/s]Extractor Estimating: 55it [00:33,  1.55it/s]Extractor Estimating: 56it [00:34,  1.62it/s]Extractor Estimating: 57it [00:34,  1.52it/s]Extractor Estimating: 58it [00:35,  1.55it/s]Extractor Estimating: 59it [00:36,  1.49it/s]Extractor Estimating: 60it [00:36,  1.49it/s]Extractor Estimating: 61it [00:37,  1.48it/s]Extractor Estimating: 62it [00:38,  1.48it/s]Extractor Estimating: 63it [00:38,  1.55it/s]Extractor Estimating: 64it [00:39,  1.56it/s]Extractor Estimating: 65it [00:40,  1.55it/s]Extractor Estimating: 66it [00:40,  1.51it/s]Extractor Estimating: 67it [00:41,  1.52it/s]Extractor Estimating: 68it [00:42,  1.48it/s]Extractor Estimating: 69it [00:42,  1.48it/s]Extractor Estimating: 70it [00:43,  1.51it/s]Extractor Estimating: 71it [00:44,  1.54it/s]Extractor Estimating: 72it [00:44,  1.54it/s]Extractor Estimating: 73it [00:45,  1.56it/s]Extractor Estimating: 74it [00:46,  1.57it/s]Extractor Estimating: 75it [00:46,  1.57it/s]Extractor Estimating: 76it [00:47,  1.51it/s]Extractor Estimating: 77it [00:48,  1.47it/s]Extractor Estimating: 78it [00:48,  1.50it/s]Extractor Estimating: 79it [00:49,  1.47it/s]Extractor Estimating: 80it [00:50,  1.47it/s]Extractor Estimating: 81it [00:50,  1.46it/s]Extractor Estimating: 82it [00:51,  1.44it/s]Extractor Estimating: 83it [00:52,  1.46it/s]Extractor Estimating: 84it [00:52,  1.49it/s]Extractor Estimating: 85it [00:53,  1.47it/s]Extractor Estimating: 86it [00:54,  1.48it/s]Extractor Estimating: 87it [00:54,  1.47it/s]Extractor Estimating: 88it [00:55,  1.51it/s]Extractor Estimating: 89it [00:56,  1.52it/s]Extractor Estimating: 90it [00:57,  1.38it/s]Extractor Estimating: 91it [00:57,  1.38it/s]Extractor Estimating: 92it [00:58,  1.43it/s]Extractor Estimating: 93it [00:59,  1.42it/s]Extractor Estimating: 94it [00:59,  1.45it/s]Extractor Estimating: 95it [01:00,  1.45it/s]Extractor Estimating: 96it [01:01,  1.42it/s]Extractor Estimating: 97it [01:01,  1.42it/s]Extractor Estimating: 98it [01:02,  1.44it/s]Extractor Estimating: 99it [01:03,  1.42it/s]Extractor Estimating: 100it [01:04,  1.39it/s]Extractor Estimating: 101it [01:04,  1.46it/s]Extractor Estimating: 102it [01:05,  1.51it/s]Extractor Estimating: 103it [01:05,  1.55it/s]Extractor Estimating: 104it [01:06,  1.57it/s]Extractor Estimating: 105it [01:07,  1.59it/s]Extractor Estimating: 106it [01:07,  1.65it/s]Extractor Estimating: 107it [01:08,  1.64it/s]Extractor Estimating: 108it [01:09,  1.58it/s]Extractor Estimating: 109it [01:09,  1.59it/s]Extractor Estimating: 110it [01:10,  1.69it/s]Extractor Estimating: 111it [01:10,  1.69it/s]Extractor Estimating: 112it [01:11,  1.66it/s]Extractor Estimating: 113it [01:11,  1.65it/s]Extractor Estimating: 114it [01:12,  1.62it/s]Extractor Estimating: 115it [01:13,  1.64it/s]Extractor Estimating: 116it [01:13,  1.61it/s]Extractor Estimating: 117it [01:14,  1.57it/s]Extractor Estimating: 118it [01:15,  1.54it/s]Extractor Estimating: 119it [01:15,  1.55it/s]Extractor Estimating: 120it [01:16,  1.52it/s]Extractor Estimating: 121it [01:17,  1.50it/s]Extractor Estimating: 122it [01:17,  1.53it/s]Extractor Estimating: 123it [01:18,  1.52it/s]Extractor Estimating: 124it [01:19,  1.58it/s]Extractor Estimating: 125it [01:19,  1.59it/s]Extractor Estimating: 126it [01:20,  1.59it/s]Extractor Estimating: 127it [01:21,  1.53it/s]Extractor Estimating: 128it [01:21,  1.57it/s]Extractor Estimating: 129it [01:22,  1.53it/s]Extractor Estimating: 130it [01:23,  1.51it/s]Extractor Estimating: 131it [01:23,  1.48it/s]Extractor Estimating: 132it [01:24,  1.49it/s]Extractor Estimating: 133it [01:25,  1.50it/s]Extractor Estimating: 134it [01:25,  1.47it/s]Extractor Estimating: 135it [01:26,  1.46it/s]Extractor Estimating: 136it [01:27,  1.45it/s]Extractor Estimating: 137it [01:27,  1.44it/s]Extractor Estimating: 138it [01:28,  1.48it/s]Extractor Estimating: 139it [01:29,  1.49it/s]Extractor Estimating: 140it [01:29,  1.50it/s]Extractor Estimating: 141it [01:30,  1.46it/s]Extractor Estimating: 142it [01:31,  1.45it/s]Extractor Estimating: 143it [01:32,  1.40it/s]Extractor Estimating: 144it [01:32,  1.44it/s]Extractor Estimating: 145it [01:33,  1.40it/s]Extractor Estimating: 146it [01:34,  1.44it/s]Extractor Estimating: 147it [01:34,  1.39it/s]Extractor Estimating: 148it [01:35,  1.36it/s]Extractor Estimating: 149it [01:36,  1.36it/s]Extractor Estimating: 150it [01:37,  1.42it/s]Extractor Estimating: 151it [01:37,  1.48it/s]Extractor Estimating: 152it [01:38,  1.56it/s]Extractor Estimating: 153it [01:38,  1.57it/s]Extractor Estimating: 154it [01:39,  1.54it/s]Extractor Estimating: 155it [01:40,  1.57it/s]Extractor Estimating: 156it [01:40,  1.62it/s]Extractor Estimating: 157it [01:41,  1.63it/s]Extractor Estimating: 158it [01:41,  1.67it/s]Extractor Estimating: 159it [01:42,  1.68it/s]Extractor Estimating: 160it [01:42,  1.73it/s]Extractor Estimating: 161it [01:43,  1.73it/s]Extractor Estimating: 162it [01:44,  1.69it/s]Extractor Estimating: 163it [01:44,  1.66it/s]Extractor Estimating: 164it [01:45,  1.67it/s]Extractor Estimating: 165it [01:45,  1.73it/s]Extractor Estimating: 166it [01:46,  1.71it/s]Extractor Estimating: 167it [01:47,  1.69it/s]Extractor Estimating: 168it [01:47,  1.52it/s]Extractor Estimating: 169it [01:48,  1.56it/s]Extractor Estimating: 170it [01:49,  1.64it/s]Extractor Estimating: 171it [01:49,  1.66it/s]Extractor Estimating: 172it [01:50,  1.67it/s]Extractor Estimating: 173it [01:50,  1.65it/s]Extractor Estimating: 174it [01:51,  1.65it/s]Extractor Estimating: 175it [01:52,  1.65it/s]Extractor Estimating: 176it [01:52,  1.69it/s]Extractor Estimating: 177it [01:53,  1.69it/s]Extractor Estimating: 178it [01:53,  1.69it/s]Extractor Estimating: 179it [01:54,  1.68it/s]Extractor Estimating: 180it [01:55,  1.68it/s]Extractor Estimating: 181it [01:55,  1.62it/s]Extractor Estimating: 182it [01:56,  1.62it/s]Extractor Estimating: 183it [01:56,  1.66it/s]Extractor Estimating: 184it [01:57,  1.69it/s]Extractor Estimating: 185it [01:58,  1.66it/s]Extractor Estimating: 186it [01:58,  1.66it/s]Extractor Estimating: 187it [01:59,  1.62it/s]Extractor Estimating: 188it [01:59,  1.64it/s]Extractor Estimating: 189it [02:00,  1.68it/s]Extractor Estimating: 190it [02:01,  1.67it/s]Extractor Estimating: 191it [02:01,  1.72it/s]Extractor Estimating: 192it [02:02,  1.72it/s]Extractor Estimating: 193it [02:02,  1.73it/s]Extractor Estimating: 194it [02:03,  1.67it/s]Extractor Estimating: 195it [02:03,  1.69it/s]Extractor Estimating: 196it [02:04,  1.73it/s]Extractor Estimating: 197it [02:05,  1.72it/s]Extractor Estimating: 198it [02:05,  1.76it/s]Extractor Estimating: 199it [02:06,  1.76it/s]Extractor Estimating: 200it [02:06,  1.75it/s]Extractor Estimating: 201it [02:07,  1.62it/s]Extractor Estimating: 202it [02:08,  1.59it/s]Extractor Estimating: 203it [02:08,  1.55it/s]Extractor Estimating: 204it [02:09,  1.55it/s]Extractor Estimating: 205it [02:10,  1.55it/s]Extractor Estimating: 206it [02:10,  1.50it/s]Extractor Estimating: 207it [02:11,  1.52it/s]Extractor Estimating: 208it [02:12,  1.55it/s]Extractor Estimating: 209it [02:12,  1.49it/s]Extractor Estimating: 210it [02:13,  1.52it/s]Extractor Estimating: 211it [02:14,  1.51it/s]Extractor Estimating: 212it [02:14,  1.44it/s]Extractor Estimating: 213it [02:15,  1.45it/s]Extractor Estimating: 214it [02:16,  1.48it/s]Extractor Estimating: 215it [02:17,  1.42it/s]Extractor Estimating: 216it [02:17,  1.43it/s]Extractor Estimating: 217it [02:18,  1.49it/s]Extractor Estimating: 218it [02:18,  1.52it/s]Extractor Estimating: 219it [02:19,  1.46it/s]Extractor Estimating: 220it [02:20,  1.51it/s]Extractor Estimating: 221it [02:20,  1.52it/s]Extractor Estimating: 222it [02:21,  1.53it/s]Extractor Estimating: 223it [02:22,  1.50it/s]Extractor Estimating: 224it [02:23,  1.43it/s]Extractor Estimating: 225it [02:23,  1.42it/s]Extractor Estimating: 226it [02:24,  1.48it/s]Extractor Estimating: 227it [02:25,  1.47it/s]Extractor Estimating: 228it [02:25,  1.51it/s]Extractor Estimating: 229it [02:26,  1.51it/s]Extractor Estimating: 230it [02:27,  1.45it/s]Extractor Estimating: 231it [02:27,  1.43it/s]Extractor Estimating: 232it [02:28,  1.44it/s]Extractor Estimating: 233it [02:29,  1.45it/s]Extractor Estimating: 234it [02:29,  1.54it/s]Extractor Estimating: 235it [02:30,  1.55it/s]Extractor Estimating: 236it [02:31,  1.53it/s]Extractor Estimating: 237it [02:31,  1.53it/s]Extractor Estimating: 238it [02:32,  1.54it/s]Extractor Estimating: 239it [02:33,  1.50it/s]Extractor Estimating: 240it [02:33,  1.52it/s]Extractor Estimating: 241it [02:34,  1.51it/s]Extractor Estimating: 242it [02:35,  1.40it/s]Extractor Estimating: 243it [02:35,  1.41it/s]Extractor Estimating: 244it [02:36,  1.38it/s]Extractor Estimating: 245it [02:37,  1.42it/s]Extractor Estimating: 246it [02:38,  1.38it/s]Extractor Estimating: 247it [02:38,  1.41it/s]Extractor Estimating: 248it [02:39,  1.43it/s]Extractor Estimating: 249it [02:40,  1.42it/s]Extractor Estimating: 250it [02:40,  1.47it/s]Extractor Estimating: 251it [02:41,  1.48it/s]Extractor Estimating: 252it [02:42,  1.49it/s]Extractor Estimating: 253it [02:42,  1.46it/s]Extractor Estimating: 254it [02:43,  1.35it/s]Extractor Estimating: 255it [02:44,  1.38it/s]Extractor Estimating: 256it [02:45,  1.40it/s]Extractor Estimating: 257it [02:45,  1.47it/s]Extractor Estimating: 258it [02:46,  1.50it/s]Extractor Estimating: 259it [02:46,  1.51it/s]Extractor Estimating: 260it [02:47,  1.52it/s]Extractor Estimating: 261it [02:48,  1.48it/s]Extractor Estimating: 262it [02:48,  1.49it/s]Extractor Estimating: 263it [02:49,  1.48it/s]Extractor Estimating: 264it [02:50,  1.46it/s]Extractor Estimating: 265it [02:50,  1.51it/s]Extractor Estimating: 266it [02:51,  1.52it/s]Extractor Estimating: 267it [02:52,  1.56it/s]Extractor Estimating: 268it [02:52,  1.60it/s]Extractor Estimating: 269it [02:53,  1.54it/s]Extractor Estimating: 270it [02:54,  1.53it/s]Extractor Estimating: 271it [02:54,  1.52it/s]Extractor Estimating: 272it [02:55,  1.56it/s]Extractor Estimating: 273it [02:56,  1.50it/s]Extractor Estimating: 274it [02:56,  1.49it/s]Extractor Estimating: 275it [02:57,  1.48it/s]Extractor Estimating: 276it [02:58,  1.44it/s]Extractor Estimating: 277it [02:58,  1.48it/s]Extractor Estimating: 278it [02:59,  1.53it/s]Extractor Estimating: 279it [03:00,  1.56it/s]Extractor Estimating: 280it [03:00,  1.56it/s]Extractor Estimating: 281it [03:01,  1.57it/s]Extractor Estimating: 282it [03:02,  1.57it/s]Extractor Estimating: 283it [03:02,  1.55it/s]Extractor Estimating: 284it [03:03,  1.48it/s]Extractor Estimating: 285it [03:04,  1.53it/s]Extractor Estimating: 286it [03:04,  1.50it/s]Extractor Estimating: 287it [03:05,  1.52it/s]Extractor Estimating: 288it [03:05,  1.56it/s]Extractor Estimating: 289it [03:06,  1.52it/s]Extractor Estimating: 290it [03:07,  1.52it/s]Extractor Estimating: 291it [03:07,  1.57it/s]Extractor Estimating: 292it [03:08,  1.59it/s]Extractor Estimating: 293it [03:09,  1.58it/s]Extractor Estimating: 294it [03:09,  1.59it/s]Extractor Estimating: 295it [03:10,  1.50it/s]Extractor Estimating: 296it [03:11,  1.57it/s]Extractor Estimating: 297it [03:11,  1.53it/s]Extractor Estimating: 298it [03:12,  1.54it/s]Extractor Estimating: 299it [03:13,  1.48it/s]Extractor Estimating: 300it [03:13,  1.49it/s]Extractor Estimating: 301it [03:14,  1.55it/s]Extractor Estimating: 302it [03:15,  1.55it/s]Extractor Estimating: 303it [03:15,  1.55it/s]Extractor Estimating: 304it [03:16,  1.60it/s]Extractor Estimating: 305it [03:16,  1.60it/s]Extractor Estimating: 306it [03:17,  1.60it/s]Extractor Estimating: 307it [03:18,  1.62it/s]Extractor Estimating: 308it [03:18,  1.59it/s]Extractor Estimating: 309it [03:19,  1.57it/s]Extractor Estimating: 310it [03:20,  1.61it/s]Extractor Estimating: 311it [03:20,  1.57it/s]Extractor Estimating: 312it [03:21,  1.61it/s]Extractor Estimating: 313it [03:21,  1.62it/s]Extractor Estimating: 314it [03:22,  1.61it/s]Extractor Estimating: 315it [03:23,  1.64it/s]Extractor Estimating: 316it [03:23,  1.61it/s]Extractor Estimating: 317it [03:24,  1.65it/s]Extractor Estimating: 318it [03:25,  1.62it/s]Extractor Estimating: 319it [03:25,  1.61it/s]Extractor Estimating: 320it [03:26,  1.57it/s]Extractor Estimating: 321it [03:26,  1.61it/s]Extractor Estimating: 322it [03:27,  1.64it/s]Extractor Estimating: 323it [03:28,  1.64it/s]Extractor Estimating: 324it [03:28,  1.63it/s]Extractor Estimating: 325it [03:29,  1.65it/s]Extractor Estimating: 326it [03:29,  1.67it/s]Extractor Estimating: 327it [03:30,  1.67it/s]Extractor Estimating: 328it [03:31,  1.67it/s]Extractor Estimating: 329it [03:31,  1.61it/s]Extractor Estimating: 330it [03:32,  1.61it/s]Extractor Estimating: 331it [03:33,  1.53it/s]Extractor Estimating: 332it [03:33,  1.53it/s]Extractor Estimating: 333it [03:34,  1.56it/s]Extractor Estimating: 334it [03:35,  1.41it/s]Extractor Estimating: 335it [03:35,  1.47it/s]Extractor Estimating: 336it [03:36,  1.52it/s]Extractor Estimating: 337it [03:37,  1.57it/s]Extractor Estimating: 338it [03:37,  1.52it/s]Extractor Estimating: 339it [03:38,  1.55it/s]Extractor Estimating: 340it [03:38,  1.62it/s]Extractor Estimating: 341it [03:39,  1.61it/s]Extractor Estimating: 342it [03:40,  1.64it/s]Extractor Estimating: 343it [03:40,  1.58it/s]Extractor Estimating: 344it [03:41,  1.57it/s]Extractor Estimating: 345it [03:42,  1.55it/s]Extractor Estimating: 346it [03:42,  1.59it/s]Extractor Estimating: 347it [03:43,  1.63it/s]Extractor Estimating: 348it [03:43,  1.63it/s]Extractor Estimating: 349it [03:44,  1.63it/s]Extractor Estimating: 350it [03:45,  1.64it/s]Extractor Estimating: 351it [03:45,  1.62it/s]Extractor Estimating: 352it [03:46,  1.58it/s]Extractor Estimating: 353it [03:47,  1.56it/s]Extractor Estimating: 354it [03:47,  1.53it/s]Extractor Estimating: 355it [03:48,  1.50it/s]Extractor Estimating: 356it [03:49,  1.52it/s]Extractor Estimating: 357it [03:49,  1.54it/s]Extractor Estimating: 358it [03:50,  1.59it/s]Extractor Estimating: 359it [03:50,  1.56it/s]Extractor Estimating: 360it [03:51,  1.57it/s]Extractor Estimating: 361it [03:52,  1.58it/s]Extractor Estimating: 362it [03:52,  1.59it/s]Extractor Estimating: 363it [03:53,  1.59it/s]Extractor Estimating: 364it [03:54,  1.59it/s]Extractor Estimating: 365it [03:54,  1.58it/s]Extractor Estimating: 366it [03:55,  1.54it/s]Extractor Estimating: 367it [03:56,  1.54it/s]Extractor Estimating: 368it [03:56,  1.53it/s]Extractor Estimating: 369it [03:57,  1.50it/s]Extractor Estimating: 370it [03:58,  1.51it/s]Extractor Estimating: 371it [03:58,  1.46it/s]Extractor Estimating: 372it [03:59,  1.48it/s]Extractor Estimating: 373it [04:00,  1.52it/s]Extractor Estimating: 374it [04:00,  1.51it/s]Extractor Estimating: 375it [04:01,  1.51it/s]Extractor Estimating: 376it [04:02,  1.52it/s]Extractor Estimating: 377it [04:02,  1.50it/s]Extractor Estimating: 378it [04:03,  1.52it/s]Extractor Estimating: 379it [04:04,  1.48it/s]Extractor Estimating: 380it [04:04,  1.46it/s]Extractor Estimating: 381it [04:05,  1.48it/s]Extractor Estimating: 382it [04:06,  1.54it/s]Extractor Estimating: 383it [04:06,  1.50it/s]Extractor Estimating: 384it [04:07,  1.49it/s]Extractor Estimating: 385it [04:08,  1.51it/s]Extractor Estimating: 386it [04:08,  1.51it/s]Extractor Estimating: 387it [04:09,  1.55it/s]Extractor Estimating: 388it [04:10,  1.54it/s]Extractor Estimating: 389it [04:10,  1.48it/s]Extractor Estimating: 390it [04:11,  1.53it/s]Extractor Estimating: 391it [04:12,  1.53it/s]Extractor Estimating: 392it [04:12,  1.57it/s]Extractor Estimating: 393it [04:13,  1.53it/s]Extractor Estimating: 394it [04:14,  1.50it/s]Extractor Estimating: 395it [04:14,  1.45it/s]Extractor Estimating: 396it [04:15,  1.48it/s]Extractor Estimating: 397it [04:16,  1.53it/s]Extractor Estimating: 398it [04:16,  1.52it/s]Extractor Estimating: 399it [04:17,  1.54it/s]Extractor Estimating: 400it [04:17,  1.54it/s]Extractor Estimating: 401it [04:18,  1.58it/s]Extractor Estimating: 402it [04:19,  1.54it/s]Extractor Estimating: 403it [04:19,  1.53it/s]Extractor Estimating: 404it [04:20,  1.58it/s]Extractor Estimating: 405it [04:21,  1.59it/s]Extractor Estimating: 406it [04:21,  1.59it/s]Extractor Estimating: 407it [04:22,  1.58it/s]Extractor Estimating: 408it [04:22,  1.61it/s]Extractor Estimating: 409it [04:23,  1.61it/s]Extractor Estimating: 410it [04:24,  1.62it/s]Extractor Estimating: 411it [04:24,  1.65it/s]Extractor Estimating: 412it [04:25,  1.67it/s]Extractor Estimating: 413it [04:26,  1.59it/s]Extractor Estimating: 414it [04:26,  1.59it/s]Extractor Estimating: 415it [04:27,  1.56it/s]Extractor Estimating: 416it [04:28,  1.56it/s]Extractor Estimating: 417it [04:28,  1.60it/s]Extractor Estimating: 418it [04:29,  1.62it/s]Extractor Estimating: 419it [04:29,  1.64it/s]Extractor Estimating: 420it [04:30,  1.62it/s]Extractor Estimating: 421it [04:30,  1.66it/s]Extractor Estimating: 422it [04:31,  1.67it/s]Extractor Estimating: 423it [04:32,  1.69it/s]Extractor Estimating: 424it [04:32,  1.72it/s]Extractor Estimating: 425it [04:33,  1.66it/s]Extractor Estimating: 426it [04:34,  1.55it/s]Extractor Estimating: 427it [04:34,  1.54it/s]Extractor Estimating: 428it [04:35,  1.52it/s]Extractor Estimating: 429it [04:36,  1.50it/s]Extractor Estimating: 430it [04:36,  1.52it/s]Extractor Estimating: 431it [04:37,  1.50it/s]Extractor Estimating: 432it [04:38,  1.51it/s]Extractor Estimating: 433it [04:38,  1.37it/s]Extractor Estimating: 434it [04:39,  1.39it/s]Extractor Estimating: 435it [04:40,  1.37it/s]Extractor Estimating: 436it [04:41,  1.42it/s]Extractor Estimating: 437it [04:41,  1.42it/s]Extractor Estimating: 438it [04:42,  1.46it/s]Extractor Estimating: 439it [04:43,  1.50it/s]Extractor Estimating: 440it [04:43,  1.51it/s]Extractor Estimating: 441it [04:44,  1.52it/s]Extractor Estimating: 442it [04:45,  1.52it/s]Extractor Estimating: 443it [04:45,  1.49it/s]Extractor Estimating: 444it [04:46,  1.44it/s]Extractor Estimating: 445it [04:47,  1.46it/s]Extractor Estimating: 446it [04:47,  1.43it/s]Extractor Estimating: 447it [04:48,  1.47it/s]Extractor Estimating: 448it [04:49,  1.48it/s]Extractor Estimating: 449it [04:49,  1.51it/s]Extractor Estimating: 450it [04:50,  1.53it/s]Extractor Estimating: 451it [04:51,  1.57it/s]Extractor Estimating: 452it [04:51,  1.57it/s]Extractor Estimating: 453it [04:52,  1.62it/s]Extractor Estimating: 454it [04:52,  1.62it/s]Extractor Estimating: 455it [04:53,  1.63it/s]Extractor Estimating: 456it [04:54,  1.66it/s]Extractor Estimating: 457it [04:54,  1.70it/s]Extractor Estimating: 458it [04:55,  1.70it/s]Extractor Estimating: 459it [04:55,  1.72it/s]Extractor Estimating: 460it [04:56,  1.72it/s]Extractor Estimating: 461it [04:56,  1.76it/s]Extractor Estimating: 462it [04:57,  1.80it/s]Extractor Estimating: 463it [04:57,  1.77it/s]Extractor Estimating: 464it [04:58,  1.72it/s]Extractor Estimating: 465it [04:59,  1.69it/s]Extractor Estimating: 466it [04:59,  1.62it/s]Extractor Estimating: 467it [05:00,  1.63it/s]Extractor Estimating: 468it [05:01,  1.65it/s]Extractor Estimating: 469it [05:01,  1.69it/s]Extractor Estimating: 470it [05:02,  1.68it/s]Extractor Estimating: 471it [05:02,  1.72it/s]Extractor Estimating: 472it [05:03,  1.70it/s]Extractor Estimating: 473it [05:04,  1.64it/s]Extractor Estimating: 474it [05:04,  1.68it/s]Extractor Estimating: 475it [05:05,  1.63it/s]Extractor Estimating: 476it [05:05,  1.56it/s]Extractor Estimating: 477it [05:06,  1.53it/s]Extractor Estimating: 478it [05:07,  1.50it/s]Extractor Estimating: 479it [05:08,  1.50it/s]Extractor Estimating: 480it [05:08,  1.46it/s]Extractor Estimating: 481it [05:09,  1.48it/s]Extractor Estimating: 482it [05:10,  1.47it/s]Extractor Estimating: 483it [05:10,  1.50it/s]Extractor Estimating: 484it [05:11,  1.44it/s]Extractor Estimating: 485it [05:12,  1.40it/s]Extractor Estimating: 486it [05:12,  1.41it/s]Extractor Estimating: 487it [05:13,  1.37it/s]Extractor Estimating: 488it [05:14,  1.41it/s]Extractor Estimating: 489it [05:15,  1.44it/s]Extractor Estimating: 490it [05:15,  1.50it/s]Extractor Estimating: 491it [05:16,  1.53it/s]Extractor Estimating: 492it [05:16,  1.55it/s]Extractor Estimating: 493it [05:17,  1.51it/s]Extractor Estimating: 494it [05:18,  1.50it/s]Extractor Estimating: 495it [05:18,  1.51it/s]Extractor Estimating: 496it [05:19,  1.48it/s]Extractor Estimating: 497it [05:20,  1.44it/s]Extractor Estimating: 498it [05:21,  1.40it/s]Extractor Estimating: 499it [05:21,  1.40it/s]Extractor Estimating: 500it [05:22,  1.47it/s]Extractor Estimating: 500it [05:22,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:36,386 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:36,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:36,392 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:36,392 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:36,392 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:41:37,035 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:41:37,037 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:41:37,614 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:41:38,691 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:41:38,691 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:41,734 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:41,741 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:41,741 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:41,741 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:41:41,741 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:41:42,388 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:41:42,389 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:41:42,981 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:41:43,148 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:41:43,148 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:50:55,364 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:50:55,494 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9995 mean pseudo reward: 0.9451629224082908
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 25574
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25674, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25674, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.084, loss:670.4793
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.092, loss:655.6419
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.098, loss:677.4842
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.097, loss:662.9158
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.103, loss:604.5201
>> valid entity prec:0.5933, rec:0.5262, f1:0.5577
>> valid relation prec:0.1330, rec:0.0256, f1:0.0429
>> valid relation with NER prec:0.1330, rec:0.0256, f1:0.0429
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.484, loss:622.4450
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.101, loss:658.4504
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.085, loss:649.9131
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.087, loss:653.9278
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.107, loss:656.7361
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5747, rec:0.5900, f1:0.5822
>> valid relation prec:0.1112, rec:0.0284, f1:0.0453
>> valid relation with NER prec:0.1112, rec:0.0284, f1:0.0453
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.484, loss:643.4698
g_step 1200, step 366, avg_time 1.096, loss:642.5082
g_step 1300, step 49, avg_time 1.097, loss:645.9522
g_step 1400, step 149, avg_time 1.090, loss:616.7842
g_step 1500, step 249, avg_time 1.106, loss:627.8921
>> valid entity prec:0.5329, rec:0.6081, f1:0.5680
>> valid relation prec:0.1411, rec:0.0299, f1:0.0493
>> valid relation with NER prec:0.1411, rec:0.0299, f1:0.0493
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.495, loss:612.8862
g_step 1700, step 32, avg_time 1.080, loss:631.1702
g_step 1800, step 132, avg_time 1.101, loss:591.3840
g_step 1900, step 232, avg_time 1.084, loss:596.9483
g_step 2000, step 332, avg_time 1.111, loss:629.2543
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5295, rec:0.5955, f1:0.5606
>> valid relation prec:0.1250, rec:0.0253, f1:0.0420
>> valid relation with NER prec:0.1250, rec:0.0253, f1:0.0420
g_step 2100, step 15, avg_time 2.487, loss:595.6610
g_step 2200, step 115, avg_time 1.097, loss:538.3954
g_step 2300, step 215, avg_time 1.107, loss:578.7550
g_step 2400, step 315, avg_time 1.091, loss:584.0084
g_step 2500, step 415, avg_time 1.081, loss:584.1848
>> valid entity prec:0.5899, rec:0.5249, f1:0.5555
>> valid relation prec:0.1512, rec:0.0342, f1:0.0558
>> valid relation with NER prec:0.1512, rec:0.0342, f1:0.0558
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 98, avg_time 2.479, loss:520.7384
g_step 2700, step 198, avg_time 1.094, loss:531.7384
g_step 2800, step 298, avg_time 1.096, loss:553.1483
g_step 2900, step 398, avg_time 1.100, loss:567.7931
g_step 3000, step 81, avg_time 1.093, loss:517.3110
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5575, rec:0.5819, f1:0.5694
>> valid relation prec:0.0960, rec:0.0238, f1:0.0382
>> valid relation with NER prec:0.0960, rec:0.0238, f1:0.0382
g_step 3100, step 181, avg_time 2.489, loss:502.9022
g_step 3200, step 281, avg_time 1.086, loss:544.3023
g_step 3300, step 381, avg_time 1.095, loss:530.0427
g_step 3400, step 64, avg_time 1.091, loss:491.9111
g_step 3500, step 164, avg_time 1.091, loss:499.3143
>> valid entity prec:0.5628, rec:0.5527, f1:0.5577
>> valid relation prec:0.0340, rec:0.0098, f1:0.0152
>> valid relation with NER prec:0.0340, rec:0.0098, f1:0.0152
g_step 3600, step 264, avg_time 2.475, loss:508.8585
g_step 3700, step 364, avg_time 1.104, loss:523.5709
g_step 3800, step 47, avg_time 1.080, loss:476.5762
g_step 3900, step 147, avg_time 1.082, loss:478.5192
g_step 4000, step 247, avg_time 1.098, loss:473.7142
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5537, rec:0.5428, f1:0.5482
>> valid relation prec:0.1280, rec:0.0396, f1:0.0605
>> valid relation with NER prec:0.1280, rec:0.0396, f1:0.0605
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 347, avg_time 2.500, loss:500.8174
g_step 4200, step 30, avg_time 1.102, loss:479.1803
g_step 4300, step 130, avg_time 1.095, loss:457.7161
g_step 4400, step 230, avg_time 1.104, loss:470.1462
g_step 4500, step 330, avg_time 1.083, loss:474.4185
>> valid entity prec:0.6171, rec:0.4983, f1:0.5513
>> valid relation prec:0.1262, rec:0.0302, f1:0.0487
>> valid relation with NER prec:0.1262, rec:0.0302, f1:0.0487
g_step 4600, step 13, avg_time 2.476, loss:468.1099
g_step 4700, step 113, avg_time 1.100, loss:432.6957
g_step 4800, step 213, avg_time 1.082, loss:429.2595
g_step 4900, step 313, avg_time 1.106, loss:439.7784
g_step 5000, step 413, avg_time 1.095, loss:484.5251
learning rate was adjusted to 0.0008
>> valid entity prec:0.5754, rec:0.5308, f1:0.5522
>> valid relation prec:0.1035, rec:0.0273, f1:0.0432
>> valid relation with NER prec:0.1035, rec:0.0273, f1:0.0432
g_step 5100, step 96, avg_time 2.478, loss:403.7177
g_step 5200, step 196, avg_time 1.101, loss:419.3476
g_step 5300, step 296, avg_time 1.095, loss:454.1068
g_step 5400, step 396, avg_time 1.097, loss:432.1446
g_step 5500, step 79, avg_time 1.072, loss:403.0979
>> valid entity prec:0.5875, rec:0.4860, f1:0.5319
>> valid relation prec:0.1201, rec:0.0299, f1:0.0478
>> valid relation with NER prec:0.1201, rec:0.0299, f1:0.0478
g_step 5600, step 179, avg_time 2.474, loss:405.2159
g_step 5700, step 279, avg_time 1.108, loss:412.6008
g_step 5800, step 379, avg_time 1.095, loss:453.3609
g_step 5900, step 62, avg_time 1.089, loss:402.7984
g_step 6000, step 162, avg_time 1.091, loss:391.5700
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5686, rec:0.5275, f1:0.5473
>> valid relation prec:0.1061, rec:0.0258, f1:0.0416
>> valid relation with NER prec:0.1061, rec:0.0258, f1:0.0416
g_step 6100, step 262, avg_time 2.484, loss:403.8983
g_step 6200, step 362, avg_time 1.081, loss:387.8635
g_step 6300, step 45, avg_time 1.100, loss:399.8741
g_step 6400, step 145, avg_time 1.115, loss:368.0855
g_step 6500, step 245, avg_time 1.083, loss:376.5581
>> valid entity prec:0.5666, rec:0.5314, f1:0.5484
>> valid relation prec:0.0981, rec:0.0287, f1:0.0444
>> valid relation with NER prec:0.0981, rec:0.0287, f1:0.0444
g_step 6600, step 345, avg_time 2.488, loss:393.6811
g_step 6700, step 28, avg_time 1.088, loss:390.3727
g_step 6800, step 128, avg_time 1.090, loss:347.6974
g_step 6900, step 228, avg_time 1.092, loss:358.8818
g_step 7000, step 328, avg_time 1.108, loss:379.2918
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6068, rec:0.4638, f1:0.5258
>> valid relation prec:0.0822, rec:0.0198, f1:0.0319
>> valid relation with NER prec:0.0822, rec:0.0198, f1:0.0319
g_step 7100, step 11, avg_time 2.476, loss:386.9747
g_step 7200, step 111, avg_time 1.083, loss:339.9322
g_step 7300, step 211, avg_time 1.092, loss:339.2406
g_step 7400, step 311, avg_time 1.101, loss:385.5089
g_step 7500, step 411, avg_time 1.102, loss:371.4936
>> valid entity prec:0.5858, rec:0.5107, f1:0.5457
>> valid relation prec:0.0783, rec:0.0195, f1:0.0313
>> valid relation with NER prec:0.0783, rec:0.0195, f1:0.0313
g_step 7600, step 94, avg_time 2.486, loss:328.4706
g_step 7700, step 194, avg_time 1.086, loss:332.6358
g_step 7800, step 294, avg_time 1.099, loss:362.2092
g_step 7900, step 394, avg_time 1.094, loss:355.3010
g_step 8000, step 77, avg_time 1.090, loss:321.0767
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5908, rec:0.4773, f1:0.5280
>> valid relation prec:0.0982, rec:0.0247, f1:0.0395
>> valid relation with NER prec:0.0982, rec:0.0247, f1:0.0395
g_step 8100, step 177, avg_time 2.476, loss:316.0317
g_step 8200, step 277, avg_time 1.103, loss:345.6376
g_step 8300, step 377, avg_time 1.092, loss:338.7339
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:50:55 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:50:55 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-50-55_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:50:56 - WARNING - datasets.builder -   Using custom data configuration default-5ef9a271c3b28f31
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5ef9a271c3b28f31/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:50:57,090 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:50:57,091 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:50:57,092 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:50:57,093 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:50:57,105 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:50:57,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:50:57,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:50:57,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:50:57,131 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:50:57,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:50:57,131 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:50:57,599 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:51:00,769 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:51:00,769 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5ef9a271c3b28f31/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.55ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.54ba/s] 27%|██▋       | 3/11 [00:00<00:02,  3.91ba/s] 36%|███▋      | 4/11 [00:01<00:01,  4.17ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.32ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  3.63ba/s] 64%|██████▎   | 7/11 [00:01<00:01,  3.91ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.12ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.28ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.39ba/s]100%|██████████| 11/11 [00:02<00:00,  4.44ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.62ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.01ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.16ba/s]100%|██████████| 4/4 [00:00<00:00,  5.25ba/s]100%|██████████| 4/4 [00:00<00:00,  4.70ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  7.50ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.28ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.71ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.88ba/s] 82%|████████▏ | 9/11 [00:00<00:00,  9.95ba/s]100%|██████████| 11/11 [00:01<00:00, 10.74ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.53ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.15ba/s]100%|██████████| 4/4 [00:00<00:00, 10.42ba/s]
[INFO|trainer.py:414] 2023-08-28 17:51:06,818 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:51:06,906 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:51:06,906 >>   Num examples = 10006
[INFO|trainer.py:1149] 2023-08-28 17:51:06,906 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:51:06,906 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:51:06,906 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:51:06,906 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:51:06,906 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:55,  3.31it/s]  0%|          | 2/780 [00:00<03:49,  3.39it/s]  0%|          | 3/780 [00:00<03:47,  3.42it/s]  1%|          | 4/780 [00:01<03:46,  3.43it/s]  1%|          | 5/780 [00:01<03:45,  3.44it/s]  1%|          | 6/780 [00:01<03:44,  3.45it/s]  1%|          | 7/780 [00:02<03:44,  3.45it/s]  1%|          | 8/780 [00:02<03:43,  3.45it/s]  1%|          | 9/780 [00:02<03:43,  3.45it/s]  1%|▏         | 10/780 [00:02<03:43,  3.45it/s]  1%|▏         | 11/780 [00:03<03:42,  3.45it/s]  2%|▏         | 12/780 [00:03<03:42,  3.45it/s]  2%|▏         | 13/780 [00:03<03:42,  3.45it/s]  2%|▏         | 14/780 [00:04<03:41,  3.45it/s]  2%|▏         | 15/780 [00:04<03:41,  3.45it/s]  2%|▏         | 16/780 [00:04<03:41,  3.45it/s]  2%|▏         | 17/780 [00:04<03:43,  3.41it/s]  2%|▏         | 18/780 [00:05<03:42,  3.43it/s]  2%|▏         | 19/780 [00:05<03:41,  3.43it/s]  3%|▎         | 20/780 [00:05<03:40,  3.44it/s]  3%|▎         | 21/780 [00:06<03:40,  3.44it/s]  3%|▎         | 22/780 [00:06<03:39,  3.45it/s]  3%|▎         | 23/780 [00:06<03:39,  3.45it/s]  3%|▎         | 24/780 [00:06<03:42,  3.40it/s]  3%|▎         | 25/780 [00:07<03:41,  3.41it/s]  3%|▎         | 26/780 [00:07<03:40,  3.42it/s]  3%|▎         | 27/780 [00:07<03:39,  3.43it/s]  4%|▎         | 28/780 [00:08<03:38,  3.44it/s]  4%|▎         | 29/780 [00:08<03:38,  3.44it/s]  4%|▍         | 30/780 [00:08<03:37,  3.44it/s]  4%|▍         | 31/780 [00:09<03:37,  3.45it/s]  4%|▍         | 32/780 [00:09<03:36,  3.45it/s]  4%|▍         | 33/780 [00:09<03:36,  3.45it/s]  4%|▍         | 34/780 [00:09<03:36,  3.44it/s]  4%|▍         | 35/780 [00:10<03:38,  3.40it/s]  5%|▍         | 36/780 [00:10<03:37,  3.42it/s]  5%|▍         | 37/780 [00:10<03:36,  3.43it/s]  5%|▍         | 38/780 [00:11<03:36,  3.43it/s]  5%|▌         | 39/780 [00:11<03:35,  3.44it/s]  5%|▌         | 40/780 [00:11<03:34,  3.44it/s]  5%|▌         | 41/780 [00:11<03:34,  3.45it/s]  5%|▌         | 42/780 [00:12<03:34,  3.45it/s]  6%|▌         | 43/780 [00:12<03:33,  3.45it/s]  6%|▌         | 44/780 [00:12<03:33,  3.45it/s]  6%|▌         | 45/780 [00:13<03:33,  3.45it/s]  6%|▌         | 46/780 [00:13<03:32,  3.45it/s]  6%|▌         | 47/780 [00:13<03:32,  3.45it/s]  6%|▌         | 48/780 [00:13<03:32,  3.45it/s]  6%|▋         | 49/780 [00:14<03:31,  3.45it/s]  6%|▋         | 50/780 [00:14<03:31,  3.45it/s]  7%|▋         | 51/780 [00:14<03:31,  3.45it/s]  7%|▋         | 52/780 [00:15<03:40,  3.29it/s]  7%|▋         | 53/780 [00:15<03:37,  3.34it/s]  7%|▋         | 54/780 [00:15<03:35,  3.37it/s]  7%|▋         | 55/780 [00:16<03:33,  3.39it/s]  7%|▋         | 56/780 [00:16<03:32,  3.41it/s]  7%|▋         | 57/780 [00:16<03:31,  3.42it/s]  7%|▋         | 58/780 [00:16<03:30,  3.43it/s]  8%|▊         | 59/780 [00:17<03:29,  3.43it/s]  8%|▊         | 60/780 [00:17<03:29,  3.44it/s]  8%|▊         | 61/780 [00:17<03:29,  3.44it/s]  8%|▊         | 62/780 [00:18<03:28,  3.44it/s]  8%|▊         | 63/780 [00:18<03:28,  3.44it/s]  8%|▊         | 64/780 [00:18<03:28,  3.44it/s]  8%|▊         | 65/780 [00:18<03:27,  3.44it/s]  8%|▊         | 66/780 [00:19<03:27,  3.44it/s]  9%|▊         | 67/780 [00:19<03:27,  3.44it/s]  9%|▊         | 68/780 [00:19<03:26,  3.45it/s]  9%|▉         | 69/780 [00:20<03:26,  3.44it/s]  9%|▉         | 70/780 [00:20<03:26,  3.43it/s]  9%|▉         | 71/780 [00:20<03:26,  3.44it/s]  9%|▉         | 72/780 [00:20<03:25,  3.44it/s]  9%|▉         | 73/780 [00:21<03:25,  3.44it/s]  9%|▉         | 74/780 [00:21<03:25,  3.44it/s] 10%|▉         | 75/780 [00:21<03:24,  3.44it/s] 10%|▉         | 76/780 [00:22<03:24,  3.44it/s] 10%|▉         | 77/780 [00:22<03:24,  3.44it/s] 10%|█         | 78/780 [00:22<03:23,  3.44it/s] 10%|█         | 79/780 [00:23<03:23,  3.44it/s] 10%|█         | 80/780 [00:23<03:23,  3.44it/s] 10%|█         | 81/780 [00:23<03:23,  3.44it/s] 11%|█         | 82/780 [00:23<03:22,  3.44it/s] 11%|█         | 83/780 [00:24<03:22,  3.44it/s] 11%|█         | 84/780 [00:24<03:22,  3.44it/s] 11%|█         | 85/780 [00:24<03:21,  3.44it/s] 11%|█         | 86/780 [00:25<03:21,  3.44it/s] 11%|█         | 87/780 [00:25<03:22,  3.42it/s] 11%|█▏        | 88/780 [00:25<03:21,  3.43it/s] 11%|█▏        | 89/780 [00:25<03:22,  3.42it/s] 12%|█▏        | 90/780 [00:26<03:21,  3.43it/s] 12%|█▏        | 91/780 [00:26<03:20,  3.43it/s] 12%|█▏        | 92/780 [00:26<03:20,  3.43it/s] 12%|█▏        | 93/780 [00:27<03:20,  3.43it/s] 12%|█▏        | 94/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 95/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 96/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 97/780 [00:28<03:18,  3.44it/s] 13%|█▎        | 98/780 [00:28<03:18,  3.44it/s] 13%|█▎        | 99/780 [00:28<03:18,  3.44it/s] 13%|█▎        | 100/780 [00:29<03:17,  3.44it/s] 13%|█▎        | 101/780 [00:29<03:17,  3.44it/s] 13%|█▎        | 102/780 [00:29<03:17,  3.44it/s] 13%|█▎        | 103/780 [00:29<03:16,  3.44it/s] 13%|█▎        | 104/780 [00:30<03:16,  3.44it/s] 13%|█▎        | 105/780 [00:30<03:16,  3.43it/s] 14%|█▎        | 106/780 [00:30<03:16,  3.43it/s] 14%|█▎        | 107/780 [00:31<03:16,  3.43it/s] 14%|█▍        | 108/780 [00:31<03:15,  3.43it/s] 14%|█▍        | 109/780 [00:31<03:15,  3.44it/s] 14%|█▍        | 110/780 [00:32<03:15,  3.43it/s] 14%|█▍        | 111/780 [00:32<03:14,  3.44it/s] 14%|█▍        | 112/780 [00:32<03:14,  3.43it/s] 14%|█▍        | 113/780 [00:32<03:14,  3.44it/s] 15%|█▍        | 114/780 [00:33<03:14,  3.43it/s] 15%|█▍        | 115/780 [00:33<03:13,  3.43it/s] 15%|█▍        | 116/780 [00:33<03:14,  3.42it/s] 15%|█▌        | 117/780 [00:34<03:13,  3.43it/s] 15%|█▌        | 118/780 [00:34<03:13,  3.43it/s] 15%|█▌        | 119/780 [00:34<03:12,  3.43it/s] 15%|█▌        | 120/780 [00:34<03:12,  3.43it/s] 16%|█▌        | 121/780 [00:35<03:11,  3.43it/s] 16%|█▌        | 122/780 [00:35<03:11,  3.43it/s] 16%|█▌        | 123/780 [00:35<03:11,  3.43it/s] 16%|█▌        | 124/780 [00:36<03:11,  3.43it/s] 16%|█▌        | 125/780 [00:36<03:10,  3.44it/s] 16%|█▌        | 126/780 [00:36<03:10,  3.43it/s] 16%|█▋        | 127/780 [00:37<03:14,  3.36it/s] 16%|█▋        | 128/780 [00:37<03:12,  3.38it/s] 17%|█▋        | 129/780 [00:37<03:11,  3.40it/s] 17%|█▋        | 130/780 [00:37<03:10,  3.41it/s] 17%|█▋        | 131/780 [00:38<03:09,  3.42it/s] 17%|█▋        | 132/780 [00:38<03:09,  3.42it/s] 17%|█▋        | 133/780 [00:38<03:09,  3.42it/s] 17%|█▋        | 134/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 135/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 136/780 [00:39<03:07,  3.43it/s] 18%|█▊        | 137/780 [00:39<03:07,  3.43it/s] 18%|█▊        | 138/780 [00:40<03:07,  3.43it/s] 18%|█▊        | 139/780 [00:40<03:06,  3.44it/s] 18%|█▊        | 140/780 [00:40<03:06,  3.42it/s] 18%|█▊        | 141/780 [00:41<03:06,  3.43it/s] 18%|█▊        | 142/780 [00:41<03:06,  3.43it/s] 18%|█▊        | 143/780 [00:41<03:05,  3.43it/s] 18%|█▊        | 144/780 [00:41<03:05,  3.43it/s] 19%|█▊        | 145/780 [00:42<03:04,  3.43it/s] 19%|█▊        | 146/780 [00:42<03:04,  3.44it/s] 19%|█▉        | 147/780 [00:42<03:15,  3.23it/s] 19%|█▉        | 148/780 [00:43<03:11,  3.29it/s] 19%|█▉        | 149/780 [00:43<03:09,  3.33it/s] 19%|█▉        | 150/780 [00:43<03:07,  3.37it/s] 19%|█▉        | 151/780 [00:44<03:05,  3.38it/s] 19%|█▉        | 152/780 [00:44<03:04,  3.40it/s] 20%|█▉        | 153/780 [00:44<03:03,  3.41it/s] 20%|█▉        | 154/780 [00:44<03:03,  3.42it/s] 20%|█▉        | 155/780 [00:45<03:02,  3.43it/s] 20%|██        | 156/780 [00:45<03:01,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 17:51:52,464 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:51:52,476 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:51:52,476 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.46it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.18it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.44it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.54it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.04it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.67it/s][A
  9%|▊         | 38/436 [00:00<00:08, 45.46it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.38it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.30it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.28it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.33it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.32it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.25it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.26it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.15it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.06it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.96it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.03it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.02it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.09it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.23it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.30it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.16it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.11it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.03it/s][A
 31%|███       | 133/436 [00:02<00:07, 43.09it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 42.98it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 43.95it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 44.69it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.14it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.52it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.69it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.97it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.03it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.71it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.66it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.80it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.96it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.99it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.16it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.20it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.14it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.05it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.85it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.83it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.82it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.87it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.95it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.11it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.21it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.27it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.17it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.02it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.98it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.96it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.04it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.09it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.11it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.12it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.21it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.15it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.11it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.04it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.95it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.00it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.01it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.03it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.15it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.20it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.19it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.06it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.99it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.94it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.97it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.03it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.07it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.13it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.15it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.17it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.07it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.10it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.07it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.93it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.03it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.02it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.08it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.08it/s][A 20%|██        | 156/780 [00:55<03:01,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:52:02,127 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 17:52:02,181 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:52:05,503 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:52:05,522 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:52:05,530 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:06<1:07:41,  6.52s/it] 20%|██        | 158/780 [01:06<48:19,  4.66s/it]   20%|██        | 159/780 [01:07<34:40,  3.35s/it] 21%|██        | 160/780 [01:07<25:08,  2.43s/it] 21%|██        | 161/780 [01:07<18:27,  1.79s/it] 21%|██        | 162/780 [01:08<13:48,  1.34s/it] 21%|██        | 163/780 [01:08<10:32,  1.02s/it] 21%|██        | 164/780 [01:08<08:15,  1.24it/s] 21%|██        | 165/780 [01:08<06:39,  1.54it/s] 21%|██▏       | 166/780 [01:09<05:32,  1.84it/s] 21%|██▏       | 167/780 [01:09<04:46,  2.14it/s] 22%|██▏       | 168/780 [01:09<04:13,  2.42it/s] 22%|██▏       | 169/780 [01:10<03:55,  2.60it/s] 22%|██▏       | 170/780 [01:10<03:37,  2.80it/s] 22%|██▏       | 171/780 [01:10<03:25,  2.97it/s] 22%|██▏       | 172/780 [01:10<03:16,  3.09it/s] 22%|██▏       | 173/780 [01:11<03:10,  3.19it/s] 22%|██▏       | 174/780 [01:11<03:05,  3.26it/s] 22%|██▏       | 175/780 [01:11<03:02,  3.31it/s] 23%|██▎       | 176/780 [01:12<03:00,  3.35it/s] 23%|██▎       | 177/780 [01:12<02:58,  3.37it/s] 23%|██▎       | 178/780 [01:12<02:57,  3.39it/s] 23%|██▎       | 179/780 [01:13<02:56,  3.41it/s] 23%|██▎       | 180/780 [01:13<03:20,  3.00it/s] 23%|██▎       | 181/780 [01:13<03:12,  3.12it/s] 23%|██▎       | 182/780 [01:14<03:06,  3.21it/s] 23%|██▎       | 183/780 [01:14<03:02,  3.27it/s] 24%|██▎       | 184/780 [01:14<02:59,  3.32it/s] 24%|██▎       | 185/780 [01:14<02:57,  3.36it/s] 24%|██▍       | 186/780 [01:15<02:55,  3.38it/s] 24%|██▍       | 187/780 [01:15<02:54,  3.39it/s] 24%|██▍       | 188/780 [01:15<02:53,  3.40it/s] 24%|██▍       | 189/780 [01:16<02:53,  3.42it/s] 24%|██▍       | 190/780 [01:16<02:53,  3.40it/s] 24%|██▍       | 191/780 [01:16<02:52,  3.41it/s] 25%|██▍       | 192/780 [01:16<02:52,  3.42it/s] 25%|██▍       | 193/780 [01:17<02:51,  3.42it/s] 25%|██▍       | 194/780 [01:17<02:51,  3.42it/s] 25%|██▌       | 195/780 [01:17<02:50,  3.43it/s] 25%|██▌       | 196/780 [01:18<02:50,  3.43it/s] 25%|██▌       | 197/780 [01:18<02:49,  3.43it/s] 25%|██▌       | 198/780 [01:18<02:49,  3.43it/s] 26%|██▌       | 199/780 [01:18<02:49,  3.43it/s] 26%|██▌       | 200/780 [01:19<02:49,  3.43it/s] 26%|██▌       | 201/780 [01:19<02:50,  3.40it/s] 26%|██▌       | 202/780 [01:19<02:49,  3.41it/s] 26%|██▌       | 203/780 [01:20<02:48,  3.41it/s] 26%|██▌       | 204/780 [01:20<02:48,  3.42it/s] 26%|██▋       | 205/780 [01:20<02:48,  3.42it/s] 26%|██▋       | 206/780 [01:21<02:47,  3.43it/s] 27%|██▋       | 207/780 [01:21<02:46,  3.43it/s] 27%|██▋       | 208/780 [01:21<02:46,  3.43it/s] 27%|██▋       | 209/780 [01:21<02:46,  3.43it/s] 27%|██▋       | 210/780 [01:22<02:45,  3.43it/s] 27%|██▋       | 211/780 [01:22<02:45,  3.44it/s] 27%|██▋       | 212/780 [01:22<02:51,  3.30it/s] 27%|██▋       | 213/780 [01:23<02:49,  3.34it/s] 27%|██▋       | 214/780 [01:23<02:47,  3.37it/s] 28%|██▊       | 215/780 [01:23<02:46,  3.39it/s] 28%|██▊       | 216/780 [01:23<02:45,  3.40it/s] 28%|██▊       | 217/780 [01:24<02:44,  3.42it/s] 28%|██▊       | 218/780 [01:24<02:44,  3.42it/s] 28%|██▊       | 219/780 [01:24<02:43,  3.43it/s] 28%|██▊       | 220/780 [01:25<02:43,  3.43it/s] 28%|██▊       | 221/780 [01:25<02:42,  3.43it/s] 28%|██▊       | 222/780 [01:25<02:42,  3.43it/s] 29%|██▊       | 223/780 [01:26<02:42,  3.43it/s] 29%|██▊       | 224/780 [01:26<02:42,  3.43it/s] 29%|██▉       | 225/780 [01:26<02:41,  3.43it/s] 29%|██▉       | 226/780 [01:26<02:45,  3.34it/s] 29%|██▉       | 227/780 [01:27<02:44,  3.37it/s] 29%|██▉       | 228/780 [01:27<02:42,  3.39it/s] 29%|██▉       | 229/780 [01:27<02:41,  3.40it/s] 29%|██▉       | 230/780 [01:28<02:41,  3.41it/s] 30%|██▉       | 231/780 [01:28<02:40,  3.42it/s] 30%|██▉       | 232/780 [01:28<02:39,  3.43it/s] 30%|██▉       | 233/780 [01:28<02:39,  3.43it/s] 30%|███       | 234/780 [01:29<02:38,  3.43it/s] 30%|███       | 235/780 [01:29<02:38,  3.43it/s] 30%|███       | 236/780 [01:29<02:38,  3.43it/s] 30%|███       | 237/780 [01:30<02:41,  3.37it/s] 31%|███       | 238/780 [01:30<02:39,  3.39it/s] 31%|███       | 239/780 [01:30<02:38,  3.41it/s] 31%|███       | 240/780 [01:31<02:38,  3.41it/s] 31%|███       | 241/780 [01:31<02:37,  3.42it/s] 31%|███       | 242/780 [01:31<02:37,  3.42it/s] 31%|███       | 243/780 [01:31<02:36,  3.43it/s] 31%|███▏      | 244/780 [01:32<02:36,  3.43it/s] 31%|███▏      | 245/780 [01:32<02:35,  3.44it/s] 32%|███▏      | 246/780 [01:32<02:35,  3.44it/s] 32%|███▏      | 247/780 [01:33<02:35,  3.44it/s] 32%|███▏      | 248/780 [01:33<02:51,  3.11it/s] 32%|███▏      | 249/780 [01:33<02:45,  3.20it/s] 32%|███▏      | 250/780 [01:34<02:42,  3.27it/s] 32%|███▏      | 251/780 [01:34<02:39,  3.31it/s] 32%|███▏      | 252/780 [01:34<02:37,  3.35it/s] 32%|███▏      | 253/780 [01:34<02:36,  3.38it/s] 33%|███▎      | 254/780 [01:35<02:34,  3.40it/s] 33%|███▎      | 255/780 [01:35<02:34,  3.41it/s] 33%|███▎      | 256/780 [01:35<02:33,  3.42it/s] 33%|███▎      | 257/780 [01:36<02:33,  3.42it/s] 33%|███▎      | 258/780 [01:36<02:40,  3.26it/s] 33%|███▎      | 259/780 [01:36<02:37,  3.31it/s] 33%|███▎      | 260/780 [01:36<02:35,  3.35it/s] 33%|███▎      | 261/780 [01:37<02:34,  3.37it/s] 34%|███▎      | 262/780 [01:37<02:32,  3.39it/s] 34%|███▎      | 263/780 [01:37<02:31,  3.40it/s] 34%|███▍      | 264/780 [01:38<02:31,  3.41it/s] 34%|███▍      | 265/780 [01:38<02:30,  3.42it/s] 34%|███▍      | 266/780 [01:38<02:30,  3.42it/s] 34%|███▍      | 267/780 [01:39<02:29,  3.42it/s] 34%|███▍      | 268/780 [01:39<02:29,  3.42it/s] 34%|███▍      | 269/780 [01:39<02:30,  3.41it/s] 35%|███▍      | 270/780 [01:39<02:29,  3.41it/s] 35%|███▍      | 271/780 [01:40<02:28,  3.42it/s] 35%|███▍      | 272/780 [01:40<02:28,  3.42it/s] 35%|███▌      | 273/780 [01:40<02:27,  3.43it/s] 35%|███▌      | 274/780 [01:41<02:27,  3.43it/s] 35%|███▌      | 275/780 [01:41<02:27,  3.43it/s] 35%|███▌      | 276/780 [01:41<02:26,  3.43it/s] 36%|███▌      | 277/780 [01:41<02:26,  3.43it/s] 36%|███▌      | 278/780 [01:42<02:26,  3.43it/s] 36%|███▌      | 279/780 [01:42<02:25,  3.43it/s] 36%|███▌      | 280/780 [01:42<02:31,  3.30it/s] 36%|███▌      | 281/780 [01:43<02:29,  3.34it/s] 36%|███▌      | 282/780 [01:43<02:27,  3.37it/s] 36%|███▋      | 283/780 [01:43<02:26,  3.39it/s] 36%|███▋      | 284/780 [01:44<02:25,  3.40it/s] 37%|███▋      | 285/780 [01:44<02:25,  3.41it/s] 37%|███▋      | 286/780 [01:44<02:24,  3.42it/s] 37%|███▋      | 287/780 [01:44<02:24,  3.42it/s] 37%|███▋      | 288/780 [01:45<02:23,  3.43it/s] 37%|███▋      | 289/780 [01:45<02:23,  3.43it/s] 37%|███▋      | 290/780 [01:45<02:22,  3.43it/s] 37%|███▋      | 291/780 [01:46<02:22,  3.42it/s] 37%|███▋      | 292/780 [01:46<02:22,  3.43it/s] 38%|███▊      | 293/780 [01:46<02:22,  3.43it/s] 38%|███▊      | 294/780 [01:46<02:21,  3.43it/s] 38%|███▊      | 295/780 [01:47<02:21,  3.43it/s] 38%|███▊      | 296/780 [01:47<02:21,  3.43it/s] 38%|███▊      | 297/780 [01:47<02:20,  3.43it/s] 38%|███▊      | 298/780 [01:48<02:20,  3.43it/s] 38%|███▊      | 299/780 [01:48<02:20,  3.43it/s] 38%|███▊      | 300/780 [01:48<02:27,  3.25it/s] 39%|███▊      | 301/780 [01:49<02:25,  3.30it/s] 39%|███▊      | 302/780 [01:49<02:25,  3.29it/s] 39%|███▉      | 303/780 [01:49<02:23,  3.33it/s] 39%|███▉      | 304/780 [01:49<02:21,  3.36it/s] 39%|███▉      | 305/780 [01:50<02:20,  3.38it/s] 39%|███▉      | 306/780 [01:50<02:19,  3.40it/s] 39%|███▉      | 307/780 [01:50<02:18,  3.41it/s] 39%|███▉      | 308/780 [01:51<02:18,  3.41it/s] 40%|███▉      | 309/780 [01:51<02:17,  3.42it/s] 40%|███▉      | 310/780 [01:51<02:17,  3.42it/s] 40%|███▉      | 311/780 [01:51<02:16,  3.43it/s] 40%|████      | 312/780 [01:52<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 17:52:59,226 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:52:59,226 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:52:59,226 >>   Batch size = 8
{'eval_loss': 1.0518537759780884, 'eval_runtime': 9.5335, 'eval_samples_per_second': 365.237, 'eval_steps_per_second': 45.733, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.30it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.81it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.20it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.55it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.92it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.72it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.15it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.77it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.92it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.99it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.05it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 45.90it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.15it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.21it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.25it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.93it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.72it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.75it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.92it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.08it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.07it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.13it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.08it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.12it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.90it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.86it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.81it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.92it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.89it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.00it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.98it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.06it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.07it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.95it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.83it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.79it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.86it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.95it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.96it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.01it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.98it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.05it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.94it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.92it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 44.98it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.31it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.46it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.62it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.72it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.85it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.91it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.85it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.78it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.77it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.85it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.92it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.96it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.96it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.93it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.97it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.99it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.84it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.86it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.94it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.84it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.91it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.96it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.02it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.04it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.92it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.91it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.84it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 42.18it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 43.40it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 44.10it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 44.83it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.28it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.54it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.65it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.81it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.44it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 45.43it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.58it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.69it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.93it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.98it/s][A                                                 
                                                 [A 40%|████      | 312/780 [02:01<02:16,  3.43it/s]
100%|██████████| 436/436 [00:09<00:00, 45.98it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:53:08,908 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 17:53:08,937 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:53:15,343 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:53:15,367 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:53:15,381 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:19<1:05:25,  8.41s/it] 40%|████      | 314/780 [02:19<46:24,  5.98s/it]   40%|████      | 315/780 [02:20<33:05,  4.27s/it] 41%|████      | 316/780 [02:20<23:47,  3.08s/it] 41%|████      | 317/780 [02:20<17:17,  2.24s/it] 41%|████      | 318/780 [02:21<12:44,  1.66s/it] 41%|████      | 319/780 [02:21<09:34,  1.25s/it] 41%|████      | 320/780 [02:21<07:20,  1.04it/s] 41%|████      | 321/780 [02:21<05:47,  1.32it/s] 41%|████▏     | 322/780 [02:22<04:42,  1.62it/s] 41%|████▏     | 323/780 [02:22<03:57,  1.92it/s] 42%|████▏     | 324/780 [02:22<03:25,  2.22it/s] 42%|████▏     | 325/780 [02:23<03:03,  2.48it/s] 42%|████▏     | 326/780 [02:23<02:47,  2.71it/s] 42%|████▏     | 327/780 [02:23<02:36,  2.89it/s] 42%|████▏     | 328/780 [02:23<02:28,  3.04it/s] 42%|████▏     | 329/780 [02:24<02:23,  3.15it/s] 42%|████▏     | 330/780 [02:24<02:19,  3.23it/s] 42%|████▏     | 331/780 [02:24<02:16,  3.30it/s] 43%|████▎     | 332/780 [02:25<02:14,  3.34it/s] 43%|████▎     | 333/780 [02:25<02:12,  3.37it/s] 43%|████▎     | 334/780 [02:25<02:11,  3.39it/s] 43%|████▎     | 335/780 [02:25<02:10,  3.41it/s] 43%|████▎     | 336/780 [02:26<02:09,  3.42it/s] 43%|████▎     | 337/780 [02:26<02:09,  3.43it/s] 43%|████▎     | 338/780 [02:26<02:08,  3.43it/s] 43%|████▎     | 339/780 [02:27<02:08,  3.44it/s] 44%|████▎     | 340/780 [02:27<02:08,  3.44it/s] 44%|████▎     | 341/780 [02:27<02:07,  3.43it/s] 44%|████▍     | 342/780 [02:28<02:07,  3.43it/s] 44%|████▍     | 343/780 [02:28<02:07,  3.42it/s] 44%|████▍     | 344/780 [02:28<02:07,  3.43it/s] 44%|████▍     | 345/780 [02:28<02:06,  3.43it/s] 44%|████▍     | 346/780 [02:29<02:06,  3.44it/s] 44%|████▍     | 347/780 [02:29<02:06,  3.43it/s] 45%|████▍     | 348/780 [02:29<02:05,  3.44it/s] 45%|████▍     | 349/780 [02:30<02:05,  3.44it/s] 45%|████▍     | 350/780 [02:30<02:05,  3.44it/s] 45%|████▌     | 351/780 [02:30<02:04,  3.44it/s] 45%|████▌     | 352/780 [02:30<02:04,  3.44it/s] 45%|████▌     | 353/780 [02:31<02:04,  3.44it/s] 45%|████▌     | 354/780 [02:31<02:08,  3.32it/s] 46%|████▌     | 355/780 [02:31<02:06,  3.35it/s] 46%|████▌     | 356/780 [02:32<02:05,  3.38it/s] 46%|████▌     | 357/780 [02:32<02:04,  3.40it/s] 46%|████▌     | 358/780 [02:32<02:03,  3.41it/s] 46%|████▌     | 359/780 [02:33<02:03,  3.42it/s] 46%|████▌     | 360/780 [02:33<02:02,  3.43it/s] 46%|████▋     | 361/780 [02:33<02:01,  3.44it/s] 46%|████▋     | 362/780 [02:33<02:01,  3.43it/s] 47%|████▋     | 363/780 [02:34<02:01,  3.43it/s] 47%|████▋     | 364/780 [02:34<02:01,  3.44it/s] 47%|████▋     | 365/780 [02:34<02:01,  3.41it/s] 47%|████▋     | 366/780 [02:35<02:00,  3.42it/s] 47%|████▋     | 367/780 [02:35<02:00,  3.42it/s] 47%|████▋     | 368/780 [02:35<02:00,  3.43it/s] 47%|████▋     | 369/780 [02:35<01:59,  3.43it/s] 47%|████▋     | 370/780 [02:36<01:59,  3.44it/s] 48%|████▊     | 371/780 [02:36<01:59,  3.44it/s] 48%|████▊     | 372/780 [02:36<01:58,  3.44it/s] 48%|████▊     | 373/780 [02:37<01:58,  3.44it/s] 48%|████▊     | 374/780 [02:37<01:58,  3.44it/s] 48%|████▊     | 375/780 [02:37<01:57,  3.44it/s] 48%|████▊     | 376/780 [02:37<01:57,  3.43it/s] 48%|████▊     | 377/780 [02:38<01:57,  3.43it/s] 48%|████▊     | 378/780 [02:38<01:57,  3.43it/s] 49%|████▊     | 379/780 [02:38<01:56,  3.43it/s] 49%|████▊     | 380/780 [02:39<01:56,  3.44it/s] 49%|████▉     | 381/780 [02:39<01:56,  3.43it/s] 49%|████▉     | 382/780 [02:39<01:55,  3.44it/s] 49%|████▉     | 383/780 [02:39<01:55,  3.44it/s] 49%|████▉     | 384/780 [02:40<01:55,  3.44it/s] 49%|████▉     | 385/780 [02:40<01:55,  3.43it/s] 49%|████▉     | 386/780 [02:40<01:54,  3.43it/s] 50%|████▉     | 387/780 [02:41<01:55,  3.40it/s] 50%|████▉     | 388/780 [02:41<01:54,  3.42it/s] 50%|████▉     | 389/780 [02:41<01:54,  3.42it/s] 50%|█████     | 390/780 [02:42<01:53,  3.43it/s] 50%|█████     | 391/780 [02:42<01:53,  3.43it/s] 50%|█████     | 392/780 [02:42<01:52,  3.43it/s] 50%|█████     | 393/780 [02:42<01:52,  3.43it/s] 51%|█████     | 394/780 [02:43<01:52,  3.44it/s] 51%|█████     | 395/780 [02:43<01:52,  3.44it/s] 51%|█████     | 396/780 [02:43<01:51,  3.44it/s] 51%|█████     | 397/780 [02:44<01:51,  3.44it/s] 51%|█████     | 398/780 [02:44<01:56,  3.28it/s] 51%|█████     | 399/780 [02:44<01:54,  3.32it/s] 51%|█████▏    | 400/780 [02:44<01:53,  3.36it/s] 51%|█████▏    | 401/780 [02:45<01:52,  3.38it/s] 52%|█████▏    | 402/780 [02:45<01:51,  3.40it/s] 52%|█████▏    | 403/780 [02:45<01:50,  3.41it/s] 52%|█████▏    | 404/780 [02:46<01:50,  3.42it/s] 52%|█████▏    | 405/780 [02:46<01:49,  3.42it/s] 52%|█████▏    | 406/780 [02:46<01:49,  3.43it/s] 52%|█████▏    | 407/780 [02:47<01:48,  3.43it/s] 52%|█████▏    | 408/780 [02:47<01:48,  3.42it/s] 52%|█████▏    | 409/780 [02:47<01:49,  3.40it/s] 53%|█████▎    | 410/780 [02:47<01:48,  3.41it/s] 53%|█████▎    | 411/780 [02:48<01:48,  3.42it/s] 53%|█████▎    | 412/780 [02:49<03:44,  1.64it/s] 53%|█████▎    | 413/780 [02:49<03:09,  1.94it/s] 53%|█████▎    | 414/780 [02:50<02:44,  2.23it/s] 53%|█████▎    | 415/780 [02:50<02:26,  2.49it/s] 53%|█████▎    | 416/780 [02:50<02:19,  2.61it/s] 53%|█████▎    | 417/780 [02:51<02:09,  2.81it/s] 54%|█████▎    | 418/780 [02:51<02:01,  2.98it/s] 54%|█████▎    | 419/780 [02:51<01:56,  3.10it/s] 54%|█████▍    | 420/780 [02:51<01:52,  3.20it/s] 54%|█████▍    | 421/780 [02:52<01:49,  3.27it/s] 54%|█████▍    | 422/780 [02:52<01:48,  3.31it/s] 54%|█████▍    | 423/780 [02:52<01:46,  3.35it/s] 54%|█████▍    | 424/780 [02:53<01:45,  3.38it/s] 54%|█████▍    | 425/780 [02:53<01:44,  3.40it/s] 55%|█████▍    | 426/780 [02:53<01:43,  3.41it/s] 55%|█████▍    | 427/780 [02:53<01:44,  3.39it/s] 55%|█████▍    | 428/780 [02:54<01:43,  3.41it/s] 55%|█████▌    | 429/780 [02:54<01:42,  3.42it/s] 55%|█████▌    | 430/780 [02:54<01:42,  3.43it/s] 55%|█████▌    | 431/780 [02:55<01:41,  3.43it/s] 55%|█████▌    | 432/780 [02:55<01:41,  3.43it/s] 56%|█████▌    | 433/780 [02:55<01:41,  3.43it/s] 56%|█████▌    | 434/780 [02:56<01:40,  3.43it/s] 56%|█████▌    | 435/780 [02:56<01:40,  3.43it/s] 56%|█████▌    | 436/780 [02:56<01:40,  3.44it/s] 56%|█████▌    | 437/780 [02:56<01:39,  3.44it/s] 56%|█████▌    | 438/780 [02:57<01:39,  3.44it/s] 56%|█████▋    | 439/780 [02:57<01:39,  3.44it/s] 56%|█████▋    | 440/780 [02:57<01:38,  3.44it/s] 57%|█████▋    | 441/780 [02:58<01:38,  3.44it/s] 57%|█████▋    | 442/780 [02:58<01:38,  3.44it/s] 57%|█████▋    | 443/780 [02:58<01:37,  3.44it/s] 57%|█████▋    | 444/780 [02:58<01:37,  3.43it/s] 57%|█████▋    | 445/780 [02:59<01:37,  3.43it/s] 57%|█████▋    | 446/780 [02:59<01:37,  3.43it/s] 57%|█████▋    | 447/780 [02:59<01:36,  3.43it/s] 57%|█████▋    | 448/780 [03:00<01:36,  3.44it/s] 58%|█████▊    | 449/780 [03:00<01:36,  3.44it/s] 58%|█████▊    | 450/780 [03:00<01:35,  3.44it/s] 58%|█████▊    | 451/780 [03:00<01:35,  3.44it/s] 58%|█████▊    | 452/780 [03:01<01:35,  3.44it/s] 58%|█████▊    | 453/780 [03:01<01:35,  3.43it/s] 58%|█████▊    | 454/780 [03:01<01:34,  3.43it/s] 58%|█████▊    | 455/780 [03:02<01:35,  3.40it/s] 58%|█████▊    | 456/780 [03:02<01:34,  3.41it/s] 59%|█████▊    | 457/780 [03:02<01:34,  3.42it/s] 59%|█████▊    | 458/780 [03:03<01:34,  3.42it/s] 59%|█████▉    | 459/780 [03:03<01:33,  3.43it/s] 59%|█████▉    | 460/780 [03:03<01:33,  3.43it/s] 59%|█████▉    | 461/780 [03:03<01:32,  3.43it/s] 59%|█████▉    | 462/780 [03:04<01:32,  3.43it/s] 59%|█████▉    | 463/780 [03:04<01:32,  3.43it/s] 59%|█████▉    | 464/780 [03:04<01:31,  3.44it/s] 60%|█████▉    | 465/780 [03:05<01:31,  3.43it/s] 60%|█████▉    | 466/780 [03:05<01:32,  3.40it/s] 60%|█████▉    | 467/780 [03:05<01:31,  3.41it/s] 60%|██████    | 468/780 [03:05<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 17:54:12,889 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:54:12,889 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:54:12,889 >>   Batch size = 8
{'eval_loss': 1.0728598833084106, 'eval_runtime': 9.5652, 'eval_samples_per_second': 364.027, 'eval_steps_per_second': 45.582, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.85it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.96it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.31it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.57it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.80it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.44it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.08it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.96it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.05it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.12it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.16it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.23it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.27it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.14it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.10it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.94it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.71it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.89it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.87it/s][A
 24%|██▎       | 103/436 [00:02<00:08, 41.56it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 42.86it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 43.89it/s][A
 27%|██▋       | 118/436 [00:02<00:07, 44.51it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.07it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.43it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.74it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 45.86it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.23it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.31it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.55it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.81it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.88it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.04it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.02it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.10it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.13it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.93it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.84it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.88it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.91it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.92it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.01it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.04it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.14it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.17it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.02it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.97it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 42.82it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 43.81it/s][A
 58%|█████▊    | 253/436 [00:05<00:04, 44.53it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 44.99it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.26it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.57it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.76it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.94it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.60it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.67it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.79it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.91it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.00it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.05it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.04it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.15it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 46.12it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.90it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.83it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.89it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.90it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.92it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.02it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.09it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.17it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 46.07it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.04it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.95it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 36.53it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 39.01it/s][A
 90%|█████████ | 393/436 [00:08<00:01, 40.89it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 42.41it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 43.46it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 44.20it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 44.81it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.20it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.09it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.34it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.45it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.45it/s][A 60%|██████    | 468/780 [03:15<01:31,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:54:22,644 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 17:54:22,777 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:54:29,746 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:54:29,789 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:54:29,797 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:34<45:48,  8.84s/it] 60%|██████    | 470/780 [03:35<32:26,  6.28s/it] 60%|██████    | 471/780 [03:35<23:04,  4.48s/it] 61%|██████    | 472/780 [03:35<16:33,  3.22s/it] 61%|██████    | 473/780 [03:35<11:59,  2.34s/it] 61%|██████    | 474/780 [03:36<08:48,  1.73s/it] 61%|██████    | 475/780 [03:36<06:35,  1.30s/it] 61%|██████    | 476/780 [03:36<05:02,  1.01it/s] 61%|██████    | 477/780 [03:37<03:57,  1.28it/s] 61%|██████▏   | 478/780 [03:37<03:11,  1.57it/s] 61%|██████▏   | 479/780 [03:37<02:39,  1.88it/s] 62%|██████▏   | 480/780 [03:37<02:17,  2.18it/s] 62%|██████▏   | 481/780 [03:38<02:02,  2.44it/s] 62%|██████▏   | 482/780 [03:38<01:51,  2.67it/s] 62%|██████▏   | 483/780 [03:38<01:43,  2.86it/s] 62%|██████▏   | 484/780 [03:39<01:38,  3.02it/s] 62%|██████▏   | 485/780 [03:39<01:34,  3.13it/s] 62%|██████▏   | 486/780 [03:39<01:31,  3.22it/s] 62%|██████▏   | 487/780 [03:39<01:29,  3.29it/s] 63%|██████▎   | 488/780 [03:40<01:27,  3.34it/s] 63%|██████▎   | 489/780 [03:40<01:26,  3.37it/s] 63%|██████▎   | 490/780 [03:40<01:25,  3.39it/s] 63%|██████▎   | 491/780 [03:41<01:24,  3.41it/s] 63%|██████▎   | 492/780 [03:41<01:25,  3.36it/s] 63%|██████▎   | 493/780 [03:41<01:24,  3.38it/s] 63%|██████▎   | 494/780 [03:41<01:24,  3.40it/s] 63%|██████▎   | 495/780 [03:42<01:23,  3.42it/s] 64%|██████▎   | 496/780 [03:42<01:22,  3.43it/s] 64%|██████▎   | 497/780 [03:42<01:22,  3.43it/s] 64%|██████▍   | 498/780 [03:43<01:22,  3.44it/s] 64%|██████▍   | 499/780 [03:43<01:21,  3.44it/s] 64%|██████▍   | 500/780 [03:43<01:21,  3.44it/s]                                                  64%|██████▍   | 500/780 [03:43<01:21,  3.44it/s] 64%|██████▍   | 501/780 [03:44<01:21,  3.44it/s] 64%|██████▍   | 502/780 [03:44<01:20,  3.45it/s] 64%|██████▍   | 503/780 [03:44<01:20,  3.42it/s] 65%|██████▍   | 504/780 [03:44<01:20,  3.43it/s] 65%|██████▍   | 505/780 [03:45<01:20,  3.43it/s] 65%|██████▍   | 506/780 [03:45<01:19,  3.44it/s] 65%|██████▌   | 507/780 [03:45<01:19,  3.44it/s] 65%|██████▌   | 508/780 [03:46<01:19,  3.44it/s] 65%|██████▌   | 509/780 [03:46<01:18,  3.44it/s] 65%|██████▌   | 510/780 [03:46<01:18,  3.44it/s] 66%|██████▌   | 511/780 [03:46<01:18,  3.44it/s] 66%|██████▌   | 512/780 [03:47<01:17,  3.44it/s] 66%|██████▌   | 513/780 [03:47<01:17,  3.44it/s] 66%|██████▌   | 514/780 [03:47<01:17,  3.43it/s] 66%|██████▌   | 515/780 [03:48<01:17,  3.43it/s] 66%|██████▌   | 516/780 [03:48<01:16,  3.44it/s] 66%|██████▋   | 517/780 [03:48<01:16,  3.44it/s] 66%|██████▋   | 518/780 [03:48<01:16,  3.44it/s] 67%|██████▋   | 519/780 [03:49<01:15,  3.45it/s] 67%|██████▋   | 520/780 [03:49<01:16,  3.39it/s] 67%|██████▋   | 521/780 [03:49<01:15,  3.41it/s] 67%|██████▋   | 522/780 [03:50<01:15,  3.42it/s] 67%|██████▋   | 523/780 [03:50<01:15,  3.42it/s] 67%|██████▋   | 524/780 [03:50<01:14,  3.43it/s] 67%|██████▋   | 525/780 [03:51<01:17,  3.29it/s] 67%|██████▋   | 526/780 [03:51<01:16,  3.34it/s] 68%|██████▊   | 527/780 [03:51<01:15,  3.37it/s] 68%|██████▊   | 528/780 [03:51<01:14,  3.39it/s] 68%|██████▊   | 529/780 [03:52<01:13,  3.40it/s] 68%|██████▊   | 530/780 [03:52<01:13,  3.41it/s] 68%|██████▊   | 531/780 [03:52<01:12,  3.42it/s] 68%|██████▊   | 532/780 [03:53<01:12,  3.43it/s] 68%|██████▊   | 533/780 [03:53<01:11,  3.43it/s] 68%|██████▊   | 534/780 [03:53<01:11,  3.44it/s] 69%|██████▊   | 535/780 [03:53<01:11,  3.44it/s] 69%|██████▊   | 536/780 [03:54<01:11,  3.42it/s] 69%|██████▉   | 537/780 [03:54<01:10,  3.42it/s] 69%|██████▉   | 538/780 [03:54<01:10,  3.43it/s] 69%|██████▉   | 539/780 [03:55<01:10,  3.43it/s] 69%|██████▉   | 540/780 [03:55<01:10,  3.43it/s] 69%|██████▉   | 541/780 [03:55<01:09,  3.43it/s] 69%|██████▉   | 542/780 [03:56<01:09,  3.43it/s] 70%|██████▉   | 543/780 [03:56<01:08,  3.44it/s] 70%|██████▉   | 544/780 [03:56<01:08,  3.44it/s] 70%|██████▉   | 545/780 [03:56<01:08,  3.44it/s] 70%|███████   | 546/780 [03:57<01:07,  3.44it/s] 70%|███████   | 547/780 [03:57<01:07,  3.44it/s] 70%|███████   | 548/780 [03:57<01:07,  3.44it/s] 70%|███████   | 549/780 [03:58<01:07,  3.44it/s] 71%|███████   | 550/780 [03:58<01:06,  3.44it/s] 71%|███████   | 551/780 [03:58<01:06,  3.44it/s] 71%|███████   | 552/780 [03:58<01:06,  3.44it/s] 71%|███████   | 553/780 [03:59<01:05,  3.44it/s] 71%|███████   | 554/780 [03:59<01:05,  3.44it/s] 71%|███████   | 555/780 [03:59<01:05,  3.44it/s] 71%|███████▏  | 556/780 [04:00<01:05,  3.44it/s] 71%|███████▏  | 557/780 [04:00<01:07,  3.29it/s] 72%|███████▏  | 558/780 [04:00<01:06,  3.34it/s] 72%|███████▏  | 559/780 [04:00<01:05,  3.37it/s] 72%|███████▏  | 560/780 [04:01<01:04,  3.39it/s] 72%|███████▏  | 561/780 [04:01<01:04,  3.40it/s] 72%|███████▏  | 562/780 [04:01<01:03,  3.42it/s] 72%|███████▏  | 563/780 [04:02<01:03,  3.42it/s] 72%|███████▏  | 564/780 [04:02<01:03,  3.43it/s] 72%|███████▏  | 565/780 [04:02<01:02,  3.43it/s] 73%|███████▎  | 566/780 [04:03<01:02,  3.44it/s] 73%|███████▎  | 567/780 [04:03<01:01,  3.44it/s] 73%|███████▎  | 568/780 [04:03<01:22,  2.56it/s] 73%|███████▎  | 569/780 [04:04<01:16,  2.77it/s] 73%|███████▎  | 570/780 [04:04<01:11,  2.94it/s] 73%|███████▎  | 571/780 [04:04<01:08,  3.07it/s] 73%|███████▎  | 572/780 [04:05<01:05,  3.17it/s] 73%|███████▎  | 573/780 [04:05<01:03,  3.25it/s] 74%|███████▎  | 574/780 [04:05<01:02,  3.30it/s] 74%|███████▎  | 575/780 [04:05<01:01,  3.34it/s] 74%|███████▍  | 576/780 [04:06<01:00,  3.37it/s] 74%|███████▍  | 577/780 [04:06<00:59,  3.39it/s] 74%|███████▍  | 578/780 [04:07<01:11,  2.84it/s] 74%|███████▍  | 579/780 [04:07<01:07,  3.00it/s] 74%|███████▍  | 580/780 [04:07<01:04,  3.12it/s] 74%|███████▍  | 581/780 [04:07<01:01,  3.21it/s] 75%|███████▍  | 582/780 [04:08<01:00,  3.28it/s] 75%|███████▍  | 583/780 [04:08<00:59,  3.32it/s] 75%|███████▍  | 584/780 [04:08<00:58,  3.36it/s] 75%|███████▌  | 585/780 [04:09<00:57,  3.39it/s] 75%|███████▌  | 586/780 [04:09<00:57,  3.40it/s] 75%|███████▌  | 587/780 [04:09<00:56,  3.41it/s] 75%|███████▌  | 588/780 [04:10<01:06,  2.90it/s] 76%|███████▌  | 589/780 [04:10<01:02,  3.04it/s] 76%|███████▌  | 590/780 [04:10<01:00,  3.15it/s] 76%|███████▌  | 591/780 [04:11<00:58,  3.23it/s] 76%|███████▌  | 592/780 [04:11<00:57,  3.29it/s] 76%|███████▌  | 593/780 [04:11<00:56,  3.33it/s] 76%|███████▌  | 594/780 [04:11<00:55,  3.36it/s] 76%|███████▋  | 595/780 [04:12<00:54,  3.38it/s] 76%|███████▋  | 596/780 [04:12<00:54,  3.40it/s] 77%|███████▋  | 597/780 [04:12<00:53,  3.41it/s] 77%|███████▋  | 598/780 [04:13<00:54,  3.35it/s] 77%|███████▋  | 599/780 [04:13<00:53,  3.37it/s] 77%|███████▋  | 600/780 [04:13<00:53,  3.39it/s] 77%|███████▋  | 601/780 [04:13<00:52,  3.40it/s] 77%|███████▋  | 602/780 [04:14<00:52,  3.41it/s] 77%|███████▋  | 603/780 [04:14<00:51,  3.41it/s] 77%|███████▋  | 604/780 [04:14<00:51,  3.42it/s] 78%|███████▊  | 605/780 [04:15<00:51,  3.42it/s] 78%|███████▊  | 606/780 [04:15<00:50,  3.43it/s] 78%|███████▊  | 607/780 [04:15<00:50,  3.43it/s] 78%|███████▊  | 608/780 [04:15<00:50,  3.43it/s] 78%|███████▊  | 609/780 [04:16<00:54,  3.13it/s] 78%|███████▊  | 610/780 [04:16<00:52,  3.22it/s] 78%|███████▊  | 611/780 [04:16<00:51,  3.28it/s] 78%|███████▊  | 612/780 [04:17<00:50,  3.33it/s] 79%|███████▊  | 613/780 [04:17<00:49,  3.36it/s] 79%|███████▊  | 614/780 [04:17<00:49,  3.38it/s] 79%|███████▉  | 615/780 [04:18<00:48,  3.40it/s] 79%|███████▉  | 616/780 [04:18<00:48,  3.41it/s] 79%|███████▉  | 617/780 [04:18<00:47,  3.41it/s] 79%|███████▉  | 618/780 [04:18<00:47,  3.42it/s] 79%|███████▉  | 619/780 [04:19<00:51,  3.11it/s] 79%|███████▉  | 620/780 [04:19<00:49,  3.20it/s] 80%|███████▉  | 621/780 [04:19<00:48,  3.27it/s] 80%|███████▉  | 622/780 [04:20<00:47,  3.32it/s] 80%|███████▉  | 623/780 [04:20<00:46,  3.35it/s] 80%|████████  | 624/780 [04:20<00:46,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 17:55:27,780 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:55:27,780 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:55:27,780 >>   Batch size = 8
{'eval_loss': 1.0858168601989746, 'eval_runtime': 9.6723, 'eval_samples_per_second': 359.998, 'eval_steps_per_second': 45.077, 'epoch': 3.0}
{'loss': 0.6562, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.70it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.20it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.32it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.53it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.10it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.77it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.32it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.12it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.11it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.12it/s][A
 13%|█▎        | 58/436 [00:01<00:09, 40.86it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 42.44it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 43.52it/s][A
 17%|█▋        | 73/436 [00:01<00:08, 44.22it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 44.90it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.25it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.62it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.79it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.29it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.49it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.68it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.86it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.00it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.03it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.08it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.21it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 46.06it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.84it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.79it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.74it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.99it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.13it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.22it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.23it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.26it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.11it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.99it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.74it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.80it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.96it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.96it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.15it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.19it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.21it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.10it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.02it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.86it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.01it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.08it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.01it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.08it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.17it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.23it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.06it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.06it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.97it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.98it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.02it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.06it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.01it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.15it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.18it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 39.66it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 41.43it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 42.74it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 43.73it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 44.52it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.07it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.39it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.57it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.33it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.50it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.67it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.87it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.02it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.07it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.13it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.20it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.15it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.92it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 45.87it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.84it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.03it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.14it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.07it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [04:30<00:46,  3.37it/s]
100%|██████████| 436/436 [00:09<00:00, 46.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:55:37,421 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 17:55:37,446 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:55:40,965 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:55:40,989 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:55:41,003 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:42<17:06,  6.62s/it] 80%|████████  | 626/780 [04:42<12:07,  4.72s/it] 80%|████████  | 627/780 [04:42<08:39,  3.39s/it] 81%|████████  | 628/780 [04:43<06:14,  2.46s/it] 81%|████████  | 629/780 [04:43<04:33,  1.81s/it] 81%|████████  | 630/780 [04:43<03:23,  1.35s/it] 81%|████████  | 631/780 [04:43<02:34,  1.04s/it] 81%|████████  | 632/780 [04:44<02:00,  1.23it/s] 81%|████████  | 633/780 [04:44<01:36,  1.53it/s] 81%|████████▏ | 634/780 [04:44<01:19,  1.83it/s] 81%|████████▏ | 635/780 [04:45<01:08,  2.13it/s] 82%|████████▏ | 636/780 [04:45<00:59,  2.41it/s] 82%|████████▏ | 637/780 [04:45<00:56,  2.54it/s] 82%|████████▏ | 638/780 [04:46<00:51,  2.76it/s] 82%|████████▏ | 639/780 [04:46<00:48,  2.93it/s] 82%|████████▏ | 640/780 [04:46<00:45,  3.05it/s] 82%|████████▏ | 641/780 [04:46<00:44,  3.16it/s] 82%|████████▏ | 642/780 [04:47<00:42,  3.24it/s] 82%|████████▏ | 643/780 [04:47<00:41,  3.30it/s] 83%|████████▎ | 644/780 [04:47<00:40,  3.34it/s] 83%|████████▎ | 645/780 [04:48<00:40,  3.37it/s] 83%|████████▎ | 646/780 [04:48<00:39,  3.39it/s] 83%|████████▎ | 647/780 [04:49<01:07,  1.97it/s] 83%|████████▎ | 648/780 [04:49<01:01,  2.15it/s] 83%|████████▎ | 649/780 [04:50<00:54,  2.42it/s] 83%|████████▎ | 650/780 [04:50<00:48,  2.66it/s] 83%|████████▎ | 651/780 [04:50<00:45,  2.85it/s] 84%|████████▎ | 652/780 [04:50<00:42,  3.00it/s] 84%|████████▎ | 653/780 [04:51<00:40,  3.12it/s] 84%|████████▍ | 654/780 [04:51<00:39,  3.21it/s] 84%|████████▍ | 655/780 [04:51<00:38,  3.27it/s] 84%|████████▍ | 656/780 [04:52<00:37,  3.32it/s] 84%|████████▍ | 657/780 [04:52<00:36,  3.35it/s] 84%|████████▍ | 658/780 [04:52<00:36,  3.38it/s] 84%|████████▍ | 659/780 [04:52<00:35,  3.37it/s] 85%|████████▍ | 660/780 [04:53<00:35,  3.39it/s] 85%|████████▍ | 661/780 [04:53<00:34,  3.41it/s] 85%|████████▍ | 662/780 [04:53<00:34,  3.42it/s] 85%|████████▌ | 663/780 [04:54<00:34,  3.43it/s] 85%|████████▌ | 664/780 [04:54<00:33,  3.43it/s] 85%|████████▌ | 665/780 [04:54<00:33,  3.44it/s] 85%|████████▌ | 666/780 [04:54<00:33,  3.44it/s] 86%|████████▌ | 667/780 [04:55<00:32,  3.44it/s] 86%|████████▌ | 668/780 [04:55<00:32,  3.44it/s] 86%|████████▌ | 669/780 [04:55<00:32,  3.45it/s] 86%|████████▌ | 670/780 [04:56<00:36,  2.97it/s] 86%|████████▌ | 671/780 [04:56<00:35,  3.10it/s] 86%|████████▌ | 672/780 [04:56<00:33,  3.20it/s] 86%|████████▋ | 673/780 [04:57<00:32,  3.27it/s] 86%|████████▋ | 674/780 [04:57<00:31,  3.32it/s] 87%|████████▋ | 675/780 [04:57<00:31,  3.36it/s] 87%|████████▋ | 676/780 [04:58<00:30,  3.38it/s] 87%|████████▋ | 677/780 [04:58<00:30,  3.40it/s] 87%|████████▋ | 678/780 [04:58<00:29,  3.41it/s] 87%|████████▋ | 679/780 [04:58<00:29,  3.42it/s] 87%|████████▋ | 680/780 [04:59<00:29,  3.43it/s] 87%|████████▋ | 681/780 [04:59<00:28,  3.43it/s] 87%|████████▋ | 682/780 [04:59<00:28,  3.44it/s] 88%|████████▊ | 683/780 [05:00<00:28,  3.44it/s] 88%|████████▊ | 684/780 [05:00<00:27,  3.44it/s] 88%|████████▊ | 685/780 [05:00<00:27,  3.44it/s] 88%|████████▊ | 686/780 [05:00<00:27,  3.44it/s] 88%|████████▊ | 687/780 [05:01<00:27,  3.44it/s] 88%|████████▊ | 688/780 [05:01<00:26,  3.44it/s] 88%|████████▊ | 689/780 [05:01<00:26,  3.43it/s] 88%|████████▊ | 690/780 [05:02<00:26,  3.43it/s] 89%|████████▊ | 691/780 [05:02<00:25,  3.44it/s] 89%|████████▊ | 692/780 [05:02<00:25,  3.44it/s] 89%|████████▉ | 693/780 [05:02<00:25,  3.44it/s] 89%|████████▉ | 694/780 [05:03<00:25,  3.44it/s] 89%|████████▉ | 695/780 [05:03<00:24,  3.44it/s] 89%|████████▉ | 696/780 [05:03<00:24,  3.44it/s] 89%|████████▉ | 697/780 [05:04<00:24,  3.44it/s] 89%|████████▉ | 698/780 [05:04<00:23,  3.44it/s] 90%|████████▉ | 699/780 [05:04<00:23,  3.44it/s] 90%|████████▉ | 700/780 [05:05<00:24,  3.29it/s] 90%|████████▉ | 701/780 [05:05<00:23,  3.33it/s] 90%|█████████ | 702/780 [05:05<00:23,  3.36it/s] 90%|█████████ | 703/780 [05:05<00:22,  3.39it/s] 90%|█████████ | 704/780 [05:06<00:22,  3.40it/s] 90%|█████████ | 705/780 [05:06<00:21,  3.41it/s] 91%|█████████ | 706/780 [05:06<00:21,  3.42it/s] 91%|█████████ | 707/780 [05:07<00:21,  3.42it/s] 91%|█████████ | 708/780 [05:07<00:21,  3.43it/s] 91%|█████████ | 709/780 [05:07<00:20,  3.43it/s] 91%|█████████ | 710/780 [05:07<00:20,  3.43it/s] 91%|█████████ | 711/780 [05:08<00:20,  3.29it/s] 91%|█████████▏| 712/780 [05:08<00:20,  3.33it/s] 91%|█████████▏| 713/780 [05:08<00:19,  3.36it/s] 92%|█████████▏| 714/780 [05:09<00:19,  3.38it/s] 92%|█████████▏| 715/780 [05:09<00:19,  3.40it/s] 92%|█████████▏| 716/780 [05:09<00:18,  3.41it/s] 92%|█████████▏| 717/780 [05:10<00:18,  3.42it/s] 92%|█████████▏| 718/780 [05:10<00:18,  3.43it/s] 92%|█████████▏| 719/780 [05:10<00:17,  3.43it/s] 92%|█████████▏| 720/780 [05:10<00:17,  3.44it/s] 92%|█████████▏| 721/780 [05:11<00:17,  3.44it/s] 93%|█████████▎| 722/780 [05:11<00:17,  3.27it/s] 93%|█████████▎| 723/780 [05:11<00:17,  3.32it/s] 93%|█████████▎| 724/780 [05:12<00:16,  3.35it/s] 93%|█████████▎| 725/780 [05:12<00:16,  3.37it/s] 93%|█████████▎| 726/780 [05:12<00:15,  3.39it/s] 93%|█████████▎| 727/780 [05:13<00:15,  3.40it/s] 93%|█████████▎| 728/780 [05:13<00:15,  3.41it/s] 93%|█████████▎| 729/780 [05:13<00:14,  3.42it/s] 94%|█████████▎| 730/780 [05:13<00:14,  3.43it/s] 94%|█████████▎| 731/780 [05:14<00:14,  3.43it/s] 94%|█████████▍| 732/780 [05:14<00:13,  3.43it/s] 94%|█████████▍| 733/780 [05:15<00:17,  2.66it/s] 94%|█████████▍| 734/780 [05:15<00:16,  2.85it/s] 94%|█████████▍| 735/780 [05:15<00:14,  3.01it/s] 94%|█████████▍| 736/780 [05:15<00:14,  3.12it/s] 94%|█████████▍| 737/780 [05:16<00:13,  3.21it/s] 95%|█████████▍| 738/780 [05:16<00:12,  3.27it/s] 95%|█████████▍| 739/780 [05:16<00:12,  3.32it/s] 95%|█████████▍| 740/780 [05:17<00:11,  3.36it/s] 95%|█████████▌| 741/780 [05:17<00:11,  3.38it/s] 95%|█████████▌| 742/780 [05:17<00:11,  3.40it/s] 95%|█████████▌| 743/780 [05:18<00:17,  2.12it/s] 95%|█████████▌| 744/780 [05:18<00:15,  2.40it/s] 96%|█████████▌| 745/780 [05:19<00:13,  2.64it/s] 96%|█████████▌| 746/780 [05:19<00:11,  2.84it/s] 96%|█████████▌| 747/780 [05:19<00:11,  3.00it/s] 96%|█████████▌| 748/780 [05:19<00:10,  3.11it/s] 96%|█████████▌| 749/780 [05:20<00:09,  3.21it/s] 96%|█████████▌| 750/780 [05:20<00:09,  3.27it/s] 96%|█████████▋| 751/780 [05:20<00:08,  3.32it/s] 96%|█████████▋| 752/780 [05:21<00:08,  3.14it/s] 97%|█████████▋| 753/780 [05:21<00:08,  3.22it/s] 97%|█████████▋| 754/780 [05:21<00:07,  3.28it/s] 97%|█████████▋| 755/780 [05:22<00:07,  3.33it/s] 97%|█████████▋| 756/780 [05:22<00:07,  3.36it/s] 97%|█████████▋| 757/780 [05:22<00:06,  3.39it/s] 97%|█████████▋| 758/780 [05:22<00:06,  3.40it/s] 97%|█████████▋| 759/780 [05:23<00:06,  3.42it/s] 97%|█████████▋| 760/780 [05:23<00:05,  3.43it/s] 98%|█████████▊| 761/780 [05:23<00:05,  3.43it/s] 98%|█████████▊| 762/780 [05:24<00:05,  3.43it/s] 98%|█████████▊| 763/780 [05:24<00:05,  3.21it/s] 98%|█████████▊| 764/780 [05:24<00:04,  3.28it/s] 98%|█████████▊| 765/780 [05:25<00:04,  3.32it/s] 98%|█████████▊| 766/780 [05:25<00:04,  3.35it/s] 98%|█████████▊| 767/780 [05:25<00:03,  3.38it/s] 98%|█████████▊| 768/780 [05:25<00:03,  3.39it/s] 99%|█████████▊| 769/780 [05:26<00:03,  3.40it/s] 99%|█████████▊| 770/780 [05:26<00:02,  3.41it/s] 99%|█████████▉| 771/780 [05:26<00:02,  3.42it/s] 99%|█████████▉| 772/780 [05:27<00:02,  3.42it/s] 99%|█████████▉| 773/780 [05:27<00:02,  3.43it/s] 99%|█████████▉| 774/780 [05:27<00:01,  3.23it/s] 99%|█████████▉| 775/780 [05:28<00:01,  3.29it/s] 99%|█████████▉| 776/780 [05:28<00:01,  3.33it/s]100%|█████████▉| 777/780 [05:28<00:00,  3.36it/s]100%|█████████▉| 778/780 [05:28<00:00,  3.39it/s]100%|█████████▉| 779/780 [05:29<00:00,  3.40it/s]100%|██████████| 780/780 [05:29<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 17:56:36,401 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:56:36,401 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:56:36,401 >>   Batch size = 8
{'eval_loss': 1.0978662967681885, 'eval_runtime': 9.6085, 'eval_samples_per_second': 362.388, 'eval_steps_per_second': 45.377, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.84it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.13it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.27it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.50it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.98it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.54it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.13it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.96it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.95it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.05it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.19it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.20it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.11it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.20it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.14it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.05it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.94it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.85it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.97it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.06it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.10it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.18it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.15it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.08it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.07it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.99it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.98it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.97it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.95it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.02it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.09it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.17it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.16it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.01it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.02it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.97it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.86it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.92it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.94it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.00it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.13it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.15it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.07it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.09it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.90it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.98it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.98it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.02it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.98it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.08it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.13it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.13it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.09it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.95it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.02it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.92it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.99it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.04it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.09it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.07it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.13it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.99it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.98it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.01it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.94it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.97it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.03it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.93it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.06it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.09it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.10it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.04it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.03it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.92it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.00it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.01it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.98it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.03it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.97it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.01it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.02it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.00it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.96it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.90it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.03it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.07it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.07it/s][A100%|██████████| 780/780 [05:38<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:56:45,926 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 17:56:45,953 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:56:50,497 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:56:50,522 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:56:50,532 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:57:00,509 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:57:00,512 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156 (score: 1.0518537759780884).
                                                 100%|██████████| 780/780 [05:56<00:00,  3.42it/s]100%|██████████| 780/780 [05:56<00:00,  2.19it/s]
[INFO|trainer.py:1894] 2023-08-28 17:57:03,826 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 17:57:03,849 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:57:07,333 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:57:07,348 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:57:07,357 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:57:07,549 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:07,549 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:07,549 >>   train_loss               =     0.6444
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:07,549 >>   train_runtime            = 0:05:56.91
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:07,549 >>   train_samples            =      10006
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:07,549 >>   train_samples_per_second =    140.175
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:07,549 >>   train_steps_per_second   =      2.185
{'eval_loss': 1.1008647680282593, 'eval_runtime': 9.4648, 'eval_samples_per_second': 367.888, 'eval_steps_per_second': 46.065, 'epoch': 5.0}
{'train_runtime': 356.9106, 'train_samples_per_second': 140.175, 'train_steps_per_second': 2.185, 'train_loss': 0.6443550305488782, 'epoch': 5.0}
08/28/2023 17:57:07 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:57:07,591 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:57:07,591 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 17:57:07,591 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 58.33it/s]  3%|▎         | 12/436 [00:00<00:08, 50.95it/s]  4%|▍         | 18/436 [00:00<00:08, 48.82it/s]  5%|▌         | 23/436 [00:00<00:08, 48.08it/s]  6%|▋         | 28/436 [00:00<00:08, 47.51it/s]  8%|▊         | 33/436 [00:00<00:08, 47.19it/s]  9%|▊         | 38/436 [00:00<00:08, 47.03it/s] 10%|▉         | 43/436 [00:00<00:08, 46.72it/s] 11%|█         | 48/436 [00:01<00:08, 46.46it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.38it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.49it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.42it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.44it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.50it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.44it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.39it/s] 20%|██        | 88/436 [00:01<00:07, 46.41it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.36it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.42it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.42it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.35it/s] 26%|██▌       | 113/436 [00:02<00:07, 43.61it/s] 27%|██▋       | 118/436 [00:02<00:07, 44.41it/s] 28%|██▊       | 123/436 [00:02<00:06, 45.08it/s] 29%|██▉       | 128/436 [00:02<00:06, 45.48it/s] 31%|███       | 133/436 [00:02<00:06, 45.75it/s] 32%|███▏      | 138/436 [00:02<00:06, 45.92it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.09it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.18it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.10it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.13it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.17it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.10it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.25it/s] 41%|████      | 178/436 [00:03<00:05, 46.29it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.34it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.42it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.31it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.29it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.24it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.24it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.31it/s] 50%|█████     | 218/436 [00:04<00:04, 46.34it/s] 51%|█████     | 223/436 [00:04<00:04, 46.34it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.30it/s] 53%|█████▎    | 233/436 [00:05<00:04, 46.29it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.35it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.30it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.27it/s] 58%|█████▊    | 253/436 [00:05<00:04, 45.72it/s] 59%|█████▉    | 258/436 [00:05<00:03, 45.97it/s] 60%|██████    | 263/436 [00:05<00:03, 46.05it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.18it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.26it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.17it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.25it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.18it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.18it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.22it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.20it/s] 71%|███████   | 308/436 [00:06<00:02, 46.20it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.27it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.38it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.31it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.28it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.22it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.23it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.22it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.22it/s] 81%|████████  | 353/436 [00:07<00:01, 46.20it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.26it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.21it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.28it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.28it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.27it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.23it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.04it/s] 90%|█████████ | 393/436 [00:08<00:01, 42.21it/s] 91%|█████████▏| 398/436 [00:08<00:00, 43.40it/s] 92%|█████████▏| 403/436 [00:08<00:00, 44.27it/s] 94%|█████████▎| 408/436 [00:08<00:00, 44.91it/s] 95%|█████████▍| 413/436 [00:08<00:00, 45.20it/s] 96%|█████████▌| 418/436 [00:09<00:00, 45.49it/s] 97%|█████████▋| 423/436 [00:09<00:00, 45.84it/s] 98%|█████████▊| 428/436 [00:09<00:00, 45.99it/s] 99%|█████████▉| 433/436 [00:09<00:00, 45.81it/s]100%|██████████| 436/436 [00:09<00:00, 46.14it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:57:17,065 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:17,065 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:17,065 >>   eval_loss               =     1.0519
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:17,065 >>   eval_runtime            = 0:00:09.47
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:17,065 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:17,065 >>   eval_samples_per_second =    367.539
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:17,065 >>   eval_steps_per_second   =     46.022
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:57:17,065 >>   perplexity              =      2.863
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:24,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:24,564 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:24,564 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:24,564 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:24,565 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:57:25,196 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:57:25,197 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:57:25,851 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:57:27,468 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:57:27,468 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:30,941 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:31,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:31,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:31,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:57:31,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:57:31,961 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:57:31,962 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:57:32,313 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:57:32,535 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:57:32,535 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.47it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.43it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:16,  1.52it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:18,  1.45it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:20,  1.47it/s]Extractor Predicting: 31it [00:20,  1.47it/s]Extractor Predicting: 32it [00:21,  1.47it/s]Extractor Predicting: 33it [00:22,  1.46it/s]Extractor Predicting: 34it [00:22,  1.44it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.42it/s]Extractor Predicting: 37it [00:25,  1.43it/s]Extractor Predicting: 38it [00:25,  1.44it/s]Extractor Predicting: 39it [00:26,  1.44it/s]Extractor Predicting: 40it [00:27,  1.41it/s]Extractor Predicting: 41it [00:27,  1.43it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:29,  1.45it/s]Extractor Predicting: 44it [00:29,  1.43it/s]Extractor Predicting: 45it [00:30,  1.46it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:31,  1.45it/s]Extractor Predicting: 48it [00:32,  1.44it/s]Extractor Predicting: 49it [00:33,  1.31it/s]Extractor Predicting: 50it [00:34,  1.35it/s]Extractor Predicting: 51it [00:34,  1.37it/s]Extractor Predicting: 52it [00:35,  1.40it/s]Extractor Predicting: 53it [00:36,  1.41it/s]Extractor Predicting: 54it [00:37,  1.40it/s]Extractor Predicting: 55it [00:37,  1.42it/s]Extractor Predicting: 56it [00:38,  1.42it/s]Extractor Predicting: 57it [00:39,  1.45it/s]Extractor Predicting: 58it [00:39,  1.43it/s]Extractor Predicting: 59it [00:40,  1.42it/s]Extractor Predicting: 60it [00:41,  1.42it/s]Extractor Predicting: 61it [00:41,  1.44it/s]Extractor Predicting: 62it [00:42,  1.46it/s]Extractor Predicting: 63it [00:43,  1.42it/s]Extractor Predicting: 64it [00:44,  1.41it/s]Extractor Predicting: 65it [00:44,  1.45it/s]Extractor Predicting: 66it [00:45,  1.44it/s]Extractor Predicting: 67it [00:46,  1.47it/s]Extractor Predicting: 68it [00:46,  1.47it/s]Extractor Predicting: 69it [00:47,  1.47it/s]Extractor Predicting: 70it [00:48,  1.51it/s]Extractor Predicting: 71it [00:48,  1.38it/s]Extractor Predicting: 72it [00:49,  1.42it/s]Extractor Predicting: 73it [00:50,  1.49it/s]Extractor Predicting: 74it [00:50,  1.51it/s]Extractor Predicting: 75it [00:51,  1.49it/s]Extractor Predicting: 76it [00:52,  1.45it/s]Extractor Predicting: 77it [00:52,  1.47it/s]Extractor Predicting: 78it [00:53,  1.47it/s]Extractor Predicting: 79it [00:54,  1.46it/s]Extractor Predicting: 80it [00:54,  1.46it/s]Extractor Predicting: 81it [00:55,  1.46it/s]Extractor Predicting: 82it [00:56,  1.45it/s]Extractor Predicting: 83it [00:56,  1.47it/s]Extractor Predicting: 84it [00:57,  1.44it/s]Extractor Predicting: 85it [00:58,  1.49it/s]Extractor Predicting: 86it [00:59,  1.45it/s]Extractor Predicting: 87it [00:59,  1.48it/s]Extractor Predicting: 88it [01:00,  1.45it/s]Extractor Predicting: 89it [01:01,  1.41it/s]Extractor Predicting: 90it [01:01,  1.40it/s]Extractor Predicting: 91it [01:02,  1.39it/s]Extractor Predicting: 92it [01:03,  1.40it/s]Extractor Predicting: 93it [01:03,  1.44it/s]Extractor Predicting: 94it [01:04,  1.39it/s]Extractor Predicting: 95it [01:05,  1.43it/s]Extractor Predicting: 96it [01:06,  1.44it/s]Extractor Predicting: 97it [01:06,  1.44it/s]Extractor Predicting: 98it [01:07,  1.43it/s]Extractor Predicting: 99it [01:08,  1.35it/s]Extractor Predicting: 100it [01:09,  1.34it/s]Extractor Predicting: 101it [01:09,  1.39it/s]Extractor Predicting: 102it [01:10,  1.38it/s]Extractor Predicting: 103it [01:11,  1.38it/s]Extractor Predicting: 104it [01:12,  1.15it/s]Extractor Predicting: 105it [01:13,  1.23it/s]Extractor Predicting: 106it [01:13,  1.25it/s]Extractor Predicting: 107it [01:14,  1.30it/s]Extractor Predicting: 108it [01:15,  1.29it/s]Extractor Predicting: 109it [01:15,  1.36it/s]Extractor Predicting: 110it [01:16,  1.40it/s]Extractor Predicting: 111it [01:17,  1.47it/s]Extractor Predicting: 112it [01:17,  1.46it/s]Extractor Predicting: 113it [01:18,  1.38it/s]Extractor Predicting: 114it [01:19,  1.39it/s]Extractor Predicting: 115it [01:20,  1.39it/s]Extractor Predicting: 116it [01:20,  1.41it/s]Extractor Predicting: 117it [01:21,  1.42it/s]Extractor Predicting: 118it [01:22,  1.41it/s]Extractor Predicting: 119it [01:22,  1.42it/s]Extractor Predicting: 120it [01:23,  1.46it/s]Extractor Predicting: 121it [01:24,  1.44it/s]Extractor Predicting: 122it [01:25,  1.46it/s]Extractor Predicting: 123it [01:25,  1.41it/s]Extractor Predicting: 124it [01:26,  1.43it/s]Extractor Predicting: 125it [01:27,  1.44it/s]Extractor Predicting: 126it [01:27,  1.45it/s]Extractor Predicting: 127it [01:28,  1.45it/s]Extractor Predicting: 128it [01:29,  1.44it/s]Extractor Predicting: 129it [01:29,  1.44it/s]Extractor Predicting: 130it [01:30,  1.42it/s]Extractor Predicting: 131it [01:31,  1.40it/s]Extractor Predicting: 132it [01:32,  1.43it/s]Extractor Predicting: 133it [01:32,  1.40it/s]Extractor Predicting: 134it [01:33,  1.37it/s]Extractor Predicting: 135it [01:34,  1.39it/s]Extractor Predicting: 136it [01:34,  1.39it/s]Extractor Predicting: 137it [01:35,  1.40it/s]Extractor Predicting: 138it [01:36,  1.42it/s]Extractor Predicting: 139it [01:37,  1.40it/s]Extractor Predicting: 140it [01:37,  1.39it/s]Extractor Predicting: 141it [01:38,  1.35it/s]Extractor Predicting: 142it [01:39,  1.40it/s]Extractor Predicting: 143it [01:39,  1.47it/s]Extractor Predicting: 143it [01:39,  1.43it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:34,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:34,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:34,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:34,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:34,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:59:34,927 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:59:34,928 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:59:35,507 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:59:36,550 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:59:36,550 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:39,727 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:39,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:39,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:39,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:59:39,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:59:40,413 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:59:40,415 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:59:40,982 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:59:41,133 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:59:41,133 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3770491803278688,
  "recall": 0.04623779437105112,
  "score": 0.0823740086978767,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.47it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.56it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.57it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.59it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.28it/s]Extractor Predicting: 32it [00:21,  1.32it/s]Extractor Predicting: 33it [00:21,  1.35it/s]Extractor Predicting: 34it [00:22,  1.37it/s]Extractor Predicting: 35it [00:23,  1.38it/s]Extractor Predicting: 36it [00:23,  1.40it/s]Extractor Predicting: 37it [00:24,  1.42it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:26,  1.45it/s]Extractor Predicting: 40it [00:26,  1.31it/s]Extractor Predicting: 41it [00:27,  1.36it/s]Extractor Predicting: 42it [00:28,  1.40it/s]Extractor Predicting: 43it [00:28,  1.39it/s]Extractor Predicting: 44it [00:29,  1.42it/s]Extractor Predicting: 45it [00:30,  1.41it/s]Extractor Predicting: 46it [00:31,  1.42it/s]Extractor Predicting: 47it [00:31,  1.43it/s]Extractor Predicting: 48it [00:32,  1.44it/s]Extractor Predicting: 49it [00:33,  1.44it/s]Extractor Predicting: 50it [00:33,  1.41it/s]Extractor Predicting: 51it [00:34,  1.43it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:35,  1.45it/s]Extractor Predicting: 54it [00:36,  1.45it/s]Extractor Predicting: 55it [00:37,  1.40it/s]Extractor Predicting: 56it [00:37,  1.47it/s]Extractor Predicting: 57it [00:38,  1.46it/s]Extractor Predicting: 58it [00:39,  1.47it/s]Extractor Predicting: 59it [00:40,  1.46it/s]Extractor Predicting: 60it [00:40,  1.45it/s]Extractor Predicting: 61it [00:41,  1.44it/s]Extractor Predicting: 62it [00:42,  1.46it/s]Extractor Predicting: 63it [00:42,  1.51it/s]Extractor Predicting: 64it [00:43,  1.52it/s]Extractor Predicting: 65it [00:44,  1.51it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:45,  1.49it/s]Extractor Predicting: 68it [00:46,  1.48it/s]Extractor Predicting: 69it [00:46,  1.50it/s]Extractor Predicting: 70it [00:47,  1.49it/s]Extractor Predicting: 71it [00:48,  1.48it/s]Extractor Predicting: 72it [00:48,  1.48it/s]Extractor Predicting: 73it [00:49,  1.47it/s]Extractor Predicting: 74it [00:50,  1.34it/s]Extractor Predicting: 75it [00:51,  1.39it/s]Extractor Predicting: 76it [00:51,  1.42it/s]Extractor Predicting: 77it [00:52,  1.46it/s]Extractor Predicting: 78it [00:52,  1.51it/s]Extractor Predicting: 79it [00:53,  1.51it/s]Extractor Predicting: 80it [00:54,  1.48it/s]Extractor Predicting: 81it [00:55,  1.47it/s]Extractor Predicting: 82it [00:55,  1.50it/s]Extractor Predicting: 83it [00:56,  1.48it/s]Extractor Predicting: 84it [00:57,  1.47it/s]Extractor Predicting: 85it [00:57,  1.46it/s]Extractor Predicting: 86it [00:58,  1.47it/s]Extractor Predicting: 87it [00:59,  1.50it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [01:00,  1.53it/s]Extractor Predicting: 90it [01:00,  1.53it/s]Extractor Predicting: 91it [01:01,  1.52it/s]Extractor Predicting: 92it [01:02,  1.49it/s]Extractor Predicting: 93it [01:03,  1.49it/s]Extractor Predicting: 94it [01:03,  1.47it/s]Extractor Predicting: 95it [01:04,  1.46it/s]Extractor Predicting: 96it [01:05,  1.45it/s]Extractor Predicting: 97it [01:05,  1.47it/s]Extractor Predicting: 98it [01:06,  1.45it/s]Extractor Predicting: 99it [01:07,  1.46it/s]Extractor Predicting: 100it [01:07,  1.45it/s]Extractor Predicting: 101it [01:08,  1.44it/s]Extractor Predicting: 102it [01:09,  1.47it/s]Extractor Predicting: 103it [01:09,  1.47it/s]Extractor Predicting: 104it [01:10,  1.47it/s]Extractor Predicting: 105it [01:11,  1.43it/s]Extractor Predicting: 106it [01:12,  1.44it/s]Extractor Predicting: 107it [01:12,  1.44it/s]Extractor Predicting: 108it [01:13,  1.46it/s]Extractor Predicting: 109it [01:14,  1.45it/s]Extractor Predicting: 110it [01:14,  1.44it/s]Extractor Predicting: 111it [01:15,  1.44it/s]Extractor Predicting: 112it [01:16,  1.43it/s]Extractor Predicting: 113it [01:16,  1.45it/s]Extractor Predicting: 114it [01:17,  1.45it/s]Extractor Predicting: 115it [01:18,  1.44it/s]Extractor Predicting: 116it [01:18,  1.50it/s]Extractor Predicting: 117it [01:19,  1.58it/s]Extractor Predicting: 118it [01:19,  1.61it/s]Extractor Predicting: 119it [01:20,  1.60it/s]Extractor Predicting: 120it [01:21,  1.55it/s]Extractor Predicting: 121it [01:21,  1.54it/s]Extractor Predicting: 122it [01:22,  1.53it/s]Extractor Predicting: 123it [01:23,  1.52it/s]Extractor Predicting: 124it [01:23,  1.56it/s]Extractor Predicting: 125it [01:24,  1.60it/s]Extractor Predicting: 126it [01:25,  1.59it/s]Extractor Predicting: 127it [01:25,  1.61it/s]Extractor Predicting: 128it [01:26,  1.63it/s]Extractor Predicting: 129it [01:26,  1.61it/s]Extractor Predicting: 130it [01:27,  1.59it/s]Extractor Predicting: 131it [01:28,  1.61it/s]Extractor Predicting: 132it [01:28,  1.62it/s]Extractor Predicting: 133it [01:29,  1.61it/s]Extractor Predicting: 134it [01:30,  1.62it/s]Extractor Predicting: 135it [01:30,  1.64it/s]Extractor Predicting: 136it [01:31,  1.66it/s]Extractor Predicting: 137it [01:31,  1.67it/s]Extractor Predicting: 138it [01:32,  1.63it/s]Extractor Predicting: 139it [01:33,  1.60it/s]Extractor Predicting: 140it [01:33,  1.61it/s]Extractor Predicting: 141it [01:34,  1.57it/s]Extractor Predicting: 142it [01:35,  1.57it/s]Extractor Predicting: 143it [01:35,  1.59it/s]Extractor Predicting: 144it [01:36,  1.57it/s]Extractor Predicting: 145it [01:36,  1.57it/s]Extractor Predicting: 146it [01:37,  1.55it/s]Extractor Predicting: 147it [01:38,  1.55it/s]Extractor Predicting: 148it [01:38,  1.53it/s]Extractor Predicting: 149it [01:39,  1.59it/s]Extractor Predicting: 150it [01:40,  1.57it/s]Extractor Predicting: 151it [01:40,  1.52it/s]Extractor Predicting: 152it [01:41,  1.56it/s]Extractor Predicting: 153it [01:42,  1.61it/s]Extractor Predicting: 154it [01:42,  1.62it/s]Extractor Predicting: 155it [01:43,  1.62it/s]Extractor Predicting: 156it [01:43,  1.60it/s]Extractor Predicting: 157it [01:44,  1.59it/s]Extractor Predicting: 158it [01:45,  1.60it/s]Extractor Predicting: 159it [01:45,  1.56it/s]Extractor Predicting: 160it [01:46,  1.57it/s]Extractor Predicting: 161it [01:47,  1.57it/s]Extractor Predicting: 162it [01:47,  1.56it/s]Extractor Predicting: 163it [01:48,  1.57it/s]Extractor Predicting: 164it [01:49,  1.57it/s]Extractor Predicting: 165it [01:49,  1.59it/s]Extractor Predicting: 166it [01:50,  1.58it/s]Extractor Predicting: 167it [01:50,  1.58it/s]Extractor Predicting: 168it [01:51,  1.55it/s]Extractor Predicting: 169it [01:52,  1.56it/s]Extractor Predicting: 170it [01:52,  1.53it/s]Extractor Predicting: 171it [01:53,  1.51it/s]Extractor Predicting: 172it [01:54,  1.57it/s]Extractor Predicting: 173it [01:54,  1.52it/s]Extractor Predicting: 174it [01:55,  1.50it/s]Extractor Predicting: 175it [01:56,  1.48it/s]Extractor Predicting: 176it [01:56,  1.49it/s]Extractor Predicting: 177it [01:57,  1.47it/s]Extractor Predicting: 178it [01:58,  1.46it/s]Extractor Predicting: 179it [01:58,  1.46it/s]Extractor Predicting: 180it [01:59,  1.50it/s]Extractor Predicting: 181it [02:00,  1.49it/s]Extractor Predicting: 182it [02:01,  1.47it/s]Extractor Predicting: 183it [02:01,  1.47it/s]Extractor Predicting: 184it [02:02,  1.42it/s]Extractor Predicting: 185it [02:03,  1.42it/s]Extractor Predicting: 186it [02:04,  1.28it/s]Extractor Predicting: 187it [02:04,  1.31it/s]Extractor Predicting: 188it [02:05,  1.34it/s]Extractor Predicting: 189it [02:06,  1.38it/s]Extractor Predicting: 190it [02:06,  1.41it/s]Extractor Predicting: 191it [02:07,  1.40it/s]Extractor Predicting: 192it [02:08,  1.41it/s]Extractor Predicting: 193it [02:09,  1.39it/s]Extractor Predicting: 194it [02:09,  1.40it/s]Extractor Predicting: 195it [02:10,  1.40it/s]Extractor Predicting: 196it [02:11,  1.43it/s]Extractor Predicting: 197it [02:11,  1.42it/s]Extractor Predicting: 198it [02:12,  1.42it/s]Extractor Predicting: 199it [02:13,  1.43it/s]Extractor Predicting: 200it [02:13,  1.42it/s]Extractor Predicting: 201it [02:14,  1.44it/s]Extractor Predicting: 202it [02:15,  1.44it/s]Extractor Predicting: 203it [02:16,  1.45it/s]Extractor Predicting: 204it [02:16,  1.47it/s]Extractor Predicting: 205it [02:17,  1.52it/s]Extractor Predicting: 206it [02:17,  1.51it/s]Extractor Predicting: 207it [02:18,  1.49it/s]Extractor Predicting: 208it [02:19,  1.49it/s]Extractor Predicting: 209it [02:19,  1.49it/s]Extractor Predicting: 210it [02:20,  1.50it/s]Extractor Predicting: 211it [02:21,  1.54it/s]Extractor Predicting: 212it [02:21,  1.54it/s]Extractor Predicting: 213it [02:22,  1.56it/s]Extractor Predicting: 214it [02:23,  1.54it/s]Extractor Predicting: 215it [02:23,  1.50it/s]Extractor Predicting: 216it [02:24,  1.51it/s]Extractor Predicting: 217it [02:25,  1.51it/s]Extractor Predicting: 218it [02:25,  1.54it/s]Extractor Predicting: 219it [02:26,  1.56it/s]Extractor Predicting: 220it [02:27,  1.53it/s]Extractor Predicting: 221it [02:27,  1.56it/s]Extractor Predicting: 222it [02:28,  1.57it/s]Extractor Predicting: 223it [02:29,  1.54it/s]Extractor Predicting: 224it [02:29,  1.55it/s]Extractor Predicting: 225it [02:30,  1.53it/s]Extractor Predicting: 226it [02:31,  1.49it/s]Extractor Predicting: 227it [02:31,  1.49it/s]Extractor Predicting: 228it [02:32,  1.49it/s]Extractor Predicting: 229it [02:33,  1.48it/s]Extractor Predicting: 230it [02:33,  1.47it/s]Extractor Predicting: 231it [02:34,  1.49it/s]Extractor Predicting: 232it [02:35,  1.52it/s]Extractor Predicting: 233it [02:35,  1.57it/s]Extractor Predicting: 234it [02:36,  1.59it/s]Extractor Predicting: 235it [02:36,  1.59it/s]Extractor Predicting: 236it [02:37,  1.63it/s]Extractor Predicting: 237it [02:38,  1.63it/s]Extractor Predicting: 238it [02:38,  1.64it/s]Extractor Predicting: 239it [02:39,  1.63it/s]Extractor Predicting: 240it [02:39,  1.64it/s]Extractor Predicting: 241it [02:40,  1.64it/s]Extractor Predicting: 242it [02:41,  1.64it/s]Extractor Predicting: 243it [02:41,  1.68it/s]Extractor Predicting: 244it [02:42,  1.66it/s]Extractor Predicting: 245it [02:42,  1.72it/s]Extractor Predicting: 246it [02:43,  1.75it/s]Extractor Predicting: 247it [02:44,  1.69it/s]Extractor Predicting: 248it [02:44,  1.65it/s]Extractor Predicting: 249it [02:45,  1.65it/s]Extractor Predicting: 250it [02:45,  1.65it/s]Extractor Predicting: 251it [02:46,  1.67it/s]Extractor Predicting: 252it [02:47,  1.68it/s]Extractor Predicting: 253it [02:47,  1.67it/s]Extractor Predicting: 254it [02:48,  1.65it/s]Extractor Predicting: 255it [02:48,  1.68it/s]Extractor Predicting: 256it [02:49,  1.67it/s]Extractor Predicting: 257it [02:50,  1.69it/s]Extractor Predicting: 258it [02:50,  1.69it/s]Extractor Predicting: 259it [02:51,  1.71it/s]Extractor Predicting: 260it [02:51,  1.69it/s]Extractor Predicting: 261it [02:52,  1.59it/s]Extractor Predicting: 262it [02:53,  1.59it/s]Extractor Predicting: 263it [02:53,  1.54it/s]Extractor Predicting: 264it [02:54,  1.50it/s]Extractor Predicting: 265it [02:55,  1.49it/s]Extractor Predicting: 266it [02:55,  1.50it/s]Extractor Predicting: 267it [02:56,  1.47it/s]Extractor Predicting: 268it [02:57,  1.49it/s]Extractor Predicting: 269it [02:57,  1.46it/s]Extractor Predicting: 270it [02:58,  1.43it/s]Extractor Predicting: 271it [02:59,  1.43it/s]Extractor Predicting: 272it [03:00,  1.44it/s]Extractor Predicting: 273it [03:00,  1.45it/s]Extractor Predicting: 274it [03:01,  1.45it/s]Extractor Predicting: 275it [03:02,  1.45it/s]Extractor Predicting: 276it [03:02,  1.46it/s]Extractor Predicting: 277it [03:03,  1.45it/s]Extractor Predicting: 278it [03:04,  1.45it/s]Extractor Predicting: 279it [03:04,  1.48it/s]Extractor Predicting: 280it [03:05,  1.46it/s]Extractor Predicting: 281it [03:06,  1.45it/s]Extractor Predicting: 282it [03:06,  1.44it/s]Extractor Predicting: 283it [03:07,  1.45it/s]Extractor Predicting: 284it [03:08,  1.44it/s]Extractor Predicting: 285it [03:08,  1.46it/s]Extractor Predicting: 286it [03:09,  1.45it/s]Extractor Predicting: 287it [03:10,  1.44it/s]Extractor Predicting: 288it [03:11,  1.49it/s]Extractor Predicting: 289it [03:11,  1.49it/s]Extractor Predicting: 290it [03:12,  1.51it/s]Extractor Predicting: 291it [03:12,  1.52it/s]Extractor Predicting: 292it [03:13,  1.51it/s]Extractor Predicting: 293it [03:14,  1.51it/s]Extractor Predicting: 294it [03:14,  1.53it/s]Extractor Predicting: 295it [03:15,  1.53it/s]Extractor Predicting: 296it [03:16,  1.51it/s]Extractor Predicting: 297it [03:16,  1.52it/s]Extractor Predicting: 298it [03:17,  1.56it/s]Extractor Predicting: 299it [03:18,  1.52it/s]Extractor Predicting: 300it [03:18,  1.53it/s]Extractor Predicting: 301it [03:19,  1.52it/s]Extractor Predicting: 302it [03:20,  1.46it/s]Extractor Predicting: 303it [03:20,  1.47it/s]Extractor Predicting: 304it [03:21,  1.47it/s]Extractor Predicting: 305it [03:22,  1.47it/s]Extractor Predicting: 306it [03:23,  1.31it/s]Extractor Predicting: 307it [03:23,  1.35it/s]Extractor Predicting: 308it [03:24,  1.39it/s]Extractor Predicting: 309it [03:25,  1.39it/s]Extractor Predicting: 310it [03:26,  1.42it/s]Extractor Predicting: 311it [03:26,  1.44it/s]Extractor Predicting: 312it [03:27,  1.44it/s]Extractor Predicting: 313it [03:28,  1.46it/s]Extractor Predicting: 314it [03:28,  1.46it/s]Extractor Predicting: 315it [03:29,  1.49it/s]Extractor Predicting: 316it [03:30,  1.48it/s]Extractor Predicting: 317it [03:30,  1.47it/s]Extractor Predicting: 318it [03:31,  1.51it/s]Extractor Predicting: 319it [03:32,  1.52it/s]Extractor Predicting: 320it [03:32,  1.51it/s]Extractor Predicting: 321it [03:33,  1.50it/s]Extractor Predicting: 322it [03:34,  1.50it/s]Extractor Predicting: 323it [03:34,  1.50it/s]Extractor Predicting: 324it [03:35,  1.51it/s]Extractor Predicting: 325it [03:36,  1.51it/s]Extractor Predicting: 326it [03:36,  1.52it/s]Extractor Predicting: 327it [03:37,  1.52it/s]Extractor Predicting: 328it [03:38,  1.50it/s]Extractor Predicting: 329it [03:38,  1.50it/s]Extractor Predicting: 330it [03:39,  1.50it/s]Extractor Predicting: 331it [03:40,  1.49it/s]Extractor Predicting: 332it [03:40,  1.52it/s]Extractor Predicting: 333it [03:41,  1.50it/s]Extractor Predicting: 334it [03:41,  1.52it/s]Extractor Predicting: 335it [03:42,  1.51it/s]Extractor Predicting: 336it [03:43,  1.53it/s]Extractor Predicting: 337it [03:43,  1.51it/s]Extractor Predicting: 338it [03:44,  1.52it/s]Extractor Predicting: 339it [03:45,  1.52it/s]Extractor Predicting: 340it [03:45,  1.50it/s]Extractor Predicting: 341it [03:46,  1.52it/s]Extractor Predicting: 342it [03:47,  1.54it/s]Extractor Predicting: 343it [03:47,  1.52it/s]Extractor Predicting: 344it [03:48,  1.55it/s]Extractor Predicting: 345it [03:49,  1.52it/s]Extractor Predicting: 346it [03:49,  1.52it/s]Extractor Predicting: 347it [03:50,  1.53it/s]Extractor Predicting: 348it [03:51,  1.48it/s]Extractor Predicting: 349it [03:51,  1.50it/s]Extractor Predicting: 350it [03:52,  1.48it/s]Extractor Predicting: 351it [03:53,  1.50it/s]Extractor Predicting: 352it [03:53,  1.49it/s]Extractor Predicting: 353it [03:54,  1.49it/s]Extractor Predicting: 354it [03:55,  1.50it/s]Extractor Predicting: 355it [03:55,  1.49it/s]Extractor Predicting: 356it [03:56,  1.50it/s]Extractor Predicting: 357it [03:57,  1.48it/s]Extractor Predicting: 358it [03:57,  1.47it/s]Extractor Predicting: 359it [03:58,  1.48it/s]Extractor Predicting: 360it [03:59,  1.49it/s]Extractor Predicting: 361it [03:59,  1.49it/s]Extractor Predicting: 362it [04:00,  1.53it/s]Extractor Predicting: 363it [04:01,  1.48it/s]Extractor Predicting: 364it [04:01,  1.52it/s]Extractor Predicting: 365it [04:02,  1.51it/s]Extractor Predicting: 366it [04:03,  1.52it/s]Extractor Predicting: 367it [04:03,  1.52it/s]Extractor Predicting: 368it [04:04,  1.53it/s]Extractor Predicting: 369it [04:05,  1.49it/s]Extractor Predicting: 370it [04:05,  1.49it/s]Extractor Predicting: 371it [04:06,  1.49it/s]Extractor Predicting: 372it [04:07,  1.51it/s]Extractor Predicting: 373it [04:07,  1.50it/s]Extractor Predicting: 374it [04:08,  1.50it/s]Extractor Predicting: 375it [04:09,  1.49it/s]Extractor Predicting: 376it [04:09,  1.48it/s]Extractor Predicting: 377it [04:10,  1.54it/s]Extractor Predicting: 378it [04:11,  1.54it/s]Extractor Predicting: 379it [04:11,  1.56it/s]Extractor Predicting: 380it [04:12,  1.53it/s]Extractor Predicting: 381it [04:13,  1.53it/s]Extractor Predicting: 382it [04:13,  1.52it/s]Extractor Predicting: 383it [04:14,  1.50it/s]Extractor Predicting: 384it [04:15,  1.49it/s]Extractor Predicting: 385it [04:15,  1.50it/s]Extractor Predicting: 386it [04:16,  1.47it/s]Extractor Predicting: 387it [04:17,  1.47it/s]Extractor Predicting: 388it [04:17,  1.43it/s]Extractor Predicting: 389it [04:18,  1.44it/s]Extractor Predicting: 390it [04:19,  1.45it/s]Extractor Predicting: 391it [04:19,  1.48it/s]Extractor Predicting: 392it [04:20,  1.50it/s]Extractor Predicting: 393it [04:21,  1.49it/s]Extractor Predicting: 394it [04:21,  1.49it/s]Extractor Predicting: 395it [04:22,  1.49it/s]Extractor Predicting: 396it [04:23,  1.48it/s]Extractor Predicting: 397it [04:23,  1.49it/s]Extractor Predicting: 398it [04:24,  1.47it/s]Extractor Predicting: 399it [04:25,  1.49it/s]Extractor Predicting: 400it [04:26,  1.48it/s]Extractor Predicting: 401it [04:26,  1.48it/s]Extractor Predicting: 402it [04:27,  1.48it/s]Extractor Predicting: 403it [04:28,  1.47it/s]Extractor Predicting: 404it [04:28,  1.49it/s]Extractor Predicting: 405it [04:29,  1.50it/s]Extractor Predicting: 406it [04:30,  1.34it/s]Extractor Predicting: 407it [04:30,  1.38it/s]Extractor Predicting: 408it [04:31,  1.41it/s]Extractor Predicting: 409it [04:32,  1.44it/s]Extractor Predicting: 410it [04:32,  1.46it/s]Extractor Predicting: 411it [04:33,  1.47it/s]Extractor Predicting: 412it [04:34,  1.50it/s]Extractor Predicting: 413it [04:34,  1.50it/s]Extractor Predicting: 414it [04:35,  1.50it/s]Extractor Predicting: 415it [04:36,  1.40it/s]Extractor Predicting: 416it [04:37,  1.46it/s]Extractor Predicting: 417it [04:37,  1.47it/s]Extractor Predicting: 418it [04:38,  1.51it/s]Extractor Predicting: 419it [04:38,  1.52it/s]Extractor Predicting: 420it [04:39,  1.43it/s]Extractor Predicting: 421it [04:40,  1.44it/s]Extractor Predicting: 422it [04:41,  1.47it/s]Extractor Predicting: 423it [04:41,  1.49it/s]Extractor Predicting: 424it [04:42,  1.50it/s]Extractor Predicting: 425it [04:43,  1.46it/s]Extractor Predicting: 426it [04:43,  1.51it/s]Extractor Predicting: 427it [04:44,  1.51it/s]Extractor Predicting: 428it [04:45,  1.47it/s]Extractor Predicting: 429it [04:45,  1.69it/s]Extractor Predicting: 429it [04:45,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:36,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:36,988 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:36,988 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:36,988 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:36,988 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:04:37,325 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:04:37,326 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:04:38,011 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:04:39,386 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:04:39,386 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:41,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:41,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:41,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:41,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:04:41,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:04:41,598 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:04:41,599 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:04:42,289 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:04:42,469 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:04:42,469 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3433356594556874,
  "recall": 0.09570122544252091,
  "score": 0.14968055978095526,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.44it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 5it [00:03,  1.53it/s]
[INFO|configuration_utils.py:515] 2023-08-28 18:04:47,063 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:04:47,064 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:04:47,087 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:04:47,089 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 18:04:47,107 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:04:52,945 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 18:04:52,950 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 18:04:52,965 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:04:52,966 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:04:52,975 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:04:52,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:04:52,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:04:52,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:04:52,979 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:04:52,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:04:52,979 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3125,
  "recall": 0.022935779816513763,
  "score": 0.04273504273504274,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 18:04:53,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:53,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:54,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:55,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:56,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:56,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:57,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:58,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:58,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:04:59,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:00,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:01,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:01,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:02,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:03,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:03,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:04,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:05,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:06,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:06,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:07,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:40, 14.74s/it][WARNING|generation_utils.py:914] 2023-08-28 18:05:07,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:08,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:09,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:10,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:10,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:11,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:12,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:12,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:13,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:14,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:14,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:15,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:15,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:16,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:17,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:17,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:18,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:19,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:19,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:20,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:21,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:28<04:15, 14.20s/it][WARNING|generation_utils.py:914] 2023-08-28 18:05:21,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:22,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:23,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:24,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:24,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:25,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:26,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:26,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:27,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:28,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:29,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:30,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:31,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:32,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:32,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:33,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:34,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:35,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:36,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:36,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:37,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:38,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:46<04:27, 15.71s/it][WARNING|generation_utils.py:914] 2023-08-28 18:05:39,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:40,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:40,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:41,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:42,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:42,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:43,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:44,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:45,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:46,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:46,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:47,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:48,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:49,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:49,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:50,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:51,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:52,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:52,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:53,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:01<04:07, 15.49s/it][WARNING|generation_utils.py:914] 2023-08-28 18:05:54,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:55,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:55,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:56,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:57,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:58,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:58,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:05:59,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:00,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:01,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:02,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:03,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:03,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:05,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:05,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:06,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:07,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:08,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:08,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:09,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:10,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:17<03:57, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:10,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:11,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:12,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:13,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:13,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:14,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:15,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:16,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:17,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:17,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:18,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:19,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:20,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:21,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:21,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:22,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:23,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:24,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:25,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:25,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:33<03:41, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:26,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:27,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:28,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:28,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:29,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:30,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:31,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:31,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:32,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:32,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:33,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:34,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:34,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:35,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:36,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:37,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:37,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:38,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:38,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:39,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:40,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:41,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:42,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:43,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:50<03:32, 16.34s/it][WARNING|generation_utils.py:914] 2023-08-28 18:06:44,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:44,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:45,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:46,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:46,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:47,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:48,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:48,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:49,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:50,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:50,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:51,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:52,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:53,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:54,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:54,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:55,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:56,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:56,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:57,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:58,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:58,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:06:59,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:00,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:07<03:17, 16.48s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:00,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:01,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:02,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:03,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:04,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:05,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:06,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:06,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:07,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:08,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:09,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:09,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:10,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:11,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:12,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:12,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:13,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:14,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:15,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:16,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:17,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:24<03:03, 16.72s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:18,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:18,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:19,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:20,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:21,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:21,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:22,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:23,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:24,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:25,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:26,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:27,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:27,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:28,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:29,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:30,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:31,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:32,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:32,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:33,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:34,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:41<02:47, 16.80s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:35,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:35,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:36,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:37,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:38,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:39,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:40,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:40,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:41,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:42,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:42,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:43,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:44,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:44,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:45,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:46,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:46,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:47,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:48,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:48,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:49,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:50,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:58<02:29, 16.62s/it][WARNING|generation_utils.py:914] 2023-08-28 18:07:51,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:52,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:52,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:53,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:54,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:54,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:55,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:56,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:57,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:57,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:58,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:07:59,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:00,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:00,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:01,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:02,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:02,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:03,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:04,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:05,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:05,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:06,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:07,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:14<02:12, 16.62s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:07,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:08,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:09,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:10,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:10,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:11,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:12,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:13,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:14,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:15,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:16,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:17,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:18,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:19,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:19,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:20,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:21,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:21,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:29<01:52, 16.08s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:22,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:23,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:24,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:25,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:25,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:26,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:27,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:28,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:28,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:29,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:30,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:31,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:32,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:32,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:33,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:34,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:35,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:36,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:37,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:37,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:45<01:35, 16.00s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:38,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:39,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:39,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:40,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:41,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:42,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:42,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:43,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:44,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:45,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:45,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:46,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:47,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:47,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:48,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:49,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:50,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:51,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:51,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:52,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:53,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:54,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:01<01:20, 16.13s/it][WARNING|generation_utils.py:914] 2023-08-28 18:08:55,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:55,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:56,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:58,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:58,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:08:59,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:00,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:01,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:02,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:02,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:03,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:04,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:04,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:05,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:06,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:07,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:07,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:08,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:09,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:10,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:11,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:11,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:19<01:06, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:12,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:13,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:13,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:14,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:15,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:16,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:16,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:17,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:18,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:18,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:19,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:20,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:21,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:21,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:22,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:23,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:24,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:25,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:26,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:27,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:34<00:48, 16.28s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:28,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:28,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:29,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:30,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:30,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:31,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:32,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:32,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:33,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:34,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:35,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:35,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:36,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:37,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:37,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:38,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:39,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:40,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:40,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:41,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:48<00:31, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:42,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:42,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:43,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:44,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:44,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:45,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:46,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:46,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:47,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:48,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:48,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:49,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:50,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:50,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:51,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:52,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:52,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:53,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:54,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:54,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:55,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:56,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:03<00:15, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-28 18:09:57,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:57,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:58,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:09:59,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:00,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:00,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:01,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:02,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:03,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:04,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:04,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:06,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:06,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:07,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:08,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:08,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:09,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:10,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:11,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:11,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 18:10:12,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:20<00:00, 15.70s/it]Generating: 100%|██████████| 20/20 [05:20<00:00, 16.01s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:21,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:21,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:21,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:21,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:21,421 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:10:22,057 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:10:22,058 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:10:22,716 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:10:23,820 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:10:23,820 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:26,777 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:26,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:26,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:26,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:10:26,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:10:27,447 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:10:27,448 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:10:28,023 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:10:28,190 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:10:28,191 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : head of government .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 480, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 553, 'raw': 704}
{'target': 600, 'success': 581, 'raw': 736}
{'target': 600, 'success': 607, 'raw': 768}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.7903645833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : has part .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.8835227272727273, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8491847826086957, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 631, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8963068181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
["Relation : place served by transport hub . Context : The city 's most important railway station , the Gander railway station , forms part of the station 's main line . Head Entity : Gander railway station , Tail Entity : Gander rail station .\n"]
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.95, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : winner .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : work location .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 11416
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11516, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.66it/s]Extractor Estimating: 2it [00:01,  1.51it/s]Extractor Estimating: 3it [00:01,  1.62it/s]Extractor Estimating: 4it [00:02,  1.62it/s]Extractor Estimating: 5it [00:03,  1.66it/s]Extractor Estimating: 6it [00:03,  1.63it/s]Extractor Estimating: 7it [00:04,  1.71it/s]Extractor Estimating: 8it [00:04,  1.66it/s]Extractor Estimating: 9it [00:05,  1.69it/s]Extractor Estimating: 10it [00:06,  1.63it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.70it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.74it/s]Extractor Estimating: 15it [00:08,  1.69it/s]Extractor Estimating: 16it [00:09,  1.66it/s]Extractor Estimating: 17it [00:10,  1.61it/s]Extractor Estimating: 18it [00:10,  1.66it/s]Extractor Estimating: 19it [00:11,  1.64it/s]Extractor Estimating: 20it [00:11,  1.75it/s]Extractor Estimating: 21it [00:12,  1.73it/s]Extractor Estimating: 22it [00:13,  1.75it/s]Extractor Estimating: 23it [00:13,  1.77it/s]Extractor Estimating: 24it [00:14,  1.72it/s]Extractor Estimating: 25it [00:14,  1.78it/s]Extractor Estimating: 26it [00:15,  1.73it/s]Extractor Estimating: 27it [00:16,  1.68it/s]Extractor Estimating: 28it [00:16,  1.71it/s]Extractor Estimating: 29it [00:17,  1.72it/s]Extractor Estimating: 30it [00:17,  1.79it/s]Extractor Estimating: 31it [00:18,  1.64it/s]Extractor Estimating: 32it [00:19,  1.65it/s]Extractor Estimating: 33it [00:19,  1.68it/s]Extractor Estimating: 34it [00:20,  1.70it/s]Extractor Estimating: 35it [00:20,  1.68it/s]Extractor Estimating: 36it [00:21,  1.67it/s]Extractor Estimating: 37it [00:21,  1.71it/s]Extractor Estimating: 38it [00:22,  1.72it/s]Extractor Estimating: 39it [00:23,  1.75it/s]Extractor Estimating: 40it [00:23,  1.81it/s]Extractor Estimating: 41it [00:24,  1.77it/s]Extractor Estimating: 42it [00:24,  1.79it/s]Extractor Estimating: 43it [00:25,  1.72it/s]Extractor Estimating: 44it [00:25,  1.72it/s]Extractor Estimating: 45it [00:26,  1.70it/s]Extractor Estimating: 46it [00:27,  1.71it/s]Extractor Estimating: 47it [00:27,  1.76it/s]Extractor Estimating: 48it [00:28,  1.71it/s]Extractor Estimating: 49it [00:28,  1.70it/s]Extractor Estimating: 50it [00:29,  1.71it/s]Extractor Estimating: 51it [00:30,  1.67it/s]Extractor Estimating: 52it [00:30,  1.67it/s]Extractor Estimating: 53it [00:31,  1.65it/s]Extractor Estimating: 54it [00:31,  1.64it/s]Extractor Estimating: 55it [00:32,  1.66it/s]Extractor Estimating: 56it [00:32,  1.71it/s]Extractor Estimating: 57it [00:33,  1.69it/s]Extractor Estimating: 58it [00:34,  1.68it/s]Extractor Estimating: 59it [00:34,  1.62it/s]Extractor Estimating: 60it [00:35,  1.64it/s]Extractor Estimating: 61it [00:36,  1.62it/s]Extractor Estimating: 62it [00:36,  1.54it/s]Extractor Estimating: 63it [00:37,  1.56it/s]Extractor Estimating: 64it [00:38,  1.59it/s]Extractor Estimating: 65it [00:38,  1.61it/s]Extractor Estimating: 66it [00:39,  1.57it/s]Extractor Estimating: 67it [00:39,  1.58it/s]Extractor Estimating: 68it [00:40,  1.60it/s]Extractor Estimating: 69it [00:41,  1.58it/s]Extractor Estimating: 70it [00:42,  1.46it/s]Extractor Estimating: 71it [00:42,  1.51it/s]Extractor Estimating: 72it [00:43,  1.54it/s]Extractor Estimating: 73it [00:43,  1.56it/s]Extractor Estimating: 74it [00:44,  1.50it/s]Extractor Estimating: 75it [00:45,  1.45it/s]Extractor Estimating: 76it [00:45,  1.49it/s]Extractor Estimating: 77it [00:46,  1.48it/s]Extractor Estimating: 78it [00:47,  1.48it/s]Extractor Estimating: 79it [00:47,  1.49it/s]Extractor Estimating: 80it [00:48,  1.50it/s]Extractor Estimating: 81it [00:49,  1.50it/s]Extractor Estimating: 82it [00:49,  1.51it/s]Extractor Estimating: 83it [00:50,  1.52it/s]Extractor Estimating: 84it [00:51,  1.52it/s]Extractor Estimating: 85it [00:51,  1.56it/s]Extractor Estimating: 86it [00:52,  1.51it/s]Extractor Estimating: 87it [00:53,  1.55it/s]Extractor Estimating: 88it [00:53,  1.59it/s]Extractor Estimating: 89it [00:54,  1.58it/s]Extractor Estimating: 90it [00:55,  1.59it/s]Extractor Estimating: 91it [00:55,  1.53it/s]Extractor Estimating: 92it [00:56,  1.45it/s]Extractor Estimating: 93it [00:57,  1.50it/s]Extractor Estimating: 94it [00:57,  1.49it/s]Extractor Estimating: 95it [00:58,  1.52it/s]Extractor Estimating: 96it [00:59,  1.53it/s]Extractor Estimating: 97it [00:59,  1.57it/s]Extractor Estimating: 98it [01:00,  1.54it/s]Extractor Estimating: 99it [01:00,  1.55it/s]Extractor Estimating: 100it [01:01,  1.51it/s]Extractor Estimating: 101it [01:02,  1.56it/s]Extractor Estimating: 102it [01:02,  1.59it/s]Extractor Estimating: 103it [01:03,  1.66it/s]Extractor Estimating: 104it [01:04,  1.62it/s]Extractor Estimating: 105it [01:04,  1.62it/s]Extractor Estimating: 106it [01:05,  1.66it/s]Extractor Estimating: 107it [01:05,  1.67it/s]Extractor Estimating: 108it [01:06,  1.70it/s]Extractor Estimating: 109it [01:06,  1.76it/s]Extractor Estimating: 110it [01:07,  1.74it/s]Extractor Estimating: 111it [01:08,  1.77it/s]Extractor Estimating: 112it [01:08,  1.79it/s]Extractor Estimating: 113it [01:09,  1.79it/s]Extractor Estimating: 114it [01:09,  1.73it/s]Extractor Estimating: 115it [01:10,  1.73it/s]Extractor Estimating: 116it [01:10,  1.71it/s]Extractor Estimating: 117it [01:11,  1.66it/s]Extractor Estimating: 118it [01:12,  1.70it/s]Extractor Estimating: 119it [01:12,  1.76it/s]Extractor Estimating: 120it [01:13,  1.78it/s]Extractor Estimating: 121it [01:13,  1.70it/s]Extractor Estimating: 122it [01:14,  1.74it/s]Extractor Estimating: 123it [01:14,  1.78it/s]Extractor Estimating: 124it [01:15,  1.77it/s]Extractor Estimating: 125it [01:16,  1.77it/s]Extractor Estimating: 126it [01:16,  1.55it/s]Extractor Estimating: 127it [01:17,  1.55it/s]Extractor Estimating: 128it [01:18,  1.57it/s]Extractor Estimating: 129it [01:18,  1.56it/s]Extractor Estimating: 130it [01:19,  1.54it/s]Extractor Estimating: 131it [01:20,  1.56it/s]Extractor Estimating: 132it [01:20,  1.51it/s]Extractor Estimating: 133it [01:21,  1.51it/s]Extractor Estimating: 134it [01:22,  1.52it/s]Extractor Estimating: 135it [01:22,  1.48it/s]Extractor Estimating: 136it [01:23,  1.51it/s]Extractor Estimating: 137it [01:24,  1.49it/s]Extractor Estimating: 138it [01:24,  1.44it/s]Extractor Estimating: 139it [01:25,  1.47it/s]Extractor Estimating: 140it [01:26,  1.39it/s]Extractor Estimating: 141it [01:27,  1.40it/s]Extractor Estimating: 142it [01:27,  1.45it/s]Extractor Estimating: 143it [01:28,  1.47it/s]Extractor Estimating: 144it [01:28,  1.53it/s]Extractor Estimating: 145it [01:29,  1.51it/s]Extractor Estimating: 146it [01:30,  1.53it/s]Extractor Estimating: 147it [01:30,  1.50it/s]Extractor Estimating: 148it [01:31,  1.45it/s]Extractor Estimating: 149it [01:32,  1.43it/s]Extractor Estimating: 150it [01:33,  1.43it/s]Extractor Estimating: 151it [01:33,  1.50it/s]Extractor Estimating: 152it [01:34,  1.56it/s]Extractor Estimating: 153it [01:34,  1.61it/s]Extractor Estimating: 154it [01:35,  1.69it/s]Extractor Estimating: 155it [01:35,  1.74it/s]Extractor Estimating: 156it [01:36,  1.74it/s]Extractor Estimating: 157it [01:37,  1.80it/s]Extractor Estimating: 158it [01:37,  1.74it/s]Extractor Estimating: 159it [01:38,  1.79it/s]Extractor Estimating: 160it [01:38,  1.77it/s]Extractor Estimating: 161it [01:39,  1.81it/s]Extractor Estimating: 162it [01:39,  1.90it/s]Extractor Estimating: 163it [01:40,  1.87it/s]Extractor Estimating: 164it [01:40,  1.78it/s]Extractor Estimating: 165it [01:41,  1.78it/s]Extractor Estimating: 166it [01:42,  1.75it/s]Extractor Estimating: 167it [01:42,  1.77it/s]Extractor Estimating: 168it [01:43,  1.77it/s]Extractor Estimating: 169it [01:43,  1.80it/s]Extractor Estimating: 170it [01:44,  1.78it/s]Extractor Estimating: 171it [01:44,  1.78it/s]Extractor Estimating: 172it [01:45,  1.78it/s]Extractor Estimating: 173it [01:46,  1.75it/s]Extractor Estimating: 174it [01:46,  1.75it/s]Extractor Estimating: 175it [01:47,  1.72it/s]Extractor Estimating: 176it [01:47,  1.73it/s]Extractor Estimating: 177it [01:48,  1.67it/s]Extractor Estimating: 178it [01:49,  1.69it/s]Extractor Estimating: 179it [01:49,  1.74it/s]Extractor Estimating: 180it [01:50,  1.80it/s]Extractor Estimating: 181it [01:50,  1.82it/s]Extractor Estimating: 182it [01:51,  1.80it/s]Extractor Estimating: 183it [01:51,  1.84it/s]Extractor Estimating: 184it [01:52,  1.82it/s]Extractor Estimating: 185it [01:52,  1.88it/s]Extractor Estimating: 186it [01:53,  1.84it/s]Extractor Estimating: 187it [01:53,  1.89it/s]Extractor Estimating: 188it [01:54,  1.81it/s]Extractor Estimating: 189it [01:54,  1.83it/s]Extractor Estimating: 190it [01:55,  1.84it/s]Extractor Estimating: 191it [01:56,  1.83it/s]Extractor Estimating: 192it [01:56,  1.86it/s]Extractor Estimating: 193it [01:57,  1.88it/s]Extractor Estimating: 194it [01:57,  1.76it/s]Extractor Estimating: 195it [01:58,  1.80it/s]Extractor Estimating: 196it [01:58,  1.77it/s]Extractor Estimating: 197it [01:59,  1.79it/s]Extractor Estimating: 198it [01:59,  1.81it/s]Extractor Estimating: 199it [02:00,  1.78it/s]Extractor Estimating: 200it [02:01,  1.70it/s]Extractor Estimating: 201it [02:01,  1.67it/s]Extractor Estimating: 202it [02:02,  1.64it/s]Extractor Estimating: 203it [02:03,  1.59it/s]Extractor Estimating: 204it [02:03,  1.57it/s]Extractor Estimating: 205it [02:04,  1.56it/s]Extractor Estimating: 206it [02:05,  1.55it/s]Extractor Estimating: 207it [02:05,  1.56it/s]Extractor Estimating: 208it [02:06,  1.58it/s]Extractor Estimating: 209it [02:06,  1.53it/s]Extractor Estimating: 210it [02:07,  1.54it/s]Extractor Estimating: 211it [02:08,  1.33it/s]Extractor Estimating: 212it [02:09,  1.37it/s]Extractor Estimating: 213it [02:09,  1.40it/s]Extractor Estimating: 214it [02:10,  1.43it/s]Extractor Estimating: 215it [02:11,  1.44it/s]Extractor Estimating: 216it [02:11,  1.46it/s]Extractor Estimating: 217it [02:12,  1.52it/s]Extractor Estimating: 218it [02:13,  1.50it/s]Extractor Estimating: 219it [02:13,  1.47it/s]Extractor Estimating: 220it [02:14,  1.47it/s]Extractor Estimating: 221it [02:15,  1.40it/s]Extractor Estimating: 222it [02:16,  1.36it/s]Extractor Estimating: 223it [02:16,  1.48it/s]Extractor Estimating: 224it [02:17,  1.47it/s]Extractor Estimating: 225it [02:18,  1.41it/s]Extractor Estimating: 226it [02:18,  1.45it/s]Extractor Estimating: 227it [02:19,  1.47it/s]Extractor Estimating: 228it [02:20,  1.50it/s]Extractor Estimating: 229it [02:20,  1.50it/s]Extractor Estimating: 230it [02:21,  1.47it/s]Extractor Estimating: 231it [02:22,  1.44it/s]Extractor Estimating: 232it [02:23,  1.41it/s]Extractor Estimating: 233it [02:23,  1.41it/s]Extractor Estimating: 234it [02:24,  1.38it/s]Extractor Estimating: 235it [02:25,  1.37it/s]Extractor Estimating: 236it [02:25,  1.39it/s]Extractor Estimating: 237it [02:26,  1.46it/s]Extractor Estimating: 238it [02:27,  1.39it/s]Extractor Estimating: 239it [02:27,  1.46it/s]Extractor Estimating: 240it [02:28,  1.52it/s]Extractor Estimating: 241it [02:29,  1.47it/s]Extractor Estimating: 242it [02:29,  1.47it/s]Extractor Estimating: 243it [02:30,  1.39it/s]Extractor Estimating: 244it [02:31,  1.44it/s]Extractor Estimating: 245it [02:32,  1.48it/s]Extractor Estimating: 246it [02:32,  1.39it/s]Extractor Estimating: 247it [02:33,  1.40it/s]Extractor Estimating: 248it [02:34,  1.44it/s]Extractor Estimating: 249it [02:34,  1.46it/s]Extractor Estimating: 250it [02:35,  1.50it/s]Extractor Estimating: 251it [02:36,  1.51it/s]Extractor Estimating: 252it [02:36,  1.54it/s]Extractor Estimating: 253it [02:37,  1.55it/s]Extractor Estimating: 254it [02:38,  1.58it/s]Extractor Estimating: 255it [02:38,  1.59it/s]Extractor Estimating: 256it [02:39,  1.59it/s]Extractor Estimating: 257it [02:39,  1.59it/s]Extractor Estimating: 258it [02:40,  1.62it/s]Extractor Estimating: 259it [02:41,  1.66it/s]Extractor Estimating: 260it [02:41,  1.67it/s]Extractor Estimating: 261it [02:42,  1.69it/s]Extractor Estimating: 262it [02:42,  1.67it/s]Extractor Estimating: 263it [02:43,  1.66it/s]Extractor Estimating: 264it [02:44,  1.64it/s]Extractor Estimating: 265it [02:44,  1.62it/s]Extractor Estimating: 266it [02:45,  1.57it/s]Extractor Estimating: 267it [02:45,  1.60it/s]Extractor Estimating: 268it [02:46,  1.57it/s]Extractor Estimating: 269it [02:47,  1.61it/s]Extractor Estimating: 270it [02:47,  1.60it/s]Extractor Estimating: 271it [02:48,  1.62it/s]Extractor Estimating: 272it [02:49,  1.66it/s]Extractor Estimating: 273it [02:49,  1.67it/s]Extractor Estimating: 274it [02:50,  1.56it/s]Extractor Estimating: 275it [02:50,  1.57it/s]Extractor Estimating: 276it [02:51,  1.59it/s]Extractor Estimating: 277it [02:52,  1.62it/s]Extractor Estimating: 278it [02:52,  1.61it/s]Extractor Estimating: 279it [02:53,  1.66it/s]Extractor Estimating: 280it [02:53,  1.68it/s]Extractor Estimating: 281it [02:54,  1.73it/s]Extractor Estimating: 282it [02:55,  1.68it/s]Extractor Estimating: 283it [02:55,  1.64it/s]Extractor Estimating: 284it [02:56,  1.63it/s]Extractor Estimating: 285it [02:57,  1.56it/s]Extractor Estimating: 286it [02:57,  1.53it/s]Extractor Estimating: 287it [02:58,  1.59it/s]Extractor Estimating: 288it [02:58,  1.60it/s]Extractor Estimating: 289it [02:59,  1.68it/s]Extractor Estimating: 290it [03:00,  1.68it/s]Extractor Estimating: 291it [03:00,  1.69it/s]Extractor Estimating: 292it [03:01,  1.71it/s]Extractor Estimating: 293it [03:01,  1.77it/s]Extractor Estimating: 294it [03:02,  1.72it/s]Extractor Estimating: 295it [03:02,  1.71it/s]Extractor Estimating: 296it [03:03,  1.69it/s]Extractor Estimating: 297it [03:04,  1.68it/s]Extractor Estimating: 298it [03:04,  1.68it/s]Extractor Estimating: 299it [03:05,  1.69it/s]Extractor Estimating: 300it [03:06,  1.53it/s]Extractor Estimating: 301it [03:06,  1.61it/s]Extractor Estimating: 302it [03:07,  1.64it/s]Extractor Estimating: 303it [03:07,  1.64it/s]Extractor Estimating: 304it [03:08,  1.67it/s]Extractor Estimating: 305it [03:09,  1.70it/s]Extractor Estimating: 306it [03:09,  1.73it/s]Extractor Estimating: 307it [03:10,  1.70it/s]Extractor Estimating: 308it [03:10,  1.72it/s]Extractor Estimating: 309it [03:11,  1.67it/s]Extractor Estimating: 310it [03:12,  1.64it/s]Extractor Estimating: 311it [03:12,  1.68it/s]Extractor Estimating: 312it [03:13,  1.74it/s]Extractor Estimating: 313it [03:13,  1.77it/s]Extractor Estimating: 314it [03:14,  1.72it/s]Extractor Estimating: 315it [03:14,  1.79it/s]Extractor Estimating: 316it [03:15,  1.72it/s]Extractor Estimating: 317it [03:16,  1.73it/s]Extractor Estimating: 318it [03:16,  1.75it/s]Extractor Estimating: 319it [03:17,  1.74it/s]Extractor Estimating: 320it [03:17,  1.71it/s]Extractor Estimating: 321it [03:18,  1.71it/s]Extractor Estimating: 322it [03:18,  1.68it/s]Extractor Estimating: 323it [03:19,  1.71it/s]Extractor Estimating: 324it [03:20,  1.66it/s]Extractor Estimating: 325it [03:20,  1.69it/s]Extractor Estimating: 326it [03:21,  1.74it/s]Extractor Estimating: 327it [03:21,  1.70it/s]Extractor Estimating: 328it [03:22,  1.65it/s]Extractor Estimating: 329it [03:23,  1.64it/s]Extractor Estimating: 330it [03:23,  1.72it/s]Extractor Estimating: 331it [03:24,  1.74it/s]Extractor Estimating: 332it [03:24,  1.71it/s]Extractor Estimating: 333it [03:25,  1.69it/s]Extractor Estimating: 334it [03:26,  1.69it/s]Extractor Estimating: 335it [03:26,  1.68it/s]Extractor Estimating: 336it [03:27,  1.71it/s]Extractor Estimating: 337it [03:27,  1.72it/s]Extractor Estimating: 338it [03:28,  1.69it/s]Extractor Estimating: 339it [03:28,  1.69it/s]Extractor Estimating: 340it [03:29,  1.73it/s]Extractor Estimating: 341it [03:30,  1.68it/s]Extractor Estimating: 342it [03:30,  1.70it/s]Extractor Estimating: 343it [03:31,  1.71it/s]Extractor Estimating: 344it [03:31,  1.73it/s]Extractor Estimating: 345it [03:32,  1.78it/s]Extractor Estimating: 346it [03:33,  1.70it/s]Extractor Estimating: 347it [03:33,  1.69it/s]Extractor Estimating: 348it [03:34,  1.65it/s]Extractor Estimating: 349it [03:34,  1.71it/s]Extractor Estimating: 350it [03:35,  1.71it/s]Extractor Estimating: 351it [03:35,  1.73it/s]Extractor Estimating: 352it [03:36,  1.69it/s]Extractor Estimating: 353it [03:37,  1.67it/s]Extractor Estimating: 354it [03:37,  1.62it/s]Extractor Estimating: 355it [03:38,  1.62it/s]Extractor Estimating: 356it [03:39,  1.59it/s]Extractor Estimating: 357it [03:39,  1.60it/s]Extractor Estimating: 358it [03:40,  1.62it/s]Extractor Estimating: 359it [03:40,  1.60it/s]Extractor Estimating: 360it [03:41,  1.66it/s]Extractor Estimating: 361it [03:42,  1.67it/s]Extractor Estimating: 362it [03:42,  1.67it/s]Extractor Estimating: 363it [03:43,  1.70it/s]Extractor Estimating: 364it [03:44,  1.59it/s]Extractor Estimating: 365it [03:44,  1.63it/s]Extractor Estimating: 366it [03:45,  1.58it/s]Extractor Estimating: 367it [03:45,  1.63it/s]Extractor Estimating: 368it [03:46,  1.62it/s]Extractor Estimating: 369it [03:47,  1.54it/s]Extractor Estimating: 370it [03:47,  1.56it/s]Extractor Estimating: 371it [03:48,  1.54it/s]Extractor Estimating: 372it [03:49,  1.56it/s]Extractor Estimating: 373it [03:49,  1.59it/s]Extractor Estimating: 374it [03:50,  1.54it/s]Extractor Estimating: 375it [03:51,  1.52it/s]Extractor Estimating: 376it [03:51,  1.54it/s]Extractor Estimating: 377it [03:52,  1.55it/s]Extractor Estimating: 378it [03:53,  1.53it/s]Extractor Estimating: 379it [03:53,  1.55it/s]Extractor Estimating: 380it [03:54,  1.59it/s]Extractor Estimating: 381it [03:54,  1.57it/s]Extractor Estimating: 382it [03:55,  1.53it/s]Extractor Estimating: 383it [03:56,  1.59it/s]Extractor Estimating: 384it [03:56,  1.50it/s]Extractor Estimating: 385it [03:57,  1.54it/s]Extractor Estimating: 386it [03:58,  1.62it/s]Extractor Estimating: 387it [03:58,  1.43it/s]Extractor Estimating: 388it [03:59,  1.50it/s]Extractor Estimating: 389it [04:00,  1.59it/s]Extractor Estimating: 390it [04:00,  1.57it/s]Extractor Estimating: 391it [04:01,  1.61it/s]Extractor Estimating: 392it [04:01,  1.61it/s]Extractor Estimating: 393it [04:02,  1.60it/s]Extractor Estimating: 394it [04:03,  1.61it/s]Extractor Estimating: 395it [04:03,  1.58it/s]Extractor Estimating: 396it [04:04,  1.57it/s]Extractor Estimating: 397it [04:05,  1.60it/s]Extractor Estimating: 398it [04:05,  1.61it/s]Extractor Estimating: 399it [04:06,  1.58it/s]Extractor Estimating: 400it [04:07,  1.57it/s]Extractor Estimating: 401it [04:07,  1.61it/s]Extractor Estimating: 402it [04:08,  1.61it/s]Extractor Estimating: 403it [04:08,  1.66it/s]Extractor Estimating: 404it [04:09,  1.64it/s]Extractor Estimating: 405it [04:10,  1.65it/s]Extractor Estimating: 406it [04:10,  1.65it/s]Extractor Estimating: 407it [04:11,  1.64it/s]Extractor Estimating: 408it [04:11,  1.68it/s]Extractor Estimating: 409it [04:12,  1.70it/s]Extractor Estimating: 410it [04:12,  1.71it/s]Extractor Estimating: 411it [04:13,  1.74it/s]Extractor Estimating: 412it [04:14,  1.74it/s]Extractor Estimating: 413it [04:14,  1.69it/s]Extractor Estimating: 414it [04:15,  1.72it/s]Extractor Estimating: 415it [04:15,  1.69it/s]Extractor Estimating: 416it [04:16,  1.70it/s]Extractor Estimating: 417it [04:17,  1.69it/s]Extractor Estimating: 418it [04:17,  1.69it/s]Extractor Estimating: 419it [04:18,  1.72it/s]Extractor Estimating: 420it [04:18,  1.73it/s]Extractor Estimating: 421it [04:19,  1.71it/s]Extractor Estimating: 422it [04:20,  1.67it/s]Extractor Estimating: 423it [04:20,  1.67it/s]Extractor Estimating: 424it [04:21,  1.68it/s]Extractor Estimating: 425it [04:21,  1.65it/s]Extractor Estimating: 426it [04:22,  1.69it/s]Extractor Estimating: 427it [04:22,  1.69it/s]Extractor Estimating: 428it [04:23,  1.65it/s]Extractor Estimating: 429it [04:24,  1.68it/s]Extractor Estimating: 430it [04:24,  1.62it/s]Extractor Estimating: 431it [04:25,  1.62it/s]Extractor Estimating: 432it [04:26,  1.58it/s]Extractor Estimating: 433it [04:26,  1.53it/s]Extractor Estimating: 434it [04:27,  1.50it/s]Extractor Estimating: 435it [04:28,  1.55it/s]Extractor Estimating: 436it [04:28,  1.54it/s]Extractor Estimating: 437it [04:29,  1.59it/s]Extractor Estimating: 438it [04:30,  1.58it/s]Extractor Estimating: 439it [04:30,  1.60it/s]Extractor Estimating: 440it [04:31,  1.57it/s]Extractor Estimating: 441it [04:31,  1.53it/s]Extractor Estimating: 442it [04:32,  1.59it/s]Extractor Estimating: 443it [04:33,  1.59it/s]Extractor Estimating: 444it [04:33,  1.61it/s]Extractor Estimating: 445it [04:34,  1.60it/s]Extractor Estimating: 446it [04:35,  1.47it/s]Extractor Estimating: 447it [04:35,  1.52it/s]Extractor Estimating: 448it [04:36,  1.54it/s]Extractor Estimating: 449it [04:37,  1.56it/s]Extractor Estimating: 450it [04:37,  1.57it/s]Extractor Estimating: 451it [04:38,  1.64it/s]Extractor Estimating: 452it [04:38,  1.67it/s]Extractor Estimating: 453it [04:39,  1.75it/s]Extractor Estimating: 454it [04:39,  1.74it/s]Extractor Estimating: 455it [04:40,  1.77it/s]Extractor Estimating: 456it [04:40,  1.82it/s]Extractor Estimating: 457it [04:41,  1.75it/s]Extractor Estimating: 458it [04:42,  1.69it/s]Extractor Estimating: 459it [04:42,  1.74it/s]Extractor Estimating: 460it [04:43,  1.81it/s]Extractor Estimating: 461it [04:43,  1.85it/s]Extractor Estimating: 462it [04:44,  1.82it/s]Extractor Estimating: 463it [04:44,  1.78it/s]Extractor Estimating: 464it [04:45,  1.79it/s]Extractor Estimating: 465it [04:46,  1.78it/s]Extractor Estimating: 466it [04:46,  1.79it/s]Extractor Estimating: 467it [04:47,  1.81it/s]Extractor Estimating: 468it [04:47,  1.81it/s]Extractor Estimating: 469it [04:48,  1.73it/s]Extractor Estimating: 470it [04:48,  1.77it/s]Extractor Estimating: 471it [04:49,  1.79it/s]Extractor Estimating: 472it [04:50,  1.61it/s]Extractor Estimating: 473it [04:50,  1.66it/s]Extractor Estimating: 474it [04:51,  1.63it/s]Extractor Estimating: 475it [04:51,  1.65it/s]Extractor Estimating: 476it [04:52,  1.56it/s]Extractor Estimating: 477it [04:53,  1.61it/s]Extractor Estimating: 478it [04:54,  1.54it/s]Extractor Estimating: 479it [04:54,  1.55it/s]Extractor Estimating: 480it [04:55,  1.54it/s]Extractor Estimating: 481it [04:55,  1.60it/s]Extractor Estimating: 482it [04:56,  1.51it/s]Extractor Estimating: 483it [04:57,  1.55it/s]Extractor Estimating: 484it [04:57,  1.49it/s]Extractor Estimating: 485it [04:58,  1.54it/s]Extractor Estimating: 486it [04:59,  1.54it/s]Extractor Estimating: 487it [04:59,  1.53it/s]Extractor Estimating: 488it [05:00,  1.49it/s]Extractor Estimating: 489it [05:01,  1.49it/s]Extractor Estimating: 490it [05:01,  1.54it/s]Extractor Estimating: 491it [05:02,  1.58it/s]Extractor Estimating: 492it [05:03,  1.61it/s]Extractor Estimating: 493it [05:03,  1.54it/s]Extractor Estimating: 494it [05:04,  1.54it/s]Extractor Estimating: 495it [05:05,  1.50it/s]Extractor Estimating: 496it [05:05,  1.43it/s]Extractor Estimating: 497it [05:06,  1.41it/s]Extractor Estimating: 498it [05:07,  1.46it/s]Extractor Estimating: 499it [05:07,  1.48it/s]Extractor Estimating: 500it [05:08,  1.56it/s]Extractor Estimating: 500it [05:08,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:50,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:50,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:50,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:50,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:50,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:15:50,684 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:15:50,685 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:15:50,951 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:15:52,037 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:15:52,037 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:53,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:53,927 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:53,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:53,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:15:53,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:15:54,304 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:15:54,305 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:15:54,633 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:15:54,866 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:15:54,867 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:22:59,898 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:22:59,920 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9990 mean pseudo reward: 0.9462743235942512
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 21796
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21896, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21896, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.081, loss:623.3947
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.126, loss:608.2611
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.061, loss:601.2958
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.081, loss:590.6127
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.069, loss:562.6417
>> valid entity prec:0.5755, rec:0.5294, f1:0.5515
>> valid relation prec:0.0857, rec:0.0155, f1:0.0263
>> valid relation with NER prec:0.0857, rec:0.0155, f1:0.0263
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.472, loss:541.8926
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.068, loss:605.6978
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.097, loss:590.5217
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.081, loss:584.9650
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.086, loss:570.9146
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5259, rec:0.6032, f1:0.5619
>> valid relation prec:0.1064, rec:0.0181, f1:0.0309
>> valid relation with NER prec:0.1064, rec:0.0181, f1:0.0309
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.469, loss:604.3403
g_step 1200, step 366, avg_time 1.069, loss:569.2177
g_step 1300, step 49, avg_time 1.050, loss:560.0652
g_step 1400, step 149, avg_time 1.067, loss:545.6220
g_step 1500, step 249, avg_time 1.070, loss:530.0360
>> valid entity prec:0.5657, rec:0.5809, f1:0.5732
>> valid relation prec:0.0940, rec:0.0247, f1:0.0391
>> valid relation with NER prec:0.0940, rec:0.0247, f1:0.0391
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.490, loss:573.6596
g_step 1700, step 32, avg_time 1.081, loss:569.1746
g_step 1800, step 132, avg_time 1.073, loss:517.7276
g_step 1900, step 232, avg_time 1.092, loss:512.0239
g_step 2000, step 332, avg_time 1.061, loss:534.2759
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5651, rec:0.6086, f1:0.5860
>> valid relation prec:0.1360, rec:0.0307, f1:0.0501
>> valid relation with NER prec:0.1360, rec:0.0307, f1:0.0501
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 15, avg_time 2.471, loss:528.3775
g_step 2200, step 115, avg_time 1.075, loss:491.9664
g_step 2300, step 215, avg_time 1.080, loss:500.6465
g_step 2400, step 315, avg_time 1.072, loss:504.2804
g_step 2500, step 415, avg_time 1.080, loss:491.7310
>> valid entity prec:0.4969, rec:0.6188, f1:0.5512
>> valid relation prec:0.0711, rec:0.0190, f1:0.0299
>> valid relation with NER prec:0.0711, rec:0.0190, f1:0.0299
g_step 2600, step 98, avg_time 2.466, loss:457.7772
g_step 2700, step 198, avg_time 1.084, loss:480.2951
g_step 2800, step 298, avg_time 1.072, loss:463.8287
g_step 2900, step 398, avg_time 1.089, loss:509.1061
g_step 3000, step 81, avg_time 1.071, loss:428.2484
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6112, rec:0.4586, f1:0.5240
>> valid relation prec:0.0684, rec:0.0115, f1:0.0197
>> valid relation with NER prec:0.0684, rec:0.0115, f1:0.0197
g_step 3100, step 181, avg_time 2.477, loss:453.4495
g_step 3200, step 281, avg_time 1.076, loss:460.8885
g_step 3300, step 381, avg_time 1.094, loss:457.4280
g_step 3400, step 64, avg_time 1.074, loss:444.4666
g_step 3500, step 164, avg_time 1.085, loss:447.0951
>> valid entity prec:0.5545, rec:0.5391, f1:0.5467
>> valid relation prec:0.0810, rec:0.0184, f1:0.0300
>> valid relation with NER prec:0.0810, rec:0.0184, f1:0.0300
g_step 3600, step 264, avg_time 2.480, loss:439.3457
g_step 3700, step 364, avg_time 1.068, loss:437.0785
g_step 3800, step 47, avg_time 1.071, loss:413.9239
g_step 3900, step 147, avg_time 1.074, loss:416.0309
g_step 4000, step 247, avg_time 1.073, loss:420.2079
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5275, rec:0.5861, f1:0.5553
>> valid relation prec:0.0341, rec:0.0075, f1:0.0123
>> valid relation with NER prec:0.0341, rec:0.0075, f1:0.0123
g_step 4100, step 347, avg_time 2.471, loss:434.1111
g_step 4200, step 30, avg_time 1.074, loss:397.6051
g_step 4300, step 130, avg_time 1.062, loss:376.4634
g_step 4400, step 230, avg_time 1.074, loss:408.1904
g_step 4500, step 330, avg_time 1.107, loss:428.2342
>> valid entity prec:0.5516, rec:0.4967, f1:0.5227
>> valid relation prec:0.0684, rec:0.0164, f1:0.0264
>> valid relation with NER prec:0.0684, rec:0.0164, f1:0.0264
g_step 4600, step 13, avg_time 2.460, loss:410.5275
g_step 4700, step 113, avg_time 1.085, loss:370.0866
g_step 4800, step 213, avg_time 1.073, loss:390.0120
g_step 4900, step 313, avg_time 1.077, loss:386.3899
g_step 5000, step 413, avg_time 1.081, loss:397.3314
learning rate was adjusted to 0.0008
>> valid entity prec:0.5638, rec:0.5650, f1:0.5644
>> valid relation prec:0.0896, rec:0.0244, f1:0.0384
>> valid relation with NER prec:0.0896, rec:0.0244, f1:0.0384
g_step 5100, step 96, avg_time 2.451, loss:361.1844
g_step 5200, step 196, avg_time 1.087, loss:375.2485
g_step 5300, step 296, avg_time 1.076, loss:363.1328
g_step 5400, step 396, avg_time 1.088, loss:375.7939
g_step 5500, step 79, avg_time 1.075, loss:349.4935
>> valid entity prec:0.5489, rec:0.5371, f1:0.5429
>> valid relation prec:0.0983, rec:0.0250, f1:0.0398
>> valid relation with NER prec:0.0983, rec:0.0250, f1:0.0398
g_step 5600, step 179, avg_time 2.482, loss:353.7288
g_step 5700, step 279, avg_time 1.087, loss:364.2487
g_step 5800, step 379, avg_time 1.073, loss:373.0270
g_step 5900, step 62, avg_time 1.074, loss:354.0022
g_step 6000, step 162, avg_time 1.086, loss:353.6628
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5545, rec:0.5230, f1:0.5383
>> valid relation prec:0.0883, rec:0.0218, f1:0.0350
>> valid relation with NER prec:0.0883, rec:0.0218, f1:0.0350
g_step 6100, step 262, avg_time 2.472, loss:347.2063
g_step 6200, step 362, avg_time 1.069, loss:356.5065
g_step 6300, step 45, avg_time 1.065, loss:341.1585
g_step 6400, step 145, avg_time 1.083, loss:330.7290
g_step 6500, step 245, avg_time 1.073, loss:340.6395
>> valid entity prec:0.5492, rec:0.5214, f1:0.5349
>> valid relation prec:0.0908, rec:0.0204, f1:0.0333
>> valid relation with NER prec:0.0908, rec:0.0204, f1:0.0333
g_step 6600, step 345, avg_time 2.485, loss:354.2164
g_step 6700, step 28, avg_time 1.067, loss:317.9389
g_step 6800, step 128, avg_time 1.085, loss:309.3760
g_step 6900, step 228, avg_time 1.072, loss:326.5120
g_step 7000, step 328, avg_time 1.076, loss:331.5107
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5481, rec:0.5437, f1:0.5459
>> valid relation prec:0.0976, rec:0.0293, f1:0.0451
>> valid relation with NER prec:0.0976, rec:0.0293, f1:0.0451
g_step 7100, step 11, avg_time 2.471, loss:326.6490
g_step 7200, step 111, avg_time 1.081, loss:295.7281
g_step 7300, step 211, avg_time 1.081, loss:297.2900
g_step 7400, step 311, avg_time 1.078, loss:305.7820
g_step 7500, step 411, avg_time 1.075, loss:319.2340
>> valid entity prec:0.5331, rec:0.5006, f1:0.5163
>> valid relation prec:0.0937, rec:0.0256, f1:0.0402
>> valid relation with NER prec:0.0937, rec:0.0256, f1:0.0402
g_step 7600, step 94, avg_time 2.470, loss:277.8264
g_step 7700, step 194, avg_time 1.083, loss:298.7326
g_step 7800, step 294, avg_time 1.078, loss:286.8997
g_step 7900, step 394, avg_time 1.077, loss:307.5471
g_step 8000, step 77, avg_time 1.059, loss:290.8360
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5067, rec:0.5279, f1:0.5171
>> valid relation prec:0.0950, rec:0.0224, f1:0.0363
>> valid relation with NER prec:0.0950, rec:0.0224, f1:0.0363
g_step 8100, step 177, avg_time 2.489, loss:282.6420
g_step 8200, step 277, avg_time 1.069, loss:291.1169
g_step 8300, step 377, avg_time 1.078, loss:303.5983
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:22:59 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:22:59 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-22-59_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:23:01 - WARNING - datasets.builder -   Using custom data configuration default-256e9e6c033e283b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-256e9e6c033e283b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:23:01,911 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:23:01,912 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:23:01,912 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:23:01,913 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:23:01,936 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:01,942 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:01,942 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:01,942 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:01,942 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:01,942 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:23:01,942 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:23:02,070 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:23:05,217 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:23:05,251 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-256e9e6c033e283b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.57ba/s] 20%|██        | 2/10 [00:00<00:01,  4.26ba/s] 30%|███       | 3/10 [00:00<00:01,  4.56ba/s] 40%|████      | 4/10 [00:00<00:01,  4.70ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.74ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.78ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.80ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.80ba/s] 90%|█████████ | 9/10 [00:01<00:00,  4.83ba/s]100%|██████████| 10/10 [00:02<00:00,  4.85ba/s]100%|██████████| 10/10 [00:02<00:00,  4.71ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.38ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.91ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.11ba/s]100%|██████████| 4/4 [00:00<00:00,  4.96ba/s]100%|██████████| 4/4 [00:00<00:00,  4.49ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  6.95ba/s] 30%|███       | 3/10 [00:00<00:00,  9.12ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.68ba/s] 60%|██████    | 6/10 [00:00<00:00,  9.74ba/s] 80%|████████  | 8/10 [00:00<00:00,  9.99ba/s]100%|██████████| 10/10 [00:01<00:00, 10.08ba/s]100%|██████████| 10/10 [00:01<00:00,  9.77ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.28ba/s] 50%|█████     | 2/4 [00:00<00:00,  7.25ba/s]100%|██████████| 4/4 [00:00<00:00, 10.30ba/s]100%|██████████| 4/4 [00:00<00:00,  9.15ba/s]
[INFO|trainer.py:414] 2023-08-28 21:23:10,634 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:23:10,662 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:23:10,662 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-28 21:23:10,662 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:23:10,662 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:23:10,662 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:23:10,662 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:23:10,662 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<04:00,  3.24it/s]  0%|          | 2/780 [00:00<03:51,  3.36it/s]  0%|          | 3/780 [00:00<03:48,  3.41it/s]  1%|          | 4/780 [00:01<03:57,  3.27it/s]  1%|          | 5/780 [00:01<03:52,  3.33it/s]  1%|          | 6/780 [00:01<03:49,  3.38it/s]  1%|          | 7/780 [00:02<03:47,  3.40it/s]  1%|          | 8/780 [00:02<03:45,  3.42it/s]  1%|          | 9/780 [00:02<03:45,  3.42it/s]  1%|▏         | 10/780 [00:02<03:44,  3.43it/s]  1%|▏         | 11/780 [00:03<03:43,  3.44it/s]  2%|▏         | 12/780 [00:03<03:43,  3.44it/s]  2%|▏         | 13/780 [00:03<03:42,  3.44it/s]  2%|▏         | 14/780 [00:04<03:42,  3.45it/s]  2%|▏         | 15/780 [00:04<03:42,  3.44it/s]  2%|▏         | 16/780 [00:04<03:41,  3.45it/s]  2%|▏         | 17/780 [00:04<03:41,  3.45it/s]  2%|▏         | 18/780 [00:05<03:40,  3.45it/s]  2%|▏         | 19/780 [00:05<03:40,  3.45it/s]  3%|▎         | 20/780 [00:05<03:40,  3.45it/s]  3%|▎         | 21/780 [00:06<03:40,  3.45it/s]  3%|▎         | 22/780 [00:06<03:39,  3.45it/s]  3%|▎         | 23/780 [00:06<03:39,  3.45it/s]  3%|▎         | 24/780 [00:07<03:39,  3.45it/s]  3%|▎         | 25/780 [00:07<03:39,  3.44it/s]  3%|▎         | 26/780 [00:07<03:41,  3.41it/s]  3%|▎         | 27/780 [00:07<03:40,  3.42it/s]  4%|▎         | 28/780 [00:08<03:39,  3.43it/s]  4%|▎         | 29/780 [00:08<03:38,  3.43it/s]  4%|▍         | 30/780 [00:08<03:38,  3.44it/s]  4%|▍         | 31/780 [00:09<03:37,  3.45it/s]  4%|▍         | 32/780 [00:09<03:37,  3.45it/s]  4%|▍         | 33/780 [00:09<03:36,  3.45it/s]  4%|▍         | 34/780 [00:09<03:36,  3.45it/s]  4%|▍         | 35/780 [00:10<03:35,  3.45it/s]  5%|▍         | 36/780 [00:10<03:35,  3.45it/s]  5%|▍         | 37/780 [00:10<03:36,  3.43it/s]  5%|▍         | 38/780 [00:11<03:35,  3.44it/s]  5%|▌         | 39/780 [00:11<03:35,  3.44it/s]  5%|▌         | 40/780 [00:11<03:35,  3.44it/s]  5%|▌         | 41/780 [00:11<03:34,  3.44it/s]  5%|▌         | 42/780 [00:12<03:34,  3.44it/s]  6%|▌         | 43/780 [00:12<03:34,  3.44it/s]  6%|▌         | 44/780 [00:12<03:33,  3.45it/s]  6%|▌         | 45/780 [00:13<03:33,  3.45it/s]  6%|▌         | 46/780 [00:13<03:32,  3.45it/s]  6%|▌         | 47/780 [00:13<03:32,  3.45it/s]  6%|▌         | 48/780 [00:13<03:32,  3.44it/s]  6%|▋         | 49/780 [00:14<03:32,  3.44it/s]  6%|▋         | 50/780 [00:14<03:31,  3.44it/s]  7%|▋         | 51/780 [00:14<03:31,  3.44it/s]  7%|▋         | 52/780 [00:15<03:31,  3.44it/s]  7%|▋         | 53/780 [00:15<03:31,  3.44it/s]  7%|▋         | 54/780 [00:15<03:30,  3.45it/s]  7%|▋         | 55/780 [00:16<03:30,  3.44it/s]  7%|▋         | 56/780 [00:16<03:30,  3.44it/s]  7%|▋         | 57/780 [00:16<03:30,  3.44it/s]  7%|▋         | 58/780 [00:16<03:29,  3.44it/s]  8%|▊         | 59/780 [00:17<03:30,  3.42it/s]  8%|▊         | 60/780 [00:17<03:53,  3.08it/s]  8%|▊         | 61/780 [00:17<03:46,  3.17it/s]  8%|▊         | 62/780 [00:18<03:40,  3.25it/s]  8%|▊         | 63/780 [00:18<03:36,  3.31it/s]  8%|▊         | 64/780 [00:18<03:34,  3.35it/s]  8%|▊         | 65/780 [00:19<03:31,  3.37it/s]  8%|▊         | 66/780 [00:19<03:30,  3.39it/s]  9%|▊         | 67/780 [00:19<03:29,  3.41it/s]  9%|▊         | 68/780 [00:19<03:28,  3.41it/s]  9%|▉         | 69/780 [00:20<03:28,  3.41it/s]  9%|▉         | 70/780 [00:20<03:27,  3.42it/s]  9%|▉         | 71/780 [00:20<03:26,  3.43it/s]  9%|▉         | 72/780 [00:21<03:26,  3.43it/s]  9%|▉         | 73/780 [00:21<03:26,  3.43it/s]  9%|▉         | 74/780 [00:21<03:25,  3.43it/s] 10%|▉         | 75/780 [00:21<03:25,  3.43it/s] 10%|▉         | 76/780 [00:22<03:24,  3.44it/s] 10%|▉         | 77/780 [00:22<03:24,  3.44it/s] 10%|█         | 78/780 [00:22<03:23,  3.44it/s] 10%|█         | 79/780 [00:23<03:23,  3.44it/s] 10%|█         | 80/780 [00:23<03:23,  3.44it/s] 10%|█         | 81/780 [00:23<03:23,  3.44it/s] 11%|█         | 82/780 [00:23<03:22,  3.44it/s] 11%|█         | 83/780 [00:24<03:22,  3.44it/s] 11%|█         | 84/780 [00:24<03:22,  3.44it/s] 11%|█         | 85/780 [00:24<03:21,  3.44it/s] 11%|█         | 86/780 [00:25<03:21,  3.44it/s] 11%|█         | 87/780 [00:25<03:21,  3.44it/s] 11%|█▏        | 88/780 [00:25<03:21,  3.44it/s] 11%|█▏        | 89/780 [00:26<03:20,  3.44it/s] 12%|█▏        | 90/780 [00:26<03:25,  3.35it/s] 12%|█▏        | 91/780 [00:26<03:23,  3.38it/s] 12%|█▏        | 92/780 [00:26<03:22,  3.40it/s] 12%|█▏        | 93/780 [00:27<03:21,  3.41it/s] 12%|█▏        | 94/780 [00:27<03:41,  3.10it/s] 12%|█▏        | 95/780 [00:27<03:34,  3.19it/s] 12%|█▏        | 96/780 [00:28<03:30,  3.26it/s] 12%|█▏        | 97/780 [00:28<03:26,  3.31it/s] 13%|█▎        | 98/780 [00:28<03:24,  3.34it/s] 13%|█▎        | 99/780 [00:29<03:22,  3.37it/s] 13%|█▎        | 100/780 [00:29<03:23,  3.34it/s] 13%|█▎        | 101/780 [00:29<03:21,  3.37it/s] 13%|█▎        | 102/780 [00:29<03:20,  3.39it/s] 13%|█▎        | 103/780 [00:30<03:19,  3.40it/s] 13%|█▎        | 104/780 [00:30<03:18,  3.41it/s] 13%|█▎        | 105/780 [00:30<03:17,  3.42it/s] 14%|█▎        | 106/780 [00:31<03:16,  3.42it/s] 14%|█▎        | 107/780 [00:31<03:16,  3.42it/s] 14%|█▍        | 108/780 [00:31<03:16,  3.43it/s] 14%|█▍        | 109/780 [00:31<03:15,  3.43it/s] 14%|█▍        | 110/780 [00:32<03:15,  3.43it/s] 14%|█▍        | 111/780 [00:32<03:15,  3.42it/s] 14%|█▍        | 112/780 [00:32<03:15,  3.42it/s] 14%|█▍        | 113/780 [00:33<03:14,  3.42it/s] 15%|█▍        | 114/780 [00:33<03:14,  3.42it/s] 15%|█▍        | 115/780 [00:33<03:16,  3.39it/s] 15%|█▍        | 116/780 [00:34<03:15,  3.40it/s] 15%|█▌        | 117/780 [00:34<03:14,  3.41it/s] 15%|█▌        | 118/780 [00:34<03:13,  3.42it/s] 15%|█▌        | 119/780 [00:34<03:13,  3.42it/s] 15%|█▌        | 120/780 [00:35<03:12,  3.42it/s] 16%|█▌        | 121/780 [00:35<03:12,  3.43it/s] 16%|█▌        | 122/780 [00:35<03:12,  3.42it/s] 16%|█▌        | 123/780 [00:36<03:11,  3.43it/s] 16%|█▌        | 124/780 [00:36<03:11,  3.43it/s] 16%|█▌        | 125/780 [00:36<03:11,  3.43it/s] 16%|█▌        | 126/780 [00:36<03:10,  3.43it/s] 16%|█▋        | 127/780 [00:37<03:10,  3.43it/s] 16%|█▋        | 128/780 [00:37<03:09,  3.43it/s] 17%|█▋        | 129/780 [00:37<03:09,  3.43it/s] 17%|█▋        | 130/780 [00:38<03:09,  3.43it/s] 17%|█▋        | 131/780 [00:38<03:09,  3.43it/s] 17%|█▋        | 132/780 [00:38<03:09,  3.43it/s] 17%|█▋        | 133/780 [00:39<03:16,  3.30it/s] 17%|█▋        | 134/780 [00:39<03:13,  3.33it/s] 17%|█▋        | 135/780 [00:39<03:11,  3.36it/s] 17%|█▋        | 136/780 [00:39<03:10,  3.38it/s] 18%|█▊        | 137/780 [00:40<03:09,  3.40it/s] 18%|█▊        | 138/780 [00:40<03:08,  3.41it/s] 18%|█▊        | 139/780 [00:40<03:07,  3.42it/s] 18%|█▊        | 140/780 [00:41<03:07,  3.42it/s] 18%|█▊        | 141/780 [00:41<03:06,  3.42it/s] 18%|█▊        | 142/780 [00:41<03:06,  3.43it/s] 18%|█▊        | 143/780 [00:41<03:05,  3.42it/s] 18%|█▊        | 144/780 [00:42<03:13,  3.28it/s] 19%|█▊        | 145/780 [00:42<03:10,  3.33it/s] 19%|█▊        | 146/780 [00:42<03:08,  3.36it/s] 19%|█▉        | 147/780 [00:43<03:07,  3.38it/s] 19%|█▉        | 148/780 [00:43<03:06,  3.39it/s] 19%|█▉        | 149/780 [00:43<03:05,  3.40it/s] 19%|█▉        | 150/780 [00:44<03:04,  3.41it/s] 19%|█▉        | 151/780 [00:44<03:04,  3.41it/s] 19%|█▉        | 152/780 [00:44<03:03,  3.42it/s] 20%|█▉        | 153/780 [00:44<03:03,  3.43it/s] 20%|█▉        | 154/780 [00:45<03:02,  3.43it/s] 20%|█▉        | 155/780 [00:45<03:09,  3.29it/s] 20%|██        | 156/780 [00:45<03:07,  3.33it/s][INFO|trainer.py:2140] 2023-08-28 21:23:56,540 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:23:56,540 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:23:56,540 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.81it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.02it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.17it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.44it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.10it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.66it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.35it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.03it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.04it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.01it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.15it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.21it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.16it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.23it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.06it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.89it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.84it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.87it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.96it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.08it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.10it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.58it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 45.86it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 45.83it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.83it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.83it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.90it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.94it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.93it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.91it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.85it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.01it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.07it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.09it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.82it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.88it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.94it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.07it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.16it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.04it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.02it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.02it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.09it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.01it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.96it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.96it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.00it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.07it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.03it/s][A
 58%|█████▊    | 253/436 [00:05<00:04, 41.87it/s][A
 59%|█████▉    | 258/436 [00:05<00:04, 43.07it/s][A
 60%|██████    | 263/436 [00:05<00:03, 43.90it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 44.64it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.02it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.32it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.67it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.85it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.44it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.65it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.76it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.79it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.02it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.11it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.90it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.09it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.98it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.85it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.87it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.95it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.95it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.05it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.14it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 46.11it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.90it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.88it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.88it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.85it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.89it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.88it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.08it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.90it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.11it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.03it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.05it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.94it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.80it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.80it/s][A 20%|██        | 156/780 [00:55<03:07,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:24:06,256 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 21:24:06,328 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:24:11,337 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:24:11,386 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:24:11,398 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:13<1:27:07,  8.39s/it] 20%|██        | 158/780 [01:13<1:01:50,  5.96s/it] 20%|██        | 159/780 [01:13<44:06,  4.26s/it]   21%|██        | 160/780 [01:13<31:43,  3.07s/it] 21%|██        | 161/780 [01:14<23:04,  2.24s/it] 21%|██        | 162/780 [01:14<17:01,  1.65s/it] 21%|██        | 163/780 [01:14<12:47,  1.24s/it] 21%|██        | 164/780 [01:15<09:50,  1.04it/s] 21%|██        | 165/780 [01:15<07:45,  1.32it/s] 21%|██▏       | 166/780 [01:15<06:18,  1.62it/s] 21%|██▏       | 167/780 [01:16<05:18,  1.93it/s] 22%|██▏       | 168/780 [01:16<04:35,  2.22it/s] 22%|██▏       | 169/780 [01:16<04:06,  2.48it/s] 22%|██▏       | 170/780 [01:16<03:45,  2.71it/s] 22%|██▏       | 171/780 [01:17<03:30,  2.89it/s] 22%|██▏       | 172/780 [01:17<03:20,  3.04it/s] 22%|██▏       | 173/780 [01:17<03:30,  2.88it/s] 22%|██▏       | 174/780 [01:18<03:20,  3.02it/s] 22%|██▏       | 175/780 [01:18<03:12,  3.14it/s] 23%|██▎       | 176/780 [01:18<03:07,  3.22it/s] 23%|██▎       | 177/780 [01:19<03:03,  3.29it/s] 23%|██▎       | 178/780 [01:19<03:00,  3.33it/s] 23%|██▎       | 179/780 [01:19<03:03,  3.28it/s] 23%|██▎       | 180/780 [01:19<03:00,  3.33it/s] 23%|██▎       | 181/780 [01:20<02:58,  3.36it/s] 23%|██▎       | 182/780 [01:20<02:56,  3.39it/s] 23%|██▎       | 183/780 [01:20<02:55,  3.40it/s] 24%|██▎       | 184/780 [01:21<02:54,  3.42it/s] 24%|██▎       | 185/780 [01:21<02:53,  3.42it/s] 24%|██▍       | 186/780 [01:21<02:53,  3.43it/s] 24%|██▍       | 187/780 [01:21<02:52,  3.43it/s] 24%|██▍       | 188/780 [01:22<02:52,  3.44it/s] 24%|██▍       | 189/780 [01:22<02:51,  3.44it/s] 24%|██▍       | 190/780 [01:22<02:52,  3.43it/s] 24%|██▍       | 191/780 [01:23<02:51,  3.43it/s] 25%|██▍       | 192/780 [01:23<02:51,  3.44it/s] 25%|██▍       | 193/780 [01:23<02:50,  3.44it/s] 25%|██▍       | 194/780 [01:23<02:50,  3.44it/s] 25%|██▌       | 195/780 [01:24<02:50,  3.44it/s] 25%|██▌       | 196/780 [01:24<02:49,  3.44it/s] 25%|██▌       | 197/780 [01:24<02:49,  3.44it/s] 25%|██▌       | 198/780 [01:25<02:49,  3.44it/s] 26%|██▌       | 199/780 [01:25<02:48,  3.44it/s] 26%|██▌       | 200/780 [01:25<02:48,  3.44it/s] 26%|██▌       | 201/780 [01:26<02:48,  3.44it/s] 26%|██▌       | 202/780 [01:26<02:48,  3.44it/s] 26%|██▌       | 203/780 [01:26<02:48,  3.43it/s] 26%|██▌       | 204/780 [01:26<02:47,  3.44it/s] 26%|██▋       | 205/780 [01:27<02:47,  3.44it/s] 26%|██▋       | 206/780 [01:27<02:47,  3.44it/s] 27%|██▋       | 207/780 [01:27<03:03,  3.13it/s] 27%|██▋       | 208/780 [01:28<02:58,  3.21it/s] 27%|██▋       | 209/780 [01:28<02:54,  3.27it/s] 27%|██▋       | 210/780 [01:28<02:51,  3.32it/s] 27%|██▋       | 211/780 [01:29<02:49,  3.35it/s] 27%|██▋       | 212/780 [01:29<02:48,  3.37it/s] 27%|██▋       | 213/780 [01:29<02:47,  3.39it/s] 27%|██▋       | 214/780 [01:29<02:46,  3.41it/s] 28%|██▊       | 215/780 [01:30<02:45,  3.42it/s] 28%|██▊       | 216/780 [01:30<02:44,  3.42it/s] 28%|██▊       | 217/780 [01:30<02:44,  3.42it/s] 28%|██▊       | 218/780 [01:31<02:44,  3.42it/s] 28%|██▊       | 219/780 [01:31<02:43,  3.43it/s] 28%|██▊       | 220/780 [01:31<02:43,  3.43it/s] 28%|██▊       | 221/780 [01:31<02:42,  3.43it/s] 28%|██▊       | 222/780 [01:32<02:42,  3.43it/s] 29%|██▊       | 223/780 [01:32<02:42,  3.44it/s] 29%|██▊       | 224/780 [01:32<02:41,  3.44it/s] 29%|██▉       | 225/780 [01:33<02:41,  3.43it/s] 29%|██▉       | 226/780 [01:33<02:41,  3.43it/s] 29%|██▉       | 227/780 [01:33<02:41,  3.43it/s] 29%|██▉       | 228/780 [01:34<02:52,  3.21it/s] 29%|██▉       | 229/780 [01:34<02:48,  3.27it/s] 29%|██▉       | 230/780 [01:34<02:45,  3.31it/s] 30%|██▉       | 231/780 [01:34<02:43,  3.35it/s] 30%|██▉       | 232/780 [01:35<02:42,  3.38it/s] 30%|██▉       | 233/780 [01:35<02:41,  3.39it/s] 30%|███       | 234/780 [01:35<02:40,  3.40it/s] 30%|███       | 235/780 [01:36<02:39,  3.41it/s] 30%|███       | 236/780 [01:36<02:39,  3.42it/s] 30%|███       | 237/780 [01:36<02:38,  3.43it/s] 31%|███       | 238/780 [01:36<02:38,  3.43it/s] 31%|███       | 239/780 [01:37<02:39,  3.39it/s] 31%|███       | 240/780 [01:37<02:38,  3.40it/s] 31%|███       | 241/780 [01:37<02:38,  3.41it/s] 31%|███       | 242/780 [01:38<02:37,  3.42it/s] 31%|███       | 243/780 [01:38<02:37,  3.41it/s] 31%|███▏      | 244/780 [01:38<02:36,  3.42it/s] 31%|███▏      | 245/780 [01:39<02:36,  3.42it/s] 32%|███▏      | 246/780 [01:39<02:35,  3.42it/s] 32%|███▏      | 247/780 [01:39<02:35,  3.43it/s] 32%|███▏      | 248/780 [01:39<02:35,  3.43it/s] 32%|███▏      | 249/780 [01:40<02:34,  3.43it/s] 32%|███▏      | 250/780 [01:40<02:42,  3.25it/s] 32%|███▏      | 251/780 [01:40<02:39,  3.31it/s] 32%|███▏      | 252/780 [01:41<02:37,  3.35it/s] 32%|███▏      | 253/780 [01:41<02:36,  3.37it/s] 33%|███▎      | 254/780 [01:41<02:35,  3.39it/s] 33%|███▎      | 255/780 [01:41<02:34,  3.40it/s] 33%|███▎      | 256/780 [01:42<02:33,  3.41it/s] 33%|███▎      | 257/780 [01:42<02:33,  3.42it/s] 33%|███▎      | 258/780 [01:42<02:32,  3.43it/s] 33%|███▎      | 259/780 [01:43<02:32,  3.43it/s] 33%|███▎      | 260/780 [01:43<02:31,  3.43it/s] 33%|███▎      | 261/780 [01:43<02:37,  3.30it/s] 34%|███▎      | 262/780 [01:44<02:35,  3.34it/s] 34%|███▎      | 263/780 [01:44<02:33,  3.36it/s] 34%|███▍      | 264/780 [01:44<02:32,  3.38it/s] 34%|███▍      | 265/780 [01:44<02:31,  3.40it/s] 34%|███▍      | 266/780 [01:45<02:30,  3.41it/s] 34%|███▍      | 267/780 [01:45<02:30,  3.41it/s] 34%|███▍      | 268/780 [01:45<02:29,  3.42it/s] 34%|███▍      | 269/780 [01:46<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:46<02:29,  3.42it/s] 35%|███▍      | 271/780 [01:46<02:28,  3.42it/s] 35%|███▍      | 272/780 [01:47<02:34,  3.30it/s] 35%|███▌      | 273/780 [01:47<02:32,  3.33it/s] 35%|███▌      | 274/780 [01:47<02:30,  3.36it/s] 35%|███▌      | 275/780 [01:47<02:29,  3.38it/s] 35%|███▌      | 276/780 [01:48<02:28,  3.39it/s] 36%|███▌      | 277/780 [01:48<02:27,  3.41it/s] 36%|███▌      | 278/780 [01:48<02:27,  3.41it/s] 36%|███▌      | 279/780 [01:49<02:26,  3.42it/s] 36%|███▌      | 280/780 [01:49<02:27,  3.38it/s] 36%|███▌      | 281/780 [01:49<02:26,  3.40it/s] 36%|███▌      | 282/780 [01:49<02:26,  3.41it/s] 36%|███▋      | 283/780 [01:50<02:25,  3.41it/s] 36%|███▋      | 284/780 [01:50<02:25,  3.41it/s] 37%|███▋      | 285/780 [01:50<02:24,  3.42it/s] 37%|███▋      | 286/780 [01:51<02:24,  3.42it/s] 37%|███▋      | 287/780 [01:51<02:24,  3.42it/s] 37%|███▋      | 288/780 [01:51<02:23,  3.42it/s] 37%|███▋      | 289/780 [01:51<02:23,  3.42it/s] 37%|███▋      | 290/780 [01:52<02:23,  3.43it/s] 37%|███▋      | 291/780 [01:52<02:22,  3.43it/s] 37%|███▋      | 292/780 [01:52<02:22,  3.43it/s] 38%|███▊      | 293/780 [01:53<02:22,  3.43it/s] 38%|███▊      | 294/780 [01:53<02:22,  3.40it/s] 38%|███▊      | 295/780 [01:53<02:22,  3.41it/s] 38%|███▊      | 296/780 [01:54<02:21,  3.42it/s] 38%|███▊      | 297/780 [01:54<02:21,  3.42it/s] 38%|███▊      | 298/780 [01:54<02:20,  3.42it/s] 38%|███▊      | 299/780 [01:54<02:20,  3.43it/s] 38%|███▊      | 300/780 [01:55<02:19,  3.43it/s] 39%|███▊      | 301/780 [01:55<02:19,  3.43it/s] 39%|███▊      | 302/780 [01:55<02:19,  3.43it/s] 39%|███▉      | 303/780 [01:56<02:18,  3.43it/s] 39%|███▉      | 304/780 [01:56<02:18,  3.43it/s] 39%|███▉      | 305/780 [01:56<02:18,  3.43it/s] 39%|███▉      | 306/780 [01:56<02:18,  3.43it/s] 39%|███▉      | 307/780 [01:57<02:18,  3.41it/s] 39%|███▉      | 308/780 [01:57<02:18,  3.42it/s] 40%|███▉      | 309/780 [01:57<02:17,  3.42it/s] 40%|███▉      | 310/780 [01:58<02:17,  3.42it/s] 40%|███▉      | 311/780 [01:58<02:18,  3.39it/s] 40%|████      | 312/780 [01:58<02:17,  3.40it/s][INFO|trainer.py:2140] 2023-08-28 21:25:09,432 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:25:09,432 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:25:09,432 >>   Batch size = 8
{'eval_loss': 1.081212043762207, 'eval_runtime': 9.5326, 'eval_samples_per_second': 365.271, 'eval_steps_per_second': 45.738, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.69it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.79it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.33it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.61it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.18it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.52it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.16it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.85it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.01it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.11it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.16it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.31it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.12it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.14it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.96it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.88it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.73it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.93it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.00it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.99it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.10it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.09it/s][A
 27%|██▋       | 118/436 [00:02<00:07, 42.52it/s][A
 28%|██▊       | 123/436 [00:02<00:07, 43.52it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 44.15it/s][A
 31%|███       | 133/436 [00:02<00:06, 44.82it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 45.26it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.48it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.75it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.99it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.53it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.69it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 45.74it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.84it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.81it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.00it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.01it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.03it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.94it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.93it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 45.80it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.74it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.88it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.92it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.03it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.02it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.03it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.00it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.82it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.85it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 44.82it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.13it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.47it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.76it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.80it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.03it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.05it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.15it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.87it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 45.85it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.79it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.95it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.96it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 46.08it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.04it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.11it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.95it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.92it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.94it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.85it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.97it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.99it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 46.14it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.09it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.92it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.09it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.06it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.96it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.93it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.86it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.89it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.04it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.16it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.12it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.07it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.88it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.88it/s][A 40%|████      | 312/780 [02:08<02:17,  3.40it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:25:19,020 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 21:25:19,199 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:25:23,885 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:25:23,917 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:25:23,942 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:25<1:03:23,  8.15s/it] 40%|████      | 314/780 [02:25<44:58,  5.79s/it]   40%|████      | 315/780 [02:25<32:05,  4.14s/it] 41%|████      | 316/780 [02:26<23:05,  2.98s/it] 41%|████      | 317/780 [02:26<16:47,  2.18s/it] 41%|████      | 318/780 [02:26<12:23,  1.61s/it] 41%|████      | 319/780 [02:26<09:19,  1.21s/it] 41%|████      | 320/780 [02:27<07:10,  1.07it/s] 41%|████      | 321/780 [02:27<05:40,  1.35it/s] 41%|████▏     | 322/780 [02:27<04:48,  1.59it/s] 41%|████▏     | 323/780 [02:28<04:01,  1.89it/s] 42%|████▏     | 324/780 [02:28<03:28,  2.19it/s] 42%|████▏     | 325/780 [02:28<03:05,  2.46it/s] 42%|████▏     | 326/780 [02:29<02:49,  2.68it/s] 42%|████▏     | 327/780 [02:29<02:37,  2.87it/s] 42%|████▏     | 328/780 [02:29<02:29,  3.02it/s] 42%|████▏     | 329/780 [02:29<02:23,  3.14it/s] 42%|████▏     | 330/780 [02:30<02:19,  3.23it/s] 42%|████▏     | 331/780 [02:30<02:16,  3.29it/s] 43%|████▎     | 332/780 [02:30<02:14,  3.33it/s] 43%|████▎     | 333/780 [02:31<02:12,  3.37it/s] 43%|████▎     | 334/780 [02:31<02:11,  3.39it/s] 43%|████▎     | 335/780 [02:31<02:10,  3.41it/s] 43%|████▎     | 336/780 [02:31<02:10,  3.41it/s] 43%|████▎     | 337/780 [02:32<02:11,  3.38it/s] 43%|████▎     | 338/780 [02:32<02:10,  3.40it/s] 43%|████▎     | 339/780 [02:32<02:09,  3.41it/s] 44%|████▎     | 340/780 [02:33<02:08,  3.42it/s] 44%|████▎     | 341/780 [02:33<02:08,  3.42it/s] 44%|████▍     | 342/780 [02:33<02:07,  3.43it/s] 44%|████▍     | 343/780 [02:33<02:07,  3.43it/s] 44%|████▍     | 344/780 [02:34<02:06,  3.44it/s] 44%|████▍     | 345/780 [02:34<02:06,  3.44it/s] 44%|████▍     | 346/780 [02:34<02:06,  3.44it/s] 44%|████▍     | 347/780 [02:35<02:05,  3.44it/s] 45%|████▍     | 348/780 [02:35<02:06,  3.41it/s] 45%|████▍     | 349/780 [02:35<02:06,  3.42it/s] 45%|████▍     | 350/780 [02:36<02:05,  3.42it/s] 45%|████▌     | 351/780 [02:36<02:05,  3.43it/s] 45%|████▌     | 352/780 [02:36<02:04,  3.43it/s] 45%|████▌     | 353/780 [02:36<02:04,  3.44it/s] 45%|████▌     | 354/780 [02:37<02:03,  3.44it/s] 46%|████▌     | 355/780 [02:37<02:03,  3.44it/s] 46%|████▌     | 356/780 [02:37<02:03,  3.44it/s] 46%|████▌     | 357/780 [02:38<02:03,  3.44it/s] 46%|████▌     | 358/780 [02:38<02:02,  3.44it/s] 46%|████▌     | 359/780 [02:38<02:06,  3.32it/s] 46%|████▌     | 360/780 [02:38<02:05,  3.35it/s] 46%|████▋     | 361/780 [02:39<02:04,  3.37it/s] 46%|████▋     | 362/780 [02:39<02:03,  3.39it/s] 47%|████▋     | 363/780 [02:39<02:02,  3.40it/s] 47%|████▋     | 364/780 [02:40<02:01,  3.42it/s] 47%|████▋     | 365/780 [02:40<02:01,  3.42it/s] 47%|████▋     | 366/780 [02:40<02:00,  3.43it/s] 47%|████▋     | 367/780 [02:41<02:00,  3.43it/s] 47%|████▋     | 368/780 [02:41<01:59,  3.43it/s] 47%|████▋     | 369/780 [02:41<01:59,  3.43it/s] 47%|████▋     | 370/780 [02:41<01:59,  3.42it/s] 48%|████▊     | 371/780 [02:42<01:59,  3.42it/s] 48%|████▊     | 372/780 [02:42<01:59,  3.43it/s] 48%|████▊     | 373/780 [02:42<01:58,  3.43it/s] 48%|████▊     | 374/780 [02:43<01:58,  3.43it/s] 48%|████▊     | 375/780 [02:43<01:58,  3.43it/s] 48%|████▊     | 376/780 [02:43<01:57,  3.43it/s] 48%|████▊     | 377/780 [02:43<01:57,  3.44it/s] 48%|████▊     | 378/780 [02:44<01:56,  3.44it/s] 49%|████▊     | 379/780 [02:44<01:56,  3.43it/s] 49%|████▊     | 380/780 [02:44<01:56,  3.43it/s] 49%|████▉     | 381/780 [02:45<01:58,  3.37it/s] 49%|████▉     | 382/780 [02:45<01:57,  3.38it/s] 49%|████▉     | 383/780 [02:45<01:56,  3.40it/s] 49%|████▉     | 384/780 [02:45<01:56,  3.41it/s] 49%|████▉     | 385/780 [02:46<01:55,  3.42it/s] 49%|████▉     | 386/780 [02:46<01:55,  3.42it/s] 50%|████▉     | 387/780 [02:46<01:54,  3.43it/s] 50%|████▉     | 388/780 [02:47<01:54,  3.43it/s] 50%|████▉     | 389/780 [02:47<01:53,  3.43it/s] 50%|█████     | 390/780 [02:47<01:53,  3.43it/s] 50%|█████     | 391/780 [02:48<01:53,  3.43it/s] 50%|█████     | 392/780 [02:48<01:54,  3.38it/s] 50%|█████     | 393/780 [02:48<01:54,  3.39it/s] 51%|█████     | 394/780 [02:48<01:53,  3.41it/s] 51%|█████     | 395/780 [02:49<01:52,  3.41it/s] 51%|█████     | 396/780 [02:49<01:56,  3.29it/s] 51%|█████     | 397/780 [02:49<01:55,  3.32it/s] 51%|█████     | 398/780 [02:50<01:53,  3.35it/s] 51%|█████     | 399/780 [02:50<01:52,  3.38it/s] 51%|█████▏    | 400/780 [02:50<01:51,  3.40it/s] 51%|█████▏    | 401/780 [02:50<01:51,  3.41it/s] 52%|█████▏    | 402/780 [02:51<01:50,  3.42it/s] 52%|█████▏    | 403/780 [02:51<01:52,  3.34it/s] 52%|█████▏    | 404/780 [02:51<01:51,  3.37it/s] 52%|█████▏    | 405/780 [02:52<01:50,  3.39it/s] 52%|█████▏    | 406/780 [02:52<01:49,  3.40it/s] 52%|█████▏    | 407/780 [02:52<01:49,  3.41it/s] 52%|█████▏    | 408/780 [02:53<01:48,  3.42it/s] 52%|█████▏    | 409/780 [02:53<01:48,  3.42it/s] 53%|█████▎    | 410/780 [02:53<01:47,  3.43it/s] 53%|█████▎    | 411/780 [02:53<01:47,  3.43it/s] 53%|█████▎    | 412/780 [02:54<01:47,  3.43it/s] 53%|█████▎    | 413/780 [02:54<01:46,  3.43it/s] 53%|█████▎    | 414/780 [02:54<01:48,  3.37it/s] 53%|█████▎    | 415/780 [02:55<01:47,  3.39it/s] 53%|█████▎    | 416/780 [02:55<01:46,  3.41it/s] 53%|█████▎    | 417/780 [02:55<01:46,  3.41it/s] 54%|█████▎    | 418/780 [02:55<01:45,  3.42it/s] 54%|█████▎    | 419/780 [02:56<01:45,  3.42it/s] 54%|█████▍    | 420/780 [02:56<01:45,  3.43it/s] 54%|█████▍    | 421/780 [02:56<01:44,  3.43it/s] 54%|█████▍    | 422/780 [02:57<01:44,  3.43it/s] 54%|█████▍    | 423/780 [02:57<01:43,  3.43it/s] 54%|█████▍    | 424/780 [02:57<01:43,  3.43it/s] 54%|█████▍    | 425/780 [02:58<01:43,  3.43it/s] 55%|█████▍    | 426/780 [02:58<01:42,  3.44it/s] 55%|█████▍    | 427/780 [02:58<01:42,  3.43it/s] 55%|█████▍    | 428/780 [02:58<01:42,  3.43it/s] 55%|█████▌    | 429/780 [02:59<01:42,  3.44it/s] 55%|█████▌    | 430/780 [02:59<01:41,  3.43it/s] 55%|█████▌    | 431/780 [02:59<01:42,  3.42it/s] 55%|█████▌    | 432/780 [03:00<01:41,  3.42it/s] 56%|█████▌    | 433/780 [03:00<01:41,  3.43it/s] 56%|█████▌    | 434/780 [03:00<01:40,  3.43it/s] 56%|█████▌    | 435/780 [03:00<01:40,  3.43it/s] 56%|█████▌    | 436/780 [03:01<01:40,  3.43it/s] 56%|█████▌    | 437/780 [03:01<01:39,  3.43it/s] 56%|█████▌    | 438/780 [03:01<01:39,  3.43it/s] 56%|█████▋    | 439/780 [03:02<01:39,  3.43it/s] 56%|█████▋    | 440/780 [03:02<01:39,  3.43it/s] 57%|█████▋    | 441/780 [03:02<01:38,  3.43it/s] 57%|█████▋    | 442/780 [03:03<01:42,  3.28it/s] 57%|█████▋    | 443/780 [03:03<01:41,  3.32it/s] 57%|█████▋    | 444/780 [03:03<01:40,  3.36it/s] 57%|█████▋    | 445/780 [03:03<01:39,  3.38it/s] 57%|█████▋    | 446/780 [03:04<01:38,  3.39it/s] 57%|█████▋    | 447/780 [03:04<01:37,  3.40it/s] 57%|█████▋    | 448/780 [03:04<01:37,  3.41it/s] 58%|█████▊    | 449/780 [03:05<01:36,  3.42it/s] 58%|█████▊    | 450/780 [03:05<01:36,  3.43it/s] 58%|█████▊    | 451/780 [03:05<01:35,  3.43it/s] 58%|█████▊    | 452/780 [03:05<01:35,  3.43it/s] 58%|█████▊    | 453/780 [03:06<01:40,  3.25it/s] 58%|█████▊    | 454/780 [03:06<01:38,  3.30it/s] 58%|█████▊    | 455/780 [03:06<01:37,  3.34it/s] 58%|█████▊    | 456/780 [03:07<01:36,  3.37it/s] 59%|█████▊    | 457/780 [03:07<01:35,  3.39it/s] 59%|█████▊    | 458/780 [03:07<01:34,  3.41it/s] 59%|█████▉    | 459/780 [03:08<01:33,  3.42it/s] 59%|█████▉    | 460/780 [03:08<01:33,  3.42it/s] 59%|█████▉    | 461/780 [03:08<01:33,  3.42it/s] 59%|█████▉    | 462/780 [03:08<01:32,  3.43it/s] 59%|█████▉    | 463/780 [03:09<01:32,  3.43it/s] 59%|█████▉    | 464/780 [03:09<01:34,  3.34it/s] 60%|█████▉    | 465/780 [03:09<01:33,  3.36it/s] 60%|█████▉    | 466/780 [03:10<01:32,  3.38it/s] 60%|█████▉    | 467/780 [03:10<01:32,  3.40it/s] 60%|██████    | 468/780 [03:10<01:31,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:26:21,384 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:26:21,385 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:26:21,385 >>   Batch size = 8
{'eval_loss': 1.1031092405319214, 'eval_runtime': 9.5337, 'eval_samples_per_second': 365.231, 'eval_steps_per_second': 45.732, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.60it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.28it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.42it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.64it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.06it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.70it/s][A
  9%|▊         | 38/436 [00:00<00:08, 45.93it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.29it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.26it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.17it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.22it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.34it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.36it/s][A
 17%|█▋        | 73/436 [00:01<00:16, 21.66it/s][A
 18%|█▊        | 78/436 [00:02<00:13, 25.82it/s][A
 19%|█▉        | 83/436 [00:02<00:11, 29.78it/s][A
 20%|██        | 88/436 [00:02<00:10, 33.35it/s][A
 21%|██▏       | 93/436 [00:02<00:09, 36.45it/s][A
 22%|██▏       | 98/436 [00:02<00:08, 38.87it/s][A
 24%|██▎       | 103/436 [00:02<00:08, 40.91it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 42.38it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 43.03it/s][A
 27%|██▋       | 118/436 [00:02<00:07, 43.92it/s][A
 28%|██▊       | 123/436 [00:03<00:07, 44.68it/s][A
 29%|██▉       | 128/436 [00:03<00:06, 45.19it/s][A
 31%|███       | 133/436 [00:03<00:06, 45.53it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 45.77it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.84it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.01it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.98it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.77it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.87it/s][A
 39%|███▊      | 168/436 [00:04<00:05, 45.67it/s][A
 40%|███▉      | 173/436 [00:04<00:05, 45.92it/s][A
 41%|████      | 178/436 [00:04<00:05, 46.00it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 46.07it/s][A
 43%|████▎     | 188/436 [00:05<00:05, 46.02it/s][A
 44%|████▍     | 193/436 [00:05<00:13, 18.43it/s][A
 45%|████▌     | 198/436 [00:05<00:10, 22.50it/s][A
 47%|████▋     | 203/436 [00:05<00:08, 26.63it/s][A
 48%|████▊     | 208/436 [00:05<00:07, 30.53it/s][A
 49%|████▉     | 213/436 [00:05<00:06, 33.96it/s][A
 50%|█████     | 218/436 [00:05<00:05, 37.00it/s][A
 51%|█████     | 223/436 [00:05<00:05, 39.40it/s][A
 52%|█████▏    | 228/436 [00:05<00:05, 41.14it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 42.28it/s][A
 55%|█████▍    | 238/436 [00:06<00:04, 43.35it/s][A
 56%|█████▌    | 243/436 [00:06<00:04, 44.12it/s][A
 57%|█████▋    | 248/436 [00:06<00:04, 44.75it/s][A
 58%|█████▊    | 253/436 [00:06<00:04, 45.24it/s][A
 59%|█████▉    | 258/436 [00:06<00:03, 45.58it/s][A
 60%|██████    | 263/436 [00:06<00:03, 45.79it/s][A
 61%|██████▏   | 268/436 [00:06<00:03, 45.97it/s][A
 63%|██████▎   | 273/436 [00:06<00:03, 45.83it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.81it/s][A
 65%|██████▍   | 283/436 [00:07<00:03, 45.82it/s][A
 66%|██████▌   | 288/436 [00:07<00:03, 45.83it/s][A
 67%|██████▋   | 293/436 [00:07<00:03, 45.97it/s][A
 68%|██████▊   | 298/436 [00:07<00:02, 46.09it/s][A
 69%|██████▉   | 303/436 [00:07<00:03, 37.77it/s][A
 71%|███████   | 308/436 [00:07<00:03, 35.76it/s][A
 72%|███████▏  | 313/436 [00:07<00:03, 38.48it/s][A
 73%|███████▎  | 318/436 [00:07<00:02, 40.53it/s][A
 74%|███████▍  | 323/436 [00:08<00:02, 42.13it/s][A
 75%|███████▌  | 328/436 [00:08<00:02, 43.26it/s][A
 76%|███████▋  | 333/436 [00:08<00:02, 44.16it/s][A
 78%|███████▊  | 338/436 [00:08<00:02, 44.80it/s][A
 79%|███████▊  | 343/436 [00:08<00:02, 45.15it/s][A
 80%|███████▉  | 348/436 [00:08<00:01, 45.40it/s][A
 81%|████████  | 353/436 [00:08<00:01, 45.42it/s][A
 82%|████████▏ | 358/436 [00:08<00:01, 45.48it/s][A
 83%|████████▎ | 363/436 [00:08<00:01, 45.59it/s][A
 84%|████████▍ | 368/436 [00:09<00:01, 45.88it/s][A
 86%|████████▌ | 373/436 [00:09<00:01, 45.95it/s][A
 87%|████████▋ | 378/436 [00:09<00:01, 46.11it/s][A
 88%|████████▊ | 383/436 [00:09<00:01, 46.10it/s][A
 89%|████████▉ | 388/436 [00:09<00:01, 46.12it/s][A
 90%|█████████ | 393/436 [00:09<00:00, 46.06it/s][A
 91%|█████████▏| 398/436 [00:09<00:00, 45.99it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 45.85it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 45.83it/s][A
 95%|█████████▍| 413/436 [00:10<00:00, 45.97it/s][A
 96%|█████████▌| 418/436 [00:10<00:00, 46.05it/s][A
 97%|█████████▋| 423/436 [00:10<00:00, 46.18it/s][A
 98%|█████████▊| 428/436 [00:10<00:00, 46.24it/s][A
 99%|█████████▉| 433/436 [00:10<00:00, 46.17it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [03:21<01:31,  3.41it/s]
100%|██████████| 436/436 [00:10<00:00, 46.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:26:31,983 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:26:32,007 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:26:37,485 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:26:37,499 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:26:37,506 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:36<40:29,  7.81s/it] 60%|██████    | 470/780 [03:36<28:47,  5.57s/it] 60%|██████    | 471/780 [03:36<20:32,  3.99s/it] 61%|██████    | 472/780 [03:36<14:46,  2.88s/it] 61%|██████    | 473/780 [03:37<10:45,  2.10s/it] 61%|██████    | 474/780 [03:37<07:56,  1.56s/it] 61%|██████    | 475/780 [03:37<05:59,  1.18s/it] 61%|██████    | 476/780 [03:38<04:37,  1.10it/s] 61%|██████    | 477/780 [03:38<03:39,  1.38it/s] 61%|██████▏   | 478/780 [03:38<02:59,  1.68it/s] 61%|██████▏   | 479/780 [03:38<02:31,  1.99it/s] 62%|██████▏   | 480/780 [03:39<02:11,  2.27it/s] 62%|██████▏   | 481/780 [03:39<01:59,  2.50it/s] 62%|██████▏   | 482/780 [03:39<01:49,  2.73it/s] 62%|██████▏   | 483/780 [03:40<01:42,  2.91it/s] 62%|██████▏   | 484/780 [03:40<01:37,  3.05it/s] 62%|██████▏   | 485/780 [03:40<01:33,  3.16it/s] 62%|██████▏   | 486/780 [03:41<01:30,  3.24it/s] 62%|██████▏   | 487/780 [03:41<01:28,  3.29it/s] 63%|██████▎   | 488/780 [03:41<01:27,  3.34it/s] 63%|██████▎   | 489/780 [03:41<01:26,  3.37it/s] 63%|██████▎   | 490/780 [03:42<01:25,  3.39it/s] 63%|██████▎   | 491/780 [03:42<01:24,  3.40it/s] 63%|██████▎   | 492/780 [03:42<01:37,  2.95it/s] 63%|██████▎   | 493/780 [03:43<01:33,  3.08it/s] 63%|██████▎   | 494/780 [03:43<01:29,  3.18it/s] 63%|██████▎   | 495/780 [03:43<01:27,  3.26it/s] 64%|██████▎   | 496/780 [03:44<01:25,  3.31it/s] 64%|██████▎   | 497/780 [03:44<01:24,  3.35it/s] 64%|██████▍   | 498/780 [03:44<01:23,  3.38it/s] 64%|██████▍   | 499/780 [03:44<01:22,  3.40it/s] 64%|██████▍   | 500/780 [03:45<01:22,  3.41it/s]                                                  64%|██████▍   | 500/780 [03:45<01:22,  3.41it/s] 64%|██████▍   | 501/780 [03:45<01:21,  3.42it/s] 64%|██████▍   | 502/780 [03:45<01:21,  3.40it/s] 64%|██████▍   | 503/780 [03:46<01:21,  3.42it/s] 65%|██████▍   | 504/780 [03:46<01:20,  3.42it/s] 65%|██████▍   | 505/780 [03:46<01:20,  3.43it/s] 65%|██████▍   | 506/780 [03:47<01:19,  3.43it/s] 65%|██████▌   | 507/780 [03:47<01:19,  3.44it/s] 65%|██████▌   | 508/780 [03:47<01:19,  3.44it/s] 65%|██████▌   | 509/780 [03:47<01:18,  3.44it/s] 65%|██████▌   | 510/780 [03:48<01:18,  3.44it/s] 66%|██████▌   | 511/780 [03:48<01:18,  3.44it/s] 66%|██████▌   | 512/780 [03:48<01:17,  3.44it/s] 66%|██████▌   | 513/780 [03:49<01:18,  3.39it/s] 66%|██████▌   | 514/780 [03:49<01:18,  3.40it/s] 66%|██████▌   | 515/780 [03:49<01:20,  3.31it/s] 66%|██████▌   | 516/780 [03:49<01:19,  3.34it/s] 66%|██████▋   | 517/780 [03:50<01:18,  3.37it/s] 66%|██████▋   | 518/780 [03:50<01:17,  3.39it/s] 67%|██████▋   | 519/780 [03:50<01:16,  3.40it/s] 67%|██████▋   | 520/780 [03:51<01:16,  3.41it/s] 67%|██████▋   | 521/780 [03:51<01:15,  3.42it/s] 67%|██████▋   | 522/780 [03:51<01:15,  3.43it/s] 67%|██████▋   | 523/780 [03:52<01:15,  3.43it/s] 67%|██████▋   | 524/780 [03:52<01:15,  3.41it/s] 67%|██████▋   | 525/780 [03:52<01:14,  3.42it/s] 67%|██████▋   | 526/780 [03:52<01:14,  3.43it/s] 68%|██████▊   | 527/780 [03:53<01:13,  3.43it/s] 68%|██████▊   | 528/780 [03:53<01:13,  3.43it/s] 68%|██████▊   | 529/780 [03:53<01:13,  3.43it/s] 68%|██████▊   | 530/780 [03:54<01:12,  3.43it/s] 68%|██████▊   | 531/780 [03:54<01:12,  3.43it/s] 68%|██████▊   | 532/780 [03:54<01:12,  3.44it/s] 68%|██████▊   | 533/780 [03:54<01:11,  3.44it/s] 68%|██████▊   | 534/780 [03:55<01:11,  3.43it/s] 69%|██████▊   | 535/780 [03:55<01:11,  3.43it/s] 69%|██████▊   | 536/780 [03:55<01:11,  3.43it/s] 69%|██████▉   | 537/780 [03:56<01:10,  3.43it/s] 69%|██████▉   | 538/780 [03:56<01:10,  3.43it/s] 69%|██████▉   | 539/780 [03:56<01:10,  3.43it/s] 69%|██████▉   | 540/780 [03:56<01:09,  3.44it/s] 69%|██████▉   | 541/780 [03:57<01:09,  3.44it/s] 69%|██████▉   | 542/780 [03:57<01:09,  3.44it/s] 70%|██████▉   | 543/780 [03:57<01:08,  3.44it/s] 70%|██████▉   | 544/780 [03:58<01:14,  3.15it/s] 70%|██████▉   | 545/780 [03:58<01:12,  3.24it/s] 70%|███████   | 546/780 [03:58<01:11,  3.29it/s] 70%|███████   | 547/780 [03:59<01:09,  3.33it/s] 70%|███████   | 548/780 [03:59<01:09,  3.36it/s] 70%|███████   | 549/780 [03:59<01:08,  3.38it/s] 71%|███████   | 550/780 [03:59<01:07,  3.40it/s] 71%|███████   | 551/780 [04:00<01:07,  3.41it/s] 71%|███████   | 552/780 [04:00<01:06,  3.42it/s] 71%|███████   | 553/780 [04:00<01:06,  3.42it/s] 71%|███████   | 554/780 [04:01<01:06,  3.39it/s] 71%|███████   | 555/780 [04:01<01:06,  3.40it/s] 71%|███████▏  | 556/780 [04:01<01:05,  3.41it/s] 71%|███████▏  | 557/780 [04:02<01:05,  3.42it/s] 72%|███████▏  | 558/780 [04:02<01:04,  3.43it/s] 72%|███████▏  | 559/780 [04:02<01:04,  3.43it/s] 72%|███████▏  | 560/780 [04:02<01:04,  3.43it/s] 72%|███████▏  | 561/780 [04:03<01:03,  3.44it/s] 72%|███████▏  | 562/780 [04:03<01:03,  3.44it/s] 72%|███████▏  | 563/780 [04:03<01:03,  3.44it/s] 72%|███████▏  | 564/780 [04:04<01:02,  3.44it/s] 72%|███████▏  | 565/780 [04:04<01:02,  3.42it/s] 73%|███████▎  | 566/780 [04:04<01:02,  3.43it/s] 73%|███████▎  | 567/780 [04:04<01:02,  3.43it/s] 73%|███████▎  | 568/780 [04:05<01:01,  3.43it/s] 73%|███████▎  | 569/780 [04:05<01:01,  3.43it/s] 73%|███████▎  | 570/780 [04:05<01:01,  3.43it/s] 73%|███████▎  | 571/780 [04:06<01:00,  3.43it/s] 73%|███████▎  | 572/780 [04:06<01:00,  3.44it/s] 73%|███████▎  | 573/780 [04:06<01:00,  3.44it/s] 74%|███████▎  | 574/780 [04:06<00:59,  3.44it/s] 74%|███████▎  | 575/780 [04:07<00:59,  3.44it/s] 74%|███████▍  | 576/780 [04:07<00:59,  3.43it/s] 74%|███████▍  | 577/780 [04:07<00:59,  3.43it/s] 74%|███████▍  | 578/780 [04:08<00:58,  3.43it/s] 74%|███████▍  | 579/780 [04:08<00:58,  3.44it/s] 74%|███████▍  | 580/780 [04:08<00:58,  3.44it/s] 74%|███████▍  | 581/780 [04:08<00:57,  3.44it/s] 75%|███████▍  | 582/780 [04:09<00:57,  3.44it/s] 75%|███████▍  | 583/780 [04:09<00:57,  3.44it/s] 75%|███████▍  | 584/780 [04:09<00:57,  3.43it/s] 75%|███████▌  | 585/780 [04:10<00:56,  3.44it/s] 75%|███████▌  | 586/780 [04:10<00:56,  3.44it/s] 75%|███████▌  | 587/780 [04:10<00:56,  3.43it/s] 75%|███████▌  | 588/780 [04:11<00:55,  3.43it/s] 76%|███████▌  | 589/780 [04:11<00:55,  3.43it/s] 76%|███████▌  | 590/780 [04:11<00:55,  3.43it/s] 76%|███████▌  | 591/780 [04:11<00:55,  3.43it/s] 76%|███████▌  | 592/780 [04:12<00:54,  3.43it/s] 76%|███████▌  | 593/780 [04:12<00:54,  3.44it/s] 76%|███████▌  | 594/780 [04:12<00:54,  3.44it/s] 76%|███████▋  | 595/780 [04:13<00:53,  3.43it/s] 76%|███████▋  | 596/780 [04:13<00:53,  3.43it/s] 77%|███████▋  | 597/780 [04:13<00:53,  3.43it/s] 77%|███████▋  | 598/780 [04:13<00:53,  3.40it/s] 77%|███████▋  | 599/780 [04:14<00:53,  3.41it/s] 77%|███████▋  | 600/780 [04:14<00:52,  3.41it/s] 77%|███████▋  | 601/780 [04:14<00:52,  3.42it/s] 77%|███████▋  | 602/780 [04:15<00:52,  3.42it/s] 77%|███████▋  | 603/780 [04:15<00:51,  3.43it/s] 77%|███████▋  | 604/780 [04:15<00:51,  3.43it/s] 78%|███████▊  | 605/780 [04:15<00:50,  3.43it/s] 78%|███████▊  | 606/780 [04:16<00:50,  3.43it/s] 78%|███████▊  | 607/780 [04:16<00:50,  3.43it/s] 78%|███████▊  | 608/780 [04:16<00:50,  3.44it/s] 78%|███████▊  | 609/780 [04:17<01:08,  2.51it/s] 78%|███████▊  | 610/780 [04:17<01:04,  2.65it/s] 78%|███████▊  | 611/780 [04:18<00:59,  2.85it/s] 78%|███████▊  | 612/780 [04:18<01:24,  1.99it/s] 79%|███████▊  | 613/780 [04:19<01:13,  2.27it/s] 79%|███████▊  | 614/780 [04:19<01:05,  2.53it/s] 79%|███████▉  | 615/780 [04:19<01:00,  2.74it/s] 79%|███████▉  | 616/780 [04:20<00:59,  2.76it/s] 79%|███████▉  | 617/780 [04:20<00:55,  2.93it/s] 79%|███████▉  | 618/780 [04:20<00:52,  3.06it/s] 79%|███████▉  | 619/780 [04:21<00:50,  3.16it/s] 79%|███████▉  | 620/780 [04:21<00:49,  3.23it/s] 80%|███████▉  | 621/780 [04:21<00:48,  3.29it/s] 80%|███████▉  | 622/780 [04:21<00:47,  3.33it/s] 80%|███████▉  | 623/780 [04:22<00:46,  3.36it/s] 80%|████████  | 624/780 [04:22<00:46,  3.39it/s][INFO|trainer.py:2140] 2023-08-28 21:27:33,269 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:27:33,270 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:27:33,270 >>   Batch size = 8
{'eval_loss': 1.1144137382507324, 'eval_runtime': 10.5436, 'eval_samples_per_second': 330.248, 'eval_steps_per_second': 41.352, 'epoch': 3.0}
{'loss': 0.5352, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.84it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.05it/s][A
  4%|▍         | 18/436 [00:00<00:17, 24.52it/s][A
  5%|▌         | 23/436 [00:00<00:14, 29.43it/s][A
  6%|▋         | 28/436 [00:00<00:12, 33.49it/s][A
  8%|▊         | 33/436 [00:00<00:10, 36.85it/s][A
  9%|▊         | 38/436 [00:01<00:10, 39.36it/s][A
 10%|▉         | 43/436 [00:01<00:09, 41.31it/s][A
 11%|█         | 48/436 [00:01<00:09, 42.77it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 43.71it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 44.17it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 44.79it/s][A
 16%|█▌        | 68/436 [00:01<00:08, 45.18it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 45.48it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.87it/s][A
 19%|█▉        | 83/436 [00:02<00:07, 45.92it/s][A
 20%|██        | 88/436 [00:02<00:07, 46.06it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 46.12it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.09it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.94it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.90it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.03it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.00it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.17it/s][A
 29%|██▉       | 128/436 [00:03<00:06, 46.21it/s][A
 31%|███       | 133/436 [00:03<00:06, 46.29it/s][A
 32%|███▏      | 138/436 [00:03<00:06, 46.30it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.11it/s][A
 34%|███▍      | 148/436 [00:03<00:10, 26.65it/s][A
 35%|███▌      | 153/436 [00:03<00:09, 30.50it/s][A
 36%|███▌      | 158/436 [00:03<00:08, 33.95it/s][A
 37%|███▋      | 163/436 [00:04<00:07, 36.98it/s][A
 39%|███▊      | 168/436 [00:04<00:06, 39.40it/s][A
 40%|███▉      | 173/436 [00:04<00:06, 41.26it/s][A
 41%|████      | 178/436 [00:04<00:06, 42.60it/s][A
 42%|████▏     | 183/436 [00:04<00:05, 43.72it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 44.63it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.04it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.48it/s][A
 48%|████▊     | 208/436 [00:05<00:04, 45.66it/s][A
 49%|████▉     | 213/436 [00:05<00:04, 45.87it/s][A
 50%|█████     | 218/436 [00:05<00:09, 22.15it/s][A
 51%|█████     | 223/436 [00:05<00:08, 26.30it/s][A
 52%|█████▏    | 228/436 [00:05<00:06, 30.18it/s][A
 53%|█████▎    | 233/436 [00:05<00:06, 33.74it/s][A
 55%|█████▍    | 238/436 [00:06<00:05, 35.10it/s][A
 56%|█████▌    | 243/436 [00:06<00:05, 37.84it/s][A
 57%|█████▋    | 248/436 [00:06<00:04, 40.00it/s][A
 58%|█████▊    | 253/436 [00:06<00:04, 41.75it/s][A
 59%|█████▉    | 258/436 [00:06<00:04, 42.94it/s][A
 60%|██████    | 263/436 [00:06<00:03, 43.85it/s][A
 61%|██████▏   | 268/436 [00:06<00:03, 44.57it/s][A
 63%|██████▎   | 273/436 [00:06<00:03, 45.15it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.11it/s][A
 65%|██████▍   | 283/436 [00:07<00:03, 45.45it/s][A
 66%|██████▌   | 288/436 [00:07<00:03, 45.71it/s][A
 67%|██████▋   | 293/436 [00:07<00:03, 45.93it/s][A
 68%|██████▊   | 298/436 [00:07<00:03, 45.99it/s][A
 69%|██████▉   | 303/436 [00:07<00:02, 45.99it/s][A
 71%|███████   | 308/436 [00:07<00:02, 46.07it/s][A
 72%|███████▏  | 313/436 [00:07<00:02, 46.05it/s][A
 73%|███████▎  | 318/436 [00:07<00:02, 46.18it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 46.04it/s][A
 75%|███████▌  | 328/436 [00:08<00:02, 46.03it/s][A
 76%|███████▋  | 333/436 [00:08<00:02, 46.00it/s][A
 78%|███████▊  | 338/436 [00:08<00:02, 46.09it/s][A
 79%|███████▊  | 343/436 [00:08<00:02, 46.20it/s][A
 80%|███████▉  | 348/436 [00:08<00:01, 46.11it/s][A
 81%|████████  | 353/436 [00:08<00:01, 46.14it/s][A
 82%|████████▏ | 358/436 [00:08<00:01, 46.04it/s][A
 83%|████████▎ | 363/436 [00:08<00:01, 46.05it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 45.96it/s][A
 86%|████████▌ | 373/436 [00:09<00:01, 40.02it/s][A
 87%|████████▋ | 378/436 [00:09<00:01, 41.70it/s][A
 88%|████████▊ | 383/436 [00:09<00:01, 42.97it/s][A
 89%|████████▉ | 388/436 [00:09<00:01, 43.90it/s][A
 90%|█████████ | 393/436 [00:09<00:00, 44.69it/s][A
 91%|█████████▏| 398/436 [00:09<00:00, 45.21it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 45.53it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 45.78it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 45.42it/s][A
 96%|█████████▌| 418/436 [00:10<00:00, 45.45it/s][A
 97%|█████████▋| 423/436 [00:10<00:00, 45.68it/s][A
 98%|█████████▊| 428/436 [00:10<00:00, 45.90it/s][A
 99%|█████████▉| 433/436 [00:10<00:00, 46.02it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:10<00:00, 46.02it/s][A 80%|████████  | 624/780 [04:33<00:46,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:27:43,742 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 21:27:43,767 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:27:49,822 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:27:49,837 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:27:49,849 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:50<22:05,  8.55s/it] 80%|████████  | 626/780 [04:50<15:36,  6.08s/it] 80%|████████  | 627/780 [04:50<11:04,  4.34s/it] 81%|████████  | 628/780 [04:51<07:55,  3.13s/it] 81%|████████  | 629/780 [04:51<05:43,  2.28s/it] 81%|████████  | 630/780 [04:51<04:11,  1.68s/it] 81%|████████  | 631/780 [04:52<03:08,  1.26s/it] 81%|████████  | 632/780 [04:52<02:23,  1.03it/s] 81%|████████  | 633/780 [04:52<01:52,  1.30it/s] 81%|████████▏ | 634/780 [04:53<01:31,  1.60it/s] 81%|████████▏ | 635/780 [04:53<01:15,  1.91it/s] 82%|████████▏ | 636/780 [04:53<01:05,  2.21it/s] 82%|████████▏ | 637/780 [04:53<00:57,  2.47it/s] 82%|████████▏ | 638/780 [04:54<00:52,  2.70it/s] 82%|████████▏ | 639/780 [04:54<00:48,  2.89it/s] 82%|████████▏ | 640/780 [04:54<00:46,  3.03it/s] 82%|████████▏ | 641/780 [04:55<00:44,  3.15it/s] 82%|████████▏ | 642/780 [04:55<00:42,  3.23it/s] 82%|████████▏ | 643/780 [04:55<00:41,  3.29it/s] 83%|████████▎ | 644/780 [04:55<00:40,  3.34it/s] 83%|████████▎ | 645/780 [04:56<00:40,  3.37it/s] 83%|████████▎ | 646/780 [04:56<00:39,  3.39it/s] 83%|████████▎ | 647/780 [04:56<00:39,  3.41it/s] 83%|████████▎ | 648/780 [04:57<00:40,  3.25it/s] 83%|████████▎ | 649/780 [04:57<00:39,  3.30it/s] 83%|████████▎ | 650/780 [04:57<00:38,  3.35it/s] 83%|████████▎ | 651/780 [04:57<00:38,  3.37it/s] 84%|████████▎ | 652/780 [04:58<00:37,  3.40it/s] 84%|████████▎ | 653/780 [04:58<00:37,  3.41it/s] 84%|████████▍ | 654/780 [04:58<00:36,  3.42it/s] 84%|████████▍ | 655/780 [04:59<00:36,  3.43it/s] 84%|████████▍ | 656/780 [04:59<00:36,  3.43it/s] 84%|████████▍ | 657/780 [04:59<00:35,  3.44it/s] 84%|████████▍ | 658/780 [05:00<00:35,  3.44it/s] 84%|████████▍ | 659/780 [05:00<00:35,  3.44it/s] 85%|████████▍ | 660/780 [05:00<00:34,  3.45it/s] 85%|████████▍ | 661/780 [05:00<00:34,  3.45it/s] 85%|████████▍ | 662/780 [05:01<00:34,  3.45it/s] 85%|████████▌ | 663/780 [05:01<00:33,  3.45it/s] 85%|████████▌ | 664/780 [05:01<00:33,  3.45it/s] 85%|████████▌ | 665/780 [05:02<00:33,  3.45it/s] 85%|████████▌ | 666/780 [05:02<00:33,  3.45it/s] 86%|████████▌ | 667/780 [05:02<00:32,  3.43it/s] 86%|████████▌ | 668/780 [05:02<00:32,  3.43it/s] 86%|████████▌ | 669/780 [05:03<00:32,  3.44it/s] 86%|████████▌ | 670/780 [05:03<00:31,  3.44it/s] 86%|████████▌ | 671/780 [05:03<00:31,  3.44it/s] 86%|████████▌ | 672/780 [05:04<00:31,  3.45it/s] 86%|████████▋ | 673/780 [05:04<00:31,  3.45it/s] 86%|████████▋ | 674/780 [05:04<00:30,  3.45it/s] 87%|████████▋ | 675/780 [05:04<00:30,  3.45it/s] 87%|████████▋ | 676/780 [05:05<00:30,  3.45it/s] 87%|████████▋ | 677/780 [05:05<00:29,  3.45it/s] 87%|████████▋ | 678/780 [05:05<00:30,  3.32it/s] 87%|████████▋ | 679/780 [05:06<00:30,  3.36it/s] 87%|████████▋ | 680/780 [05:06<00:29,  3.39it/s] 87%|████████▋ | 681/780 [05:06<00:29,  3.40it/s] 87%|████████▋ | 682/780 [05:07<00:28,  3.42it/s] 88%|████████▊ | 683/780 [05:07<00:28,  3.43it/s] 88%|████████▊ | 684/780 [05:07<00:27,  3.43it/s] 88%|████████▊ | 685/780 [05:07<00:27,  3.44it/s] 88%|████████▊ | 686/780 [05:08<00:27,  3.44it/s] 88%|████████▊ | 687/780 [05:08<00:27,  3.44it/s] 88%|████████▊ | 688/780 [05:08<00:26,  3.44it/s] 88%|████████▊ | 689/780 [05:09<00:27,  3.26it/s] 88%|████████▊ | 690/780 [05:09<00:27,  3.31it/s] 89%|████████▊ | 691/780 [05:09<00:26,  3.35it/s] 89%|████████▊ | 692/780 [05:09<00:26,  3.38it/s] 89%|████████▉ | 693/780 [05:10<00:25,  3.40it/s] 89%|████████▉ | 694/780 [05:10<00:25,  3.41it/s] 89%|████████▉ | 695/780 [05:10<00:24,  3.42it/s] 89%|████████▉ | 696/780 [05:11<00:24,  3.42it/s] 89%|████████▉ | 697/780 [05:11<00:24,  3.42it/s] 89%|████████▉ | 698/780 [05:11<00:23,  3.43it/s] 90%|████████▉ | 699/780 [05:12<00:23,  3.43it/s] 90%|████████▉ | 700/780 [05:12<00:24,  3.25it/s] 90%|████████▉ | 701/780 [05:12<00:23,  3.31it/s] 90%|█████████ | 702/780 [05:12<00:23,  3.35it/s] 90%|█████████ | 703/780 [05:13<00:22,  3.38it/s] 90%|█████████ | 704/780 [05:13<00:22,  3.40it/s] 90%|█████████ | 705/780 [05:13<00:21,  3.41it/s] 91%|█████████ | 706/780 [05:14<00:21,  3.42it/s] 91%|█████████ | 707/780 [05:14<00:21,  3.43it/s] 91%|█████████ | 708/780 [05:14<00:20,  3.43it/s] 91%|█████████ | 709/780 [05:14<00:20,  3.43it/s] 91%|█████████ | 710/780 [05:15<00:20,  3.44it/s] 91%|█████████ | 711/780 [05:15<00:20,  3.42it/s] 91%|█████████▏| 712/780 [05:15<00:19,  3.42it/s] 91%|█████████▏| 713/780 [05:16<00:19,  3.43it/s] 92%|█████████▏| 714/780 [05:16<00:19,  3.43it/s] 92%|█████████▏| 715/780 [05:16<00:18,  3.44it/s] 92%|█████████▏| 716/780 [05:17<00:18,  3.44it/s] 92%|█████████▏| 717/780 [05:17<00:18,  3.44it/s] 92%|█████████▏| 718/780 [05:17<00:18,  3.44it/s] 92%|█████████▏| 719/780 [05:17<00:18,  3.30it/s] 92%|█████████▏| 720/780 [05:18<00:18,  3.33it/s] 92%|█████████▏| 721/780 [05:18<00:17,  3.35it/s] 93%|█████████▎| 722/780 [05:18<00:17,  3.32it/s] 93%|█████████▎| 723/780 [05:19<00:16,  3.36it/s] 93%|█████████▎| 724/780 [05:19<00:16,  3.38it/s] 93%|█████████▎| 725/780 [05:19<00:16,  3.40it/s] 93%|█████████▎| 726/780 [05:19<00:15,  3.41it/s] 93%|█████████▎| 727/780 [05:20<00:15,  3.41it/s] 93%|█████████▎| 728/780 [05:20<00:15,  3.42it/s] 93%|█████████▎| 729/780 [05:20<00:14,  3.42it/s] 94%|█████████▎| 730/780 [05:21<00:14,  3.43it/s] 94%|█████████▎| 731/780 [05:21<00:14,  3.43it/s] 94%|█████████▍| 732/780 [05:21<00:13,  3.44it/s] 94%|█████████▍| 733/780 [05:22<00:13,  3.43it/s] 94%|█████████▍| 734/780 [05:22<00:13,  3.43it/s] 94%|█████████▍| 735/780 [05:22<00:13,  3.43it/s] 94%|█████████▍| 736/780 [05:22<00:12,  3.44it/s] 94%|█████████▍| 737/780 [05:23<00:12,  3.44it/s] 95%|█████████▍| 738/780 [05:23<00:12,  3.44it/s] 95%|█████████▍| 739/780 [05:23<00:11,  3.44it/s] 95%|█████████▍| 740/780 [05:24<00:11,  3.44it/s] 95%|█████████▌| 741/780 [05:24<00:11,  3.44it/s] 95%|█████████▌| 742/780 [05:24<00:11,  3.44it/s] 95%|█████████▌| 743/780 [05:24<00:10,  3.44it/s] 95%|█████████▌| 744/780 [05:25<00:10,  3.37it/s] 96%|█████████▌| 745/780 [05:25<00:10,  3.39it/s] 96%|█████████▌| 746/780 [05:25<00:09,  3.40it/s] 96%|█████████▌| 747/780 [05:26<00:09,  3.42it/s] 96%|█████████▌| 748/780 [05:26<00:09,  3.42it/s] 96%|█████████▌| 749/780 [05:26<00:09,  3.43it/s] 96%|█████████▌| 750/780 [05:26<00:08,  3.43it/s] 96%|█████████▋| 751/780 [05:27<00:08,  3.43it/s] 96%|█████████▋| 752/780 [05:27<00:08,  3.43it/s] 97%|█████████▋| 753/780 [05:27<00:07,  3.42it/s] 97%|█████████▋| 754/780 [05:28<00:07,  3.43it/s] 97%|█████████▋| 755/780 [05:28<00:07,  3.31it/s] 97%|█████████▋| 756/780 [05:28<00:07,  3.35it/s] 97%|█████████▋| 757/780 [05:29<00:06,  3.37it/s] 97%|█████████▋| 758/780 [05:29<00:06,  3.40it/s] 97%|█████████▋| 759/780 [05:29<00:06,  3.41it/s] 97%|█████████▋| 760/780 [05:29<00:05,  3.42it/s] 98%|█████████▊| 761/780 [05:30<00:05,  3.42it/s] 98%|█████████▊| 762/780 [05:30<00:05,  3.42it/s] 98%|█████████▊| 763/780 [05:30<00:04,  3.42it/s] 98%|█████████▊| 764/780 [05:31<00:04,  3.43it/s] 98%|█████████▊| 765/780 [05:31<00:04,  3.43it/s] 98%|█████████▊| 766/780 [05:31<00:04,  3.43it/s] 98%|█████████▊| 767/780 [05:31<00:03,  3.44it/s] 98%|█████████▊| 768/780 [05:32<00:03,  3.44it/s] 99%|█████████▊| 769/780 [05:32<00:03,  3.44it/s] 99%|█████████▊| 770/780 [05:32<00:02,  3.44it/s] 99%|█████████▉| 771/780 [05:33<00:02,  3.44it/s] 99%|█████████▉| 772/780 [05:33<00:02,  3.32it/s] 99%|█████████▉| 773/780 [05:33<00:02,  3.35it/s] 99%|█████████▉| 774/780 [05:34<00:01,  3.38it/s] 99%|█████████▉| 775/780 [05:34<00:01,  3.40it/s] 99%|█████████▉| 776/780 [05:34<00:01,  3.41it/s]100%|█████████▉| 777/780 [05:34<00:00,  3.27it/s]100%|█████████▉| 778/780 [05:35<00:00,  3.31it/s]100%|█████████▉| 779/780 [05:35<00:00,  3.35it/s]100%|██████████| 780/780 [05:35<00:00,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 21:28:46,500 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:28:46,500 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:28:46,500 >>   Batch size = 8
{'eval_loss': 1.124680519104004, 'eval_runtime': 10.4504, 'eval_samples_per_second': 333.192, 'eval_steps_per_second': 41.721, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.50it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.76it/s][A
  4%|▍         | 18/436 [00:00<00:08, 47.94it/s][A
  5%|▌         | 23/436 [00:00<00:09, 45.63it/s][A
  6%|▋         | 28/436 [00:00<00:08, 45.70it/s][A
  8%|▊         | 33/436 [00:00<00:08, 45.78it/s][A
  9%|▊         | 38/436 [00:00<00:08, 45.88it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.68it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.86it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.03it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.15it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.13it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.13it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.24it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.21it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.21it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.11it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 46.12it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.12it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.16it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.20it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 45.65it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.33it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.32it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.26it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.19it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.08it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 46.07it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.94it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.07it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.15it/s][A
 37%|███▋      | 163/436 [00:03<00:10, 26.17it/s][A
 39%|███▊      | 168/436 [00:03<00:08, 30.10it/s][A
 40%|███▉      | 173/436 [00:04<00:07, 33.67it/s][A
 41%|████      | 178/436 [00:04<00:07, 36.64it/s][A
 42%|████▏     | 183/436 [00:04<00:06, 39.07it/s][A
 43%|████▎     | 188/436 [00:04<00:06, 41.05it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 42.53it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 43.63it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 44.04it/s][A
 48%|████▊     | 208/436 [00:04<00:05, 44.56it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 45.09it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.50it/s][A
 51%|█████     | 223/436 [00:05<00:04, 45.75it/s][A
 52%|█████▏    | 228/436 [00:05<00:04, 45.82it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.08it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.11it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.07it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.95it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.96it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.00it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.08it/s][A
 61%|██████▏   | 268/436 [00:06<00:03, 46.09it/s][A
 63%|██████▎   | 273/436 [00:06<00:03, 46.14it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.18it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.04it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.11it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 41.08it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 42.51it/s][A
 69%|██████▉   | 303/436 [00:06<00:03, 43.60it/s][A
 71%|███████   | 308/436 [00:06<00:02, 44.39it/s][A
 72%|███████▏  | 313/436 [00:07<00:02, 44.99it/s][A
 73%|███████▎  | 318/436 [00:07<00:02, 45.37it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.70it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.90it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.38it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.43it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.67it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 45.83it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.99it/s][A
 82%|████████▏ | 358/436 [00:08<00:01, 46.07it/s][A
 83%|████████▎ | 363/436 [00:08<00:01, 46.18it/s][A
 84%|████████▍ | 368/436 [00:08<00:01, 46.27it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.28it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.11it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.92it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.91it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.01it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.03it/s][A
 92%|█████████▏| 403/436 [00:09<00:00, 46.14it/s][A
 94%|█████████▎| 408/436 [00:09<00:00, 46.17it/s][A
 95%|█████████▍| 413/436 [00:09<00:00, 46.19it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.17it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.06it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.93it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.93it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.93it/s][A100%|██████████| 780/780 [05:45<00:00,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:28:56,340 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 21:28:56,360 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:29:01,844 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:29:01,873 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:29:01,885 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:29:21,845 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:29:21,891 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156 (score: 1.081212043762207).
                                                 100%|██████████| 780/780 [06:20<00:00,  3.37it/s]100%|██████████| 780/780 [06:20<00:00,  2.05it/s]
[INFO|trainer.py:1894] 2023-08-28 21:29:31,345 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 21:29:31,395 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:29:36,769 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:29:36,842 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:29:36,867 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:29:37,247 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:37,247 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:37,247 >>   train_loss               =     0.5253
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:37,247 >>   train_runtime            = 0:06:20.65
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:37,248 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:37,248 >>   train_samples_per_second =    131.341
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:37,248 >>   train_steps_per_second   =      2.049
{'eval_loss': 1.127209186553955, 'eval_runtime': 9.778, 'eval_samples_per_second': 356.104, 'eval_steps_per_second': 44.59, 'epoch': 5.0}
{'train_runtime': 380.6511, 'train_samples_per_second': 131.341, 'train_steps_per_second': 2.049, 'train_loss': 0.5253005590194311, 'epoch': 5.0}
08/28/2023 21:29:37 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:29:37,293 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:29:37,293 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-28 21:29:37,293 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 58.05it/s]  3%|▎         | 12/436 [00:00<00:08, 50.96it/s]  4%|▍         | 18/436 [00:00<00:08, 49.04it/s]  5%|▌         | 23/436 [00:00<00:08, 48.29it/s]  6%|▋         | 28/436 [00:00<00:08, 47.90it/s]  8%|▊         | 33/436 [00:00<00:08, 47.62it/s]  9%|▊         | 38/436 [00:00<00:08, 47.38it/s] 10%|▉         | 43/436 [00:00<00:08, 47.08it/s] 11%|█         | 48/436 [00:01<00:08, 46.85it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.76it/s] 13%|█▎        | 58/436 [00:01<00:08, 45.64it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.37it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.56it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.66it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.74it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.80it/s] 20%|██        | 88/436 [00:01<00:07, 46.74it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.59it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.66it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.56it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.66it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.60it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.64it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.80it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.80it/s] 31%|███       | 133/436 [00:02<00:06, 46.76it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.65it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.72it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.57it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.64it/s] 36%|███▌      | 158/436 [00:03<00:05, 46.68it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.72it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.67it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.63it/s] 41%|████      | 178/436 [00:03<00:05, 46.74it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.74it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.61it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.65it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.69it/s] 47%|████▋     | 203/436 [00:04<00:04, 46.65it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.65it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.62it/s] 50%|█████     | 218/436 [00:04<00:04, 46.68it/s] 51%|█████     | 223/436 [00:04<00:04, 46.73it/s] 52%|█████▏    | 228/436 [00:05<00:07, 28.43it/s] 53%|█████▎    | 233/436 [00:05<00:06, 32.23it/s] 55%|█████▍    | 238/436 [00:05<00:05, 35.55it/s] 56%|█████▌    | 243/436 [00:05<00:05, 38.33it/s] 57%|█████▋    | 248/436 [00:05<00:04, 40.52it/s] 58%|█████▊    | 253/436 [00:05<00:04, 42.23it/s] 59%|█████▉    | 258/436 [00:05<00:04, 43.53it/s] 60%|██████    | 263/436 [00:05<00:03, 44.45it/s] 61%|██████▏   | 268/436 [00:05<00:03, 44.70it/s] 63%|██████▎   | 273/436 [00:06<00:03, 45.15it/s] 64%|██████▍   | 278/436 [00:06<00:03, 45.56it/s] 65%|██████▍   | 283/436 [00:06<00:03, 45.86it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.13it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.34it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.49it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.54it/s] 71%|███████   | 308/436 [00:06<00:02, 46.59it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.52it/s] 73%|███████▎  | 318/436 [00:07<00:02, 46.40it/s] 74%|███████▍  | 323/436 [00:07<00:02, 46.43it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.55it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.60it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.64it/s] 79%|███████▊  | 343/436 [00:07<00:01, 46.60it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.61it/s] 81%|████████  | 353/436 [00:07<00:01, 46.66it/s] 82%|████████▏ | 358/436 [00:07<00:01, 45.19it/s] 83%|████████▎ | 363/436 [00:07<00:01, 45.64it/s] 84%|████████▍ | 368/436 [00:08<00:01, 45.92it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.12it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.20it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.31it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.40it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.51it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.43it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.36it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.45it/s] 95%|█████████▍| 413/436 [00:09<00:00, 46.48it/s] 96%|█████████▌| 418/436 [00:09<00:00, 46.44it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.51it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.58it/s] 99%|█████████▉| 433/436 [00:09<00:00, 46.56it/s]100%|██████████| 436/436 [00:09<00:00, 45.58it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:29:46,882 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:46,882 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:46,882 >>   eval_loss               =     1.0812
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:46,882 >>   eval_runtime            = 0:00:09.58
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:46,882 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:46,883 >>   eval_samples_per_second =    363.127
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:46,883 >>   eval_steps_per_second   =     45.469
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:29:46,883 >>   perplexity              =     2.9483
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:57,486 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:57,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:57,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:57,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:29:57,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:29:59,405 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:29:59,406 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:30:00,296 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:30:01,335 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:30:01,384 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:06,617 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:06,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:06,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:06,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:30:06,706 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:30:07,366 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:30:07,368 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:30:08,605 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:30:08,757 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:30:08,757 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.44it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:16,  1.45it/s]Extractor Predicting: 26it [00:17,  1.45it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:19,  1.44it/s]Extractor Predicting: 30it [00:20,  1.45it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:22,  1.43it/s]Extractor Predicting: 34it [00:23,  1.42it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.41it/s]Extractor Predicting: 37it [00:25,  1.42it/s]Extractor Predicting: 38it [00:26,  1.43it/s]Extractor Predicting: 39it [00:26,  1.43it/s]Extractor Predicting: 40it [00:27,  1.45it/s]Extractor Predicting: 41it [00:28,  1.46it/s]Extractor Predicting: 42it [00:28,  1.47it/s]Extractor Predicting: 43it [00:29,  1.46it/s]Extractor Predicting: 44it [00:30,  1.45it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:31,  1.48it/s]Extractor Predicting: 47it [00:32,  1.34it/s]Extractor Predicting: 48it [00:33,  1.36it/s]Extractor Predicting: 49it [00:33,  1.37it/s]Extractor Predicting: 50it [00:34,  1.38it/s]Extractor Predicting: 51it [00:35,  1.39it/s]Extractor Predicting: 52it [00:35,  1.42it/s]Extractor Predicting: 53it [00:36,  1.42it/s]Extractor Predicting: 54it [00:37,  1.40it/s]Extractor Predicting: 55it [00:38,  1.41it/s]Extractor Predicting: 56it [00:38,  1.40it/s]Extractor Predicting: 57it [00:39,  1.43it/s]Extractor Predicting: 58it [00:40,  1.41it/s]Extractor Predicting: 59it [00:40,  1.41it/s]Extractor Predicting: 60it [00:41,  1.42it/s]Extractor Predicting: 61it [00:42,  1.44it/s]Extractor Predicting: 62it [00:42,  1.45it/s]Extractor Predicting: 63it [00:43,  1.41it/s]Extractor Predicting: 64it [00:44,  1.40it/s]Extractor Predicting: 65it [00:45,  1.45it/s]Extractor Predicting: 66it [00:45,  1.44it/s]Extractor Predicting: 67it [00:46,  1.47it/s]Extractor Predicting: 68it [00:47,  1.47it/s]Extractor Predicting: 69it [00:47,  1.47it/s]Extractor Predicting: 70it [00:48,  1.51it/s]Extractor Predicting: 71it [00:49,  1.50it/s]Extractor Predicting: 72it [00:49,  1.51it/s]Extractor Predicting: 73it [00:50,  1.56it/s]Extractor Predicting: 74it [00:50,  1.56it/s]Extractor Predicting: 75it [00:51,  1.52it/s]Extractor Predicting: 76it [00:52,  1.47it/s]Extractor Predicting: 77it [00:53,  1.47it/s]Extractor Predicting: 78it [00:53,  1.48it/s]Extractor Predicting: 79it [00:54,  1.47it/s]Extractor Predicting: 80it [00:55,  1.47it/s]Extractor Predicting: 81it [00:55,  1.45it/s]Extractor Predicting: 82it [00:56,  1.45it/s]Extractor Predicting: 83it [00:57,  1.47it/s]Extractor Predicting: 84it [00:57,  1.44it/s]Extractor Predicting: 85it [00:58,  1.49it/s]Extractor Predicting: 86it [00:59,  1.39it/s]Extractor Predicting: 87it [01:00,  1.43it/s]Extractor Predicting: 88it [01:00,  1.41it/s]Extractor Predicting: 89it [01:01,  1.40it/s]Extractor Predicting: 90it [01:02,  1.40it/s]Extractor Predicting: 91it [01:02,  1.39it/s]Extractor Predicting: 92it [01:03,  1.41it/s]Extractor Predicting: 93it [01:04,  1.44it/s]Extractor Predicting: 94it [01:04,  1.41it/s]Extractor Predicting: 95it [01:05,  1.27it/s]Extractor Predicting: 96it [01:06,  1.33it/s]Extractor Predicting: 97it [01:07,  1.36it/s]Extractor Predicting: 98it [01:08,  1.37it/s]Extractor Predicting: 99it [01:08,  1.34it/s]Extractor Predicting: 100it [01:09,  1.35it/s]Extractor Predicting: 101it [01:10,  1.39it/s]Extractor Predicting: 102it [01:10,  1.39it/s]Extractor Predicting: 103it [01:11,  1.39it/s]Extractor Predicting: 104it [01:12,  1.40it/s]Extractor Predicting: 105it [01:13,  1.41it/s]Extractor Predicting: 106it [01:13,  1.38it/s]Extractor Predicting: 107it [01:14,  1.40it/s]Extractor Predicting: 108it [01:15,  1.43it/s]Extractor Predicting: 109it [01:15,  1.46it/s]Extractor Predicting: 110it [01:16,  1.47it/s]Extractor Predicting: 111it [01:17,  1.51it/s]Extractor Predicting: 112it [01:17,  1.49it/s]Extractor Predicting: 113it [01:18,  1.49it/s]Extractor Predicting: 114it [01:19,  1.43it/s]Extractor Predicting: 115it [01:19,  1.42it/s]Extractor Predicting: 116it [01:20,  1.43it/s]Extractor Predicting: 117it [01:21,  1.42it/s]Extractor Predicting: 118it [01:22,  1.42it/s]Extractor Predicting: 119it [01:22,  1.42it/s]Extractor Predicting: 120it [01:23,  1.46it/s]Extractor Predicting: 121it [01:24,  1.44it/s]Extractor Predicting: 122it [01:24,  1.46it/s]Extractor Predicting: 123it [01:25,  1.42it/s]Extractor Predicting: 124it [01:26,  1.14it/s]Extractor Predicting: 125it [01:27,  1.22it/s]Extractor Predicting: 126it [01:28,  1.29it/s]Extractor Predicting: 127it [01:28,  1.33it/s]Extractor Predicting: 128it [01:29,  1.25it/s]Extractor Predicting: 129it [01:30,  1.31it/s]Extractor Predicting: 130it [01:31,  1.22it/s]Extractor Predicting: 131it [01:32,  1.29it/s]Extractor Predicting: 132it [01:32,  1.32it/s]Extractor Predicting: 133it [01:33,  1.32it/s]Extractor Predicting: 134it [01:34,  1.31it/s]Extractor Predicting: 135it [01:35,  1.34it/s]Extractor Predicting: 136it [01:35,  1.37it/s]Extractor Predicting: 137it [01:36,  1.30it/s]Extractor Predicting: 138it [01:37,  1.34it/s]Extractor Predicting: 139it [01:38,  1.34it/s]Extractor Predicting: 140it [01:38,  1.34it/s]Extractor Predicting: 141it [01:39,  1.28it/s]Extractor Predicting: 142it [01:40,  1.34it/s]Extractor Predicting: 143it [01:40,  1.40it/s]Extractor Predicting: 143it [01:40,  1.42it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:03,112 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:03,116 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:03,117 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:03,117 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:03,117 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:32:03,541 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:32:03,542 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:32:04,224 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:32:05,251 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:32:05,251 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:06,976 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:06,991 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:06,991 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:06,991 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:32:06,991 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:32:07,736 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:32:07,737 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:32:08,007 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:32:08,150 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:32:08,150 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3834080717488789,
  "recall": 0.04910970706490523,
  "score": 0.08706720977596742,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.40it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.47it/s]Extractor Predicting: 22it [00:14,  1.45it/s]Extractor Predicting: 23it [00:15,  1.50it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.60it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.59it/s]Extractor Predicting: 29it [00:19,  1.59it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:21,  1.44it/s]Extractor Predicting: 34it [00:22,  1.44it/s]Extractor Predicting: 35it [00:23,  1.43it/s]Extractor Predicting: 36it [00:23,  1.44it/s]Extractor Predicting: 37it [00:24,  1.44it/s]Extractor Predicting: 38it [00:25,  1.44it/s]Extractor Predicting: 39it [00:26,  1.45it/s]Extractor Predicting: 40it [00:26,  1.44it/s]Extractor Predicting: 41it [00:27,  1.46it/s]Extractor Predicting: 42it [00:28,  1.47it/s]Extractor Predicting: 43it [00:28,  1.42it/s]Extractor Predicting: 44it [00:29,  1.44it/s]Extractor Predicting: 45it [00:30,  1.44it/s]Extractor Predicting: 46it [00:30,  1.45it/s]Extractor Predicting: 47it [00:31,  1.45it/s]Extractor Predicting: 48it [00:32,  1.42it/s]Extractor Predicting: 49it [00:33,  1.42it/s]Extractor Predicting: 50it [00:33,  1.44it/s]Extractor Predicting: 51it [00:34,  1.45it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.45it/s]Extractor Predicting: 54it [00:36,  1.46it/s]Extractor Predicting: 55it [00:37,  1.45it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:38,  1.48it/s]Extractor Predicting: 58it [00:39,  1.46it/s]Extractor Predicting: 59it [00:39,  1.45it/s]Extractor Predicting: 60it [00:40,  1.46it/s]Extractor Predicting: 61it [00:41,  1.44it/s]Extractor Predicting: 62it [00:41,  1.46it/s]Extractor Predicting: 63it [00:43,  1.17it/s]Extractor Predicting: 64it [00:43,  1.26it/s]Extractor Predicting: 65it [00:44,  1.33it/s]Extractor Predicting: 66it [00:45,  1.34it/s]Extractor Predicting: 67it [00:46,  1.23it/s]Extractor Predicting: 68it [00:46,  1.28it/s]Extractor Predicting: 69it [00:47,  1.35it/s]Extractor Predicting: 70it [00:48,  1.40it/s]Extractor Predicting: 71it [00:48,  1.41it/s]Extractor Predicting: 72it [00:49,  1.36it/s]Extractor Predicting: 73it [00:50,  1.38it/s]Extractor Predicting: 74it [00:50,  1.43it/s]Extractor Predicting: 75it [00:51,  1.46it/s]Extractor Predicting: 76it [00:52,  1.47it/s]Extractor Predicting: 77it [00:52,  1.48it/s]Extractor Predicting: 78it [00:53,  1.53it/s]Extractor Predicting: 79it [00:54,  1.52it/s]Extractor Predicting: 80it [00:54,  1.49it/s]Extractor Predicting: 81it [00:55,  1.48it/s]Extractor Predicting: 82it [00:56,  1.47it/s]Extractor Predicting: 83it [00:57,  1.46it/s]Extractor Predicting: 84it [00:57,  1.45it/s]Extractor Predicting: 85it [00:58,  1.45it/s]Extractor Predicting: 86it [00:59,  1.47it/s]Extractor Predicting: 87it [00:59,  1.48it/s]Extractor Predicting: 88it [01:00,  1.48it/s]Extractor Predicting: 89it [01:01,  1.52it/s]Extractor Predicting: 90it [01:01,  1.53it/s]Extractor Predicting: 91it [01:02,  1.51it/s]Extractor Predicting: 92it [01:03,  1.41it/s]Extractor Predicting: 93it [01:03,  1.43it/s]Extractor Predicting: 94it [01:04,  1.43it/s]Extractor Predicting: 95it [01:05,  1.43it/s]Extractor Predicting: 96it [01:05,  1.43it/s]Extractor Predicting: 97it [01:06,  1.45it/s]Extractor Predicting: 98it [01:07,  1.44it/s]Extractor Predicting: 99it [01:07,  1.46it/s]Extractor Predicting: 100it [01:08,  1.45it/s]Extractor Predicting: 101it [01:09,  1.45it/s]Extractor Predicting: 102it [01:10,  1.47it/s]Extractor Predicting: 103it [01:10,  1.47it/s]Extractor Predicting: 104it [01:11,  1.47it/s]Extractor Predicting: 105it [01:12,  1.46it/s]Extractor Predicting: 106it [01:12,  1.46it/s]Extractor Predicting: 107it [01:13,  1.45it/s]Extractor Predicting: 108it [01:14,  1.46it/s]Extractor Predicting: 109it [01:14,  1.46it/s]Extractor Predicting: 110it [01:15,  1.45it/s]Extractor Predicting: 111it [01:16,  1.45it/s]Extractor Predicting: 112it [01:16,  1.44it/s]Extractor Predicting: 113it [01:17,  1.46it/s]Extractor Predicting: 114it [01:18,  1.46it/s]Extractor Predicting: 115it [01:18,  1.45it/s]Extractor Predicting: 116it [01:19,  1.50it/s]Extractor Predicting: 117it [01:20,  1.57it/s]Extractor Predicting: 118it [01:20,  1.61it/s]Extractor Predicting: 119it [01:21,  1.60it/s]Extractor Predicting: 120it [01:22,  1.58it/s]Extractor Predicting: 121it [01:22,  1.38it/s]Extractor Predicting: 122it [01:23,  1.41it/s]Extractor Predicting: 123it [01:24,  1.43it/s]Extractor Predicting: 124it [01:24,  1.49it/s]Extractor Predicting: 125it [01:25,  1.56it/s]Extractor Predicting: 126it [01:26,  1.55it/s]Extractor Predicting: 127it [01:26,  1.56it/s]Extractor Predicting: 128it [01:27,  1.59it/s]Extractor Predicting: 129it [01:28,  1.57it/s]Extractor Predicting: 130it [01:28,  1.57it/s]Extractor Predicting: 131it [01:29,  1.58it/s]Extractor Predicting: 132it [01:29,  1.58it/s]Extractor Predicting: 133it [01:30,  1.57it/s]Extractor Predicting: 134it [01:31,  1.59it/s]Extractor Predicting: 135it [01:31,  1.61it/s]Extractor Predicting: 136it [01:32,  1.64it/s]Extractor Predicting: 137it [01:32,  1.64it/s]Extractor Predicting: 138it [01:33,  1.61it/s]Extractor Predicting: 139it [01:34,  1.58it/s]Extractor Predicting: 140it [01:34,  1.59it/s]Extractor Predicting: 141it [01:35,  1.56it/s]Extractor Predicting: 142it [01:36,  1.56it/s]Extractor Predicting: 143it [01:36,  1.58it/s]Extractor Predicting: 144it [01:37,  1.55it/s]Extractor Predicting: 145it [01:38,  1.55it/s]Extractor Predicting: 146it [01:38,  1.56it/s]Extractor Predicting: 147it [01:39,  1.56it/s]Extractor Predicting: 148it [01:40,  1.53it/s]Extractor Predicting: 149it [01:40,  1.58it/s]Extractor Predicting: 150it [01:41,  1.56it/s]Extractor Predicting: 151it [01:42,  1.55it/s]Extractor Predicting: 152it [01:42,  1.57it/s]Extractor Predicting: 153it [01:43,  1.62it/s]Extractor Predicting: 154it [01:43,  1.55it/s]Extractor Predicting: 155it [01:44,  1.57it/s]Extractor Predicting: 156it [01:45,  1.57it/s]Extractor Predicting: 157it [01:45,  1.57it/s]Extractor Predicting: 158it [01:46,  1.58it/s]Extractor Predicting: 159it [01:47,  1.55it/s]Extractor Predicting: 160it [01:47,  1.55it/s]Extractor Predicting: 161it [01:48,  1.56it/s]Extractor Predicting: 162it [01:49,  1.56it/s]Extractor Predicting: 163it [01:49,  1.56it/s]Extractor Predicting: 164it [01:50,  1.56it/s]Extractor Predicting: 165it [01:50,  1.57it/s]Extractor Predicting: 166it [01:51,  1.58it/s]Extractor Predicting: 167it [01:52,  1.58it/s]Extractor Predicting: 168it [01:52,  1.54it/s]Extractor Predicting: 169it [01:53,  1.51it/s]Extractor Predicting: 170it [01:54,  1.50it/s]Extractor Predicting: 171it [01:54,  1.49it/s]Extractor Predicting: 172it [01:55,  1.56it/s]Extractor Predicting: 173it [01:56,  1.51it/s]Extractor Predicting: 174it [01:56,  1.47it/s]Extractor Predicting: 175it [01:57,  1.46it/s]Extractor Predicting: 176it [01:58,  1.47it/s]Extractor Predicting: 177it [01:59,  1.45it/s]Extractor Predicting: 178it [01:59,  1.44it/s]Extractor Predicting: 179it [02:00,  1.38it/s]Extractor Predicting: 180it [02:01,  1.44it/s]Extractor Predicting: 181it [02:01,  1.44it/s]Extractor Predicting: 182it [02:02,  1.44it/s]Extractor Predicting: 183it [02:03,  1.44it/s]Extractor Predicting: 184it [02:04,  1.38it/s]Extractor Predicting: 185it [02:04,  1.40it/s]Extractor Predicting: 186it [02:05,  1.41it/s]Extractor Predicting: 187it [02:06,  1.41it/s]Extractor Predicting: 188it [02:06,  1.45it/s]Extractor Predicting: 189it [02:07,  1.40it/s]Extractor Predicting: 190it [02:08,  1.10it/s]Extractor Predicting: 191it [02:09,  1.17it/s]Extractor Predicting: 192it [02:10,  1.25it/s]Extractor Predicting: 193it [02:11,  1.28it/s]Extractor Predicting: 194it [02:11,  1.28it/s]Extractor Predicting: 195it [02:12,  1.32it/s]Extractor Predicting: 196it [02:13,  1.36it/s]Extractor Predicting: 197it [02:13,  1.37it/s]Extractor Predicting: 198it [02:14,  1.39it/s]Extractor Predicting: 199it [02:15,  1.40it/s]Extractor Predicting: 200it [02:16,  1.40it/s]Extractor Predicting: 201it [02:16,  1.42it/s]Extractor Predicting: 202it [02:17,  1.43it/s]Extractor Predicting: 203it [02:18,  1.45it/s]Extractor Predicting: 204it [02:18,  1.45it/s]Extractor Predicting: 205it [02:19,  1.50it/s]Extractor Predicting: 206it [02:20,  1.49it/s]Extractor Predicting: 207it [02:20,  1.49it/s]Extractor Predicting: 208it [02:21,  1.49it/s]Extractor Predicting: 209it [02:22,  1.27it/s]Extractor Predicting: 210it [02:23,  1.34it/s]Extractor Predicting: 211it [02:23,  1.42it/s]Extractor Predicting: 212it [02:24,  1.45it/s]Extractor Predicting: 213it [02:24,  1.50it/s]Extractor Predicting: 214it [02:25,  1.44it/s]Extractor Predicting: 215it [02:26,  1.43it/s]Extractor Predicting: 216it [02:27,  1.46it/s]Extractor Predicting: 217it [02:27,  1.48it/s]Extractor Predicting: 218it [02:28,  1.52it/s]Extractor Predicting: 219it [02:28,  1.55it/s]Extractor Predicting: 220it [02:29,  1.53it/s]Extractor Predicting: 221it [02:30,  1.56it/s]Extractor Predicting: 222it [02:30,  1.57it/s]Extractor Predicting: 223it [02:31,  1.54it/s]Extractor Predicting: 224it [02:32,  1.54it/s]Extractor Predicting: 225it [02:32,  1.54it/s]Extractor Predicting: 226it [02:33,  1.50it/s]Extractor Predicting: 227it [02:34,  1.49it/s]Extractor Predicting: 228it [02:34,  1.49it/s]Extractor Predicting: 229it [02:35,  1.44it/s]Extractor Predicting: 230it [02:36,  1.45it/s]Extractor Predicting: 231it [02:37,  1.47it/s]Extractor Predicting: 232it [02:37,  1.50it/s]Extractor Predicting: 233it [02:38,  1.57it/s]Extractor Predicting: 234it [02:38,  1.56it/s]Extractor Predicting: 235it [02:39,  1.57it/s]Extractor Predicting: 236it [02:40,  1.62it/s]Extractor Predicting: 237it [02:40,  1.62it/s]Extractor Predicting: 238it [02:41,  1.63it/s]Extractor Predicting: 239it [02:41,  1.58it/s]Extractor Predicting: 240it [02:42,  1.60it/s]Extractor Predicting: 241it [02:43,  1.61it/s]Extractor Predicting: 242it [02:43,  1.61it/s]Extractor Predicting: 243it [02:44,  1.66it/s]Extractor Predicting: 244it [02:45,  1.40it/s]Extractor Predicting: 245it [02:45,  1.52it/s]Extractor Predicting: 246it [02:46,  1.59it/s]Extractor Predicting: 247it [02:47,  1.58it/s]Extractor Predicting: 248it [02:47,  1.56it/s]Extractor Predicting: 249it [02:48,  1.58it/s]Extractor Predicting: 250it [02:48,  1.59it/s]Extractor Predicting: 251it [02:49,  1.63it/s]Extractor Predicting: 252it [02:50,  1.65it/s]Extractor Predicting: 253it [02:50,  1.64it/s]Extractor Predicting: 254it [02:51,  1.60it/s]Extractor Predicting: 255it [02:51,  1.64it/s]Extractor Predicting: 256it [02:52,  1.63it/s]Extractor Predicting: 257it [02:53,  1.66it/s]Extractor Predicting: 258it [02:53,  1.66it/s]Extractor Predicting: 259it [02:54,  1.68it/s]Extractor Predicting: 260it [02:54,  1.65it/s]Extractor Predicting: 261it [02:55,  1.56it/s]Extractor Predicting: 262it [02:56,  1.57it/s]Extractor Predicting: 263it [02:57,  1.52it/s]Extractor Predicting: 264it [02:57,  1.47it/s]Extractor Predicting: 265it [02:58,  1.47it/s]Extractor Predicting: 266it [02:59,  1.48it/s]Extractor Predicting: 267it [02:59,  1.50it/s]Extractor Predicting: 268it [03:00,  1.50it/s]Extractor Predicting: 269it [03:01,  1.47it/s]Extractor Predicting: 270it [03:01,  1.44it/s]Extractor Predicting: 271it [03:02,  1.43it/s]Extractor Predicting: 272it [03:03,  1.46it/s]Extractor Predicting: 273it [03:03,  1.45it/s]Extractor Predicting: 274it [03:04,  1.44it/s]Extractor Predicting: 275it [03:05,  1.44it/s]Extractor Predicting: 276it [03:06,  1.45it/s]Extractor Predicting: 277it [03:06,  1.43it/s]Extractor Predicting: 278it [03:07,  1.43it/s]Extractor Predicting: 279it [03:08,  1.45it/s]Extractor Predicting: 280it [03:08,  1.44it/s]Extractor Predicting: 281it [03:09,  1.43it/s]Extractor Predicting: 282it [03:10,  1.43it/s]Extractor Predicting: 283it [03:10,  1.44it/s]Extractor Predicting: 284it [03:11,  1.43it/s]Extractor Predicting: 285it [03:12,  1.45it/s]Extractor Predicting: 286it [03:12,  1.44it/s]Extractor Predicting: 287it [03:13,  1.43it/s]Extractor Predicting: 288it [03:14,  1.48it/s]Extractor Predicting: 289it [03:14,  1.48it/s]Extractor Predicting: 290it [03:15,  1.50it/s]Extractor Predicting: 291it [03:16,  1.48it/s]Extractor Predicting: 292it [03:16,  1.48it/s]Extractor Predicting: 293it [03:17,  1.49it/s]Extractor Predicting: 294it [03:18,  1.51it/s]Extractor Predicting: 295it [03:18,  1.51it/s]Extractor Predicting: 296it [03:19,  1.50it/s]Extractor Predicting: 297it [03:20,  1.51it/s]Extractor Predicting: 298it [03:20,  1.55it/s]Extractor Predicting: 299it [03:21,  1.52it/s]Extractor Predicting: 300it [03:22,  1.52it/s]Extractor Predicting: 301it [03:22,  1.46it/s]Extractor Predicting: 302it [03:23,  1.46it/s]Extractor Predicting: 303it [03:24,  1.47it/s]Extractor Predicting: 304it [03:25,  1.47it/s]Extractor Predicting: 305it [03:25,  1.47it/s]Extractor Predicting: 306it [03:26,  1.49it/s]Extractor Predicting: 307it [03:27,  1.49it/s]Extractor Predicting: 308it [03:27,  1.48it/s]Extractor Predicting: 309it [03:28,  1.46it/s]Extractor Predicting: 310it [03:29,  1.47it/s]Extractor Predicting: 311it [03:29,  1.47it/s]Extractor Predicting: 312it [03:30,  1.47it/s]Extractor Predicting: 313it [03:31,  1.48it/s]Extractor Predicting: 314it [03:31,  1.48it/s]Extractor Predicting: 315it [03:32,  1.49it/s]Extractor Predicting: 316it [03:33,  1.48it/s]Extractor Predicting: 317it [03:33,  1.51it/s]Extractor Predicting: 318it [03:34,  1.54it/s]Extractor Predicting: 319it [03:35,  1.53it/s]Extractor Predicting: 320it [03:35,  1.51it/s]Extractor Predicting: 321it [03:36,  1.49it/s]Extractor Predicting: 322it [03:37,  1.49it/s]Extractor Predicting: 323it [03:37,  1.50it/s]Extractor Predicting: 324it [03:38,  1.51it/s]Extractor Predicting: 325it [03:39,  1.51it/s]Extractor Predicting: 326it [03:39,  1.51it/s]Extractor Predicting: 327it [03:40,  1.52it/s]Extractor Predicting: 328it [03:41,  1.50it/s]Extractor Predicting: 329it [03:41,  1.51it/s]Extractor Predicting: 330it [03:42,  1.50it/s]Extractor Predicting: 331it [03:43,  1.49it/s]Extractor Predicting: 332it [03:43,  1.52it/s]Extractor Predicting: 333it [03:44,  1.49it/s]Extractor Predicting: 334it [03:45,  1.52it/s]Extractor Predicting: 335it [03:45,  1.51it/s]Extractor Predicting: 336it [03:46,  1.52it/s]Extractor Predicting: 337it [03:47,  1.50it/s]Extractor Predicting: 338it [03:47,  1.51it/s]Extractor Predicting: 339it [03:48,  1.51it/s]Extractor Predicting: 340it [03:49,  1.50it/s]Extractor Predicting: 341it [03:49,  1.52it/s]Extractor Predicting: 342it [03:50,  1.53it/s]Extractor Predicting: 343it [03:50,  1.52it/s]Extractor Predicting: 344it [03:51,  1.55it/s]Extractor Predicting: 345it [03:52,  1.52it/s]Extractor Predicting: 346it [03:52,  1.52it/s]Extractor Predicting: 347it [03:53,  1.51it/s]Extractor Predicting: 348it [03:54,  1.48it/s]Extractor Predicting: 349it [03:54,  1.50it/s]Extractor Predicting: 350it [03:55,  1.48it/s]Extractor Predicting: 351it [03:56,  1.50it/s]Extractor Predicting: 352it [03:56,  1.49it/s]Extractor Predicting: 353it [03:57,  1.49it/s]Extractor Predicting: 354it [03:58,  1.50it/s]Extractor Predicting: 355it [03:58,  1.48it/s]Extractor Predicting: 356it [03:59,  1.50it/s]Extractor Predicting: 357it [04:00,  1.48it/s]Extractor Predicting: 358it [04:00,  1.49it/s]Extractor Predicting: 359it [04:01,  1.50it/s]Extractor Predicting: 360it [04:02,  1.50it/s]Extractor Predicting: 361it [04:02,  1.49it/s]Extractor Predicting: 362it [04:03,  1.52it/s]Extractor Predicting: 363it [04:04,  1.33it/s]Extractor Predicting: 364it [04:05,  1.40it/s]Extractor Predicting: 365it [04:05,  1.41it/s]Extractor Predicting: 366it [04:06,  1.44it/s]Extractor Predicting: 367it [04:07,  1.45it/s]Extractor Predicting: 368it [04:07,  1.47it/s]Extractor Predicting: 369it [04:08,  1.44it/s]Extractor Predicting: 370it [04:09,  1.45it/s]Extractor Predicting: 371it [04:09,  1.46it/s]Extractor Predicting: 372it [04:10,  1.47it/s]Extractor Predicting: 373it [04:11,  1.47it/s]Extractor Predicting: 374it [04:11,  1.50it/s]Extractor Predicting: 375it [04:12,  1.49it/s]Extractor Predicting: 376it [04:13,  1.47it/s]Extractor Predicting: 377it [04:13,  1.51it/s]Extractor Predicting: 378it [04:14,  1.52it/s]Extractor Predicting: 379it [04:15,  1.55it/s]Extractor Predicting: 380it [04:15,  1.52it/s]Extractor Predicting: 381it [04:16,  1.52it/s]Extractor Predicting: 382it [04:17,  1.50it/s]Extractor Predicting: 383it [04:17,  1.48it/s]Extractor Predicting: 384it [04:18,  1.48it/s]Extractor Predicting: 385it [04:19,  1.49it/s]Extractor Predicting: 386it [04:20,  1.47it/s]Extractor Predicting: 387it [04:20,  1.46it/s]Extractor Predicting: 388it [04:21,  1.42it/s]Extractor Predicting: 389it [04:22,  1.44it/s]Extractor Predicting: 390it [04:22,  1.45it/s]Extractor Predicting: 391it [04:23,  1.47it/s]Extractor Predicting: 392it [04:24,  1.44it/s]Extractor Predicting: 393it [04:24,  1.45it/s]Extractor Predicting: 394it [04:25,  1.46it/s]Extractor Predicting: 395it [04:26,  1.47it/s]Extractor Predicting: 396it [04:26,  1.46it/s]Extractor Predicting: 397it [04:27,  1.47it/s]Extractor Predicting: 398it [04:28,  1.46it/s]Extractor Predicting: 399it [04:28,  1.50it/s]Extractor Predicting: 400it [04:29,  1.48it/s]Extractor Predicting: 401it [04:30,  1.49it/s]Extractor Predicting: 402it [04:30,  1.46it/s]Extractor Predicting: 403it [04:31,  1.45it/s]Extractor Predicting: 404it [04:32,  1.47it/s]Extractor Predicting: 405it [04:32,  1.48it/s]Extractor Predicting: 406it [04:33,  1.51it/s]Extractor Predicting: 407it [04:34,  1.49it/s]Extractor Predicting: 408it [04:35,  1.47it/s]Extractor Predicting: 409it [04:35,  1.49it/s]Extractor Predicting: 410it [04:36,  1.50it/s]Extractor Predicting: 411it [04:36,  1.50it/s]Extractor Predicting: 412it [04:37,  1.51it/s]Extractor Predicting: 413it [04:38,  1.51it/s]Extractor Predicting: 414it [04:38,  1.50it/s]Extractor Predicting: 415it [04:39,  1.48it/s]Extractor Predicting: 416it [04:40,  1.51it/s]Extractor Predicting: 417it [04:40,  1.51it/s]Extractor Predicting: 418it [04:41,  1.53it/s]Extractor Predicting: 419it [04:42,  1.54it/s]Extractor Predicting: 420it [04:42,  1.55it/s]Extractor Predicting: 421it [04:43,  1.52it/s]Extractor Predicting: 422it [04:44,  1.53it/s]Extractor Predicting: 423it [04:44,  1.53it/s]Extractor Predicting: 424it [04:45,  1.52it/s]Extractor Predicting: 425it [04:46,  1.51it/s]Extractor Predicting: 426it [04:46,  1.54it/s]Extractor Predicting: 427it [04:47,  1.53it/s]Extractor Predicting: 428it [04:48,  1.46it/s]Extractor Predicting: 429it [04:48,  1.67it/s]Extractor Predicting: 429it [04:48,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:09,579 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:09,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:09,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:09,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:09,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:37:10,365 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:37:10,366 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:10,997 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:12,057 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:12,057 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:16,683 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:16,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:16,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:16,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:37:16,699 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:37:17,750 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:37:17,751 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:37:18,367 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:37:18,519 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:37:18,519 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.40275445078938527,
  "recall": 0.11661155417234001,
  "score": 0.18085828493853232,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 5it [00:03,  1.53it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:37:22,482 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:37:22,483 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:37:22,508 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:37:22,509 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:37:22,526 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:37:27,872 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:37:27,954 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:37:28,033 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:37:28,033 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:37:28,053 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:28,077 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:28,077 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:28,077 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:28,077 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:28,077 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:37:28,077 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6842105263157895,
  "recall": 0.05963302752293578,
  "score": 0.10970464135021099,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:37:28,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:29,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:29,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:30,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:31,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:31,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:32,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:33,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:33,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:34,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:34,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:35,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:36,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:36,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:37,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:38,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:38,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:39,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:39,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:40,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<04:05, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-28 21:37:41,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:41,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:42,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:43,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:43,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:44,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:45,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:45,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:46,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:47,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:47,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:48,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:48,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:49,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:50,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:50,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:51,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:51,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:52,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:53,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:25<03:46, 12.61s/it][WARNING|generation_utils.py:914] 2023-08-28 21:37:53,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:54,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:55,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:55,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:56,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:57,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:58,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:37:59,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:00,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:01,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:01,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:02,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:03,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:04,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:04,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:05,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:06,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:07,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:07,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:08,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:09,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:10,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:42<04:09, 14.66s/it][WARNING|generation_utils.py:914] 2023-08-28 21:38:10,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:11,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:12,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:13,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:13,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:14,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:15,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:15,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:16,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:17,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:18,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:18,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:19,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:20,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:20,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:21,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:22,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:23,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:23,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:24,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:56<03:52, 14.50s/it][WARNING|generation_utils.py:914] 2023-08-28 21:38:25,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:25,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:26,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:27,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:27,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:28,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:29,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:29,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:30,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:31,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:31,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:32,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:33,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:34,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:35,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:36,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:37,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:37,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:38,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:39,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:40,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:13<03:48, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 21:38:41,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:42,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:43,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:43,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:44,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:45,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:45,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:46,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:47,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:48,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:48,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:49,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:50,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:51,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:51,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:52,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:53,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:53,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:54,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:55,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:27<03:29, 14.99s/it][WARNING|generation_utils.py:914] 2023-08-28 21:38:56,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:56,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:57,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:58,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:58,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:38:59,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:00,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:00,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:01,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:02,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:02,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:03,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:04,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:04,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:05,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:06,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:06,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:07,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:08,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:09,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:09,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:10,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:11,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:43<03:19, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:12,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:12,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:13,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:14,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:14,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:15,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:16,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:16,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:17,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:18,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:18,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:19,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:20,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:20,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:21,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:22,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:22,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:23,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:24,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:24,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:25,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:26,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:58<03:00, 15.07s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:26,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:27,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:28,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:28,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:29,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:30,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:31,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:31,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:32,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:33,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:33,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:34,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:35,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:36,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:37,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:38,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:38,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:39,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:40,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:41,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:41,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:14<02:48, 15.32s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:42,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:43,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:44,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:44,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:45,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:46,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:46,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:47,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:48,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:49,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:50,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:50,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:51,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:52,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:52,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:53,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:54,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:55,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:55,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:56,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:29<02:32, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 21:39:57,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:58,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:59,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:39:59,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:00,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:01,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:01,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:02,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:03,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:03,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:04,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:05,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:05,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:06,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:07,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:08,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:08,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:09,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:10,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:10,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:11,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:43<02:15, 15.05s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:12,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:12,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:13,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:14,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:14,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:15,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:16,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:16,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:17,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:18,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:18,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:19,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:20,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:20,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:21,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:22,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:22,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:23,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:24,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:24,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:25,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:26,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:58<01:58, 14.86s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:26,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:27,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:28,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:29,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:29,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:30,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:31,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:32,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:32,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:33,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:34,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:35,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:35,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:36,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:37,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:38,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:38,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:39,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:40,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:41,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:14<01:46, 15.18s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:42,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:43,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:43,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:44,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:45,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:45,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:46,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:47,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:48,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:48,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:49,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:50,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:51,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:51,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:52,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:53,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:54,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:54,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:55,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:56,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:28<01:29, 14.87s/it][WARNING|generation_utils.py:914] 2023-08-28 21:40:56,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:57,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:58,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:58,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:40:59,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:00,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:00,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:01,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:02,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:02,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:03,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:04,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:04,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:05,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:06,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:07,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:07,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:08,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:09,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:09,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:10,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:42<01:13, 14.78s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:11,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:11,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:12,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:13,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:14,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:14,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:15,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:16,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:17,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:17,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:18,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:19,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:20,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:20,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:21,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:22,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:23,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:23,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:24,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:25,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:25,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:57<00:59, 14.84s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:26,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:26,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:27,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:28,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:29,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:29,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:30,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:31,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:32,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:33,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:34,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:35,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:36,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:37,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:38,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:39,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:40,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:41,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:14<00:45, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:42,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:43,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:44,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:45,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:46,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:46,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:47,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:48,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:49,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:50,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:51,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:52,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:53,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:54,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:55,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:28<00:29, 14.90s/it][WARNING|generation_utils.py:914] 2023-08-28 21:41:56,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:57,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:57,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:58,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:41:59,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:00,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:01,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:02,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:03,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:04,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:05,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:06,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:06,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:07,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:08,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:09,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:41<00:14, 14.42s/it][WARNING|generation_utils.py:914] 2023-08-28 21:42:09,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:10,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:11,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:12,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:13,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:14,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:15,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:16,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:17,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:18,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:19,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:20,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:21,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:21,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:22,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:42:23,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:55<00:00, 14.29s/it]Generating: 100%|██████████| 20/20 [04:55<00:00, 14.77s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:31,861 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:31,886 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:31,886 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:31,886 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:31,886 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:42:32,477 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:42:32,478 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:42:33,265 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:42:34,319 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:42:34,320 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:38,476 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:38,482 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:38,482 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:38,482 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:42:38,482 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:42:39,128 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:42:39,129 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:42:39,758 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:42:39,911 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:42:39,911 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : head of government .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : licensed to broadcast to .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 313, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 467, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : performer .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : spouse .', 'success_rate': 0.9330357142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : after a work by .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8247282608695652, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : country of citizenship .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : has part . Context : Later in the year ( October 1887 ) , " The Last of Us " premiered at the Los Angeles International Film Fest from June until the death of actress and director George Lucas , who died in December 1987 . Head Entity : The Last of Us , Tail Entity : Los Angeles International Film Festival .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : has part .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : headquarters location .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : location of formation .', 'success_rate': 0.8735795454545454, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : mountain range .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : mouth of the watercourse .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.959375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : winner .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : work location .', 'success_rate': 0.95, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 9568
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9668, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.76it/s]Extractor Estimating: 2it [00:01,  1.72it/s]Extractor Estimating: 3it [00:01,  1.90it/s]Extractor Estimating: 4it [00:02,  1.92it/s]Extractor Estimating: 5it [00:02,  1.88it/s]Extractor Estimating: 6it [00:03,  1.67it/s]Extractor Estimating: 7it [00:03,  1.68it/s]Extractor Estimating: 8it [00:04,  1.69it/s]Extractor Estimating: 9it [00:05,  1.76it/s]Extractor Estimating: 10it [00:05,  1.80it/s]Extractor Estimating: 11it [00:06,  1.81it/s]Extractor Estimating: 12it [00:06,  1.83it/s]Extractor Estimating: 13it [00:07,  1.86it/s]Extractor Estimating: 14it [00:07,  1.76it/s]Extractor Estimating: 15it [00:08,  1.79it/s]Extractor Estimating: 16it [00:09,  1.72it/s]Extractor Estimating: 17it [00:09,  1.77it/s]Extractor Estimating: 18it [00:10,  1.80it/s]Extractor Estimating: 19it [00:10,  1.80it/s]Extractor Estimating: 20it [00:11,  1.83it/s]Extractor Estimating: 21it [00:11,  1.80it/s]Extractor Estimating: 22it [00:12,  1.80it/s]Extractor Estimating: 23it [00:12,  1.79it/s]Extractor Estimating: 24it [00:13,  1.78it/s]Extractor Estimating: 25it [00:14,  1.77it/s]Extractor Estimating: 26it [00:14,  1.80it/s]Extractor Estimating: 27it [00:15,  1.81it/s]Extractor Estimating: 28it [00:15,  1.75it/s]Extractor Estimating: 29it [00:16,  1.71it/s]Extractor Estimating: 30it [00:16,  1.69it/s]Extractor Estimating: 31it [00:17,  1.76it/s]Extractor Estimating: 32it [00:17,  1.81it/s]Extractor Estimating: 33it [00:18,  1.82it/s]Extractor Estimating: 34it [00:19,  1.79it/s]Extractor Estimating: 35it [00:19,  1.74it/s]Extractor Estimating: 36it [00:20,  1.78it/s]Extractor Estimating: 37it [00:20,  1.82it/s]Extractor Estimating: 38it [00:21,  1.80it/s]Extractor Estimating: 39it [00:21,  1.81it/s]Extractor Estimating: 40it [00:22,  1.81it/s]Extractor Estimating: 41it [00:22,  1.81it/s]Extractor Estimating: 42it [00:23,  1.83it/s]Extractor Estimating: 43it [00:24,  1.84it/s]Extractor Estimating: 44it [00:24,  1.78it/s]Extractor Estimating: 45it [00:25,  1.83it/s]Extractor Estimating: 46it [00:25,  1.78it/s]Extractor Estimating: 47it [00:26,  1.77it/s]Extractor Estimating: 48it [00:26,  1.76it/s]Extractor Estimating: 49it [00:27,  1.79it/s]Extractor Estimating: 50it [00:27,  1.78it/s]Extractor Estimating: 51it [00:28,  1.79it/s]Extractor Estimating: 52it [00:29,  1.79it/s]Extractor Estimating: 53it [00:29,  1.69it/s]Extractor Estimating: 54it [00:30,  1.74it/s]Extractor Estimating: 55it [00:31,  1.41it/s]Extractor Estimating: 56it [00:31,  1.53it/s]Extractor Estimating: 57it [00:32,  1.65it/s]Extractor Estimating: 58it [00:32,  1.66it/s]Extractor Estimating: 59it [00:33,  1.65it/s]Extractor Estimating: 60it [00:34,  1.66it/s]Extractor Estimating: 61it [00:34,  1.69it/s]Extractor Estimating: 62it [00:35,  1.68it/s]Extractor Estimating: 63it [00:35,  1.65it/s]Extractor Estimating: 64it [00:36,  1.67it/s]Extractor Estimating: 65it [00:37,  1.73it/s]Extractor Estimating: 66it [00:37,  1.74it/s]Extractor Estimating: 67it [00:38,  1.73it/s]Extractor Estimating: 68it [00:38,  1.71it/s]Extractor Estimating: 69it [00:39,  1.70it/s]Extractor Estimating: 70it [00:40,  1.69it/s]Extractor Estimating: 71it [00:40,  1.72it/s]Extractor Estimating: 72it [00:41,  1.69it/s]Extractor Estimating: 73it [00:41,  1.70it/s]Extractor Estimating: 74it [00:42,  1.72it/s]Extractor Estimating: 75it [00:42,  1.71it/s]Extractor Estimating: 76it [00:43,  1.70it/s]Extractor Estimating: 77it [00:44,  1.56it/s]Extractor Estimating: 78it [00:44,  1.58it/s]Extractor Estimating: 79it [00:45,  1.60it/s]Extractor Estimating: 80it [00:46,  1.59it/s]Extractor Estimating: 81it [00:46,  1.59it/s]Extractor Estimating: 82it [00:47,  1.59it/s]Extractor Estimating: 83it [00:48,  1.61it/s]Extractor Estimating: 84it [00:48,  1.62it/s]Extractor Estimating: 85it [00:49,  1.63it/s]Extractor Estimating: 86it [00:49,  1.59it/s]Extractor Estimating: 87it [00:50,  1.57it/s]Extractor Estimating: 88it [00:51,  1.56it/s]Extractor Estimating: 89it [00:51,  1.61it/s]Extractor Estimating: 90it [00:52,  1.62it/s]Extractor Estimating: 91it [00:52,  1.63it/s]Extractor Estimating: 92it [00:53,  1.64it/s]Extractor Estimating: 93it [00:54,  1.62it/s]Extractor Estimating: 94it [00:54,  1.56it/s]Extractor Estimating: 95it [00:55,  1.43it/s]Extractor Estimating: 96it [00:56,  1.44it/s]Extractor Estimating: 97it [00:57,  1.49it/s]Extractor Estimating: 98it [00:57,  1.51it/s]Extractor Estimating: 99it [00:58,  1.54it/s]Extractor Estimating: 100it [00:58,  1.59it/s]Extractor Estimating: 101it [00:59,  1.66it/s]Extractor Estimating: 102it [01:00,  1.67it/s]Extractor Estimating: 103it [01:00,  1.72it/s]Extractor Estimating: 104it [01:01,  1.73it/s]Extractor Estimating: 105it [01:01,  1.78it/s]Extractor Estimating: 106it [01:02,  1.85it/s]Extractor Estimating: 107it [01:02,  1.82it/s]Extractor Estimating: 108it [01:03,  1.83it/s]Extractor Estimating: 109it [01:03,  1.89it/s]Extractor Estimating: 110it [01:04,  1.87it/s]Extractor Estimating: 111it [01:04,  1.80it/s]Extractor Estimating: 112it [01:05,  1.87it/s]Extractor Estimating: 113it [01:05,  1.87it/s]Extractor Estimating: 114it [01:06,  1.77it/s]Extractor Estimating: 115it [01:07,  1.68it/s]Extractor Estimating: 116it [01:07,  1.73it/s]Extractor Estimating: 117it [01:08,  1.78it/s]Extractor Estimating: 118it [01:08,  1.75it/s]Extractor Estimating: 119it [01:09,  1.75it/s]Extractor Estimating: 120it [01:10,  1.70it/s]Extractor Estimating: 121it [01:10,  1.70it/s]Extractor Estimating: 122it [01:11,  1.69it/s]Extractor Estimating: 123it [01:11,  1.68it/s]Extractor Estimating: 124it [01:12,  1.76it/s]Extractor Estimating: 125it [01:12,  1.75it/s]Extractor Estimating: 126it [01:13,  1.69it/s]Extractor Estimating: 127it [01:14,  1.67it/s]Extractor Estimating: 128it [01:14,  1.70it/s]Extractor Estimating: 129it [01:15,  1.67it/s]Extractor Estimating: 130it [01:16,  1.65it/s]Extractor Estimating: 131it [01:16,  1.58it/s]Extractor Estimating: 132it [01:17,  1.61it/s]Extractor Estimating: 133it [01:17,  1.64it/s]Extractor Estimating: 134it [01:18,  1.66it/s]Extractor Estimating: 135it [01:19,  1.70it/s]Extractor Estimating: 136it [01:19,  1.71it/s]Extractor Estimating: 137it [01:20,  1.71it/s]Extractor Estimating: 138it [01:20,  1.71it/s]Extractor Estimating: 139it [01:21,  1.67it/s]Extractor Estimating: 140it [01:22,  1.64it/s]Extractor Estimating: 141it [01:22,  1.62it/s]Extractor Estimating: 142it [01:23,  1.62it/s]Extractor Estimating: 143it [01:23,  1.64it/s]Extractor Estimating: 144it [01:24,  1.59it/s]Extractor Estimating: 145it [01:25,  1.60it/s]Extractor Estimating: 146it [01:25,  1.59it/s]Extractor Estimating: 147it [01:26,  1.57it/s]Extractor Estimating: 148it [01:27,  1.61it/s]Extractor Estimating: 149it [01:27,  1.55it/s]Extractor Estimating: 150it [01:28,  1.58it/s]Extractor Estimating: 151it [01:28,  1.69it/s]Extractor Estimating: 152it [01:29,  1.75it/s]Extractor Estimating: 153it [01:29,  1.80it/s]Extractor Estimating: 154it [01:30,  1.91it/s]Extractor Estimating: 155it [01:30,  1.94it/s]Extractor Estimating: 156it [01:31,  1.90it/s]Extractor Estimating: 157it [01:31,  1.93it/s]Extractor Estimating: 158it [01:32,  1.94it/s]Extractor Estimating: 159it [01:32,  2.00it/s]Extractor Estimating: 160it [01:33,  1.99it/s]Extractor Estimating: 161it [01:33,  1.97it/s]Extractor Estimating: 162it [01:34,  1.95it/s]Extractor Estimating: 163it [01:35,  1.88it/s]Extractor Estimating: 164it [01:35,  1.88it/s]Extractor Estimating: 165it [01:36,  1.90it/s]Extractor Estimating: 166it [01:36,  1.93it/s]Extractor Estimating: 167it [01:37,  1.93it/s]Extractor Estimating: 168it [01:37,  1.95it/s]Extractor Estimating: 169it [01:38,  1.80it/s]Extractor Estimating: 170it [01:38,  1.86it/s]Extractor Estimating: 171it [01:39,  1.89it/s]Extractor Estimating: 172it [01:39,  1.84it/s]Extractor Estimating: 173it [01:40,  1.85it/s]Extractor Estimating: 174it [01:40,  1.88it/s]Extractor Estimating: 175it [01:41,  1.81it/s]Extractor Estimating: 176it [01:41,  1.87it/s]Extractor Estimating: 177it [01:42,  1.83it/s]Extractor Estimating: 178it [01:43,  1.87it/s]Extractor Estimating: 179it [01:43,  1.93it/s]Extractor Estimating: 180it [01:44,  1.95it/s]Extractor Estimating: 181it [01:44,  1.87it/s]Extractor Estimating: 182it [01:45,  1.94it/s]Extractor Estimating: 183it [01:45,  1.99it/s]Extractor Estimating: 184it [01:46,  1.95it/s]Extractor Estimating: 185it [01:46,  1.91it/s]Extractor Estimating: 186it [01:47,  1.89it/s]Extractor Estimating: 187it [01:47,  1.93it/s]Extractor Estimating: 188it [01:48,  1.93it/s]Extractor Estimating: 189it [01:48,  1.96it/s]Extractor Estimating: 190it [01:49,  1.96it/s]Extractor Estimating: 191it [01:49,  2.05it/s]Extractor Estimating: 192it [01:50,  1.92it/s]Extractor Estimating: 193it [01:50,  1.97it/s]Extractor Estimating: 194it [01:51,  2.01it/s]Extractor Estimating: 195it [01:51,  2.00it/s]Extractor Estimating: 196it [01:52,  2.00it/s]Extractor Estimating: 197it [01:52,  2.04it/s]Extractor Estimating: 198it [01:53,  1.96it/s]Extractor Estimating: 199it [01:53,  1.93it/s]Extractor Estimating: 200it [01:54,  1.89it/s]Extractor Estimating: 201it [01:54,  1.86it/s]Extractor Estimating: 202it [01:55,  1.76it/s]Extractor Estimating: 203it [01:56,  1.54it/s]Extractor Estimating: 204it [01:56,  1.56it/s]Extractor Estimating: 205it [01:57,  1.56it/s]Extractor Estimating: 206it [01:58,  1.56it/s]Extractor Estimating: 207it [01:58,  1.59it/s]Extractor Estimating: 208it [01:59,  1.63it/s]Extractor Estimating: 209it [02:00,  1.59it/s]Extractor Estimating: 210it [02:00,  1.65it/s]Extractor Estimating: 211it [02:01,  1.63it/s]Extractor Estimating: 212it [02:01,  1.66it/s]Extractor Estimating: 213it [02:02,  1.66it/s]Extractor Estimating: 214it [02:03,  1.65it/s]Extractor Estimating: 215it [02:03,  1.61it/s]Extractor Estimating: 216it [02:04,  1.53it/s]Extractor Estimating: 217it [02:05,  1.55it/s]Extractor Estimating: 218it [02:05,  1.55it/s]Extractor Estimating: 219it [02:06,  1.57it/s]Extractor Estimating: 220it [02:06,  1.61it/s]Extractor Estimating: 221it [02:07,  1.56it/s]Extractor Estimating: 222it [02:08,  1.62it/s]Extractor Estimating: 223it [02:08,  1.63it/s]Extractor Estimating: 224it [02:09,  1.64it/s]Extractor Estimating: 225it [02:09,  1.69it/s]Extractor Estimating: 226it [02:10,  1.68it/s]Extractor Estimating: 227it [02:11,  1.63it/s]Extractor Estimating: 228it [02:11,  1.56it/s]Extractor Estimating: 229it [02:12,  1.56it/s]Extractor Estimating: 230it [02:13,  1.60it/s]Extractor Estimating: 231it [02:13,  1.59it/s]Extractor Estimating: 232it [02:14,  1.56it/s]Extractor Estimating: 233it [02:15,  1.52it/s]Extractor Estimating: 234it [02:15,  1.45it/s]Extractor Estimating: 235it [02:16,  1.52it/s]Extractor Estimating: 236it [02:17,  1.48it/s]Extractor Estimating: 237it [02:17,  1.46it/s]Extractor Estimating: 238it [02:18,  1.49it/s]Extractor Estimating: 239it [02:19,  1.50it/s]Extractor Estimating: 240it [02:19,  1.54it/s]Extractor Estimating: 241it [02:20,  1.56it/s]Extractor Estimating: 242it [02:21,  1.54it/s]Extractor Estimating: 243it [02:21,  1.58it/s]Extractor Estimating: 244it [02:22,  1.58it/s]Extractor Estimating: 245it [02:22,  1.57it/s]Extractor Estimating: 246it [02:23,  1.61it/s]Extractor Estimating: 247it [02:24,  1.57it/s]Extractor Estimating: 248it [02:24,  1.60it/s]Extractor Estimating: 249it [02:25,  1.58it/s]Extractor Estimating: 250it [02:26,  1.61it/s]Extractor Estimating: 251it [02:26,  1.61it/s]Extractor Estimating: 252it [02:27,  1.65it/s]Extractor Estimating: 253it [02:27,  1.66it/s]Extractor Estimating: 254it [02:28,  1.69it/s]Extractor Estimating: 255it [02:29,  1.62it/s]Extractor Estimating: 256it [02:29,  1.69it/s]Extractor Estimating: 257it [02:30,  1.72it/s]Extractor Estimating: 258it [02:30,  1.70it/s]Extractor Estimating: 259it [02:31,  1.55it/s]Extractor Estimating: 260it [02:32,  1.62it/s]Extractor Estimating: 261it [02:32,  1.65it/s]Extractor Estimating: 262it [02:33,  1.68it/s]Extractor Estimating: 263it [02:33,  1.64it/s]Extractor Estimating: 264it [02:34,  1.69it/s]Extractor Estimating: 265it [02:35,  1.69it/s]Extractor Estimating: 266it [02:35,  1.65it/s]Extractor Estimating: 267it [02:36,  1.65it/s]Extractor Estimating: 268it [02:36,  1.65it/s]Extractor Estimating: 269it [02:37,  1.69it/s]Extractor Estimating: 270it [02:38,  1.74it/s]Extractor Estimating: 271it [02:38,  1.77it/s]Extractor Estimating: 272it [02:39,  1.76it/s]Extractor Estimating: 273it [02:39,  1.79it/s]Extractor Estimating: 274it [02:40,  1.76it/s]Extractor Estimating: 275it [02:40,  1.76it/s]Extractor Estimating: 276it [02:41,  1.75it/s]Extractor Estimating: 277it [02:41,  1.75it/s]Extractor Estimating: 278it [02:42,  1.79it/s]Extractor Estimating: 279it [02:43,  1.77it/s]Extractor Estimating: 280it [02:43,  1.64it/s]Extractor Estimating: 281it [02:44,  1.65it/s]Extractor Estimating: 282it [02:44,  1.70it/s]Extractor Estimating: 283it [02:45,  1.80it/s]Extractor Estimating: 284it [02:45,  1.86it/s]Extractor Estimating: 285it [02:46,  1.81it/s]Extractor Estimating: 286it [02:47,  1.83it/s]Extractor Estimating: 287it [02:47,  1.85it/s]Extractor Estimating: 288it [02:48,  1.88it/s]Extractor Estimating: 289it [02:48,  1.88it/s]Extractor Estimating: 290it [02:49,  1.86it/s]Extractor Estimating: 291it [02:49,  1.84it/s]Extractor Estimating: 292it [02:50,  1.81it/s]Extractor Estimating: 293it [02:50,  1.83it/s]Extractor Estimating: 294it [02:51,  1.83it/s]Extractor Estimating: 295it [02:51,  1.81it/s]Extractor Estimating: 296it [02:52,  1.79it/s]Extractor Estimating: 297it [02:53,  1.83it/s]Extractor Estimating: 298it [02:53,  1.86it/s]Extractor Estimating: 299it [02:54,  1.82it/s]Extractor Estimating: 300it [02:54,  1.79it/s]Extractor Estimating: 301it [02:55,  1.59it/s]Extractor Estimating: 302it [02:56,  1.62it/s]Extractor Estimating: 303it [02:56,  1.68it/s]Extractor Estimating: 304it [02:57,  1.69it/s]Extractor Estimating: 305it [02:57,  1.75it/s]Extractor Estimating: 306it [02:58,  1.75it/s]Extractor Estimating: 307it [02:58,  1.74it/s]Extractor Estimating: 308it [02:59,  1.75it/s]Extractor Estimating: 309it [02:59,  1.79it/s]Extractor Estimating: 310it [03:00,  1.81it/s]Extractor Estimating: 311it [03:01,  1.81it/s]Extractor Estimating: 312it [03:01,  1.76it/s]Extractor Estimating: 313it [03:02,  1.71it/s]Extractor Estimating: 314it [03:02,  1.72it/s]Extractor Estimating: 315it [03:03,  1.76it/s]Extractor Estimating: 316it [03:03,  1.77it/s]Extractor Estimating: 317it [03:04,  1.75it/s]Extractor Estimating: 318it [03:05,  1.69it/s]Extractor Estimating: 319it [03:05,  1.70it/s]Extractor Estimating: 320it [03:06,  1.73it/s]Extractor Estimating: 321it [03:06,  1.75it/s]Extractor Estimating: 322it [03:07,  1.71it/s]Extractor Estimating: 323it [03:08,  1.74it/s]Extractor Estimating: 324it [03:08,  1.70it/s]Extractor Estimating: 325it [03:09,  1.75it/s]Extractor Estimating: 326it [03:09,  1.80it/s]Extractor Estimating: 327it [03:10,  1.86it/s]Extractor Estimating: 328it [03:10,  1.86it/s]Extractor Estimating: 329it [03:11,  1.89it/s]Extractor Estimating: 330it [03:11,  1.88it/s]Extractor Estimating: 331it [03:12,  1.75it/s]Extractor Estimating: 332it [03:12,  1.82it/s]Extractor Estimating: 333it [03:13,  1.83it/s]Extractor Estimating: 334it [03:14,  1.79it/s]Extractor Estimating: 335it [03:14,  1.80it/s]Extractor Estimating: 336it [03:15,  1.79it/s]Extractor Estimating: 337it [03:15,  1.78it/s]Extractor Estimating: 338it [03:16,  1.82it/s]Extractor Estimating: 339it [03:16,  1.89it/s]Extractor Estimating: 340it [03:17,  1.82it/s]Extractor Estimating: 341it [03:17,  1.83it/s]Extractor Estimating: 342it [03:18,  1.83it/s]Extractor Estimating: 343it [03:19,  1.79it/s]Extractor Estimating: 344it [03:19,  1.75it/s]Extractor Estimating: 345it [03:20,  1.78it/s]Extractor Estimating: 346it [03:20,  1.84it/s]Extractor Estimating: 347it [03:21,  1.89it/s]Extractor Estimating: 348it [03:21,  1.89it/s]Extractor Estimating: 349it [03:22,  1.87it/s]Extractor Estimating: 350it [03:22,  1.83it/s]Extractor Estimating: 351it [03:23,  1.75it/s]Extractor Estimating: 352it [03:23,  1.78it/s]Extractor Estimating: 353it [03:24,  1.70it/s]Extractor Estimating: 354it [03:25,  1.69it/s]Extractor Estimating: 355it [03:25,  1.74it/s]Extractor Estimating: 356it [03:26,  1.74it/s]Extractor Estimating: 357it [03:26,  1.76it/s]Extractor Estimating: 358it [03:27,  1.75it/s]Extractor Estimating: 359it [03:28,  1.76it/s]Extractor Estimating: 360it [03:28,  1.78it/s]Extractor Estimating: 361it [03:29,  1.78it/s]Extractor Estimating: 362it [03:29,  1.82it/s]Extractor Estimating: 363it [03:30,  1.84it/s]Extractor Estimating: 364it [03:30,  1.86it/s]Extractor Estimating: 365it [03:31,  1.83it/s]Extractor Estimating: 366it [03:31,  1.84it/s]Extractor Estimating: 367it [03:32,  1.75it/s]Extractor Estimating: 368it [03:33,  1.78it/s]Extractor Estimating: 369it [03:33,  1.85it/s]Extractor Estimating: 370it [03:34,  1.77it/s]Extractor Estimating: 371it [03:34,  1.80it/s]Extractor Estimating: 372it [03:35,  1.82it/s]Extractor Estimating: 373it [03:35,  1.77it/s]Extractor Estimating: 374it [03:36,  1.80it/s]Extractor Estimating: 375it [03:36,  1.73it/s]Extractor Estimating: 376it [03:37,  1.69it/s]Extractor Estimating: 377it [03:38,  1.66it/s]Extractor Estimating: 378it [03:38,  1.71it/s]Extractor Estimating: 379it [03:39,  1.65it/s]Extractor Estimating: 380it [03:39,  1.67it/s]Extractor Estimating: 381it [03:40,  1.71it/s]Extractor Estimating: 382it [03:41,  1.65it/s]Extractor Estimating: 383it [03:41,  1.61it/s]Extractor Estimating: 384it [03:42,  1.58it/s]Extractor Estimating: 385it [03:43,  1.62it/s]Extractor Estimating: 386it [03:43,  1.60it/s]Extractor Estimating: 387it [03:44,  1.62it/s]Extractor Estimating: 388it [03:44,  1.67it/s]Extractor Estimating: 389it [03:45,  1.69it/s]Extractor Estimating: 390it [03:46,  1.65it/s]Extractor Estimating: 391it [03:46,  1.65it/s]Extractor Estimating: 392it [03:47,  1.64it/s]Extractor Estimating: 393it [03:47,  1.66it/s]Extractor Estimating: 394it [03:48,  1.66it/s]Extractor Estimating: 395it [03:49,  1.68it/s]Extractor Estimating: 396it [03:49,  1.67it/s]Extractor Estimating: 397it [03:50,  1.69it/s]Extractor Estimating: 398it [03:50,  1.73it/s]Extractor Estimating: 399it [03:51,  1.79it/s]Extractor Estimating: 400it [03:51,  1.79it/s]Extractor Estimating: 401it [03:52,  1.76it/s]Extractor Estimating: 402it [03:53,  1.75it/s]Extractor Estimating: 403it [03:53,  1.74it/s]Extractor Estimating: 404it [03:54,  1.72it/s]Extractor Estimating: 405it [03:54,  1.75it/s]Extractor Estimating: 406it [03:55,  1.80it/s]Extractor Estimating: 407it [03:56,  1.54it/s]Extractor Estimating: 408it [03:56,  1.63it/s]Extractor Estimating: 409it [03:57,  1.60it/s]Extractor Estimating: 410it [03:57,  1.68it/s]Extractor Estimating: 411it [03:58,  1.65it/s]Extractor Estimating: 412it [03:59,  1.65it/s]Extractor Estimating: 413it [03:59,  1.65it/s]Extractor Estimating: 414it [04:00,  1.63it/s]Extractor Estimating: 415it [04:00,  1.65it/s]Extractor Estimating: 416it [04:01,  1.67it/s]Extractor Estimating: 417it [04:02,  1.68it/s]Extractor Estimating: 418it [04:02,  1.69it/s]Extractor Estimating: 419it [04:03,  1.66it/s]Extractor Estimating: 420it [04:03,  1.68it/s]Extractor Estimating: 421it [04:04,  1.66it/s]Extractor Estimating: 422it [04:05,  1.58it/s]Extractor Estimating: 423it [04:05,  1.63it/s]Extractor Estimating: 424it [04:06,  1.65it/s]Extractor Estimating: 425it [04:07,  1.59it/s]Extractor Estimating: 426it [04:07,  1.65it/s]Extractor Estimating: 427it [04:08,  1.62it/s]Extractor Estimating: 428it [04:08,  1.63it/s]Extractor Estimating: 429it [04:09,  1.70it/s]Extractor Estimating: 430it [04:09,  1.69it/s]Extractor Estimating: 431it [04:10,  1.62it/s]Extractor Estimating: 432it [04:11,  1.64it/s]Extractor Estimating: 433it [04:11,  1.64it/s]Extractor Estimating: 434it [04:12,  1.67it/s]Extractor Estimating: 435it [04:13,  1.70it/s]Extractor Estimating: 436it [04:13,  1.66it/s]Extractor Estimating: 437it [04:14,  1.65it/s]Extractor Estimating: 438it [04:14,  1.61it/s]Extractor Estimating: 439it [04:15,  1.62it/s]Extractor Estimating: 440it [04:16,  1.58it/s]Extractor Estimating: 441it [04:16,  1.60it/s]Extractor Estimating: 442it [04:17,  1.65it/s]Extractor Estimating: 443it [04:17,  1.67it/s]Extractor Estimating: 444it [04:18,  1.70it/s]Extractor Estimating: 445it [04:19,  1.61it/s]Extractor Estimating: 446it [04:19,  1.58it/s]Extractor Estimating: 447it [04:20,  1.57it/s]Extractor Estimating: 448it [04:21,  1.60it/s]Extractor Estimating: 449it [04:21,  1.57it/s]Extractor Estimating: 450it [04:22,  1.63it/s]Extractor Estimating: 451it [04:22,  1.72it/s]Extractor Estimating: 452it [04:23,  1.77it/s]Extractor Estimating: 453it [04:23,  1.80it/s]Extractor Estimating: 454it [04:24,  1.75it/s]Extractor Estimating: 455it [04:25,  1.81it/s]Extractor Estimating: 456it [04:25,  1.82it/s]Extractor Estimating: 457it [04:26,  1.84it/s]Extractor Estimating: 458it [04:26,  1.93it/s]Extractor Estimating: 459it [04:27,  1.97it/s]Extractor Estimating: 460it [04:27,  1.84it/s]Extractor Estimating: 461it [04:28,  1.89it/s]Extractor Estimating: 462it [04:28,  1.90it/s]Extractor Estimating: 463it [04:29,  1.92it/s]Extractor Estimating: 464it [04:29,  1.95it/s]Extractor Estimating: 465it [04:30,  1.88it/s]Extractor Estimating: 466it [04:30,  1.86it/s]Extractor Estimating: 467it [04:31,  1.88it/s]Extractor Estimating: 468it [04:31,  1.93it/s]Extractor Estimating: 469it [04:32,  2.03it/s]Extractor Estimating: 470it [04:32,  2.09it/s]Extractor Estimating: 471it [04:33,  2.15it/s]Extractor Estimating: 472it [04:33,  2.17it/s]Extractor Estimating: 473it [04:34,  2.05it/s]Extractor Estimating: 474it [04:34,  2.03it/s]Extractor Estimating: 475it [04:35,  1.98it/s]Extractor Estimating: 476it [04:35,  1.85it/s]Extractor Estimating: 477it [04:36,  1.74it/s]Extractor Estimating: 478it [04:37,  1.65it/s]Extractor Estimating: 479it [04:37,  1.65it/s]Extractor Estimating: 480it [04:38,  1.64it/s]Extractor Estimating: 481it [04:38,  1.67it/s]Extractor Estimating: 482it [04:39,  1.65it/s]Extractor Estimating: 483it [04:40,  1.64it/s]Extractor Estimating: 484it [04:40,  1.62it/s]Extractor Estimating: 485it [04:41,  1.53it/s]Extractor Estimating: 486it [04:42,  1.55it/s]Extractor Estimating: 487it [04:42,  1.44it/s]Extractor Estimating: 488it [04:43,  1.52it/s]Extractor Estimating: 489it [04:44,  1.62it/s]Extractor Estimating: 490it [04:44,  1.64it/s]Extractor Estimating: 491it [04:45,  1.61it/s]Extractor Estimating: 492it [04:45,  1.65it/s]Extractor Estimating: 493it [04:46,  1.64it/s]Extractor Estimating: 494it [04:47,  1.65it/s]Extractor Estimating: 495it [04:47,  1.61it/s]Extractor Estimating: 496it [04:48,  1.65it/s]Extractor Estimating: 497it [04:48,  1.69it/s]Extractor Estimating: 498it [04:49,  1.67it/s]Extractor Estimating: 499it [04:50,  1.65it/s]Extractor Estimating: 500it [04:50,  1.83it/s]Extractor Estimating: 500it [04:50,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:46,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:46,641 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:46,641 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:46,641 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:46,641 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:47:47,223 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:47:47,224 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:47:48,206 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:47:49,264 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:47:49,264 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:54,047 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:54,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:54,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:54,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:47:54,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:47:55,268 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:47:55,269 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:47:55,865 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:47:56,019 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:47:56,019 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:50:55,362 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:50:55,368 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9989 mean pseudo reward: 0.9394943614166789
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl'}
train vocab size: 17175
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17275, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17275, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.039, loss:595.7882
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.035, loss:558.2643
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.017, loss:536.7087
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.043, loss:535.5732
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.080, loss:486.7227
>> valid entity prec:0.6013, rec:0.5426, f1:0.5704
>> valid relation prec:0.0738, rec:0.0155, f1:0.0256
>> valid relation with NER prec:0.0738, rec:0.0155, f1:0.0256
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.460, loss:528.8350
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.043, loss:506.3490
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.044, loss:542.1008
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.050, loss:500.9114
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.032, loss:480.8307
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5308, rec:0.5870, f1:0.5575
>> valid relation prec:0.0652, rec:0.0158, f1:0.0254
>> valid relation with NER prec:0.0652, rec:0.0158, f1:0.0254
g_step 1100, step 266, avg_time 2.458, loss:513.2551
g_step 1200, step 366, avg_time 1.057, loss:505.4045
g_step 1300, step 49, avg_time 1.026, loss:491.1405
g_step 1400, step 149, avg_time 1.056, loss:475.6046
g_step 1500, step 249, avg_time 1.051, loss:504.9266
>> valid entity prec:0.5340, rec:0.6644, f1:0.5921
>> valid relation prec:0.0989, rec:0.0287, f1:0.0445
>> valid relation with NER prec:0.0989, rec:0.0287, f1:0.0445
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 349, avg_time 2.427, loss:484.9217
g_step 1700, step 32, avg_time 1.045, loss:486.4022
g_step 1800, step 132, avg_time 1.064, loss:447.2024
g_step 1900, step 232, avg_time 1.042, loss:463.4802
g_step 2000, step 332, avg_time 1.052, loss:444.6228
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5294, rec:0.5916, f1:0.5588
>> valid relation prec:0.1360, rec:0.0336, f1:0.0539
>> valid relation with NER prec:0.1360, rec:0.0336, f1:0.0539
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 15, avg_time 2.445, loss:467.4083
g_step 2200, step 115, avg_time 1.042, loss:454.5648
g_step 2300, step 215, avg_time 1.042, loss:427.8117
g_step 2400, step 315, avg_time 1.039, loss:429.5263
g_step 2500, step 415, avg_time 1.051, loss:456.3963
>> valid entity prec:0.5642, rec:0.5543, f1:0.5592
>> valid relation prec:0.1154, rec:0.0299, f1:0.0475
>> valid relation with NER prec:0.1154, rec:0.0299, f1:0.0475
g_step 2600, step 98, avg_time 2.464, loss:402.9702
g_step 2700, step 198, avg_time 1.046, loss:446.9786
g_step 2800, step 298, avg_time 1.050, loss:411.4029
g_step 2900, step 398, avg_time 1.023, loss:416.9828
g_step 3000, step 81, avg_time 1.028, loss:401.8801
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5633, rec:0.5845, f1:0.5737
>> valid relation prec:0.1651, rec:0.0405, f1:0.0650
>> valid relation with NER prec:0.1651, rec:0.0405, f1:0.0650
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 181, avg_time 2.442, loss:392.9071
g_step 3200, step 281, avg_time 1.060, loss:392.2727
g_step 3300, step 381, avg_time 1.033, loss:430.4710
g_step 3400, step 64, avg_time 1.061, loss:387.7614
g_step 3500, step 164, avg_time 1.031, loss:382.8107
>> valid entity prec:0.5748, rec:0.4932, f1:0.5309
>> valid relation prec:0.1710, rec:0.0437, f1:0.0695
>> valid relation with NER prec:0.1710, rec:0.0437, f1:0.0695
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 264, avg_time 2.456, loss:402.8424
g_step 3700, step 364, avg_time 1.060, loss:394.2982
g_step 3800, step 47, avg_time 1.051, loss:380.4905
g_step 3900, step 147, avg_time 1.074, loss:376.0614
g_step 4000, step 247, avg_time 1.031, loss:370.3574
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5469, rec:0.5913, f1:0.5682
>> valid relation prec:0.0586, rec:0.0146, f1:0.0234
>> valid relation with NER prec:0.0586, rec:0.0146, f1:0.0234
g_step 4100, step 347, avg_time 2.440, loss:376.9540
g_step 4200, step 30, avg_time 1.058, loss:378.0451
g_step 4300, step 130, avg_time 1.035, loss:352.4017
g_step 4400, step 230, avg_time 1.076, loss:376.8042
g_step 4500, step 330, avg_time 1.038, loss:370.1889
>> valid entity prec:0.5477, rec:0.5072, f1:0.5267
>> valid relation prec:0.1097, rec:0.0281, f1:0.0448
>> valid relation with NER prec:0.1097, rec:0.0281, f1:0.0448
g_step 4600, step 13, avg_time 2.448, loss:375.0548
g_step 4700, step 113, avg_time 1.029, loss:336.0170
g_step 4800, step 213, avg_time 1.061, loss:347.2166
g_step 4900, step 313, avg_time 1.060, loss:383.4786
g_step 5000, step 413, avg_time 1.046, loss:361.0096
learning rate was adjusted to 0.0008
>> valid entity prec:0.5590, rec:0.5178, f1:0.5376
>> valid relation prec:0.1324, rec:0.0388, f1:0.0600
>> valid relation with NER prec:0.1324, rec:0.0388, f1:0.0600
g_step 5100, step 96, avg_time 2.452, loss:337.2332
g_step 5200, step 196, avg_time 1.038, loss:348.2730
g_step 5300, step 296, avg_time 1.070, loss:331.6060
g_step 5400, step 396, avg_time 1.051, loss:347.4335
g_step 5500, step 79, avg_time 1.048, loss:313.9617
>> valid entity prec:0.5603, rec:0.5276, f1:0.5435
>> valid relation prec:0.1468, rec:0.0439, f1:0.0676
>> valid relation with NER prec:0.1468, rec:0.0439, f1:0.0676
g_step 5600, step 179, avg_time 2.449, loss:310.4608
g_step 5700, step 279, avg_time 1.036, loss:328.1784
g_step 5800, step 379, avg_time 1.070, loss:352.1417
g_step 5900, step 62, avg_time 1.056, loss:313.7758
g_step 6000, step 162, avg_time 1.049, loss:314.9826
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5706, rec:0.5316, f1:0.5504
>> valid relation prec:0.1142, rec:0.0339, f1:0.0523
>> valid relation with NER prec:0.1142, rec:0.0339, f1:0.0523
g_step 6100, step 262, avg_time 2.441, loss:316.4519
g_step 6200, step 362, avg_time 1.048, loss:304.8145
g_step 6300, step 45, avg_time 1.041, loss:316.0862
g_step 6400, step 145, avg_time 1.041, loss:298.7385
g_step 6500, step 245, avg_time 1.050, loss:298.3907
>> valid entity prec:0.5357, rec:0.4826, f1:0.5078
>> valid relation prec:0.1263, rec:0.0393, f1:0.0600
>> valid relation with NER prec:0.1263, rec:0.0393, f1:0.0600
g_step 6600, step 345, avg_time 2.442, loss:314.0615
g_step 6700, step 28, avg_time 1.039, loss:303.8166
g_step 6800, step 128, avg_time 1.044, loss:298.2464
g_step 6900, step 228, avg_time 1.050, loss:281.9391
g_step 7000, step 328, avg_time 1.042, loss:305.6432
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5567, rec:0.5135, f1:0.5342
>> valid relation prec:0.1009, rec:0.0356, f1:0.0526
>> valid relation with NER prec:0.1009, rec:0.0356, f1:0.0526
g_step 7100, step 11, avg_time 2.458, loss:303.0013
g_step 7200, step 111, avg_time 1.051, loss:269.3176
g_step 7300, step 211, avg_time 1.066, loss:284.1305
g_step 7400, step 311, avg_time 1.038, loss:305.1054
g_step 7500, step 411, avg_time 1.047, loss:286.4345
>> valid entity prec:0.5410, rec:0.5324, f1:0.5367
>> valid relation prec:0.1167, rec:0.0370, f1:0.0562
>> valid relation with NER prec:0.1167, rec:0.0370, f1:0.0562
g_step 7600, step 94, avg_time 2.448, loss:251.7226
g_step 7700, step 194, avg_time 1.048, loss:286.8704
g_step 7800, step 294, avg_time 1.055, loss:275.4158
g_step 7900, step 394, avg_time 1.035, loss:296.1477
g_step 8000, step 77, avg_time 1.064, loss:258.5387
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5297, rec:0.5734, f1:0.5507
>> valid relation prec:0.0684, rec:0.0273, f1:0.0390
>> valid relation with NER prec:0.0684, rec:0.0273, f1:0.0390
g_step 8100, step 177, avg_time 2.442, loss:271.9927
g_step 8200, step 277, avg_time 1.037, loss:262.9966
g_step 8300, step 377, avg_time 1.051, loss:281.0249
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:50:55 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:50:55 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-50-55_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:50:56 - WARNING - datasets.builder -   Using custom data configuration default-41d76f96ec0131f2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-41d76f96ec0131f2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:50:56,640 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:50:56,641 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:50:56,642 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:50:56,643 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:50:56,654 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:50:56,660 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:50:56,660 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:50:56,660 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:50:56,660 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:50:56,660 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:50:56,660 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:50:56,844 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:50:59,933 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:50:59,939 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-41d76f96ec0131f2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.32ba/s] 20%|██        | 2/10 [00:00<00:01,  4.19ba/s] 30%|███       | 3/10 [00:00<00:01,  4.58ba/s] 40%|████      | 4/10 [00:00<00:01,  4.76ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.91ba/s] 60%|██████    | 6/10 [00:01<00:00,  5.00ba/s] 70%|███████   | 7/10 [00:01<00:00,  5.05ba/s] 80%|████████  | 8/10 [00:01<00:00,  5.09ba/s] 90%|█████████ | 9/10 [00:01<00:00,  5.10ba/s]100%|██████████| 10/10 [00:02<00:00,  5.10ba/s]100%|██████████| 10/10 [00:02<00:00,  4.88ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.48ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.31ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.74ba/s]100%|██████████| 4/4 [00:00<00:00,  4.83ba/s]100%|██████████| 4/4 [00:00<00:00,  4.07ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  7.81ba/s] 20%|██        | 2/10 [00:00<00:00,  8.80ba/s] 30%|███       | 3/10 [00:00<00:00,  9.28ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.70ba/s] 70%|███████   | 7/10 [00:00<00:00,  9.86ba/s] 80%|████████  | 8/10 [00:00<00:00,  9.85ba/s]100%|██████████| 10/10 [00:01<00:00,  9.96ba/s]100%|██████████| 10/10 [00:01<00:00,  9.71ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.10ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.55ba/s]100%|██████████| 4/4 [00:00<00:00,  9.66ba/s]
[INFO|trainer.py:414] 2023-08-29 00:51:04,894 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:51:04,915 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:51:04,915 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 00:51:04,915 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:51:04,915 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:51:04,915 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:51:04,916 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:51:04,916 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<04:03,  3.21it/s]  0%|          | 2/780 [00:00<03:52,  3.34it/s]  0%|          | 3/780 [00:00<03:49,  3.39it/s]  1%|          | 4/780 [00:01<03:47,  3.41it/s]  1%|          | 5/780 [00:01<03:47,  3.40it/s]  1%|          | 6/780 [00:01<03:46,  3.42it/s]  1%|          | 7/780 [00:02<03:45,  3.43it/s]  1%|          | 8/780 [00:02<03:44,  3.44it/s]  1%|          | 9/780 [00:02<03:44,  3.44it/s]  1%|▏         | 10/780 [00:02<03:43,  3.44it/s]  1%|▏         | 11/780 [00:03<03:43,  3.44it/s]  2%|▏         | 12/780 [00:03<03:42,  3.45it/s]  2%|▏         | 13/780 [00:03<03:42,  3.45it/s]  2%|▏         | 14/780 [00:04<03:42,  3.45it/s]  2%|▏         | 15/780 [00:04<03:41,  3.45it/s]  2%|▏         | 16/780 [00:04<03:43,  3.42it/s]  2%|▏         | 17/780 [00:04<03:42,  3.43it/s]  2%|▏         | 18/780 [00:05<03:42,  3.43it/s]  2%|▏         | 19/780 [00:05<03:41,  3.44it/s]  3%|▎         | 20/780 [00:05<03:40,  3.44it/s]  3%|▎         | 21/780 [00:06<03:40,  3.44it/s]  3%|▎         | 22/780 [00:06<03:40,  3.44it/s]  3%|▎         | 23/780 [00:06<03:39,  3.44it/s]  3%|▎         | 24/780 [00:06<03:39,  3.44it/s]  3%|▎         | 25/780 [00:07<03:39,  3.44it/s]  3%|▎         | 26/780 [00:07<03:39,  3.44it/s]  3%|▎         | 27/780 [00:07<03:38,  3.44it/s]  4%|▎         | 28/780 [00:08<03:38,  3.44it/s]  4%|▎         | 29/780 [00:08<03:38,  3.44it/s]  4%|▍         | 30/780 [00:08<03:37,  3.44it/s]  4%|▍         | 31/780 [00:09<03:37,  3.44it/s]  4%|▍         | 32/780 [00:09<03:37,  3.45it/s]  4%|▍         | 33/780 [00:09<03:37,  3.43it/s]  4%|▍         | 34/780 [00:09<03:37,  3.43it/s]  4%|▍         | 35/780 [00:10<03:36,  3.44it/s]  5%|▍         | 36/780 [00:10<03:36,  3.44it/s]  5%|▍         | 37/780 [00:10<03:36,  3.44it/s]  5%|▍         | 38/780 [00:11<03:35,  3.44it/s]  5%|▌         | 39/780 [00:11<03:35,  3.44it/s]  5%|▌         | 40/780 [00:11<03:34,  3.44it/s]  5%|▌         | 41/780 [00:11<03:39,  3.37it/s]  5%|▌         | 42/780 [00:12<03:37,  3.39it/s]  6%|▌         | 43/780 [00:12<03:36,  3.40it/s]  6%|▌         | 44/780 [00:12<03:35,  3.41it/s]  6%|▌         | 45/780 [00:13<03:34,  3.42it/s]  6%|▌         | 46/780 [00:13<03:34,  3.43it/s]  6%|▌         | 47/780 [00:13<03:33,  3.43it/s]  6%|▌         | 48/780 [00:13<03:33,  3.43it/s]  6%|▋         | 49/780 [00:14<03:32,  3.43it/s]  6%|▋         | 50/780 [00:14<03:32,  3.43it/s]  7%|▋         | 51/780 [00:14<03:33,  3.42it/s]  7%|▋         | 52/780 [00:15<03:32,  3.43it/s]  7%|▋         | 53/780 [00:15<03:32,  3.43it/s]  7%|▋         | 54/780 [00:15<03:31,  3.43it/s]  7%|▋         | 55/780 [00:16<03:31,  3.43it/s]  7%|▋         | 56/780 [00:16<03:30,  3.44it/s]  7%|▋         | 57/780 [00:16<03:30,  3.44it/s]  7%|▋         | 58/780 [00:16<03:29,  3.44it/s]  8%|▊         | 59/780 [00:17<03:29,  3.44it/s]  8%|▊         | 60/780 [00:17<03:29,  3.44it/s]  8%|▊         | 61/780 [00:17<03:29,  3.44it/s]  8%|▊         | 62/780 [00:18<03:29,  3.43it/s]  8%|▊         | 63/780 [00:18<03:28,  3.44it/s]  8%|▊         | 64/780 [00:18<03:29,  3.42it/s]  8%|▊         | 65/780 [00:18<03:29,  3.42it/s]  8%|▊         | 66/780 [00:19<03:28,  3.43it/s]  9%|▊         | 67/780 [00:19<03:28,  3.43it/s]  9%|▊         | 68/780 [00:19<03:28,  3.42it/s]  9%|▉         | 69/780 [00:20<03:27,  3.43it/s]  9%|▉         | 70/780 [00:20<03:26,  3.43it/s]  9%|▉         | 71/780 [00:20<03:26,  3.43it/s]  9%|▉         | 72/780 [00:20<03:26,  3.43it/s]  9%|▉         | 73/780 [00:21<03:25,  3.44it/s]  9%|▉         | 74/780 [00:21<03:25,  3.43it/s] 10%|▉         | 75/780 [00:21<03:25,  3.43it/s] 10%|▉         | 76/780 [00:22<03:25,  3.43it/s] 10%|▉         | 77/780 [00:22<03:24,  3.43it/s] 10%|█         | 78/780 [00:22<03:24,  3.44it/s] 10%|█         | 79/780 [00:23<03:23,  3.44it/s] 10%|█         | 80/780 [00:23<03:23,  3.44it/s] 10%|█         | 81/780 [00:23<03:23,  3.44it/s] 11%|█         | 82/780 [00:23<03:23,  3.44it/s] 11%|█         | 83/780 [00:24<03:22,  3.44it/s] 11%|█         | 84/780 [00:24<03:22,  3.44it/s] 11%|█         | 85/780 [00:24<03:22,  3.44it/s] 11%|█         | 86/780 [00:25<03:22,  3.42it/s] 11%|█         | 87/780 [00:25<03:22,  3.42it/s] 11%|█▏        | 88/780 [00:25<03:22,  3.43it/s] 11%|█▏        | 89/780 [00:25<03:21,  3.43it/s] 12%|█▏        | 90/780 [00:26<03:21,  3.43it/s] 12%|█▏        | 91/780 [00:26<03:20,  3.43it/s] 12%|█▏        | 92/780 [00:26<03:20,  3.43it/s] 12%|█▏        | 93/780 [00:27<03:20,  3.43it/s] 12%|█▏        | 94/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 95/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 96/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 97/780 [00:28<03:18,  3.44it/s] 13%|█▎        | 98/780 [00:28<03:18,  3.43it/s] 13%|█▎        | 99/780 [00:28<03:18,  3.43it/s] 13%|█▎        | 100/780 [00:29<03:18,  3.43it/s] 13%|█▎        | 101/780 [00:29<03:17,  3.43it/s] 13%|█▎        | 102/780 [00:29<03:17,  3.43it/s] 13%|█▎        | 103/780 [00:30<03:17,  3.43it/s] 13%|█▎        | 104/780 [00:30<03:17,  3.43it/s] 13%|█▎        | 105/780 [00:30<03:16,  3.43it/s] 14%|█▎        | 106/780 [00:30<03:16,  3.43it/s] 14%|█▎        | 107/780 [00:31<03:16,  3.43it/s] 14%|█▍        | 108/780 [00:31<03:15,  3.43it/s] 14%|█▍        | 109/780 [00:31<03:15,  3.43it/s] 14%|█▍        | 110/780 [00:32<03:15,  3.43it/s] 14%|█▍        | 111/780 [00:32<03:14,  3.43it/s] 14%|█▍        | 112/780 [00:32<03:14,  3.43it/s] 14%|█▍        | 113/780 [00:32<03:14,  3.43it/s] 15%|█▍        | 114/780 [00:33<03:14,  3.43it/s] 15%|█▍        | 115/780 [00:33<03:13,  3.43it/s] 15%|█▍        | 116/780 [00:33<03:13,  3.43it/s] 15%|█▌        | 117/780 [00:34<03:13,  3.43it/s] 15%|█▌        | 118/780 [00:34<03:12,  3.43it/s] 15%|█▌        | 119/780 [00:34<03:12,  3.43it/s] 15%|█▌        | 120/780 [00:34<03:12,  3.43it/s] 16%|█▌        | 121/780 [00:35<03:12,  3.42it/s] 16%|█▌        | 122/780 [00:35<03:12,  3.42it/s] 16%|█▌        | 123/780 [00:35<03:11,  3.43it/s] 16%|█▌        | 124/780 [00:36<03:11,  3.43it/s] 16%|█▌        | 125/780 [00:36<03:11,  3.43it/s] 16%|█▌        | 126/780 [00:36<03:10,  3.43it/s] 16%|█▋        | 127/780 [00:37<03:10,  3.43it/s] 16%|█▋        | 128/780 [00:37<03:10,  3.43it/s] 17%|█▋        | 129/780 [00:37<03:09,  3.43it/s] 17%|█▋        | 130/780 [00:37<03:09,  3.43it/s] 17%|█▋        | 131/780 [00:38<03:09,  3.43it/s] 17%|█▋        | 132/780 [00:38<03:08,  3.43it/s] 17%|█▋        | 133/780 [00:38<03:08,  3.43it/s] 17%|█▋        | 134/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 135/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 136/780 [00:39<03:07,  3.43it/s] 18%|█▊        | 137/780 [00:39<03:07,  3.43it/s] 18%|█▊        | 138/780 [00:40<03:07,  3.43it/s] 18%|█▊        | 139/780 [00:40<03:07,  3.42it/s] 18%|█▊        | 140/780 [00:40<03:06,  3.42it/s] 18%|█▊        | 141/780 [00:41<03:06,  3.42it/s] 18%|█▊        | 142/780 [00:41<03:06,  3.43it/s] 18%|█▊        | 143/780 [00:41<03:05,  3.43it/s] 18%|█▊        | 144/780 [00:41<03:05,  3.43it/s] 19%|█▊        | 145/780 [00:42<03:05,  3.43it/s] 19%|█▊        | 146/780 [00:42<03:04,  3.43it/s] 19%|█▉        | 147/780 [00:42<03:04,  3.43it/s] 19%|█▉        | 148/780 [00:43<03:04,  3.43it/s] 19%|█▉        | 149/780 [00:43<03:04,  3.43it/s] 19%|█▉        | 150/780 [00:43<03:03,  3.42it/s] 19%|█▉        | 151/780 [00:44<03:03,  3.43it/s] 19%|█▉        | 152/780 [00:44<03:03,  3.42it/s] 20%|█▉        | 153/780 [00:44<03:03,  3.42it/s] 20%|█▉        | 154/780 [00:44<03:02,  3.42it/s] 20%|█▉        | 155/780 [00:45<03:02,  3.43it/s] 20%|██        | 156/780 [00:45<03:02,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 00:51:50,450 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:51:50,450 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:51:50,450 >>   Batch size = 8

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.18it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.98it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.19it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.44it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.07it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.56it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.40it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.92it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.97it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.16it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.17it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.24it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.10it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.18it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.07it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.97it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.88it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.81it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 46.02it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.00it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.06it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.17it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.24it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.24it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.14it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.01it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.02it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.93it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.01it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.11it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.07it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.17it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.18it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.16it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.01it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.10it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.04it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.09it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.98it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.15it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.15it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.22it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.06it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.98it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 46.02it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.98it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.10it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.15it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.22it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.04it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.12it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.10it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.07it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.10it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 46.10it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.01it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.02it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.07it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.16it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.16it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.12it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.99it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.07it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.01it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.12it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.03it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.11it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.18it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.08it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.07it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.03it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.92it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.05it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.95it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.07it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.07it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.02it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.12it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.15it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.06it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.95it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.06it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.10it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.08it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.07it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.07it/s][A 20%|██        | 156/780 [00:55<03:02,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:51:59,972 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 00:51:59,992 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:52:02,374 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:52:02,390 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:52:02,404 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:02<55:17,  5.33s/it] 20%|██        | 158/780 [01:02<39:34,  3.82s/it] 20%|██        | 159/780 [01:03<28:34,  2.76s/it] 21%|██        | 160/780 [01:03<20:52,  2.02s/it] 21%|██        | 161/780 [01:03<15:29,  1.50s/it] 21%|██        | 162/780 [01:04<11:43,  1.14s/it] 21%|██        | 163/780 [01:04<09:05,  1.13it/s] 21%|██        | 164/780 [01:04<07:15,  1.42it/s] 21%|██        | 165/780 [01:04<05:57,  1.72it/s] 21%|██▏       | 166/780 [01:05<05:03,  2.02it/s] 21%|██▏       | 167/780 [01:05<04:26,  2.30it/s] 22%|██▏       | 168/780 [01:05<03:59,  2.55it/s] 22%|██▏       | 169/780 [01:06<03:41,  2.76it/s] 22%|██▏       | 170/780 [01:06<03:28,  2.93it/s] 22%|██▏       | 171/780 [01:06<03:18,  3.06it/s] 22%|██▏       | 172/780 [01:06<03:12,  3.16it/s] 22%|██▏       | 173/780 [01:07<03:07,  3.24it/s] 22%|██▏       | 174/780 [01:07<03:03,  3.30it/s] 22%|██▏       | 175/780 [01:07<03:01,  3.33it/s] 23%|██▎       | 176/780 [01:08<02:59,  3.36it/s] 23%|██▎       | 177/780 [01:08<02:58,  3.37it/s] 23%|██▎       | 178/780 [01:08<02:57,  3.39it/s] 23%|██▎       | 179/780 [01:08<02:56,  3.40it/s] 23%|██▎       | 180/780 [01:09<02:56,  3.40it/s] 23%|██▎       | 181/780 [01:09<02:55,  3.41it/s] 23%|██▎       | 182/780 [01:09<02:55,  3.41it/s] 23%|██▎       | 183/780 [01:10<02:54,  3.42it/s] 24%|██▎       | 184/780 [01:10<02:54,  3.42it/s] 24%|██▎       | 185/780 [01:10<02:53,  3.42it/s] 24%|██▍       | 186/780 [01:11<02:53,  3.42it/s] 24%|██▍       | 187/780 [01:11<02:53,  3.43it/s] 24%|██▍       | 188/780 [01:11<02:52,  3.42it/s] 24%|██▍       | 189/780 [01:11<02:52,  3.43it/s] 24%|██▍       | 190/780 [01:12<02:56,  3.35it/s] 24%|██▍       | 191/780 [01:12<02:55,  3.36it/s] 25%|██▍       | 192/780 [01:12<02:53,  3.38it/s] 25%|██▍       | 193/780 [01:13<02:52,  3.39it/s] 25%|██▍       | 194/780 [01:13<02:52,  3.41it/s] 25%|██▌       | 195/780 [01:13<02:51,  3.41it/s] 25%|██▌       | 196/780 [01:13<02:50,  3.42it/s] 25%|██▌       | 197/780 [01:14<02:50,  3.42it/s] 25%|██▌       | 198/780 [01:14<02:49,  3.43it/s] 26%|██▌       | 199/780 [01:14<02:49,  3.43it/s] 26%|██▌       | 200/780 [01:15<02:49,  3.43it/s] 26%|██▌       | 201/780 [01:15<02:48,  3.43it/s] 26%|██▌       | 202/780 [01:15<02:48,  3.42it/s] 26%|██▌       | 203/780 [01:16<02:48,  3.42it/s] 26%|██▌       | 204/780 [01:16<02:48,  3.43it/s] 26%|██▋       | 205/780 [01:16<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:16<02:47,  3.43it/s] 27%|██▋       | 207/780 [01:17<02:47,  3.42it/s] 27%|██▋       | 208/780 [01:17<02:46,  3.43it/s] 27%|██▋       | 209/780 [01:17<02:46,  3.43it/s] 27%|██▋       | 210/780 [01:18<02:46,  3.43it/s] 27%|██▋       | 211/780 [01:18<02:46,  3.43it/s] 27%|██▋       | 212/780 [01:18<02:45,  3.43it/s] 27%|██▋       | 213/780 [01:18<02:46,  3.41it/s] 27%|██▋       | 214/780 [01:19<02:45,  3.41it/s] 28%|██▊       | 215/780 [01:19<02:45,  3.42it/s] 28%|██▊       | 216/780 [01:19<02:44,  3.42it/s] 28%|██▊       | 217/780 [01:20<02:44,  3.42it/s] 28%|██▊       | 218/780 [01:20<02:44,  3.42it/s] 28%|██▊       | 219/780 [01:20<02:43,  3.42it/s] 28%|██▊       | 220/780 [01:20<02:43,  3.42it/s] 28%|██▊       | 221/780 [01:21<02:43,  3.42it/s] 28%|██▊       | 222/780 [01:21<02:43,  3.42it/s] 29%|██▊       | 223/780 [01:21<02:42,  3.42it/s] 29%|██▊       | 224/780 [01:22<02:42,  3.41it/s] 29%|██▉       | 225/780 [01:22<02:42,  3.41it/s] 29%|██▉       | 226/780 [01:22<02:42,  3.41it/s] 29%|██▉       | 227/780 [01:23<02:41,  3.41it/s] 29%|██▉       | 228/780 [01:23<02:42,  3.39it/s] 29%|██▉       | 229/780 [01:23<02:42,  3.40it/s] 29%|██▉       | 230/780 [01:23<02:41,  3.41it/s] 30%|██▉       | 231/780 [01:24<02:41,  3.41it/s] 30%|██▉       | 232/780 [01:24<02:40,  3.41it/s] 30%|██▉       | 233/780 [01:24<02:40,  3.41it/s] 30%|███       | 234/780 [01:25<02:39,  3.41it/s] 30%|███       | 235/780 [01:25<02:39,  3.41it/s] 30%|███       | 236/780 [01:25<02:39,  3.42it/s] 30%|███       | 237/780 [01:25<02:38,  3.42it/s] 31%|███       | 238/780 [01:26<02:38,  3.42it/s] 31%|███       | 239/780 [01:26<02:38,  3.41it/s] 31%|███       | 240/780 [01:26<02:38,  3.42it/s] 31%|███       | 241/780 [01:27<02:37,  3.42it/s] 31%|███       | 242/780 [01:27<02:37,  3.42it/s] 31%|███       | 243/780 [01:27<02:36,  3.42it/s] 31%|███▏      | 244/780 [01:28<02:36,  3.42it/s] 31%|███▏      | 245/780 [01:28<02:36,  3.42it/s] 32%|███▏      | 246/780 [01:28<02:36,  3.42it/s] 32%|███▏      | 247/780 [01:28<02:35,  3.42it/s] 32%|███▏      | 248/780 [01:29<02:35,  3.42it/s] 32%|███▏      | 249/780 [01:29<02:35,  3.42it/s] 32%|███▏      | 250/780 [01:29<02:35,  3.41it/s] 32%|███▏      | 251/780 [01:30<02:34,  3.41it/s] 32%|███▏      | 252/780 [01:30<02:34,  3.42it/s] 32%|███▏      | 253/780 [01:30<02:34,  3.41it/s] 33%|███▎      | 254/780 [01:30<02:33,  3.42it/s] 33%|███▎      | 255/780 [01:31<02:33,  3.42it/s] 33%|███▎      | 256/780 [01:31<02:33,  3.42it/s] 33%|███▎      | 257/780 [01:31<02:32,  3.42it/s] 33%|███▎      | 258/780 [01:32<02:32,  3.42it/s] 33%|███▎      | 259/780 [01:32<02:32,  3.42it/s] 33%|███▎      | 260/780 [01:32<02:32,  3.42it/s] 33%|███▎      | 261/780 [01:32<02:32,  3.41it/s] 34%|███▎      | 262/780 [01:33<02:31,  3.41it/s] 34%|███▎      | 263/780 [01:33<02:31,  3.41it/s] 34%|███▍      | 264/780 [01:33<02:31,  3.41it/s] 34%|███▍      | 265/780 [01:34<02:30,  3.42it/s] 34%|███▍      | 266/780 [01:34<02:30,  3.42it/s] 34%|███▍      | 267/780 [01:34<02:30,  3.42it/s] 34%|███▍      | 268/780 [01:35<02:29,  3.42it/s] 34%|███▍      | 269/780 [01:35<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:35<02:29,  3.42it/s] 35%|███▍      | 271/780 [01:35<02:28,  3.42it/s] 35%|███▍      | 272/780 [01:36<02:28,  3.41it/s] 35%|███▌      | 273/780 [01:36<02:28,  3.41it/s] 35%|███▌      | 274/780 [01:36<02:28,  3.41it/s] 35%|███▌      | 275/780 [01:37<02:28,  3.41it/s] 35%|███▌      | 276/780 [01:37<02:27,  3.41it/s] 36%|███▌      | 277/780 [01:37<02:27,  3.41it/s] 36%|███▌      | 278/780 [01:37<02:27,  3.41it/s] 36%|███▌      | 279/780 [01:38<02:26,  3.42it/s] 36%|███▌      | 280/780 [01:38<02:26,  3.42it/s] 36%|███▌      | 281/780 [01:38<02:26,  3.42it/s] 36%|███▌      | 282/780 [01:39<02:25,  3.42it/s] 36%|███▋      | 283/780 [01:39<02:25,  3.41it/s] 36%|███▋      | 284/780 [01:39<02:25,  3.41it/s] 37%|███▋      | 285/780 [01:40<02:25,  3.41it/s] 37%|███▋      | 286/780 [01:40<02:24,  3.41it/s] 37%|███▋      | 287/780 [01:40<02:24,  3.41it/s] 37%|███▋      | 288/780 [01:40<02:24,  3.41it/s] 37%|███▋      | 289/780 [01:41<02:23,  3.41it/s] 37%|███▋      | 290/780 [01:41<02:23,  3.42it/s] 37%|███▋      | 291/780 [01:41<02:23,  3.42it/s] 37%|███▋      | 292/780 [01:42<02:22,  3.42it/s] 38%|███▊      | 293/780 [01:42<02:22,  3.42it/s] 38%|███▊      | 294/780 [01:42<02:22,  3.41it/s] 38%|███▊      | 295/780 [01:42<02:22,  3.41it/s] 38%|███▊      | 296/780 [01:43<02:21,  3.41it/s] 38%|███▊      | 297/780 [01:43<02:21,  3.41it/s] 38%|███▊      | 298/780 [01:43<02:21,  3.41it/s] 38%|███▊      | 299/780 [01:44<02:20,  3.41it/s] 38%|███▊      | 300/780 [01:44<02:20,  3.41it/s] 39%|███▊      | 301/780 [01:44<02:20,  3.41it/s] 39%|███▊      | 302/780 [01:45<02:20,  3.41it/s] 39%|███▉      | 303/780 [01:45<02:19,  3.41it/s] 39%|███▉      | 304/780 [01:45<02:19,  3.41it/s] 39%|███▉      | 305/780 [01:45<02:19,  3.40it/s] 39%|███▉      | 306/780 [01:46<02:19,  3.40it/s] 39%|███▉      | 307/780 [01:46<02:18,  3.41it/s] 39%|███▉      | 308/780 [01:46<02:18,  3.41it/s] 40%|███▉      | 309/780 [01:47<02:18,  3.41it/s] 40%|███▉      | 310/780 [01:47<02:17,  3.41it/s] 40%|███▉      | 311/780 [01:47<02:17,  3.41it/s] 40%|████      | 312/780 [01:47<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 00:52:52,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:52:52,901 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:52:52,901 >>   Batch size = 8
{'eval_loss': 1.1328699588775635, 'eval_runtime': 9.4842, 'eval_samples_per_second': 367.138, 'eval_steps_per_second': 45.971, 'epoch': 1.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.78it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.98it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.20it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.38it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.03it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.66it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.36it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.97it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.07it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.08it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.14it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.21it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.16it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.26it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.14it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.88it/s][A
 20%|██        | 88/436 [00:01<00:07, 46.05it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.91it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.94it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.99it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.11it/s][A
 26%|██▌       | 113/436 [00:02<00:06, 46.16it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.12it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.06it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.01it/s][A
 31%|███       | 133/436 [00:02<00:06, 46.04it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.06it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.95it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 46.04it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.01it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.08it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.10it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.07it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.04it/s][A
 41%|████      | 178/436 [00:03<00:05, 46.07it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 46.00it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.01it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 46.10it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.97it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.03it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.10it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.09it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.99it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.04it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.00it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.93it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.03it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.05it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.13it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.04it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.02it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.07it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.04it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.09it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.95it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.91it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.01it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.08it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.12it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.02it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.04it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.05it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 46.00it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.04it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.02it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 46.01it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 46.04it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.07it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.06it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.12it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.94it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.01it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 46.03it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 46.05it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 46.05it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.95it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.06it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.05it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.13it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.05it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.96it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.98it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 46.02it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.99it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.00it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.00it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.00it/s][A 40%|████      | 312/780 [01:57<02:16,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:53:02,415 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 00:53:02,431 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:53:04,760 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:53:04,776 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:53:04,784 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:05<42:39,  5.48s/it] 40%|████      | 314/780 [02:05<30:28,  3.92s/it] 40%|████      | 315/780 [02:06<21:58,  2.83s/it] 41%|████      | 316/780 [02:06<16:01,  2.07s/it] 41%|████      | 317/780 [02:06<11:51,  1.54s/it] 41%|████      | 318/780 [02:06<08:57,  1.16s/it] 41%|████      | 319/780 [02:07<06:55,  1.11it/s] 41%|████      | 320/780 [02:07<05:30,  1.39it/s] 41%|████      | 321/780 [02:07<04:31,  1.69it/s] 41%|████▏     | 322/780 [02:08<03:49,  2.00it/s] 41%|████▏     | 323/780 [02:08<03:20,  2.28it/s] 42%|████▏     | 324/780 [02:08<02:59,  2.54it/s] 42%|████▏     | 325/780 [02:09<02:45,  2.75it/s] 42%|████▏     | 326/780 [02:09<02:35,  2.92it/s] 42%|████▏     | 327/780 [02:09<02:28,  3.06it/s] 42%|████▏     | 328/780 [02:09<02:22,  3.16it/s] 42%|████▏     | 329/780 [02:10<02:19,  3.24it/s] 42%|████▏     | 330/780 [02:10<02:16,  3.30it/s] 42%|████▏     | 331/780 [02:10<02:14,  3.34it/s] 43%|████▎     | 332/780 [02:11<02:13,  3.37it/s] 43%|████▎     | 333/780 [02:11<02:11,  3.39it/s] 43%|████▎     | 334/780 [02:11<02:11,  3.40it/s] 43%|████▎     | 335/780 [02:11<02:10,  3.41it/s] 43%|████▎     | 336/780 [02:12<02:11,  3.37it/s] 43%|████▎     | 337/780 [02:12<02:10,  3.39it/s] 43%|████▎     | 338/780 [02:12<02:09,  3.40it/s] 43%|████▎     | 339/780 [02:13<02:09,  3.41it/s] 44%|████▎     | 340/780 [02:13<02:08,  3.42it/s] 44%|████▎     | 341/780 [02:13<02:08,  3.42it/s] 44%|████▍     | 342/780 [02:13<02:07,  3.43it/s] 44%|████▍     | 343/780 [02:14<02:07,  3.43it/s] 44%|████▍     | 344/780 [02:14<02:07,  3.43it/s] 44%|████▍     | 345/780 [02:14<02:06,  3.43it/s] 44%|████▍     | 346/780 [02:15<02:06,  3.43it/s] 44%|████▍     | 347/780 [02:15<02:06,  3.42it/s] 45%|████▍     | 348/780 [02:15<02:06,  3.42it/s] 45%|████▍     | 349/780 [02:16<02:05,  3.43it/s] 45%|████▍     | 350/780 [02:16<02:05,  3.43it/s] 45%|████▌     | 351/780 [02:16<02:05,  3.43it/s] 45%|████▌     | 352/780 [02:16<02:04,  3.43it/s] 45%|████▌     | 353/780 [02:17<02:04,  3.43it/s] 45%|████▌     | 354/780 [02:17<02:04,  3.43it/s] 46%|████▌     | 355/780 [02:17<02:03,  3.43it/s] 46%|████▌     | 356/780 [02:18<02:03,  3.43it/s] 46%|████▌     | 357/780 [02:18<02:03,  3.43it/s] 46%|████▌     | 358/780 [02:18<02:03,  3.42it/s] 46%|████▌     | 359/780 [02:18<02:03,  3.41it/s] 46%|████▌     | 360/780 [02:19<02:02,  3.42it/s] 46%|████▋     | 361/780 [02:19<02:02,  3.42it/s] 46%|████▋     | 362/780 [02:19<02:02,  3.42it/s] 47%|████▋     | 363/780 [02:20<02:01,  3.42it/s] 47%|████▋     | 364/780 [02:20<02:01,  3.42it/s] 47%|████▋     | 365/780 [02:20<02:01,  3.42it/s] 47%|████▋     | 366/780 [02:20<02:00,  3.42it/s] 47%|████▋     | 367/780 [02:21<02:00,  3.42it/s] 47%|████▋     | 368/780 [02:21<02:00,  3.42it/s] 47%|████▋     | 369/780 [02:21<02:00,  3.41it/s] 47%|████▋     | 370/780 [02:22<01:59,  3.42it/s] 48%|████▊     | 371/780 [02:22<01:59,  3.42it/s] 48%|████▊     | 372/780 [02:22<01:59,  3.42it/s] 48%|████▊     | 373/780 [02:23<01:58,  3.42it/s] 48%|████▊     | 374/780 [02:23<01:58,  3.42it/s] 48%|████▊     | 375/780 [02:23<01:58,  3.42it/s] 48%|████▊     | 376/780 [02:23<01:58,  3.42it/s] 48%|████▊     | 377/780 [02:24<01:57,  3.42it/s] 48%|████▊     | 378/780 [02:24<01:57,  3.42it/s] 49%|████▊     | 379/780 [02:24<01:57,  3.42it/s] 49%|████▊     | 380/780 [02:25<01:56,  3.43it/s] 49%|████▉     | 381/780 [02:25<01:56,  3.43it/s] 49%|████▉     | 382/780 [02:25<01:56,  3.42it/s] 49%|████▉     | 383/780 [02:25<01:55,  3.42it/s] 49%|████▉     | 384/780 [02:26<01:55,  3.42it/s] 49%|████▉     | 385/780 [02:26<01:55,  3.42it/s] 49%|████▉     | 386/780 [02:26<01:55,  3.42it/s] 50%|████▉     | 387/780 [02:27<01:54,  3.42it/s] 50%|████▉     | 388/780 [02:27<01:54,  3.42it/s] 50%|████▉     | 389/780 [02:27<01:54,  3.42it/s] 50%|█████     | 390/780 [02:28<01:54,  3.41it/s] 50%|█████     | 391/780 [02:28<01:53,  3.42it/s] 50%|█████     | 392/780 [02:28<01:53,  3.42it/s] 50%|█████     | 393/780 [02:28<01:53,  3.42it/s] 51%|█████     | 394/780 [02:29<01:52,  3.42it/s] 51%|█████     | 395/780 [02:29<01:52,  3.42it/s] 51%|█████     | 396/780 [02:29<01:52,  3.42it/s] 51%|█████     | 397/780 [02:30<01:51,  3.43it/s] 51%|█████     | 398/780 [02:30<01:51,  3.43it/s] 51%|█████     | 399/780 [02:30<01:51,  3.43it/s] 51%|█████▏    | 400/780 [02:30<01:50,  3.42it/s] 51%|█████▏    | 401/780 [02:31<01:50,  3.42it/s] 52%|█████▏    | 402/780 [02:31<01:50,  3.42it/s] 52%|█████▏    | 403/780 [02:31<01:50,  3.42it/s] 52%|█████▏    | 404/780 [02:32<01:49,  3.42it/s] 52%|█████▏    | 405/780 [02:32<01:49,  3.42it/s] 52%|█████▏    | 406/780 [02:32<01:49,  3.42it/s] 52%|█████▏    | 407/780 [02:32<01:48,  3.43it/s] 52%|█████▏    | 408/780 [02:33<01:48,  3.42it/s] 52%|█████▏    | 409/780 [02:33<01:48,  3.42it/s] 53%|█████▎    | 410/780 [02:33<01:48,  3.42it/s] 53%|█████▎    | 411/780 [02:34<01:47,  3.42it/s] 53%|█████▎    | 412/780 [02:34<01:48,  3.41it/s] 53%|█████▎    | 413/780 [02:34<01:47,  3.41it/s] 53%|█████▎    | 414/780 [02:35<01:47,  3.42it/s] 53%|█████▎    | 415/780 [02:35<01:46,  3.42it/s] 53%|█████▎    | 416/780 [02:35<01:46,  3.42it/s] 53%|█████▎    | 417/780 [02:35<01:46,  3.42it/s] 54%|█████▎    | 418/780 [02:36<01:45,  3.42it/s] 54%|█████▎    | 419/780 [02:36<01:45,  3.42it/s] 54%|█████▍    | 420/780 [02:36<01:45,  3.42it/s] 54%|█████▍    | 421/780 [02:37<01:44,  3.42it/s] 54%|█████▍    | 422/780 [02:37<01:44,  3.42it/s] 54%|█████▍    | 423/780 [02:37<01:44,  3.41it/s] 54%|█████▍    | 424/780 [02:37<01:44,  3.41it/s] 54%|█████▍    | 425/780 [02:38<01:43,  3.42it/s] 55%|█████▍    | 426/780 [02:38<01:43,  3.42it/s] 55%|█████▍    | 427/780 [02:38<01:43,  3.42it/s] 55%|█████▍    | 428/780 [02:39<01:42,  3.42it/s] 55%|█████▌    | 429/780 [02:39<01:42,  3.42it/s] 55%|█████▌    | 430/780 [02:39<01:42,  3.42it/s] 55%|█████▌    | 431/780 [02:39<01:42,  3.42it/s] 55%|█████▌    | 432/780 [02:40<01:41,  3.42it/s] 56%|█████▌    | 433/780 [02:40<01:41,  3.42it/s] 56%|█████▌    | 434/780 [02:40<01:41,  3.41it/s] 56%|█████▌    | 435/780 [02:41<01:40,  3.42it/s] 56%|█████▌    | 436/780 [02:41<01:40,  3.42it/s] 56%|█████▌    | 437/780 [02:41<01:40,  3.42it/s] 56%|█████▌    | 438/780 [02:42<01:39,  3.42it/s] 56%|█████▋    | 439/780 [02:42<01:39,  3.42it/s] 56%|█████▋    | 440/780 [02:42<01:39,  3.42it/s] 57%|█████▋    | 441/780 [02:42<01:38,  3.43it/s] 57%|█████▋    | 442/780 [02:43<01:38,  3.42it/s] 57%|█████▋    | 443/780 [02:43<01:38,  3.42it/s] 57%|█████▋    | 444/780 [02:43<01:38,  3.42it/s] 57%|█████▋    | 445/780 [02:44<01:38,  3.42it/s] 57%|█████▋    | 446/780 [02:44<01:37,  3.42it/s] 57%|█████▋    | 447/780 [02:44<01:37,  3.42it/s] 57%|█████▋    | 448/780 [02:44<01:37,  3.42it/s] 58%|█████▊    | 449/780 [02:45<01:36,  3.42it/s] 58%|█████▊    | 450/780 [02:45<01:36,  3.42it/s] 58%|█████▊    | 451/780 [02:45<01:36,  3.42it/s] 58%|█████▊    | 452/780 [02:46<01:35,  3.42it/s] 58%|█████▊    | 453/780 [02:46<01:35,  3.42it/s] 58%|█████▊    | 454/780 [02:46<01:35,  3.42it/s] 58%|█████▊    | 455/780 [02:47<01:34,  3.42it/s] 58%|█████▊    | 456/780 [02:47<01:34,  3.41it/s] 59%|█████▊    | 457/780 [02:47<01:34,  3.42it/s] 59%|█████▊    | 458/780 [02:47<01:34,  3.42it/s] 59%|█████▉    | 459/780 [02:48<01:33,  3.42it/s] 59%|█████▉    | 460/780 [02:48<01:33,  3.42it/s] 59%|█████▉    | 461/780 [02:48<01:33,  3.42it/s] 59%|█████▉    | 462/780 [02:49<01:33,  3.42it/s] 59%|█████▉    | 463/780 [02:49<01:32,  3.42it/s] 59%|█████▉    | 464/780 [02:49<01:32,  3.42it/s] 60%|█████▉    | 465/780 [02:49<01:32,  3.42it/s] 60%|█████▉    | 466/780 [02:50<01:31,  3.42it/s] 60%|█████▉    | 467/780 [02:50<01:32,  3.38it/s] 60%|██████    | 468/780 [02:50<01:32,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 00:53:55,791 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:53:55,791 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:53:55,791 >>   Batch size = 8
{'eval_loss': 1.1546610593795776, 'eval_runtime': 9.488, 'eval_samples_per_second': 366.99, 'eval_steps_per_second': 45.953, 'epoch': 2.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.56it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.97it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.30it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.54it/s][A
  6%|▋         | 28/436 [00:00<00:08, 46.96it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.51it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.33it/s][A
 10%|▉         | 43/436 [00:00<00:08, 45.95it/s][A
 11%|█         | 48/436 [00:01<00:08, 45.85it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 45.96it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 45.99it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.11it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.16it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.10it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 45.95it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 45.85it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.71it/s][A
 21%|██▏       | 93/436 [00:02<00:07, 45.77it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.92it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 45.97it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 45.98it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.12it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.08it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.03it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 45.89it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.79it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.82it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.91it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.95it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.97it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 45.97it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 45.80it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.09it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 45.99it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.84it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.70it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.83it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.89it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.05it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.97it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.03it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.02it/s][A
 50%|█████     | 218/436 [00:04<00:04, 45.95it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.86it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.80it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.78it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.84it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 45.39it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.73it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.81it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 45.88it/s][A
 60%|██████    | 263/436 [00:05<00:03, 45.86it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.80it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 45.73it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.76it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.71it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.87it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.98it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.03it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.00it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.84it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 45.91it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.81it/s][A
 74%|███████▍  | 323/436 [00:07<00:02, 45.83it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.84it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.90it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.95it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.97it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.00it/s][A
 81%|████████  | 353/436 [00:07<00:01, 45.84it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 45.86it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.75it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.83it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.82it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.91it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 46.01it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.00it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.04it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 45.92it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 45.92it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.79it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.89it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.85it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.92it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 46.01it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.93it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.93it/s][A 60%|██████    | 468/780 [03:00<01:32,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:54:05,330 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 00:54:05,348 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:54:07,852 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:54:07,872 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:54:07,890 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:09<30:15,  5.84s/it] 60%|██████    | 470/780 [03:09<21:35,  4.18s/it] 60%|██████    | 471/780 [03:10<15:30,  3.01s/it] 61%|██████    | 472/780 [03:10<11:16,  2.20s/it] 61%|██████    | 473/780 [03:10<08:18,  1.63s/it] 61%|██████    | 474/780 [03:11<06:14,  1.22s/it] 61%|██████    | 475/780 [03:11<04:48,  1.06it/s] 61%|██████    | 476/780 [03:11<03:47,  1.33it/s] 61%|██████    | 477/780 [03:11<03:05,  1.63it/s] 61%|██████▏   | 478/780 [03:12<02:38,  1.91it/s] 61%|██████▏   | 479/780 [03:12<02:16,  2.20it/s] 62%|██████▏   | 480/780 [03:12<02:01,  2.46it/s] 62%|██████▏   | 481/780 [03:13<01:51,  2.68it/s] 62%|██████▏   | 482/780 [03:13<01:43,  2.87it/s] 62%|██████▏   | 483/780 [03:13<01:38,  3.02it/s] 62%|██████▏   | 484/780 [03:14<01:34,  3.13it/s] 62%|██████▏   | 485/780 [03:14<01:31,  3.22it/s] 62%|██████▏   | 486/780 [03:14<01:29,  3.28it/s] 62%|██████▏   | 487/780 [03:14<01:28,  3.32it/s] 63%|██████▎   | 488/780 [03:15<01:27,  3.35it/s] 63%|██████▎   | 489/780 [03:15<01:26,  3.38it/s] 63%|██████▎   | 490/780 [03:15<01:25,  3.39it/s] 63%|██████▎   | 491/780 [03:16<01:24,  3.40it/s] 63%|██████▎   | 492/780 [03:16<01:24,  3.40it/s] 63%|██████▎   | 493/780 [03:16<01:24,  3.41it/s] 63%|██████▎   | 494/780 [03:16<01:23,  3.41it/s] 63%|██████▎   | 495/780 [03:17<01:23,  3.42it/s] 64%|██████▎   | 496/780 [03:17<01:23,  3.42it/s] 64%|██████▎   | 497/780 [03:17<01:22,  3.42it/s] 64%|██████▍   | 498/780 [03:18<01:22,  3.42it/s] 64%|██████▍   | 499/780 [03:18<01:22,  3.42it/s] 64%|██████▍   | 500/780 [03:18<01:21,  3.43it/s]                                                  64%|██████▍   | 500/780 [03:18<01:21,  3.43it/s] 64%|██████▍   | 501/780 [03:18<01:21,  3.41it/s] 64%|██████▍   | 502/780 [03:19<01:21,  3.41it/s] 64%|██████▍   | 503/780 [03:19<01:21,  3.41it/s] 65%|██████▍   | 504/780 [03:19<01:20,  3.41it/s] 65%|██████▍   | 505/780 [03:20<01:20,  3.41it/s] 65%|██████▍   | 506/780 [03:20<01:20,  3.42it/s] 65%|██████▌   | 507/780 [03:20<01:19,  3.42it/s] 65%|██████▌   | 508/780 [03:21<01:19,  3.42it/s] 65%|██████▌   | 509/780 [03:21<01:19,  3.42it/s] 65%|██████▌   | 510/780 [03:21<01:18,  3.42it/s] 66%|██████▌   | 511/780 [03:21<01:18,  3.43it/s] 66%|██████▌   | 512/780 [03:22<01:18,  3.43it/s] 66%|██████▌   | 513/780 [03:22<01:17,  3.43it/s] 66%|██████▌   | 514/780 [03:22<01:17,  3.42it/s] 66%|██████▌   | 515/780 [03:23<01:17,  3.42it/s] 66%|██████▌   | 516/780 [03:23<01:17,  3.42it/s] 66%|██████▋   | 517/780 [03:23<01:16,  3.42it/s] 66%|██████▋   | 518/780 [03:23<01:16,  3.42it/s] 67%|██████▋   | 519/780 [03:24<01:16,  3.43it/s] 67%|██████▋   | 520/780 [03:24<01:15,  3.42it/s] 67%|██████▋   | 521/780 [03:24<01:15,  3.43it/s] 67%|██████▋   | 522/780 [03:25<01:15,  3.43it/s] 67%|██████▋   | 523/780 [03:25<01:14,  3.43it/s] 67%|██████▋   | 524/780 [03:25<01:14,  3.43it/s] 67%|██████▋   | 525/780 [03:25<01:14,  3.42it/s] 67%|██████▋   | 526/780 [03:26<01:14,  3.42it/s] 68%|██████▊   | 527/780 [03:26<01:13,  3.42it/s] 68%|██████▊   | 528/780 [03:26<01:13,  3.42it/s] 68%|██████▊   | 529/780 [03:27<01:13,  3.43it/s] 68%|██████▊   | 530/780 [03:27<01:12,  3.42it/s] 68%|██████▊   | 531/780 [03:27<01:12,  3.42it/s] 68%|██████▊   | 532/780 [03:28<01:12,  3.42it/s] 68%|██████▊   | 533/780 [03:28<01:12,  3.42it/s] 68%|██████▊   | 534/780 [03:28<01:11,  3.43it/s] 69%|██████▊   | 535/780 [03:28<01:11,  3.43it/s] 69%|██████▊   | 536/780 [03:29<01:11,  3.41it/s] 69%|██████▉   | 537/780 [03:29<01:11,  3.42it/s] 69%|██████▉   | 538/780 [03:29<01:10,  3.42it/s] 69%|██████▉   | 539/780 [03:30<01:10,  3.42it/s] 69%|██████▉   | 540/780 [03:30<01:10,  3.42it/s] 69%|██████▉   | 541/780 [03:30<01:09,  3.42it/s] 69%|██████▉   | 542/780 [03:30<01:09,  3.42it/s] 70%|██████▉   | 543/780 [03:31<01:09,  3.42it/s] 70%|██████▉   | 544/780 [03:31<01:08,  3.42it/s] 70%|██████▉   | 545/780 [03:31<01:08,  3.42it/s] 70%|███████   | 546/780 [03:32<01:08,  3.42it/s] 70%|███████   | 547/780 [03:32<01:08,  3.41it/s] 70%|███████   | 548/780 [03:32<01:07,  3.42it/s] 70%|███████   | 549/780 [03:33<01:07,  3.42it/s] 71%|███████   | 550/780 [03:33<01:07,  3.42it/s] 71%|███████   | 551/780 [03:33<01:06,  3.42it/s] 71%|███████   | 552/780 [03:33<01:06,  3.42it/s] 71%|███████   | 553/780 [03:34<01:06,  3.42it/s] 71%|███████   | 554/780 [03:34<01:06,  3.42it/s] 71%|███████   | 555/780 [03:34<01:05,  3.42it/s] 71%|███████▏  | 556/780 [03:35<01:05,  3.42it/s] 71%|███████▏  | 557/780 [03:35<01:05,  3.42it/s] 72%|███████▏  | 558/780 [03:35<01:05,  3.41it/s] 72%|███████▏  | 559/780 [03:35<01:04,  3.41it/s] 72%|███████▏  | 560/780 [03:36<01:04,  3.41it/s] 72%|███████▏  | 561/780 [03:36<01:04,  3.42it/s] 72%|███████▏  | 562/780 [03:36<01:03,  3.42it/s] 72%|███████▏  | 563/780 [03:37<01:03,  3.42it/s] 72%|███████▏  | 564/780 [03:37<01:03,  3.42it/s] 72%|███████▏  | 565/780 [03:37<01:02,  3.42it/s] 73%|███████▎  | 566/780 [03:37<01:02,  3.42it/s] 73%|███████▎  | 567/780 [03:38<01:02,  3.42it/s] 73%|███████▎  | 568/780 [03:38<01:01,  3.42it/s] 73%|███████▎  | 569/780 [03:38<01:02,  3.40it/s] 73%|███████▎  | 570/780 [03:39<01:01,  3.41it/s] 73%|███████▎  | 571/780 [03:39<01:01,  3.41it/s] 73%|███████▎  | 572/780 [03:39<01:00,  3.41it/s] 73%|███████▎  | 573/780 [03:40<01:00,  3.42it/s] 74%|███████▎  | 574/780 [03:40<01:00,  3.42it/s] 74%|███████▎  | 575/780 [03:40<00:59,  3.42it/s] 74%|███████▍  | 576/780 [03:40<00:59,  3.42it/s] 74%|███████▍  | 577/780 [03:41<00:59,  3.42it/s] 74%|███████▍  | 578/780 [03:41<00:59,  3.42it/s] 74%|███████▍  | 579/780 [03:41<00:58,  3.42it/s] 74%|███████▍  | 580/780 [03:42<00:58,  3.41it/s] 74%|███████▍  | 581/780 [03:42<00:58,  3.41it/s] 75%|███████▍  | 582/780 [03:42<00:58,  3.41it/s] 75%|███████▍  | 583/780 [03:42<00:57,  3.42it/s] 75%|███████▍  | 584/780 [03:43<00:57,  3.42it/s] 75%|███████▌  | 585/780 [03:43<00:57,  3.42it/s] 75%|███████▌  | 586/780 [03:43<00:56,  3.42it/s] 75%|███████▌  | 587/780 [03:44<00:56,  3.42it/s] 75%|███████▌  | 588/780 [03:44<00:56,  3.42it/s] 76%|███████▌  | 589/780 [03:44<00:55,  3.42it/s] 76%|███████▌  | 590/780 [03:45<00:55,  3.42it/s] 76%|███████▌  | 591/780 [03:45<00:55,  3.41it/s] 76%|███████▌  | 592/780 [03:45<00:55,  3.41it/s] 76%|███████▌  | 593/780 [03:45<00:54,  3.41it/s] 76%|███████▌  | 594/780 [03:46<00:54,  3.41it/s] 76%|███████▋  | 595/780 [03:46<00:54,  3.42it/s] 76%|███████▋  | 596/780 [03:46<00:53,  3.41it/s] 77%|███████▋  | 597/780 [03:47<00:53,  3.41it/s] 77%|███████▋  | 598/780 [03:47<00:53,  3.41it/s] 77%|███████▋  | 599/780 [03:47<00:52,  3.42it/s] 77%|███████▋  | 600/780 [03:47<00:52,  3.42it/s] 77%|███████▋  | 601/780 [03:48<00:52,  3.42it/s] 77%|███████▋  | 602/780 [03:48<00:52,  3.41it/s] 77%|███████▋  | 603/780 [03:48<00:51,  3.41it/s] 77%|███████▋  | 604/780 [03:49<00:51,  3.41it/s] 78%|███████▊  | 605/780 [03:49<00:51,  3.41it/s] 78%|███████▊  | 606/780 [03:49<00:50,  3.42it/s] 78%|███████▊  | 607/780 [03:49<00:50,  3.42it/s] 78%|███████▊  | 608/780 [03:50<00:50,  3.42it/s] 78%|███████▊  | 609/780 [03:50<00:50,  3.41it/s] 78%|███████▊  | 610/780 [03:50<00:49,  3.41it/s] 78%|███████▊  | 611/780 [03:51<00:49,  3.41it/s] 78%|███████▊  | 612/780 [03:51<00:49,  3.42it/s] 79%|███████▊  | 613/780 [03:51<00:49,  3.41it/s] 79%|███████▊  | 614/780 [03:52<00:48,  3.41it/s] 79%|███████▉  | 615/780 [03:52<00:48,  3.41it/s] 79%|███████▉  | 616/780 [03:52<00:48,  3.41it/s] 79%|███████▉  | 617/780 [03:52<00:47,  3.42it/s] 79%|███████▉  | 618/780 [03:53<00:47,  3.42it/s] 79%|███████▉  | 619/780 [03:53<00:47,  3.42it/s] 79%|███████▉  | 620/780 [03:53<00:46,  3.42it/s] 80%|███████▉  | 621/780 [03:54<00:46,  3.42it/s] 80%|███████▉  | 622/780 [03:54<00:46,  3.42it/s] 80%|███████▉  | 623/780 [03:54<00:45,  3.42it/s] 80%|████████  | 624/780 [03:54<00:45,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 00:54:59,932 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:54:59,933 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:54:59,933 >>   Batch size = 8
{'eval_loss': 1.172706961631775, 'eval_runtime': 9.5159, 'eval_samples_per_second': 365.915, 'eval_steps_per_second': 45.818, 'epoch': 3.0}
{'loss': 0.3968, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.69it/s][A
  3%|▎         | 12/436 [00:00<00:08, 49.82it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.16it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.45it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.06it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.68it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.41it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.08it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.06it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.10it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.12it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.14it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.03it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.13it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.15it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.03it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.95it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 45.81it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.95it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.05it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.06it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.09it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.17it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.15it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.04it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.90it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 45.93it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.92it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.89it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 45.98it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.16it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.01it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.07it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.04it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.93it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.98it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 45.91it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.89it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 45.88it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 45.98it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.06it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.16it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.09it/s][A
 51%|█████     | 223/436 [00:04<00:04, 45.93it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 45.77it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.93it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 46.00it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.10it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 45.99it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 45.98it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.12it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.04it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 45.98it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.07it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 45.84it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.92it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 45.96it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 45.99it/s][A
 68%|██████▊   | 298/436 [00:06<00:02, 46.10it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.15it/s][A
 71%|███████   | 308/436 [00:06<00:02, 46.10it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.07it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.97it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 45.98it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 45.94it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.95it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.99it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 45.91it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.08it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.12it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.07it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 45.97it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.97it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.94it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.96it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.94it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 46.00it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 46.03it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.04it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.05it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 46.00it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 46.02it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.92it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 45.97it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.93it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 45.95it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 45.95it/s][A 80%|████████  | 624/780 [04:04<00:45,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:55:09,461 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 00:55:09,497 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:55:12,305 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:55:12,344 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:55:12,371 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:13<14:32,  5.63s/it] 80%|████████  | 626/780 [04:13<10:20,  4.03s/it] 80%|████████  | 627/780 [04:13<07:25,  2.91s/it] 81%|████████  | 628/780 [04:13<05:22,  2.12s/it] 81%|████████  | 629/780 [04:14<03:57,  1.57s/it] 81%|████████  | 630/780 [04:14<02:58,  1.19s/it] 81%|████████  | 631/780 [04:14<02:17,  1.09it/s] 81%|████████  | 632/780 [04:15<01:48,  1.37it/s] 81%|████████  | 633/780 [04:15<01:28,  1.67it/s] 81%|████████▏ | 634/780 [04:15<01:14,  1.97it/s] 81%|████████▏ | 635/780 [04:15<01:04,  2.26it/s] 82%|████████▏ | 636/780 [04:16<00:57,  2.52it/s] 82%|████████▏ | 637/780 [04:16<00:52,  2.73it/s] 82%|████████▏ | 638/780 [04:16<00:48,  2.91it/s] 82%|████████▏ | 639/780 [04:17<00:46,  3.05it/s] 82%|████████▏ | 640/780 [04:17<00:44,  3.15it/s] 82%|████████▏ | 641/780 [04:17<00:42,  3.23it/s] 82%|████████▏ | 642/780 [04:18<00:41,  3.29it/s] 82%|████████▏ | 643/780 [04:18<00:41,  3.33it/s] 83%|████████▎ | 644/780 [04:18<00:40,  3.36it/s] 83%|████████▎ | 645/780 [04:18<00:39,  3.38it/s] 83%|████████▎ | 646/780 [04:19<00:39,  3.38it/s] 83%|████████▎ | 647/780 [04:19<00:39,  3.39it/s] 83%|████████▎ | 648/780 [04:19<00:39,  3.38it/s] 83%|████████▎ | 649/780 [04:20<00:38,  3.40it/s] 83%|████████▎ | 650/780 [04:20<00:38,  3.41it/s] 83%|████████▎ | 651/780 [04:20<00:37,  3.42it/s] 84%|████████▎ | 652/780 [04:20<00:37,  3.42it/s] 84%|████████▎ | 653/780 [04:21<00:37,  3.42it/s] 84%|████████▍ | 654/780 [04:21<00:36,  3.43it/s] 84%|████████▍ | 655/780 [04:21<00:36,  3.43it/s] 84%|████████▍ | 656/780 [04:22<00:36,  3.43it/s] 84%|████████▍ | 657/780 [04:22<00:35,  3.43it/s] 84%|████████▍ | 658/780 [04:22<00:35,  3.43it/s] 84%|████████▍ | 659/780 [04:22<00:35,  3.42it/s] 85%|████████▍ | 660/780 [04:23<00:35,  3.42it/s] 85%|████████▍ | 661/780 [04:23<00:34,  3.42it/s] 85%|████████▍ | 662/780 [04:23<00:34,  3.43it/s] 85%|████████▌ | 663/780 [04:24<00:34,  3.43it/s] 85%|████████▌ | 664/780 [04:24<00:33,  3.43it/s] 85%|████████▌ | 665/780 [04:24<00:33,  3.43it/s] 85%|████████▌ | 666/780 [04:25<00:33,  3.43it/s] 86%|████████▌ | 667/780 [04:25<00:32,  3.43it/s] 86%|████████▌ | 668/780 [04:25<00:32,  3.43it/s] 86%|████████▌ | 669/780 [04:25<00:32,  3.43it/s] 86%|████████▌ | 670/780 [04:26<00:32,  3.42it/s] 86%|████████▌ | 671/780 [04:26<00:31,  3.42it/s] 86%|████████▌ | 672/780 [04:26<00:31,  3.42it/s] 86%|████████▋ | 673/780 [04:27<00:31,  3.43it/s] 86%|████████▋ | 674/780 [04:27<00:30,  3.43it/s] 87%|████████▋ | 675/780 [04:27<00:30,  3.43it/s] 87%|████████▋ | 676/780 [04:27<00:30,  3.43it/s] 87%|████████▋ | 677/780 [04:28<00:30,  3.43it/s] 87%|████████▋ | 678/780 [04:28<00:29,  3.43it/s] 87%|████████▋ | 679/780 [04:28<00:29,  3.43it/s] 87%|████████▋ | 680/780 [04:29<00:29,  3.43it/s] 87%|████████▋ | 681/780 [04:29<00:28,  3.42it/s] 87%|████████▋ | 682/780 [04:29<00:28,  3.42it/s] 88%|████████▊ | 683/780 [04:29<00:28,  3.43it/s] 88%|████████▊ | 684/780 [04:30<00:27,  3.43it/s] 88%|████████▊ | 685/780 [04:30<00:27,  3.43it/s] 88%|████████▊ | 686/780 [04:30<00:27,  3.43it/s] 88%|████████▊ | 687/780 [04:31<00:27,  3.43it/s] 88%|████████▊ | 688/780 [04:31<00:26,  3.43it/s] 88%|████████▊ | 689/780 [04:31<00:26,  3.43it/s] 88%|████████▊ | 690/780 [04:32<00:26,  3.43it/s] 89%|████████▊ | 691/780 [04:32<00:25,  3.43it/s] 89%|████████▊ | 692/780 [04:32<00:25,  3.42it/s] 89%|████████▉ | 693/780 [04:32<00:25,  3.42it/s] 89%|████████▉ | 694/780 [04:33<00:25,  3.42it/s] 89%|████████▉ | 695/780 [04:33<00:24,  3.42it/s] 89%|████████▉ | 696/780 [04:33<00:24,  3.43it/s] 89%|████████▉ | 697/780 [04:34<00:24,  3.43it/s] 89%|████████▉ | 698/780 [04:34<00:23,  3.43it/s] 90%|████████▉ | 699/780 [04:34<00:23,  3.43it/s] 90%|████████▉ | 700/780 [04:34<00:23,  3.43it/s] 90%|████████▉ | 701/780 [04:35<00:23,  3.43it/s] 90%|█████████ | 702/780 [04:35<00:22,  3.43it/s] 90%|█████████ | 703/780 [04:35<00:22,  3.42it/s] 90%|█████████ | 704/780 [04:36<00:22,  3.42it/s] 90%|█████████ | 705/780 [04:36<00:21,  3.42it/s] 91%|█████████ | 706/780 [04:36<00:21,  3.43it/s] 91%|█████████ | 707/780 [04:36<00:21,  3.43it/s] 91%|█████████ | 708/780 [04:37<00:20,  3.43it/s] 91%|█████████ | 709/780 [04:37<00:20,  3.43it/s] 91%|█████████ | 710/780 [04:37<00:20,  3.43it/s] 91%|█████████ | 711/780 [04:38<00:20,  3.43it/s] 91%|█████████▏| 712/780 [04:38<00:19,  3.42it/s] 91%|█████████▏| 713/780 [04:38<00:19,  3.43it/s] 92%|█████████▏| 714/780 [04:39<00:19,  3.41it/s] 92%|█████████▏| 715/780 [04:39<00:19,  3.42it/s] 92%|█████████▏| 716/780 [04:39<00:18,  3.42it/s] 92%|█████████▏| 717/780 [04:39<00:18,  3.42it/s] 92%|█████████▏| 718/780 [04:40<00:18,  3.42it/s] 92%|█████████▏| 719/780 [04:40<00:17,  3.43it/s] 92%|█████████▏| 720/780 [04:40<00:17,  3.43it/s] 92%|█████████▏| 721/780 [04:41<00:17,  3.43it/s] 93%|█████████▎| 722/780 [04:41<00:16,  3.43it/s] 93%|█████████▎| 723/780 [04:41<00:16,  3.43it/s] 93%|█████████▎| 724/780 [04:41<00:16,  3.43it/s] 93%|█████████▎| 725/780 [04:42<00:16,  3.42it/s] 93%|█████████▎| 726/780 [04:42<00:15,  3.42it/s] 93%|█████████▎| 727/780 [04:42<00:15,  3.42it/s] 93%|█████████▎| 728/780 [04:43<00:15,  3.42it/s] 93%|█████████▎| 729/780 [04:43<00:14,  3.43it/s] 94%|█████████▎| 730/780 [04:43<00:14,  3.43it/s] 94%|█████████▎| 731/780 [04:43<00:14,  3.43it/s] 94%|█████████▍| 732/780 [04:44<00:14,  3.43it/s] 94%|█████████▍| 733/780 [04:44<00:13,  3.43it/s] 94%|█████████▍| 734/780 [04:44<00:13,  3.43it/s] 94%|█████████▍| 735/780 [04:45<00:13,  3.43it/s] 94%|█████████▍| 736/780 [04:45<00:12,  3.43it/s] 94%|█████████▍| 737/780 [04:45<00:12,  3.42it/s] 95%|█████████▍| 738/780 [04:46<00:12,  3.42it/s] 95%|█████████▍| 739/780 [04:46<00:11,  3.42it/s] 95%|█████████▍| 740/780 [04:46<00:11,  3.42it/s] 95%|█████████▌| 741/780 [04:46<00:11,  3.42it/s] 95%|█████████▌| 742/780 [04:47<00:11,  3.42it/s] 95%|█████████▌| 743/780 [04:47<00:10,  3.42it/s] 95%|█████████▌| 744/780 [04:47<00:10,  3.42it/s] 96%|█████████▌| 745/780 [04:48<00:10,  3.42it/s] 96%|█████████▌| 746/780 [04:48<00:09,  3.42it/s] 96%|█████████▌| 747/780 [04:48<00:09,  3.41it/s] 96%|█████████▌| 748/780 [04:48<00:09,  3.42it/s] 96%|█████████▌| 749/780 [04:49<00:09,  3.42it/s] 96%|█████████▌| 750/780 [04:49<00:08,  3.42it/s] 96%|█████████▋| 751/780 [04:49<00:08,  3.42it/s] 96%|█████████▋| 752/780 [04:50<00:08,  3.42it/s] 97%|█████████▋| 753/780 [04:50<00:07,  3.42it/s] 97%|█████████▋| 754/780 [04:50<00:07,  3.40it/s] 97%|█████████▋| 755/780 [04:51<00:07,  3.41it/s] 97%|█████████▋| 756/780 [04:51<00:07,  3.41it/s] 97%|█████████▋| 757/780 [04:51<00:06,  3.42it/s] 97%|█████████▋| 758/780 [04:51<00:06,  3.39it/s] 97%|█████████▋| 759/780 [04:52<00:06,  3.40it/s] 97%|█████████▋| 760/780 [04:52<00:05,  3.41it/s] 98%|█████████▊| 761/780 [04:52<00:05,  3.41it/s] 98%|█████████▊| 762/780 [04:53<00:05,  3.42it/s] 98%|█████████▊| 763/780 [04:53<00:04,  3.42it/s] 98%|█████████▊| 764/780 [04:53<00:04,  3.42it/s] 98%|█████████▊| 765/780 [04:53<00:04,  3.42it/s] 98%|█████████▊| 766/780 [04:54<00:04,  3.42it/s] 98%|█████████▊| 767/780 [04:54<00:03,  3.42it/s] 98%|█████████▊| 768/780 [04:54<00:03,  3.42it/s] 99%|█████████▊| 769/780 [04:55<00:03,  3.41it/s] 99%|█████████▊| 770/780 [04:55<00:02,  3.41it/s] 99%|█████████▉| 771/780 [04:55<00:02,  3.42it/s] 99%|█████████▉| 772/780 [04:55<00:02,  3.42it/s] 99%|█████████▉| 773/780 [04:56<00:02,  3.42it/s] 99%|█████████▉| 774/780 [04:56<00:01,  3.42it/s] 99%|█████████▉| 775/780 [04:56<00:01,  3.42it/s] 99%|█████████▉| 776/780 [04:57<00:01,  3.42it/s]100%|█████████▉| 777/780 [04:57<00:00,  3.42it/s]100%|█████████▉| 778/780 [04:57<00:00,  3.42it/s]100%|█████████▉| 779/780 [04:58<00:00,  3.42it/s]100%|██████████| 780/780 [04:58<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 00:56:03,257 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:56:03,257 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:56:03,257 >>   Batch size = 8
{'eval_loss': 1.1819500923156738, 'eval_runtime': 9.496, 'eval_samples_per_second': 366.682, 'eval_steps_per_second': 45.914, 'epoch': 4.0}

  0%|          | 0/436 [00:00<?, ?it/s][A
  1%|▏         | 6/436 [00:00<00:07, 56.21it/s][A
  3%|▎         | 12/436 [00:00<00:08, 50.06it/s][A
  4%|▍         | 18/436 [00:00<00:08, 48.14it/s][A
  5%|▌         | 23/436 [00:00<00:08, 47.50it/s][A
  6%|▋         | 28/436 [00:00<00:08, 47.06it/s][A
  8%|▊         | 33/436 [00:00<00:08, 46.67it/s][A
  9%|▊         | 38/436 [00:00<00:08, 46.37it/s][A
 10%|▉         | 43/436 [00:00<00:08, 46.19it/s][A
 11%|█         | 48/436 [00:01<00:08, 46.14it/s][A
 12%|█▏        | 53/436 [00:01<00:08, 46.05it/s][A
 13%|█▎        | 58/436 [00:01<00:08, 46.08it/s][A
 14%|█▍        | 63/436 [00:01<00:08, 46.13it/s][A
 16%|█▌        | 68/436 [00:01<00:07, 46.11it/s][A
 17%|█▋        | 73/436 [00:01<00:07, 46.17it/s][A
 18%|█▊        | 78/436 [00:01<00:07, 46.21it/s][A
 19%|█▉        | 83/436 [00:01<00:07, 46.18it/s][A
 20%|██        | 88/436 [00:01<00:07, 45.96it/s][A
 21%|██▏       | 93/436 [00:01<00:07, 46.02it/s][A
 22%|██▏       | 98/436 [00:02<00:07, 45.95it/s][A
 24%|██▎       | 103/436 [00:02<00:07, 46.05it/s][A
 25%|██▍       | 108/436 [00:02<00:07, 46.01it/s][A
 26%|██▌       | 113/436 [00:02<00:07, 46.03it/s][A
 27%|██▋       | 118/436 [00:02<00:06, 46.12it/s][A
 28%|██▊       | 123/436 [00:02<00:06, 46.07it/s][A
 29%|██▉       | 128/436 [00:02<00:06, 46.05it/s][A
 31%|███       | 133/436 [00:02<00:06, 45.97it/s][A
 32%|███▏      | 138/436 [00:02<00:06, 46.08it/s][A
 33%|███▎      | 143/436 [00:03<00:06, 45.89it/s][A
 34%|███▍      | 148/436 [00:03<00:06, 45.97it/s][A
 35%|███▌      | 153/436 [00:03<00:06, 46.03it/s][A
 36%|███▌      | 158/436 [00:03<00:06, 46.05it/s][A
 37%|███▋      | 163/436 [00:03<00:05, 46.11it/s][A
 39%|███▊      | 168/436 [00:03<00:05, 46.12it/s][A
 40%|███▉      | 173/436 [00:03<00:05, 46.09it/s][A
 41%|████      | 178/436 [00:03<00:05, 45.99it/s][A
 42%|████▏     | 183/436 [00:03<00:05, 45.98it/s][A
 43%|████▎     | 188/436 [00:04<00:05, 46.04it/s][A
 44%|████▍     | 193/436 [00:04<00:05, 45.94it/s][A
 45%|████▌     | 198/436 [00:04<00:05, 46.07it/s][A
 47%|████▋     | 203/436 [00:04<00:05, 46.02it/s][A
 48%|████▊     | 208/436 [00:04<00:04, 46.03it/s][A
 49%|████▉     | 213/436 [00:04<00:04, 46.08it/s][A
 50%|█████     | 218/436 [00:04<00:04, 46.09it/s][A
 51%|█████     | 223/436 [00:04<00:04, 46.01it/s][A
 52%|█████▏    | 228/436 [00:04<00:04, 46.02it/s][A
 53%|█████▎    | 233/436 [00:05<00:04, 45.98it/s][A
 55%|█████▍    | 238/436 [00:05<00:04, 45.98it/s][A
 56%|█████▌    | 243/436 [00:05<00:04, 46.12it/s][A
 57%|█████▋    | 248/436 [00:05<00:04, 46.12it/s][A
 58%|█████▊    | 253/436 [00:05<00:03, 46.00it/s][A
 59%|█████▉    | 258/436 [00:05<00:03, 46.10it/s][A
 60%|██████    | 263/436 [00:05<00:03, 46.07it/s][A
 61%|██████▏   | 268/436 [00:05<00:03, 46.02it/s][A
 63%|██████▎   | 273/436 [00:05<00:03, 46.03it/s][A
 64%|██████▍   | 278/436 [00:06<00:03, 46.00it/s][A
 65%|██████▍   | 283/436 [00:06<00:03, 45.93it/s][A
 66%|██████▌   | 288/436 [00:06<00:03, 46.07it/s][A
 67%|██████▋   | 293/436 [00:06<00:03, 46.06it/s][A
 68%|██████▊   | 298/436 [00:06<00:03, 45.97it/s][A
 69%|██████▉   | 303/436 [00:06<00:02, 46.04it/s][A
 71%|███████   | 308/436 [00:06<00:02, 45.98it/s][A
 72%|███████▏  | 313/436 [00:06<00:02, 46.05it/s][A
 73%|███████▎  | 318/436 [00:06<00:02, 45.96it/s][A
 74%|███████▍  | 323/436 [00:06<00:02, 46.03it/s][A
 75%|███████▌  | 328/436 [00:07<00:02, 46.03it/s][A
 76%|███████▋  | 333/436 [00:07<00:02, 45.96it/s][A
 78%|███████▊  | 338/436 [00:07<00:02, 45.98it/s][A
 79%|███████▊  | 343/436 [00:07<00:02, 46.06it/s][A
 80%|███████▉  | 348/436 [00:07<00:01, 46.04it/s][A
 81%|████████  | 353/436 [00:07<00:01, 46.10it/s][A
 82%|████████▏ | 358/436 [00:07<00:01, 46.09it/s][A
 83%|████████▎ | 363/436 [00:07<00:01, 46.04it/s][A
 84%|████████▍ | 368/436 [00:07<00:01, 45.35it/s][A
 86%|████████▌ | 373/436 [00:08<00:01, 45.52it/s][A
 87%|████████▋ | 378/436 [00:08<00:01, 45.66it/s][A
 88%|████████▊ | 383/436 [00:08<00:01, 45.85it/s][A
 89%|████████▉ | 388/436 [00:08<00:01, 45.92it/s][A
 90%|█████████ | 393/436 [00:08<00:00, 45.98it/s][A
 91%|█████████▏| 398/436 [00:08<00:00, 46.02it/s][A
 92%|█████████▏| 403/436 [00:08<00:00, 46.08it/s][A
 94%|█████████▎| 408/436 [00:08<00:00, 45.88it/s][A
 95%|█████████▍| 413/436 [00:08<00:00, 45.94it/s][A
 96%|█████████▌| 418/436 [00:09<00:00, 45.91it/s][A
 97%|█████████▋| 423/436 [00:09<00:00, 46.00it/s][A
 98%|█████████▊| 428/436 [00:09<00:00, 45.99it/s][A
 99%|█████████▉| 433/436 [00:09<00:00, 46.03it/s][A
                                                 [A                                                 
100%|██████████| 436/436 [00:09<00:00, 46.03it/s][A100%|██████████| 780/780 [05:07<00:00,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:56:12,742 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 00:56:12,759 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:56:15,003 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:56:15,019 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:56:15,031 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:56:19,846 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:56:19,851 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156 (score: 1.1328699588775635).
                                                 100%|██████████| 780/780 [05:16<00:00,  3.41it/s]100%|██████████| 780/780 [05:16<00:00,  2.46it/s]
[INFO|trainer.py:1894] 2023-08-29 00:56:21,639 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 00:56:21,659 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:56:23,955 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:56:23,970 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:56:23,985 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:56:24,185 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:24,185 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:24,185 >>   train_loss               =     0.3892
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:24,185 >>   train_runtime            = 0:05:16.71
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:24,185 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:24,185 >>   train_samples_per_second =    157.869
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:24,185 >>   train_steps_per_second   =      2.463
{'eval_loss': 1.19024658203125, 'eval_runtime': 9.4676, 'eval_samples_per_second': 367.78, 'eval_steps_per_second': 46.052, 'epoch': 5.0}
{'train_runtime': 316.7182, 'train_samples_per_second': 157.869, 'train_steps_per_second': 2.463, 'train_loss': 0.38918286837064303, 'epoch': 5.0}
08/29/2023 00:56:24 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:56:24,216 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:56:24,216 >>   Num examples = 3482
[INFO|trainer.py:2145] 2023-08-29 00:56:24,216 >>   Batch size = 8
  0%|          | 0/436 [00:00<?, ?it/s]  1%|▏         | 6/436 [00:00<00:07, 58.27it/s]  3%|▎         | 12/436 [00:00<00:08, 50.60it/s]  4%|▍         | 18/436 [00:00<00:08, 48.80it/s]  5%|▌         | 23/436 [00:00<00:08, 48.07it/s]  6%|▋         | 28/436 [00:00<00:08, 47.62it/s]  8%|▊         | 33/436 [00:00<00:08, 47.25it/s]  9%|▊         | 38/436 [00:00<00:08, 47.00it/s] 10%|▉         | 43/436 [00:00<00:08, 46.79it/s] 11%|█         | 48/436 [00:01<00:08, 46.58it/s] 12%|█▏        | 53/436 [00:01<00:08, 46.53it/s] 13%|█▎        | 58/436 [00:01<00:08, 46.44it/s] 14%|█▍        | 63/436 [00:01<00:08, 46.39it/s] 16%|█▌        | 68/436 [00:01<00:07, 46.48it/s] 17%|█▋        | 73/436 [00:01<00:07, 46.54it/s] 18%|█▊        | 78/436 [00:01<00:07, 46.37it/s] 19%|█▉        | 83/436 [00:01<00:07, 46.47it/s] 20%|██        | 88/436 [00:01<00:07, 46.38it/s] 21%|██▏       | 93/436 [00:01<00:07, 46.24it/s] 22%|██▏       | 98/436 [00:02<00:07, 46.21it/s] 24%|██▎       | 103/436 [00:02<00:07, 46.31it/s] 25%|██▍       | 108/436 [00:02<00:07, 46.36it/s] 26%|██▌       | 113/436 [00:02<00:06, 46.38it/s] 27%|██▋       | 118/436 [00:02<00:06, 46.40it/s] 28%|██▊       | 123/436 [00:02<00:06, 46.51it/s] 29%|██▉       | 128/436 [00:02<00:06, 46.40it/s] 31%|███       | 133/436 [00:02<00:06, 46.35it/s] 32%|███▏      | 138/436 [00:02<00:06, 46.35it/s] 33%|███▎      | 143/436 [00:03<00:06, 46.24it/s] 34%|███▍      | 148/436 [00:03<00:06, 46.06it/s] 35%|███▌      | 153/436 [00:03<00:06, 46.27it/s] 36%|███▌      | 158/436 [00:03<00:06, 46.31it/s] 37%|███▋      | 163/436 [00:03<00:05, 46.28it/s] 39%|███▊      | 168/436 [00:03<00:05, 46.36it/s] 40%|███▉      | 173/436 [00:03<00:05, 46.43it/s] 41%|████      | 178/436 [00:03<00:05, 46.32it/s] 42%|████▏     | 183/436 [00:03<00:05, 46.26it/s] 43%|████▎     | 188/436 [00:04<00:05, 46.18it/s] 44%|████▍     | 193/436 [00:04<00:05, 46.27it/s] 45%|████▌     | 198/436 [00:04<00:05, 46.31it/s] 47%|████▋     | 203/436 [00:04<00:05, 46.28it/s] 48%|████▊     | 208/436 [00:04<00:04, 46.39it/s] 49%|████▉     | 213/436 [00:04<00:04, 46.39it/s] 50%|█████     | 218/436 [00:04<00:04, 46.36it/s] 51%|█████     | 223/436 [00:04<00:04, 46.34it/s] 52%|█████▏    | 228/436 [00:04<00:04, 46.24it/s] 53%|█████▎    | 233/436 [00:04<00:04, 46.28it/s] 55%|█████▍    | 238/436 [00:05<00:04, 46.19it/s] 56%|█████▌    | 243/436 [00:05<00:04, 46.29it/s] 57%|█████▋    | 248/436 [00:05<00:04, 46.35it/s] 58%|█████▊    | 253/436 [00:05<00:03, 46.32it/s] 59%|█████▉    | 258/436 [00:05<00:03, 46.35it/s] 60%|██████    | 263/436 [00:05<00:03, 46.28it/s] 61%|██████▏   | 268/436 [00:05<00:03, 46.26it/s] 63%|██████▎   | 273/436 [00:05<00:03, 46.22it/s] 64%|██████▍   | 278/436 [00:05<00:03, 46.14it/s] 65%|██████▍   | 283/436 [00:06<00:03, 46.25it/s] 66%|██████▌   | 288/436 [00:06<00:03, 46.22it/s] 67%|██████▋   | 293/436 [00:06<00:03, 46.20it/s] 68%|██████▊   | 298/436 [00:06<00:02, 46.29it/s] 69%|██████▉   | 303/436 [00:06<00:02, 46.28it/s] 71%|███████   | 308/436 [00:06<00:02, 46.37it/s] 72%|███████▏  | 313/436 [00:06<00:02, 46.26it/s] 73%|███████▎  | 318/436 [00:06<00:02, 46.33it/s] 74%|███████▍  | 323/436 [00:06<00:02, 46.19it/s] 75%|███████▌  | 328/436 [00:07<00:02, 46.17it/s] 76%|███████▋  | 333/436 [00:07<00:02, 46.23it/s] 78%|███████▊  | 338/436 [00:07<00:02, 46.26it/s] 79%|███████▊  | 343/436 [00:07<00:02, 46.30it/s] 80%|███████▉  | 348/436 [00:07<00:01, 46.27it/s] 81%|████████  | 353/436 [00:07<00:01, 46.35it/s] 82%|████████▏ | 358/436 [00:07<00:01, 46.27it/s] 83%|████████▎ | 363/436 [00:07<00:01, 46.26it/s] 84%|████████▍ | 368/436 [00:07<00:01, 46.21it/s] 86%|████████▌ | 373/436 [00:08<00:01, 46.22it/s] 87%|████████▋ | 378/436 [00:08<00:01, 46.31it/s] 88%|████████▊ | 383/436 [00:08<00:01, 46.25it/s] 89%|████████▉ | 388/436 [00:08<00:01, 46.28it/s] 90%|█████████ | 393/436 [00:08<00:00, 46.31it/s] 91%|█████████▏| 398/436 [00:08<00:00, 46.24it/s] 92%|█████████▏| 403/436 [00:08<00:00, 46.26it/s] 94%|█████████▎| 408/436 [00:08<00:00, 46.24it/s] 95%|█████████▍| 413/436 [00:08<00:00, 46.27it/s] 96%|█████████▌| 418/436 [00:08<00:00, 46.18it/s] 97%|█████████▋| 423/436 [00:09<00:00, 46.18it/s] 98%|█████████▊| 428/436 [00:09<00:00, 46.35it/s] 99%|█████████▉| 433/436 [00:09<00:00, 46.37it/s]100%|██████████| 436/436 [00:09<00:00, 46.44it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:56:33,628 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:33,628 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:33,628 >>   eval_loss               =     1.1329
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:33,628 >>   eval_runtime            = 0:00:09.41
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:33,628 >>   eval_samples            =       3482
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:33,628 >>   eval_samples_per_second =    369.956
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:33,628 >>   eval_steps_per_second   =     46.324
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:56:33,628 >>   perplexity              =     3.1046
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:40,356 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:40,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:40,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:40,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:40,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:56:40,949 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:56:40,950 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:56:41,527 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:56:42,574 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:56:42,574 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:45,598 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:45,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:45,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:45,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:56:45,603 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:56:46,227 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:56:46,228 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:56:46,810 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:56:46,967 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:56:46,967 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter5/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/dev.jsonl', 'labels': ['head of government', 'licensed to broadcast to', 'mother', 'performer', 'spouse'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12865
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12965, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:05,  1.47it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:10,  1.45it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.41it/s]Extractor Predicting: 23it [00:15,  1.45it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:17,  1.43it/s]Extractor Predicting: 26it [00:17,  1.44it/s]Extractor Predicting: 27it [00:18,  1.44it/s]Extractor Predicting: 28it [00:19,  1.41it/s]Extractor Predicting: 29it [00:19,  1.43it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:21,  1.44it/s]Extractor Predicting: 33it [00:22,  1.43it/s]Extractor Predicting: 34it [00:23,  1.41it/s]Extractor Predicting: 35it [00:23,  1.44it/s]Extractor Predicting: 36it [00:24,  1.46it/s]Extractor Predicting: 37it [00:25,  1.46it/s]Extractor Predicting: 38it [00:26,  1.44it/s]Extractor Predicting: 39it [00:26,  1.44it/s]Extractor Predicting: 40it [00:27,  1.45it/s]Extractor Predicting: 41it [00:28,  1.45it/s]Extractor Predicting: 42it [00:28,  1.46it/s]Extractor Predicting: 43it [00:29,  1.45it/s]Extractor Predicting: 44it [00:30,  1.43it/s]Extractor Predicting: 45it [00:30,  1.45it/s]Extractor Predicting: 46it [00:31,  1.46it/s]Extractor Predicting: 47it [00:32,  1.43it/s]Extractor Predicting: 48it [00:32,  1.42it/s]Extractor Predicting: 49it [00:33,  1.40it/s]Extractor Predicting: 50it [00:34,  1.40it/s]Extractor Predicting: 51it [00:35,  1.41it/s]Extractor Predicting: 52it [00:35,  1.42it/s]Extractor Predicting: 53it [00:36,  1.42it/s]Extractor Predicting: 54it [00:37,  1.40it/s]Extractor Predicting: 55it [00:37,  1.41it/s]Extractor Predicting: 56it [00:38,  1.40it/s]Extractor Predicting: 57it [00:39,  1.42it/s]Extractor Predicting: 58it [00:40,  1.41it/s]Extractor Predicting: 59it [00:40,  1.40it/s]Extractor Predicting: 60it [00:41,  1.41it/s]Extractor Predicting: 61it [00:42,  1.43it/s]Extractor Predicting: 62it [00:42,  1.44it/s]Extractor Predicting: 63it [00:43,  1.39it/s]Extractor Predicting: 64it [00:44,  1.38it/s]Extractor Predicting: 65it [00:45,  1.43it/s]Extractor Predicting: 66it [00:45,  1.41it/s]Extractor Predicting: 67it [00:46,  1.44it/s]Extractor Predicting: 68it [00:47,  1.45it/s]Extractor Predicting: 69it [00:47,  1.45it/s]Extractor Predicting: 70it [00:48,  1.49it/s]Extractor Predicting: 71it [00:49,  1.48it/s]Extractor Predicting: 72it [00:49,  1.49it/s]Extractor Predicting: 73it [00:50,  1.54it/s]Extractor Predicting: 74it [00:51,  1.53it/s]Extractor Predicting: 75it [00:51,  1.50it/s]Extractor Predicting: 76it [00:52,  1.45it/s]Extractor Predicting: 77it [00:53,  1.46it/s]Extractor Predicting: 78it [00:53,  1.46it/s]Extractor Predicting: 79it [00:54,  1.44it/s]Extractor Predicting: 80it [00:55,  1.45it/s]Extractor Predicting: 81it [00:55,  1.44it/s]Extractor Predicting: 82it [00:56,  1.44it/s]Extractor Predicting: 83it [00:57,  1.46it/s]Extractor Predicting: 84it [00:58,  1.43it/s]Extractor Predicting: 85it [00:58,  1.48it/s]Extractor Predicting: 86it [00:59,  1.44it/s]Extractor Predicting: 87it [01:00,  1.47it/s]Extractor Predicting: 88it [01:00,  1.44it/s]Extractor Predicting: 89it [01:01,  1.43it/s]Extractor Predicting: 90it [01:02,  1.41it/s]Extractor Predicting: 91it [01:02,  1.40it/s]Extractor Predicting: 92it [01:03,  1.41it/s]Extractor Predicting: 93it [01:04,  1.44it/s]Extractor Predicting: 94it [01:05,  1.41it/s]Extractor Predicting: 95it [01:05,  1.45it/s]Extractor Predicting: 96it [01:06,  1.32it/s]Extractor Predicting: 97it [01:07,  1.35it/s]Extractor Predicting: 98it [01:08,  1.36it/s]Extractor Predicting: 99it [01:08,  1.34it/s]Extractor Predicting: 100it [01:09,  1.34it/s]Extractor Predicting: 101it [01:10,  1.38it/s]Extractor Predicting: 102it [01:10,  1.37it/s]Extractor Predicting: 103it [01:11,  1.37it/s]Extractor Predicting: 104it [01:12,  1.41it/s]Extractor Predicting: 105it [01:13,  1.41it/s]Extractor Predicting: 106it [01:13,  1.38it/s]Extractor Predicting: 107it [01:14,  1.39it/s]Extractor Predicting: 108it [01:15,  1.41it/s]Extractor Predicting: 109it [01:15,  1.45it/s]Extractor Predicting: 110it [01:16,  1.45it/s]Extractor Predicting: 111it [01:17,  1.49it/s]Extractor Predicting: 112it [01:17,  1.48it/s]Extractor Predicting: 113it [01:18,  1.46it/s]Extractor Predicting: 114it [01:19,  1.44it/s]Extractor Predicting: 115it [01:19,  1.42it/s]Extractor Predicting: 116it [01:20,  1.42it/s]Extractor Predicting: 117it [01:21,  1.41it/s]Extractor Predicting: 118it [01:22,  1.41it/s]Extractor Predicting: 119it [01:22,  1.41it/s]Extractor Predicting: 120it [01:23,  1.45it/s]Extractor Predicting: 121it [01:24,  1.42it/s]Extractor Predicting: 122it [01:24,  1.44it/s]Extractor Predicting: 123it [01:25,  1.41it/s]Extractor Predicting: 124it [01:26,  1.42it/s]Extractor Predicting: 125it [01:27,  1.41it/s]Extractor Predicting: 126it [01:27,  1.43it/s]Extractor Predicting: 127it [01:28,  1.42it/s]Extractor Predicting: 128it [01:29,  1.41it/s]Extractor Predicting: 129it [01:29,  1.41it/s]Extractor Predicting: 130it [01:30,  1.39it/s]Extractor Predicting: 131it [01:31,  1.41it/s]Extractor Predicting: 132it [01:31,  1.43it/s]Extractor Predicting: 133it [01:32,  1.40it/s]Extractor Predicting: 134it [01:33,  1.36it/s]Extractor Predicting: 135it [01:34,  1.38it/s]Extractor Predicting: 136it [01:34,  1.39it/s]Extractor Predicting: 137it [01:35,  1.40it/s]Extractor Predicting: 138it [01:36,  1.40it/s]Extractor Predicting: 139it [01:37,  1.39it/s]Extractor Predicting: 140it [01:37,  1.37it/s]Extractor Predicting: 141it [01:38,  1.37it/s]Extractor Predicting: 142it [01:39,  1.41it/s]Extractor Predicting: 143it [01:39,  1.44it/s]Extractor Predicting: 143it [01:39,  1.43it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:36,044 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:36,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:36,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:36,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:36,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:58:36,656 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:58:36,658 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:58:37,219 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:58:38,260 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:58:38,260 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:41,129 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:41,134 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:41,134 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:41,134 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:58:41,134 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:58:41,792 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:58:41,793 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:58:42,365 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:58:42,516 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:58:42,516 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.3875432525951557,
  "recall": 0.064330844342332,
  "score": 0.11034482758620691,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26706
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26806, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.59it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:20,  1.45it/s]Extractor Predicting: 33it [00:21,  1.44it/s]Extractor Predicting: 34it [00:22,  1.43it/s]Extractor Predicting: 35it [00:22,  1.42it/s]Extractor Predicting: 36it [00:23,  1.43it/s]Extractor Predicting: 37it [00:24,  1.43it/s]Extractor Predicting: 38it [00:25,  1.42it/s]Extractor Predicting: 39it [00:25,  1.43it/s]Extractor Predicting: 40it [00:26,  1.42it/s]Extractor Predicting: 41it [00:27,  1.43it/s]Extractor Predicting: 42it [00:27,  1.44it/s]Extractor Predicting: 43it [00:28,  1.41it/s]Extractor Predicting: 44it [00:29,  1.43it/s]Extractor Predicting: 45it [00:29,  1.43it/s]Extractor Predicting: 46it [00:30,  1.43it/s]Extractor Predicting: 47it [00:31,  1.43it/s]Extractor Predicting: 48it [00:32,  1.42it/s]Extractor Predicting: 49it [00:32,  1.41it/s]Extractor Predicting: 50it [00:33,  1.42it/s]Extractor Predicting: 51it [00:34,  1.43it/s]Extractor Predicting: 52it [00:34,  1.47it/s]Extractor Predicting: 53it [00:35,  1.43it/s]Extractor Predicting: 54it [00:36,  1.44it/s]Extractor Predicting: 55it [00:36,  1.43it/s]Extractor Predicting: 56it [00:37,  1.49it/s]Extractor Predicting: 57it [00:38,  1.46it/s]Extractor Predicting: 58it [00:38,  1.47it/s]Extractor Predicting: 59it [00:39,  1.46it/s]Extractor Predicting: 60it [00:40,  1.47it/s]Extractor Predicting: 61it [00:41,  1.44it/s]Extractor Predicting: 62it [00:41,  1.46it/s]Extractor Predicting: 63it [00:42,  1.50it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.51it/s]Extractor Predicting: 66it [00:44,  1.45it/s]Extractor Predicting: 67it [00:45,  1.48it/s]Extractor Predicting: 68it [00:45,  1.47it/s]Extractor Predicting: 69it [00:46,  1.49it/s]Extractor Predicting: 70it [00:47,  1.50it/s]Extractor Predicting: 71it [00:47,  1.48it/s]Extractor Predicting: 72it [00:48,  1.47it/s]Extractor Predicting: 73it [00:49,  1.46it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:50,  1.49it/s]Extractor Predicting: 76it [00:51,  1.49it/s]Extractor Predicting: 77it [00:51,  1.51it/s]Extractor Predicting: 78it [00:52,  1.55it/s]Extractor Predicting: 79it [00:53,  1.53it/s]Extractor Predicting: 80it [00:53,  1.50it/s]Extractor Predicting: 81it [00:54,  1.49it/s]Extractor Predicting: 82it [00:55,  1.51it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:57,  1.47it/s]Extractor Predicting: 86it [00:57,  1.47it/s]Extractor Predicting: 87it [00:58,  1.48it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [00:59,  1.53it/s]Extractor Predicting: 90it [01:00,  1.53it/s]Extractor Predicting: 91it [01:01,  1.52it/s]Extractor Predicting: 92it [01:02,  1.31it/s]Extractor Predicting: 93it [01:02,  1.35it/s]Extractor Predicting: 94it [01:03,  1.36it/s]Extractor Predicting: 95it [01:04,  1.38it/s]Extractor Predicting: 96it [01:04,  1.39it/s]Extractor Predicting: 97it [01:05,  1.42it/s]Extractor Predicting: 98it [01:06,  1.41it/s]Extractor Predicting: 99it [01:06,  1.43it/s]Extractor Predicting: 100it [01:07,  1.43it/s]Extractor Predicting: 101it [01:08,  1.42it/s]Extractor Predicting: 102it [01:09,  1.45it/s]Extractor Predicting: 103it [01:09,  1.45it/s]Extractor Predicting: 104it [01:10,  1.45it/s]Extractor Predicting: 105it [01:11,  1.44it/s]Extractor Predicting: 106it [01:11,  1.44it/s]Extractor Predicting: 107it [01:12,  1.43it/s]Extractor Predicting: 108it [01:13,  1.44it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:14,  1.43it/s]Extractor Predicting: 111it [01:15,  1.43it/s]Extractor Predicting: 112it [01:16,  1.42it/s]Extractor Predicting: 113it [01:16,  1.44it/s]Extractor Predicting: 114it [01:17,  1.43it/s]Extractor Predicting: 115it [01:18,  1.42it/s]Extractor Predicting: 116it [01:18,  1.48it/s]Extractor Predicting: 117it [01:19,  1.55it/s]Extractor Predicting: 118it [01:19,  1.59it/s]Extractor Predicting: 119it [01:20,  1.57it/s]Extractor Predicting: 120it [01:21,  1.56it/s]Extractor Predicting: 121it [01:21,  1.54it/s]Extractor Predicting: 122it [01:22,  1.52it/s]Extractor Predicting: 123it [01:23,  1.50it/s]Extractor Predicting: 124it [01:23,  1.54it/s]Extractor Predicting: 125it [01:24,  1.59it/s]Extractor Predicting: 126it [01:25,  1.58it/s]Extractor Predicting: 127it [01:25,  1.59it/s]Extractor Predicting: 128it [01:26,  1.61it/s]Extractor Predicting: 129it [01:26,  1.59it/s]Extractor Predicting: 130it [01:27,  1.57it/s]Extractor Predicting: 131it [01:28,  1.58it/s]Extractor Predicting: 132it [01:28,  1.59it/s]Extractor Predicting: 133it [01:29,  1.58it/s]Extractor Predicting: 134it [01:30,  1.59it/s]Extractor Predicting: 135it [01:30,  1.60it/s]Extractor Predicting: 136it [01:31,  1.64it/s]Extractor Predicting: 137it [01:31,  1.64it/s]Extractor Predicting: 138it [01:32,  1.61it/s]Extractor Predicting: 139it [01:33,  1.57it/s]Extractor Predicting: 140it [01:33,  1.58it/s]Extractor Predicting: 141it [01:34,  1.55it/s]Extractor Predicting: 142it [01:35,  1.55it/s]Extractor Predicting: 143it [01:35,  1.56it/s]Extractor Predicting: 144it [01:36,  1.54it/s]Extractor Predicting: 145it [01:37,  1.54it/s]Extractor Predicting: 146it [01:37,  1.56it/s]Extractor Predicting: 147it [01:38,  1.55it/s]Extractor Predicting: 148it [01:39,  1.53it/s]Extractor Predicting: 149it [01:39,  1.59it/s]Extractor Predicting: 150it [01:40,  1.57it/s]Extractor Predicting: 151it [01:40,  1.55it/s]Extractor Predicting: 152it [01:41,  1.57it/s]Extractor Predicting: 153it [01:42,  1.62it/s]Extractor Predicting: 154it [01:42,  1.62it/s]Extractor Predicting: 155it [01:43,  1.61it/s]Extractor Predicting: 156it [01:44,  1.60it/s]Extractor Predicting: 157it [01:44,  1.59it/s]Extractor Predicting: 158it [01:45,  1.60it/s]Extractor Predicting: 159it [01:45,  1.55it/s]Extractor Predicting: 160it [01:46,  1.56it/s]Extractor Predicting: 161it [01:47,  1.56it/s]Extractor Predicting: 162it [01:47,  1.57it/s]Extractor Predicting: 163it [01:48,  1.56it/s]Extractor Predicting: 164it [01:49,  1.57it/s]Extractor Predicting: 165it [01:49,  1.58it/s]Extractor Predicting: 166it [01:50,  1.59it/s]Extractor Predicting: 167it [01:51,  1.59it/s]Extractor Predicting: 168it [01:51,  1.55it/s]Extractor Predicting: 169it [01:52,  1.56it/s]Extractor Predicting: 170it [01:53,  1.53it/s]Extractor Predicting: 171it [01:53,  1.50it/s]Extractor Predicting: 172it [01:54,  1.56it/s]Extractor Predicting: 173it [01:55,  1.51it/s]Extractor Predicting: 174it [01:55,  1.49it/s]Extractor Predicting: 175it [01:56,  1.47it/s]Extractor Predicting: 176it [01:57,  1.48it/s]Extractor Predicting: 177it [01:57,  1.47it/s]Extractor Predicting: 178it [01:58,  1.45it/s]Extractor Predicting: 179it [01:59,  1.45it/s]Extractor Predicting: 180it [01:59,  1.49it/s]Extractor Predicting: 181it [02:00,  1.48it/s]Extractor Predicting: 182it [02:01,  1.47it/s]Extractor Predicting: 183it [02:01,  1.46it/s]Extractor Predicting: 184it [02:02,  1.45it/s]Extractor Predicting: 185it [02:03,  1.44it/s]Extractor Predicting: 186it [02:03,  1.44it/s]Extractor Predicting: 187it [02:04,  1.43it/s]Extractor Predicting: 188it [02:05,  1.46it/s]Extractor Predicting: 189it [02:05,  1.47it/s]Extractor Predicting: 190it [02:06,  1.48it/s]Extractor Predicting: 191it [02:07,  1.44it/s]Extractor Predicting: 192it [02:08,  1.44it/s]Extractor Predicting: 193it [02:08,  1.42it/s]Extractor Predicting: 194it [02:09,  1.42it/s]Extractor Predicting: 195it [02:10,  1.42it/s]Extractor Predicting: 196it [02:10,  1.43it/s]Extractor Predicting: 197it [02:11,  1.43it/s]Extractor Predicting: 198it [02:12,  1.43it/s]Extractor Predicting: 199it [02:13,  1.43it/s]Extractor Predicting: 200it [02:13,  1.42it/s]Extractor Predicting: 201it [02:14,  1.27it/s]Extractor Predicting: 202it [02:15,  1.31it/s]Extractor Predicting: 203it [02:16,  1.35it/s]Extractor Predicting: 204it [02:16,  1.39it/s]Extractor Predicting: 205it [02:17,  1.46it/s]Extractor Predicting: 206it [02:18,  1.46it/s]Extractor Predicting: 207it [02:18,  1.46it/s]Extractor Predicting: 208it [02:19,  1.46it/s]Extractor Predicting: 209it [02:20,  1.45it/s]Extractor Predicting: 210it [02:20,  1.48it/s]Extractor Predicting: 211it [02:21,  1.51it/s]Extractor Predicting: 212it [02:22,  1.52it/s]Extractor Predicting: 213it [02:22,  1.54it/s]Extractor Predicting: 214it [02:23,  1.52it/s]Extractor Predicting: 215it [02:24,  1.48it/s]Extractor Predicting: 216it [02:24,  1.48it/s]Extractor Predicting: 217it [02:25,  1.49it/s]Extractor Predicting: 218it [02:26,  1.52it/s]Extractor Predicting: 219it [02:26,  1.54it/s]Extractor Predicting: 220it [02:27,  1.51it/s]Extractor Predicting: 221it [02:27,  1.54it/s]Extractor Predicting: 222it [02:28,  1.55it/s]Extractor Predicting: 223it [02:29,  1.52it/s]Extractor Predicting: 224it [02:29,  1.52it/s]Extractor Predicting: 225it [02:30,  1.52it/s]Extractor Predicting: 226it [02:31,  1.47it/s]Extractor Predicting: 227it [02:32,  1.46it/s]Extractor Predicting: 228it [02:32,  1.47it/s]Extractor Predicting: 229it [02:33,  1.46it/s]Extractor Predicting: 230it [02:34,  1.45it/s]Extractor Predicting: 231it [02:34,  1.46it/s]Extractor Predicting: 232it [02:35,  1.49it/s]Extractor Predicting: 233it [02:36,  1.54it/s]Extractor Predicting: 234it [02:36,  1.56it/s]Extractor Predicting: 235it [02:37,  1.56it/s]Extractor Predicting: 236it [02:37,  1.60it/s]Extractor Predicting: 237it [02:38,  1.60it/s]Extractor Predicting: 238it [02:39,  1.61it/s]Extractor Predicting: 239it [02:39,  1.59it/s]Extractor Predicting: 240it [02:40,  1.62it/s]Extractor Predicting: 241it [02:40,  1.61it/s]Extractor Predicting: 242it [02:41,  1.61it/s]Extractor Predicting: 243it [02:42,  1.66it/s]Extractor Predicting: 244it [02:42,  1.63it/s]Extractor Predicting: 245it [02:43,  1.69it/s]Extractor Predicting: 246it [02:43,  1.73it/s]Extractor Predicting: 247it [02:44,  1.67it/s]Extractor Predicting: 248it [02:45,  1.62it/s]Extractor Predicting: 249it [02:45,  1.62it/s]Extractor Predicting: 250it [02:46,  1.63it/s]Extractor Predicting: 251it [02:47,  1.65it/s]Extractor Predicting: 252it [02:47,  1.66it/s]Extractor Predicting: 253it [02:48,  1.65it/s]Extractor Predicting: 254it [02:48,  1.61it/s]Extractor Predicting: 255it [02:49,  1.64it/s]Extractor Predicting: 256it [02:50,  1.63it/s]Extractor Predicting: 257it [02:50,  1.66it/s]Extractor Predicting: 258it [02:51,  1.66it/s]Extractor Predicting: 259it [02:51,  1.67it/s]Extractor Predicting: 260it [02:52,  1.65it/s]Extractor Predicting: 261it [02:53,  1.56it/s]Extractor Predicting: 262it [02:53,  1.56it/s]Extractor Predicting: 263it [02:54,  1.51it/s]Extractor Predicting: 264it [02:55,  1.47it/s]Extractor Predicting: 265it [02:55,  1.46it/s]Extractor Predicting: 266it [02:56,  1.47it/s]Extractor Predicting: 267it [02:57,  1.49it/s]Extractor Predicting: 268it [02:57,  1.50it/s]Extractor Predicting: 269it [02:58,  1.46it/s]Extractor Predicting: 270it [02:59,  1.44it/s]Extractor Predicting: 271it [03:00,  1.43it/s]Extractor Predicting: 272it [03:00,  1.45it/s]Extractor Predicting: 273it [03:01,  1.45it/s]Extractor Predicting: 274it [03:02,  1.44it/s]Extractor Predicting: 275it [03:02,  1.44it/s]Extractor Predicting: 276it [03:03,  1.45it/s]Extractor Predicting: 277it [03:04,  1.43it/s]Extractor Predicting: 278it [03:04,  1.43it/s]Extractor Predicting: 279it [03:05,  1.46it/s]Extractor Predicting: 280it [03:06,  1.44it/s]Extractor Predicting: 281it [03:07,  1.44it/s]Extractor Predicting: 282it [03:07,  1.44it/s]Extractor Predicting: 283it [03:08,  1.45it/s]Extractor Predicting: 284it [03:09,  1.43it/s]Extractor Predicting: 285it [03:09,  1.45it/s]Extractor Predicting: 286it [03:10,  1.44it/s]Extractor Predicting: 287it [03:11,  1.43it/s]Extractor Predicting: 288it [03:11,  1.48it/s]Extractor Predicting: 289it [03:12,  1.48it/s]Extractor Predicting: 290it [03:13,  1.50it/s]Extractor Predicting: 291it [03:13,  1.51it/s]Extractor Predicting: 292it [03:14,  1.50it/s]Extractor Predicting: 293it [03:15,  1.50it/s]Extractor Predicting: 294it [03:15,  1.52it/s]Extractor Predicting: 295it [03:16,  1.52it/s]Extractor Predicting: 296it [03:17,  1.50it/s]Extractor Predicting: 297it [03:17,  1.52it/s]Extractor Predicting: 298it [03:18,  1.55it/s]Extractor Predicting: 299it [03:19,  1.52it/s]Extractor Predicting: 300it [03:19,  1.53it/s]Extractor Predicting: 301it [03:20,  1.51it/s]Extractor Predicting: 302it [03:21,  1.49it/s]Extractor Predicting: 303it [03:21,  1.49it/s]Extractor Predicting: 304it [03:22,  1.49it/s]Extractor Predicting: 305it [03:23,  1.48it/s]Extractor Predicting: 306it [03:23,  1.50it/s]Extractor Predicting: 307it [03:24,  1.50it/s]Extractor Predicting: 308it [03:25,  1.49it/s]Extractor Predicting: 309it [03:25,  1.46it/s]Extractor Predicting: 310it [03:26,  1.47it/s]Extractor Predicting: 311it [03:27,  1.47it/s]Extractor Predicting: 312it [03:27,  1.48it/s]Extractor Predicting: 313it [03:28,  1.48it/s]Extractor Predicting: 314it [03:29,  1.48it/s]Extractor Predicting: 315it [03:29,  1.50it/s]Extractor Predicting: 316it [03:30,  1.49it/s]Extractor Predicting: 317it [03:31,  1.51it/s]Extractor Predicting: 318it [03:31,  1.53it/s]Extractor Predicting: 319it [03:32,  1.53it/s]Extractor Predicting: 320it [03:33,  1.51it/s]Extractor Predicting: 321it [03:33,  1.50it/s]Extractor Predicting: 322it [03:34,  1.49it/s]Extractor Predicting: 323it [03:35,  1.51it/s]Extractor Predicting: 324it [03:35,  1.52it/s]Extractor Predicting: 325it [03:36,  1.50it/s]Extractor Predicting: 326it [03:37,  1.52it/s]Extractor Predicting: 327it [03:37,  1.52it/s]Extractor Predicting: 328it [03:38,  1.51it/s]Extractor Predicting: 329it [03:39,  1.31it/s]Extractor Predicting: 330it [03:40,  1.35it/s]Extractor Predicting: 331it [03:40,  1.38it/s]Extractor Predicting: 332it [03:41,  1.43it/s]Extractor Predicting: 333it [03:42,  1.42it/s]Extractor Predicting: 334it [03:42,  1.47it/s]Extractor Predicting: 335it [03:43,  1.46it/s]Extractor Predicting: 336it [03:44,  1.48it/s]Extractor Predicting: 337it [03:44,  1.47it/s]Extractor Predicting: 338it [03:45,  1.48it/s]Extractor Predicting: 339it [03:46,  1.49it/s]Extractor Predicting: 340it [03:46,  1.47it/s]Extractor Predicting: 341it [03:47,  1.49it/s]Extractor Predicting: 342it [03:48,  1.50it/s]Extractor Predicting: 343it [03:48,  1.50it/s]Extractor Predicting: 344it [03:49,  1.53it/s]Extractor Predicting: 345it [03:50,  1.50it/s]Extractor Predicting: 346it [03:50,  1.49it/s]Extractor Predicting: 347it [03:51,  1.50it/s]Extractor Predicting: 348it [03:52,  1.46it/s]Extractor Predicting: 349it [03:52,  1.48it/s]Extractor Predicting: 350it [03:53,  1.46it/s]Extractor Predicting: 351it [03:54,  1.47it/s]Extractor Predicting: 352it [03:54,  1.46it/s]Extractor Predicting: 353it [03:55,  1.46it/s]Extractor Predicting: 354it [03:56,  1.47it/s]Extractor Predicting: 355it [03:56,  1.45it/s]Extractor Predicting: 356it [03:57,  1.47it/s]Extractor Predicting: 357it [03:58,  1.45it/s]Extractor Predicting: 358it [03:59,  1.46it/s]Extractor Predicting: 359it [03:59,  1.47it/s]Extractor Predicting: 360it [04:00,  1.47it/s]Extractor Predicting: 361it [04:01,  1.47it/s]Extractor Predicting: 362it [04:01,  1.49it/s]Extractor Predicting: 363it [04:02,  1.49it/s]Extractor Predicting: 364it [04:03,  1.51it/s]Extractor Predicting: 365it [04:03,  1.49it/s]Extractor Predicting: 366it [04:04,  1.50it/s]Extractor Predicting: 367it [04:05,  1.49it/s]Extractor Predicting: 368it [04:05,  1.50it/s]Extractor Predicting: 369it [04:06,  1.46it/s]Extractor Predicting: 370it [04:07,  1.46it/s]Extractor Predicting: 371it [04:07,  1.46it/s]Extractor Predicting: 372it [04:08,  1.48it/s]Extractor Predicting: 373it [04:09,  1.48it/s]Extractor Predicting: 374it [04:09,  1.51it/s]Extractor Predicting: 375it [04:10,  1.50it/s]Extractor Predicting: 376it [04:11,  1.48it/s]Extractor Predicting: 377it [04:11,  1.53it/s]Extractor Predicting: 378it [04:12,  1.53it/s]Extractor Predicting: 379it [04:13,  1.56it/s]Extractor Predicting: 380it [04:13,  1.53it/s]Extractor Predicting: 381it [04:14,  1.52it/s]Extractor Predicting: 382it [04:15,  1.51it/s]Extractor Predicting: 383it [04:15,  1.49it/s]Extractor Predicting: 384it [04:16,  1.48it/s]Extractor Predicting: 385it [04:17,  1.49it/s]Extractor Predicting: 386it [04:17,  1.48it/s]Extractor Predicting: 387it [04:18,  1.47it/s]Extractor Predicting: 388it [04:19,  1.42it/s]Extractor Predicting: 389it [04:19,  1.44it/s]Extractor Predicting: 390it [04:20,  1.45it/s]Extractor Predicting: 391it [04:21,  1.47it/s]Extractor Predicting: 392it [04:21,  1.49it/s]Extractor Predicting: 393it [04:22,  1.48it/s]Extractor Predicting: 394it [04:23,  1.49it/s]Extractor Predicting: 395it [04:23,  1.49it/s]Extractor Predicting: 396it [04:24,  1.47it/s]Extractor Predicting: 397it [04:25,  1.48it/s]Extractor Predicting: 398it [04:25,  1.47it/s]Extractor Predicting: 399it [04:26,  1.51it/s]Extractor Predicting: 400it [04:27,  1.48it/s]Extractor Predicting: 401it [04:27,  1.49it/s]Extractor Predicting: 402it [04:28,  1.49it/s]Extractor Predicting: 403it [04:29,  1.47it/s]Extractor Predicting: 404it [04:29,  1.49it/s]Extractor Predicting: 405it [04:30,  1.50it/s]Extractor Predicting: 406it [04:31,  1.52it/s]Extractor Predicting: 407it [04:31,  1.51it/s]Extractor Predicting: 408it [04:32,  1.49it/s]Extractor Predicting: 409it [04:33,  1.51it/s]Extractor Predicting: 410it [04:33,  1.51it/s]Extractor Predicting: 411it [04:34,  1.50it/s]Extractor Predicting: 412it [04:35,  1.52it/s]Extractor Predicting: 413it [04:35,  1.52it/s]Extractor Predicting: 414it [04:36,  1.50it/s]Extractor Predicting: 415it [04:37,  1.48it/s]Extractor Predicting: 416it [04:37,  1.51it/s]Extractor Predicting: 417it [04:38,  1.51it/s]Extractor Predicting: 418it [04:39,  1.53it/s]Extractor Predicting: 419it [04:39,  1.53it/s]Extractor Predicting: 420it [04:40,  1.55it/s]Extractor Predicting: 421it [04:41,  1.52it/s]Extractor Predicting: 422it [04:41,  1.52it/s]Extractor Predicting: 423it [04:42,  1.53it/s]Extractor Predicting: 424it [04:43,  1.52it/s]Extractor Predicting: 425it [04:43,  1.51it/s]Extractor Predicting: 426it [04:44,  1.54it/s]Extractor Predicting: 427it [04:45,  1.53it/s]Extractor Predicting: 428it [04:45,  1.48it/s]Extractor Predicting: 429it [04:46,  1.69it/s]Extractor Predicting: 429it [04:46,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:38,427 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:38,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:38,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:38,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:38,455 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:03:39,331 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:03:39,332 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:03:39,954 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:03:41,086 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:03:41,087 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:43,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:43,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:43,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:43,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:03:43,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:03:44,650 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:03:44,651 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:03:44,914 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:03:45,070 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:03:45,071 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3355263157894737,
  "recall": 0.11904298774557479,
  "score": 0.17573582196697776,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1177
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1277, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.42it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 5it [00:03,  1.50it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5185185185185185,
  "recall": 0.06422018348623854,
  "score": 0.11428571428571431,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/', 'labels': ['after a work by', 'competition class', 'country of citizenship', 'field of work', 'has part', 'headquarters location', 'location of formation', 'mountain range', 'mouth of the watercourse', 'occupant', 'occupation', 'place served by transport hub', 'record label', 'winner', 'work location'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_True_limit5000.json'
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_15_seed_2/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_15_seed_2', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl'}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl', 'labels': ['member of political party', 'military branch', 'occupation', 'part of the series', 'place of death'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17184
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17284, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Predicting: 1it [00:12, 12.97s/it]Extractor Predicting: 2it [00:14,  6.38s/it]Extractor Predicting: 3it [00:15,  4.01s/it]Extractor Predicting: 4it [00:16,  2.65s/it]Extractor Predicting: 5it [00:17,  1.93s/it]Extractor Predicting: 6it [00:18,  1.67s/it]Extractor Predicting: 7it [00:18,  1.33s/it]Extractor Predicting: 8it [00:21,  1.64s/it]Extractor Predicting: 9it [00:21,  1.33s/it]Extractor Predicting: 10it [00:22,  1.10s/it]Extractor Predicting: 11it [00:23,  1.06it/s]Extractor Predicting: 12it [00:23,  1.19it/s]Extractor Predicting: 13it [00:24,  1.29it/s]Extractor Predicting: 14it [00:24,  1.41it/s]Extractor Predicting: 15it [00:25,  1.45it/s]Extractor Predicting: 16it [00:26,  1.50it/s]Extractor Predicting: 17it [00:26,  1.53it/s]Extractor Predicting: 18it [00:27,  1.53it/s]Extractor Predicting: 19it [00:28,  1.54it/s]Extractor Predicting: 20it [00:28,  1.54it/s]Extractor Predicting: 21it [00:29,  1.57it/s]Extractor Predicting: 22it [00:29,  1.61it/s]Extractor Predicting: 23it [00:30,  1.61it/s]Extractor Predicting: 24it [00:31,  1.61it/s]Extractor Predicting: 25it [00:31,  1.60it/s]Extractor Predicting: 26it [00:32,  1.60it/s]Extractor Predicting: 27it [00:33,  1.59it/s]Extractor Predicting: 28it [00:33,  1.49it/s]Extractor Predicting: 29it [00:34,  1.52it/s]Extractor Predicting: 30it [00:34,  1.57it/s]Extractor Predicting: 31it [00:35,  1.60it/s]Extractor Predicting: 32it [00:36,  1.61it/s]Extractor Predicting: 33it [00:36,  1.61it/s]Extractor Predicting: 34it [00:37,  1.62it/s]Extractor Predicting: 35it [00:38,  1.63it/s]Extractor Predicting: 36it [00:38,  1.63it/s]Extractor Predicting: 37it [00:39,  1.63it/s]Extractor Predicting: 38it [00:39,  1.61it/s]Extractor Predicting: 39it [00:40,  1.59it/s]Extractor Predicting: 40it [00:41,  1.59it/s]Extractor Predicting: 41it [00:41,  1.59it/s]Extractor Predicting: 42it [00:42,  1.61it/s]Extractor Predicting: 43it [00:43,  1.64it/s]Extractor Predicting: 44it [00:43,  1.65it/s]Extractor Predicting: 45it [00:44,  1.61it/s]Extractor Predicting: 46it [00:44,  1.67it/s]Extractor Predicting: 47it [00:45,  1.71it/s]Extractor Predicting: 48it [00:45,  1.71it/s]Extractor Predicting: 49it [00:46,  1.76it/s]Extractor Predicting: 50it [00:47,  1.73it/s]Extractor Predicting: 51it [00:47,  1.71it/s]Extractor Predicting: 52it [00:48,  1.72it/s]Extractor Predicting: 53it [00:48,  1.71it/s]Extractor Predicting: 54it [00:49,  1.66it/s]Extractor Predicting: 55it [00:50,  1.67it/s]Extractor Predicting: 56it [00:50,  1.65it/s]Extractor Predicting: 57it [00:51,  1.68it/s]Extractor Predicting: 58it [00:51,  1.62it/s]Extractor Predicting: 59it [00:52,  1.62it/s]Extractor Predicting: 60it [00:53,  1.66it/s]Extractor Predicting: 61it [00:53,  1.68it/s]Extractor Predicting: 62it [00:54,  1.67it/s]Extractor Predicting: 63it [00:54,  1.69it/s]Extractor Predicting: 64it [00:55,  1.70it/s]Extractor Predicting: 65it [00:56,  1.72it/s]Extractor Predicting: 66it [00:56,  1.76it/s]Extractor Predicting: 67it [00:57,  1.75it/s]Extractor Predicting: 68it [00:57,  1.72it/s]Extractor Predicting: 69it [00:58,  1.71it/s]Extractor Predicting: 70it [00:58,  1.71it/s]Extractor Predicting: 71it [00:59,  1.71it/s]Extractor Predicting: 72it [01:00,  1.70it/s]Extractor Predicting: 73it [01:00,  1.67it/s]Extractor Predicting: 74it [01:01,  1.66it/s]Extractor Predicting: 75it [01:01,  1.66it/s]Extractor Predicting: 76it [01:02,  1.68it/s]Extractor Predicting: 77it [01:03,  1.68it/s]Extractor Predicting: 78it [01:03,  1.63it/s]Extractor Predicting: 79it [01:04,  1.62it/s]Extractor Predicting: 80it [01:05,  1.62it/s]Extractor Predicting: 81it [01:05,  1.67it/s]Extractor Predicting: 82it [01:06,  1.70it/s]Extractor Predicting: 83it [01:06,  1.75it/s]Extractor Predicting: 84it [01:07,  1.68it/s]Extractor Predicting: 85it [01:07,  1.63it/s]Extractor Predicting: 86it [01:08,  1.60it/s]Extractor Predicting: 87it [01:09,  1.56it/s]Extractor Predicting: 88it [01:09,  1.53it/s]Extractor Predicting: 89it [01:10,  1.54it/s]Extractor Predicting: 90it [01:11,  1.54it/s]Extractor Predicting: 91it [01:11,  1.53it/s]Extractor Predicting: 92it [01:12,  1.54it/s]Extractor Predicting: 93it [01:13,  1.49it/s]Extractor Predicting: 94it [01:14,  1.46it/s]Extractor Predicting: 95it [01:14,  1.53it/s]Extractor Predicting: 96it [01:15,  1.53it/s]Extractor Predicting: 97it [01:15,  1.56it/s]Extractor Predicting: 98it [01:16,  1.52it/s]Extractor Predicting: 99it [01:17,  1.51it/s]Extractor Predicting: 100it [01:17,  1.54it/s]Extractor Predicting: 101it [01:18,  1.56it/s]Extractor Predicting: 102it [01:19,  1.58it/s]Extractor Predicting: 103it [01:19,  1.61it/s]Extractor Predicting: 104it [01:20,  1.56it/s]Extractor Predicting: 105it [01:20,  1.60it/s]Extractor Predicting: 106it [01:21,  1.63it/s]Extractor Predicting: 107it [01:22,  1.60it/s]Extractor Predicting: 108it [01:22,  1.64it/s]Extractor Predicting: 109it [01:23,  1.60it/s]Extractor Predicting: 110it [01:24,  1.62it/s]Extractor Predicting: 111it [01:24,  1.60it/s]Extractor Predicting: 112it [01:25,  1.63it/s]Extractor Predicting: 113it [01:25,  1.59it/s]Extractor Predicting: 114it [01:26,  1.57it/s]Extractor Predicting: 115it [01:27,  1.60it/s]Extractor Predicting: 116it [01:27,  1.59it/s]Extractor Predicting: 117it [01:28,  1.59it/s]Extractor Predicting: 118it [01:29,  1.52it/s]Extractor Predicting: 119it [01:29,  1.54it/s]Extractor Predicting: 120it [01:30,  1.58it/s]Extractor Predicting: 121it [01:31,  1.52it/s]Extractor Predicting: 122it [01:31,  1.56it/s]Extractor Predicting: 123it [01:32,  1.58it/s]Extractor Predicting: 124it [01:32,  1.61it/s]Extractor Predicting: 125it [01:33,  1.60it/s]Extractor Predicting: 126it [01:34,  1.57it/s]Extractor Predicting: 127it [01:34,  1.58it/s]Extractor Predicting: 128it [01:35,  1.58it/s]Extractor Predicting: 129it [01:36,  1.61it/s]Extractor Predicting: 130it [01:36,  1.62it/s]Extractor Predicting: 131it [01:37,  1.46it/s]Extractor Predicting: 132it [01:38,  1.49it/s]Extractor Predicting: 133it [01:38,  1.50it/s]Extractor Predicting: 134it [01:39,  1.51it/s]Extractor Predicting: 135it [01:40,  1.49it/s]Extractor Predicting: 136it [01:40,  1.48it/s]Extractor Predicting: 137it [01:41,  1.46it/s]Extractor Predicting: 138it [01:42,  1.47it/s]Extractor Predicting: 139it [01:42,  1.47it/s]Extractor Predicting: 140it [01:43,  1.49it/s]Extractor Predicting: 141it [01:44,  1.49it/s]Extractor Predicting: 142it [01:44,  1.50it/s]Extractor Predicting: 143it [01:45,  1.50it/s]Extractor Predicting: 144it [01:46,  1.49it/s]Extractor Predicting: 145it [01:46,  1.49it/s]Extractor Predicting: 146it [01:47,  1.50it/s]Extractor Predicting: 147it [01:48,  1.46it/s]Extractor Predicting: 148it [01:48,  1.47it/s]Extractor Predicting: 149it [01:49,  1.48it/s]Extractor Predicting: 150it [01:50,  1.50it/s]Extractor Predicting: 151it [01:50,  1.52it/s]Extractor Predicting: 152it [01:51,  1.52it/s]Extractor Predicting: 153it [01:52,  1.51it/s]Extractor Predicting: 154it [01:52,  1.51it/s]Extractor Predicting: 155it [01:53,  1.47it/s]Extractor Predicting: 156it [01:54,  1.50it/s]Extractor Predicting: 157it [01:54,  1.49it/s]Extractor Predicting: 158it [01:55,  1.48it/s]Extractor Predicting: 159it [01:56,  1.48it/s]Extractor Predicting: 160it [01:56,  1.48it/s]Extractor Predicting: 161it [01:57,  1.49it/s]Extractor Predicting: 162it [01:58,  1.47it/s]Extractor Predicting: 163it [01:58,  1.52it/s]Extractor Predicting: 164it [01:59,  1.52it/s]Extractor Predicting: 165it [02:00,  1.55it/s]Extractor Predicting: 166it [02:00,  1.54it/s]Extractor Predicting: 167it [02:01,  1.54it/s]Extractor Predicting: 168it [02:02,  1.45it/s]Extractor Predicting: 169it [02:03,  1.45it/s]Extractor Predicting: 170it [02:03,  1.47it/s]Extractor Predicting: 171it [02:04,  1.47it/s]Extractor Predicting: 172it [02:05,  1.47it/s]Extractor Predicting: 173it [02:05,  1.45it/s]Extractor Predicting: 174it [02:06,  1.47it/s]Extractor Predicting: 175it [02:07,  1.42it/s]Extractor Predicting: 176it [02:07,  1.39it/s]Extractor Predicting: 177it [02:08,  1.46it/s]Extractor Predicting: 178it [02:09,  1.47it/s]Extractor Predicting: 179it [02:09,  1.49it/s]Extractor Predicting: 180it [02:10,  1.50it/s]Extractor Predicting: 181it [02:11,  1.53it/s]Extractor Predicting: 182it [02:11,  1.52it/s]Extractor Predicting: 183it [02:12,  1.49it/s]Extractor Predicting: 184it [02:13,  1.49it/s]Extractor Predicting: 185it [02:13,  1.52it/s]Extractor Predicting: 186it [02:14,  1.53it/s]Extractor Predicting: 187it [02:15,  1.58it/s]Extractor Predicting: 188it [02:15,  1.55it/s]Extractor Predicting: 189it [02:16,  1.56it/s]Extractor Predicting: 190it [02:16,  1.55it/s]Extractor Predicting: 191it [02:17,  1.57it/s]Extractor Predicting: 192it [02:18,  1.57it/s]Extractor Predicting: 193it [02:18,  1.56it/s]Extractor Predicting: 194it [02:19,  1.58it/s]Extractor Predicting: 195it [02:20,  1.58it/s]Extractor Predicting: 196it [02:20,  1.59it/s]Extractor Predicting: 197it [02:21,  1.58it/s]Extractor Predicting: 198it [02:22,  1.59it/s]Extractor Predicting: 199it [02:22,  1.57it/s]Extractor Predicting: 200it [02:23,  1.59it/s]Extractor Predicting: 201it [02:23,  1.61it/s]Extractor Predicting: 202it [02:24,  1.59it/s]Extractor Predicting: 203it [02:25,  1.58it/s]Extractor Predicting: 204it [02:25,  1.58it/s]Extractor Predicting: 205it [02:26,  1.58it/s]Extractor Predicting: 206it [02:27,  1.58it/s]Extractor Predicting: 207it [02:27,  1.55it/s]Extractor Predicting: 208it [02:28,  1.55it/s]Extractor Predicting: 209it [02:28,  1.58it/s]Extractor Predicting: 210it [02:29,  1.58it/s]Extractor Predicting: 211it [02:30,  1.52it/s]Extractor Predicting: 212it [02:30,  1.55it/s]Extractor Predicting: 213it [02:31,  1.54it/s]Extractor Predicting: 214it [02:32,  1.57it/s]Extractor Predicting: 215it [02:32,  1.57it/s]Extractor Predicting: 216it [02:33,  1.58it/s]Extractor Predicting: 217it [02:34,  1.58it/s]Extractor Predicting: 218it [02:35,  1.39it/s]Extractor Predicting: 219it [02:35,  1.48it/s]Extractor Predicting: 220it [02:36,  1.49it/s]Extractor Predicting: 221it [02:36,  1.48it/s]Extractor Predicting: 222it [02:37,  1.52it/s]Extractor Predicting: 223it [02:38,  1.55it/s]Extractor Predicting: 224it [02:38,  1.53it/s]Extractor Predicting: 225it [02:39,  1.52it/s]Extractor Predicting: 226it [02:40,  1.53it/s]Extractor Predicting: 227it [02:40,  1.56it/s]Extractor Predicting: 228it [02:41,  1.55it/s]Extractor Predicting: 229it [02:42,  1.50it/s]Extractor Predicting: 230it [02:42,  1.53it/s]Extractor Predicting: 231it [02:43,  1.48it/s]Extractor Predicting: 232it [02:44,  1.49it/s]Extractor Predicting: 233it [02:44,  1.41it/s]Extractor Predicting: 234it [02:45,  1.40it/s]Extractor Predicting: 235it [02:46,  1.41it/s]Extractor Predicting: 236it [02:47,  1.44it/s]Extractor Predicting: 237it [02:47,  1.45it/s]Extractor Predicting: 238it [02:48,  1.42it/s]Extractor Predicting: 239it [02:49,  1.48it/s]Extractor Predicting: 240it [02:49,  1.49it/s]Extractor Predicting: 241it [02:50,  1.47it/s]Extractor Predicting: 242it [02:51,  1.48it/s]Extractor Predicting: 243it [02:51,  1.44it/s]Extractor Predicting: 244it [02:52,  1.44it/s]Extractor Predicting: 245it [02:53,  1.50it/s]Extractor Predicting: 246it [02:53,  1.54it/s]Extractor Predicting: 247it [02:54,  1.52it/s]Extractor Predicting: 247it [02:54,  1.42it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6466480446927374,
  "recall": 0.07137351626329583,
  "score": 0.12855754546716647,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'manufacturer', 'mother', 'occupant', 'organization directed by the office or position', 'owned by', 'part of', 'screenwriter', 'sport', 'sports discipline competed in', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26970
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27070, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:03,  1.46it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.57it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:13,  1.59it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:23,  1.59it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.58it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.58it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:29,  1.57it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:31,  1.56it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.56it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:34,  1.53it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.53it/s]Extractor Predicting: 56it [00:36,  1.54it/s]Extractor Predicting: 57it [00:36,  1.56it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:38,  1.57it/s]Extractor Predicting: 60it [00:38,  1.60it/s]Extractor Predicting: 61it [00:39,  1.58it/s]Extractor Predicting: 62it [00:40,  1.56it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:42,  1.58it/s]Extractor Predicting: 67it [00:43,  1.58it/s]Extractor Predicting: 68it [00:43,  1.57it/s]Extractor Predicting: 69it [00:44,  1.53it/s]Extractor Predicting: 70it [00:45,  1.54it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:47,  1.56it/s]Extractor Predicting: 74it [00:47,  1.62it/s]Extractor Predicting: 75it [00:48,  1.61it/s]Extractor Predicting: 76it [00:48,  1.59it/s]Extractor Predicting: 77it [00:49,  1.57it/s]Extractor Predicting: 78it [00:50,  1.59it/s]Extractor Predicting: 79it [00:50,  1.62it/s]Extractor Predicting: 80it [00:51,  1.61it/s]Extractor Predicting: 81it [00:52,  1.62it/s]Extractor Predicting: 82it [00:52,  1.65it/s]Extractor Predicting: 83it [00:53,  1.65it/s]Extractor Predicting: 84it [00:53,  1.63it/s]Extractor Predicting: 85it [00:54,  1.67it/s]Extractor Predicting: 86it [00:54,  1.69it/s]Extractor Predicting: 87it [00:55,  1.65it/s]Extractor Predicting: 88it [00:56,  1.67it/s]Extractor Predicting: 89it [00:56,  1.64it/s]Extractor Predicting: 90it [00:57,  1.65it/s]Extractor Predicting: 91it [00:58,  1.62it/s]Extractor Predicting: 92it [00:58,  1.64it/s]Extractor Predicting: 93it [00:59,  1.62it/s]Extractor Predicting: 94it [00:59,  1.63it/s]Extractor Predicting: 95it [01:00,  1.65it/s]Extractor Predicting: 96it [01:01,  1.64it/s]Extractor Predicting: 97it [01:01,  1.62it/s]Extractor Predicting: 98it [01:02,  1.59it/s]Extractor Predicting: 99it [01:03,  1.61it/s]Extractor Predicting: 100it [01:03,  1.66it/s]Extractor Predicting: 101it [01:04,  1.64it/s]Extractor Predicting: 102it [01:04,  1.60it/s]Extractor Predicting: 103it [01:05,  1.58it/s]Extractor Predicting: 104it [01:06,  1.62it/s]Extractor Predicting: 105it [01:06,  1.61it/s]Extractor Predicting: 106it [01:07,  1.58it/s]Extractor Predicting: 107it [01:07,  1.60it/s]Extractor Predicting: 108it [01:08,  1.61it/s]Extractor Predicting: 109it [01:09,  1.64it/s]Extractor Predicting: 110it [01:09,  1.63it/s]Extractor Predicting: 111it [01:10,  1.63it/s]Extractor Predicting: 112it [01:11,  1.59it/s]Extractor Predicting: 113it [01:11,  1.61it/s]Extractor Predicting: 114it [01:12,  1.56it/s]Extractor Predicting: 115it [01:13,  1.54it/s]Extractor Predicting: 116it [01:13,  1.56it/s]Extractor Predicting: 117it [01:14,  1.53it/s]Extractor Predicting: 118it [01:14,  1.56it/s]Extractor Predicting: 119it [01:15,  1.52it/s]Extractor Predicting: 120it [01:16,  1.55it/s]Extractor Predicting: 121it [01:16,  1.56it/s]Extractor Predicting: 122it [01:17,  1.54it/s]Extractor Predicting: 123it [01:18,  1.52it/s]Extractor Predicting: 124it [01:18,  1.50it/s]Extractor Predicting: 125it [01:19,  1.50it/s]Extractor Predicting: 126it [01:20,  1.50it/s]Extractor Predicting: 127it [01:20,  1.51it/s]Extractor Predicting: 128it [01:21,  1.48it/s]Extractor Predicting: 129it [01:22,  1.48it/s]Extractor Predicting: 130it [01:22,  1.52it/s]Extractor Predicting: 131it [01:23,  1.54it/s]Extractor Predicting: 132it [01:24,  1.55it/s]Extractor Predicting: 133it [01:24,  1.56it/s]Extractor Predicting: 134it [01:25,  1.60it/s]Extractor Predicting: 135it [01:26,  1.61it/s]Extractor Predicting: 136it [01:26,  1.59it/s]Extractor Predicting: 137it [01:27,  1.64it/s]Extractor Predicting: 138it [01:27,  1.63it/s]Extractor Predicting: 139it [01:28,  1.44it/s]Extractor Predicting: 140it [01:29,  1.50it/s]Extractor Predicting: 141it [01:29,  1.51it/s]Extractor Predicting: 142it [01:30,  1.54it/s]Extractor Predicting: 143it [01:31,  1.58it/s]Extractor Predicting: 144it [01:31,  1.59it/s]Extractor Predicting: 145it [01:32,  1.62it/s]Extractor Predicting: 146it [01:33,  1.60it/s]Extractor Predicting: 147it [01:33,  1.59it/s]Extractor Predicting: 148it [01:34,  1.59it/s]Extractor Predicting: 149it [01:34,  1.59it/s]Extractor Predicting: 150it [01:35,  1.59it/s]Extractor Predicting: 151it [01:36,  1.51it/s]Extractor Predicting: 152it [01:37,  1.50it/s]Extractor Predicting: 153it [01:37,  1.51it/s]Extractor Predicting: 154it [01:38,  1.53it/s]Extractor Predicting: 155it [01:39,  1.48it/s]Extractor Predicting: 156it [01:39,  1.46it/s]Extractor Predicting: 157it [01:40,  1.44it/s]Extractor Predicting: 158it [01:41,  1.47it/s]Extractor Predicting: 159it [01:41,  1.47it/s]Extractor Predicting: 160it [01:42,  1.49it/s]Extractor Predicting: 161it [01:43,  1.50it/s]Extractor Predicting: 162it [01:43,  1.50it/s]Extractor Predicting: 163it [01:44,  1.56it/s]Extractor Predicting: 164it [01:44,  1.66it/s]Extractor Predicting: 165it [01:45,  1.63it/s]Extractor Predicting: 166it [01:46,  1.60it/s]Extractor Predicting: 167it [01:46,  1.58it/s]Extractor Predicting: 168it [01:47,  1.56it/s]Extractor Predicting: 169it [01:48,  1.54it/s]Extractor Predicting: 170it [01:48,  1.59it/s]Extractor Predicting: 171it [01:49,  1.57it/s]Extractor Predicting: 172it [01:49,  1.56it/s]Extractor Predicting: 173it [01:50,  1.58it/s]Extractor Predicting: 174it [01:51,  1.56it/s]Extractor Predicting: 175it [01:51,  1.64it/s]Extractor Predicting: 176it [01:52,  1.68it/s]Extractor Predicting: 177it [01:52,  1.69it/s]Extractor Predicting: 178it [01:53,  1.68it/s]Extractor Predicting: 179it [01:54,  1.66it/s]Extractor Predicting: 180it [01:54,  1.60it/s]Extractor Predicting: 181it [01:55,  1.59it/s]Extractor Predicting: 182it [01:56,  1.62it/s]Extractor Predicting: 183it [01:56,  1.62it/s]Extractor Predicting: 184it [01:57,  1.63it/s]Extractor Predicting: 185it [01:57,  1.67it/s]Extractor Predicting: 186it [01:58,  1.71it/s]Extractor Predicting: 187it [01:59,  1.66it/s]Extractor Predicting: 188it [01:59,  1.65it/s]Extractor Predicting: 189it [02:00,  1.65it/s]Extractor Predicting: 190it [02:00,  1.66it/s]Extractor Predicting: 191it [02:01,  1.64it/s]Extractor Predicting: 192it [02:02,  1.63it/s]Extractor Predicting: 193it [02:02,  1.60it/s]Extractor Predicting: 194it [02:03,  1.58it/s]Extractor Predicting: 195it [02:04,  1.62it/s]Extractor Predicting: 196it [02:04,  1.63it/s]Extractor Predicting: 197it [02:05,  1.65it/s]Extractor Predicting: 198it [02:05,  1.66it/s]Extractor Predicting: 199it [02:06,  1.65it/s]Extractor Predicting: 200it [02:07,  1.64it/s]Extractor Predicting: 201it [02:07,  1.65it/s]Extractor Predicting: 202it [02:08,  1.63it/s]Extractor Predicting: 203it [02:08,  1.60it/s]Extractor Predicting: 204it [02:09,  1.64it/s]Extractor Predicting: 205it [02:10,  1.65it/s]Extractor Predicting: 206it [02:10,  1.64it/s]Extractor Predicting: 207it [02:11,  1.72it/s]Extractor Predicting: 208it [02:11,  1.66it/s]Extractor Predicting: 209it [02:12,  1.65it/s]Extractor Predicting: 210it [02:13,  1.60it/s]Extractor Predicting: 211it [02:13,  1.62it/s]Extractor Predicting: 212it [02:14,  1.58it/s]Extractor Predicting: 213it [02:15,  1.54it/s]Extractor Predicting: 214it [02:15,  1.58it/s]Extractor Predicting: 215it [02:16,  1.60it/s]Extractor Predicting: 216it [02:16,  1.60it/s]Extractor Predicting: 217it [02:17,  1.59it/s]Extractor Predicting: 218it [02:18,  1.61it/s]Extractor Predicting: 219it [02:18,  1.62it/s]Extractor Predicting: 220it [02:19,  1.64it/s]Extractor Predicting: 221it [02:19,  1.67it/s]Extractor Predicting: 222it [02:20,  1.60it/s]Extractor Predicting: 223it [02:21,  1.59it/s]Extractor Predicting: 224it [02:21,  1.59it/s]Extractor Predicting: 225it [02:22,  1.61it/s]Extractor Predicting: 226it [02:23,  1.62it/s]Extractor Predicting: 227it [02:23,  1.64it/s]Extractor Predicting: 228it [02:24,  1.63it/s]Extractor Predicting: 229it [02:24,  1.63it/s]Extractor Predicting: 230it [02:25,  1.59it/s]Extractor Predicting: 231it [02:26,  1.63it/s]Extractor Predicting: 232it [02:26,  1.64it/s]Extractor Predicting: 233it [02:27,  1.66it/s]Extractor Predicting: 234it [02:27,  1.65it/s]Extractor Predicting: 235it [02:28,  1.63it/s]Extractor Predicting: 236it [02:29,  1.61it/s]Extractor Predicting: 237it [02:29,  1.58it/s]Extractor Predicting: 238it [02:30,  1.63it/s]Extractor Predicting: 239it [02:31,  1.61it/s]Extractor Predicting: 240it [02:31,  1.63it/s]Extractor Predicting: 241it [02:32,  1.63it/s]Extractor Predicting: 242it [02:32,  1.62it/s]Extractor Predicting: 243it [02:33,  1.44it/s]Extractor Predicting: 244it [02:34,  1.48it/s]Extractor Predicting: 245it [02:35,  1.53it/s]Extractor Predicting: 246it [02:35,  1.56it/s]Extractor Predicting: 247it [02:36,  1.61it/s]Extractor Predicting: 248it [02:36,  1.61it/s]Extractor Predicting: 249it [02:37,  1.64it/s]Extractor Predicting: 250it [02:38,  1.66it/s]Extractor Predicting: 251it [02:38,  1.61it/s]Extractor Predicting: 252it [02:39,  1.63it/s]Extractor Predicting: 253it [02:39,  1.67it/s]Extractor Predicting: 254it [02:40,  1.68it/s]Extractor Predicting: 255it [02:41,  1.65it/s]Extractor Predicting: 256it [02:41,  1.62it/s]Extractor Predicting: 257it [02:42,  1.55it/s]Extractor Predicting: 258it [02:43,  1.51it/s]Extractor Predicting: 259it [02:43,  1.50it/s]Extractor Predicting: 260it [02:44,  1.53it/s]Extractor Predicting: 261it [02:45,  1.54it/s]Extractor Predicting: 262it [02:45,  1.56it/s]Extractor Predicting: 263it [02:46,  1.58it/s]Extractor Predicting: 264it [02:46,  1.57it/s]Extractor Predicting: 265it [02:47,  1.60it/s]Extractor Predicting: 266it [02:48,  1.59it/s]Extractor Predicting: 267it [02:48,  1.61it/s]Extractor Predicting: 268it [02:49,  1.61it/s]Extractor Predicting: 269it [02:50,  1.61it/s]Extractor Predicting: 270it [02:50,  1.63it/s]Extractor Predicting: 271it [02:51,  1.63it/s]Extractor Predicting: 272it [02:51,  1.60it/s]Extractor Predicting: 273it [02:52,  1.63it/s]Extractor Predicting: 274it [02:53,  1.60it/s]Extractor Predicting: 275it [02:53,  1.63it/s]Extractor Predicting: 276it [02:54,  1.62it/s]Extractor Predicting: 277it [02:54,  1.62it/s]Extractor Predicting: 278it [02:55,  1.60it/s]Extractor Predicting: 279it [02:56,  1.59it/s]Extractor Predicting: 280it [02:56,  1.60it/s]Extractor Predicting: 281it [02:57,  1.62it/s]Extractor Predicting: 282it [02:58,  1.61it/s]Extractor Predicting: 283it [02:58,  1.62it/s]Extractor Predicting: 284it [02:59,  1.59it/s]Extractor Predicting: 285it [03:00,  1.58it/s]Extractor Predicting: 286it [03:00,  1.58it/s]Extractor Predicting: 287it [03:01,  1.61it/s]Extractor Predicting: 288it [03:01,  1.60it/s]Extractor Predicting: 289it [03:02,  1.61it/s]Extractor Predicting: 290it [03:03,  1.60it/s]Extractor Predicting: 291it [03:03,  1.62it/s]Extractor Predicting: 292it [03:04,  1.63it/s]Extractor Predicting: 293it [03:04,  1.63it/s]Extractor Predicting: 294it [03:05,  1.63it/s]Extractor Predicting: 295it [03:06,  1.63it/s]Extractor Predicting: 296it [03:06,  1.59it/s]Extractor Predicting: 297it [03:07,  1.58it/s]Extractor Predicting: 298it [03:08,  1.57it/s]Extractor Predicting: 299it [03:08,  1.56it/s]Extractor Predicting: 300it [03:09,  1.55it/s]Extractor Predicting: 301it [03:10,  1.54it/s]Extractor Predicting: 302it [03:10,  1.55it/s]Extractor Predicting: 303it [03:11,  1.59it/s]Extractor Predicting: 304it [03:11,  1.64it/s]Extractor Predicting: 305it [03:12,  1.64it/s]Extractor Predicting: 306it [03:13,  1.68it/s]Extractor Predicting: 307it [03:13,  1.69it/s]Extractor Predicting: 308it [03:14,  1.68it/s]Extractor Predicting: 309it [03:14,  1.70it/s]Extractor Predicting: 310it [03:15,  1.70it/s]Extractor Predicting: 311it [03:15,  1.70it/s]Extractor Predicting: 312it [03:16,  1.66it/s]Extractor Predicting: 313it [03:17,  1.68it/s]Extractor Predicting: 314it [03:17,  1.61it/s]Extractor Predicting: 315it [03:18,  1.59it/s]Extractor Predicting: 316it [03:19,  1.62it/s]Extractor Predicting: 317it [03:19,  1.66it/s]Extractor Predicting: 318it [03:20,  1.63it/s]Extractor Predicting: 319it [03:20,  1.64it/s]Extractor Predicting: 320it [03:21,  1.64it/s]Extractor Predicting: 321it [03:22,  1.64it/s]Extractor Predicting: 322it [03:22,  1.67it/s]Extractor Predicting: 323it [03:23,  1.67it/s]Extractor Predicting: 324it [03:23,  1.66it/s]Extractor Predicting: 325it [03:24,  1.62it/s]Extractor Predicting: 326it [03:25,  1.64it/s]Extractor Predicting: 327it [03:25,  1.67it/s]Extractor Predicting: 328it [03:26,  1.69it/s]Extractor Predicting: 329it [03:26,  1.72it/s]Extractor Predicting: 330it [03:27,  1.70it/s]Extractor Predicting: 331it [03:28,  1.71it/s]Extractor Predicting: 332it [03:28,  1.68it/s]Extractor Predicting: 333it [03:29,  1.68it/s]Extractor Predicting: 334it [03:29,  1.71it/s]Extractor Predicting: 335it [03:30,  1.66it/s]Extractor Predicting: 336it [03:31,  1.62it/s]Extractor Predicting: 337it [03:31,  1.56it/s]Extractor Predicting: 338it [03:32,  1.60it/s]Extractor Predicting: 339it [03:33,  1.63it/s]Extractor Predicting: 340it [03:33,  1.64it/s]Extractor Predicting: 341it [03:34,  1.63it/s]Extractor Predicting: 342it [03:34,  1.58it/s]Extractor Predicting: 343it [03:35,  1.53it/s]Extractor Predicting: 344it [03:36,  1.55it/s]Extractor Predicting: 345it [03:36,  1.55it/s]Extractor Predicting: 346it [03:37,  1.59it/s]Extractor Predicting: 347it [03:38,  1.61it/s]Extractor Predicting: 348it [03:38,  1.63it/s]Extractor Predicting: 349it [03:39,  1.64it/s]Extractor Predicting: 350it [03:39,  1.62it/s]Extractor Predicting: 351it [03:40,  1.66it/s]Extractor Predicting: 352it [03:41,  1.62it/s]Extractor Predicting: 353it [03:41,  1.66it/s]Extractor Predicting: 354it [03:42,  1.66it/s]Extractor Predicting: 355it [03:42,  1.62it/s]Extractor Predicting: 356it [03:43,  1.41it/s]Extractor Predicting: 357it [03:44,  1.47it/s]Extractor Predicting: 358it [03:45,  1.50it/s]Extractor Predicting: 359it [03:45,  1.53it/s]Extractor Predicting: 360it [03:46,  1.54it/s]Extractor Predicting: 361it [03:47,  1.52it/s]Extractor Predicting: 362it [03:47,  1.55it/s]Extractor Predicting: 363it [03:48,  1.56it/s]Extractor Predicting: 364it [03:48,  1.57it/s]Extractor Predicting: 365it [03:49,  1.54it/s]Extractor Predicting: 366it [03:50,  1.53it/s]Extractor Predicting: 367it [03:50,  1.56it/s]Extractor Predicting: 368it [03:51,  1.62it/s]Extractor Predicting: 369it [03:52,  1.64it/s]Extractor Predicting: 370it [03:52,  1.64it/s]Extractor Predicting: 371it [03:53,  1.65it/s]Extractor Predicting: 372it [03:53,  1.65it/s]Extractor Predicting: 373it [03:54,  1.63it/s]Extractor Predicting: 374it [03:55,  1.65it/s]Extractor Predicting: 375it [03:55,  1.69it/s]Extractor Predicting: 376it [03:56,  1.68it/s]Extractor Predicting: 377it [03:56,  1.59it/s]Extractor Predicting: 378it [03:57,  1.51it/s]Extractor Predicting: 379it [03:58,  1.57it/s]Extractor Predicting: 380it [03:58,  1.55it/s]Extractor Predicting: 381it [03:59,  1.58it/s]Extractor Predicting: 382it [04:00,  1.59it/s]Extractor Predicting: 383it [04:00,  1.59it/s]Extractor Predicting: 384it [04:01,  1.60it/s]Extractor Predicting: 385it [04:01,  1.63it/s]Extractor Predicting: 386it [04:02,  1.61it/s]Extractor Predicting: 387it [04:03,  1.63it/s]Extractor Predicting: 388it [04:03,  1.67it/s]Extractor Predicting: 389it [04:04,  1.65it/s]Extractor Predicting: 390it [04:05,  1.61it/s]Extractor Predicting: 391it [04:05,  1.61it/s]Extractor Predicting: 392it [04:06,  1.64it/s]Extractor Predicting: 393it [04:06,  1.63it/s]Extractor Predicting: 394it [04:07,  1.65it/s]Extractor Predicting: 395it [04:08,  1.64it/s]Extractor Predicting: 396it [04:08,  1.63it/s]Extractor Predicting: 397it [04:09,  1.65it/s]Extractor Predicting: 398it [04:09,  1.69it/s]Extractor Predicting: 399it [04:10,  1.65it/s]Extractor Predicting: 400it [04:11,  1.68it/s]Extractor Predicting: 401it [04:11,  1.67it/s]Extractor Predicting: 402it [04:12,  1.66it/s]Extractor Predicting: 403it [04:12,  1.62it/s]Extractor Predicting: 404it [04:13,  1.60it/s]Extractor Predicting: 405it [04:14,  1.60it/s]Extractor Predicting: 406it [04:14,  1.59it/s]Extractor Predicting: 407it [04:15,  1.56it/s]Extractor Predicting: 408it [04:16,  1.57it/s]Extractor Predicting: 409it [04:16,  1.52it/s]Extractor Predicting: 410it [04:17,  1.50it/s]Extractor Predicting: 411it [04:18,  1.54it/s]Extractor Predicting: 412it [04:18,  1.55it/s]Extractor Predicting: 413it [04:19,  1.52it/s]Extractor Predicting: 414it [04:20,  1.52it/s]Extractor Predicting: 415it [04:20,  1.57it/s]Extractor Predicting: 416it [04:21,  1.53it/s]Extractor Predicting: 417it [04:22,  1.53it/s]Extractor Predicting: 418it [04:22,  1.47it/s]Extractor Predicting: 419it [04:23,  1.51it/s]Extractor Predicting: 420it [04:24,  1.15it/s]Extractor Predicting: 421it [04:25,  1.23it/s]Extractor Predicting: 422it [04:26,  1.29it/s]Extractor Predicting: 423it [04:26,  1.39it/s]Extractor Predicting: 424it [04:27,  1.44it/s]Extractor Predicting: 425it [04:27,  1.47it/s]Extractor Predicting: 426it [04:28,  1.48it/s]Extractor Predicting: 427it [04:29,  1.48it/s]Extractor Predicting: 428it [04:30,  1.49it/s]Extractor Predicting: 429it [04:30,  1.54it/s]Extractor Predicting: 430it [04:31,  1.48it/s]Extractor Predicting: 431it [04:32,  1.48it/s]Extractor Predicting: 432it [04:32,  1.49it/s]Extractor Predicting: 433it [04:33,  1.52it/s]Extractor Predicting: 434it [04:33,  1.55it/s]Extractor Predicting: 435it [04:34,  1.62it/s]Extractor Predicting: 436it [04:34,  1.71it/s]Extractor Predicting: 437it [04:35,  1.79it/s]Extractor Predicting: 438it [04:36,  1.74it/s]Extractor Predicting: 439it [04:36,  1.76it/s]Extractor Predicting: 440it [04:37,  1.86it/s]Extractor Predicting: 441it [04:37,  1.72it/s]Extractor Predicting: 442it [04:38,  1.61it/s]Extractor Predicting: 443it [04:39,  1.53it/s]Extractor Predicting: 444it [04:39,  1.51it/s]Extractor Predicting: 445it [04:40,  1.49it/s]Extractor Predicting: 446it [04:41,  1.47it/s]Extractor Predicting: 447it [04:42,  1.45it/s]Extractor Predicting: 448it [04:42,  1.44it/s]Extractor Predicting: 449it [04:43,  1.44it/s]Extractor Predicting: 450it [04:44,  1.46it/s]Extractor Predicting: 451it [04:44,  1.45it/s]Extractor Predicting: 452it [04:45,  1.44it/s]Extractor Predicting: 453it [04:46,  1.46it/s]Extractor Predicting: 454it [04:46,  1.47it/s]Extractor Predicting: 455it [04:47,  1.46it/s]Extractor Predicting: 456it [04:48,  1.44it/s]Extractor Predicting: 457it [04:48,  1.42it/s]Extractor Predicting: 458it [04:49,  1.43it/s]Extractor Predicting: 459it [04:50,  1.47it/s]Extractor Predicting: 460it [04:50,  1.47it/s]Extractor Predicting: 461it [04:51,  1.48it/s]Extractor Predicting: 462it [04:52,  1.50it/s]Extractor Predicting: 463it [04:52,  1.49it/s]Extractor Predicting: 464it [04:53,  1.47it/s]Extractor Predicting: 465it [04:54,  1.52it/s]Extractor Predicting: 466it [04:54,  1.76it/s]Extractor Predicting: 466it [04:54,  1.58it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35509904622157007,
  "recall": 0.043334228668636406,
  "score": 0.07724225981487393,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'manufacturer', 'mother', 'occupant', 'organization directed by the office or position', 'owned by', 'part of', 'screenwriter', 'sport', 'sports discipline competed in', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8266
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8366, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.49it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.41it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:14,  1.53it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.54it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.48it/s]Extractor Predicting: 41it [00:27,  1.46it/s]Extractor Predicting: 42it [00:28,  1.45it/s]Extractor Predicting: 43it [00:28,  1.44it/s]Extractor Predicting: 44it [00:29,  1.43it/s]Extractor Predicting: 45it [00:30,  1.43it/s]Extractor Predicting: 46it [00:30,  1.43it/s]Extractor Predicting: 47it [00:31,  1.42it/s]Extractor Predicting: 48it [00:32,  1.41it/s]Extractor Predicting: 49it [00:32,  1.41it/s]Extractor Predicting: 50it [00:33,  1.42it/s]Extractor Predicting: 51it [00:34,  1.42it/s]Extractor Predicting: 52it [00:35,  1.40it/s]Extractor Predicting: 53it [00:35,  1.36it/s]Extractor Predicting: 54it [00:36,  1.41it/s]Extractor Predicting: 55it [00:37,  1.46it/s]Extractor Predicting: 56it [00:37,  1.46it/s]Extractor Predicting: 57it [00:38,  1.47it/s]Extractor Predicting: 58it [00:39,  1.45it/s]Extractor Predicting: 59it [00:39,  1.45it/s]Extractor Predicting: 60it [00:40,  1.44it/s]Extractor Predicting: 61it [00:41,  1.46it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:43,  1.50it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.52it/s]Extractor Predicting: 67it [00:45,  1.53it/s]Extractor Predicting: 68it [00:45,  1.55it/s]Extractor Predicting: 69it [00:46,  1.57it/s]Extractor Predicting: 70it [00:47,  1.58it/s]Extractor Predicting: 71it [00:47,  1.54it/s]Extractor Predicting: 72it [00:48,  1.53it/s]Extractor Predicting: 73it [00:49,  1.52it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:50,  1.52it/s]Extractor Predicting: 76it [00:51,  1.49it/s]Extractor Predicting: 77it [00:51,  1.47it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:53,  1.50it/s]Extractor Predicting: 80it [00:53,  1.49it/s]Extractor Predicting: 81it [00:54,  1.48it/s]Extractor Predicting: 82it [00:55,  1.50it/s]Extractor Predicting: 83it [00:55,  1.50it/s]Extractor Predicting: 84it [00:56,  1.50it/s]Extractor Predicting: 85it [00:57,  1.51it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:58,  1.51it/s]Extractor Predicting: 88it [00:59,  1.48it/s]Extractor Predicting: 89it [00:59,  1.51it/s]Extractor Predicting: 90it [01:00,  1.55it/s]Extractor Predicting: 91it [01:01,  1.56it/s]Extractor Predicting: 92it [01:01,  1.71it/s]Extractor Predicting: 92it [01:01,  1.50it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.1297071129707113,
  "recall": 0.00608320251177394,
  "score": 0.01162136832239925,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_2/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:43, 14.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:33<05:03, 16.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:49<04:46, 16.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:08<04:39, 17.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:26<04:24, 17.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:41<03:55, 16.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:55<03:27, 15.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:11<03:11, 15.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:30<03:04, 16.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:45<02:41, 16.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:03<02:31, 16.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:20<02:15, 16.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:36<01:56, 16.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:52<01:38, 16.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:09<01:23, 16.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:24<01:04, 16.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:41<00:49, 16.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:58<00:33, 16.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:17<00:17, 17.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:39<00:00, 18.76s/it]Generating: 100%|██████████| 20/20 [05:39<00:00, 16.99s/it]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8806818181818182, 'errors': {''}}
['Relation : military branch . Context : The FSB studied the Communist Party under Nikita Khrushchev and other leaders at the Moscow State Academy of Sciences ( MANS ) in 1958 . Head Entity : the FSB , Tail Entity : Communist Party .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 350, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 422, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 468, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 518, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 568, 'raw': 768}
{'target': 600, 'success': 593, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : military branch .', 'success_rate': 0.7439903846153846, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 364, 'raw': 480}
{'target': 600, 'success': 391, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 462, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 576, 'raw': 768}
{'target': 600, 'success': 598, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : part of the series .', 'success_rate': 0.7463942307692307, 'errors': {'', "('Eleanor Mabel de Lea', 'part of the series', '', 'His parents were Josephine Giorgio de Cordero ( 1911 &ndash; 24 July 1912 ) and Eleanor Mabel de Lea ( d.')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 604, 'raw': 736}
{'prompt': 'Relation : place of death .', 'success_rate': 0.8206521739130435, 'errors': {'', 'too many values to unpack (expected 2)', "('John', 'place of death', '', 'It is also notable for being , among many others , the main character in Peterborough s John in 1775 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : director .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 395, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : main subject .', 'success_rate': 0.77375, 'errors': {'', "('Kingdom of Malagasy', 'main subject', '', 'He continued this journey and made three more conquests in the Indian Ocean , with the same success , to find the island of Madagascar , in the territory of the Kingdom of Malagasy .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9196428571428571, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 281, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8301630434782609, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupant . Context : Later in the year ( 1143 ) , he purchased the lands of Lauterbach in Bavaria , near Leake , now the Kriege , and became Chief of Staff of Lejeune &ndash; 1st Duke of Saxony . Head Entity : Lauterbach , Tail Entity : William of Leake .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8369565217391305, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.8396739130434783, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8707386363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : part of .', 'success_rate': 0.8274456521739131, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 489, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : sport . Context : On his debut year ( 2009 ) at the University of Michigan , he started all nine games in the first half for Michigan Football . Head Entity : Michigan Football , Tail Entity : football .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 388, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : sport .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 472, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 544, 'raw': 704}
{'target': 600, 'success': 568, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.7725, 'errors': {'', "('John Johnstone', 'sports discipline competed in', '', 'John Johnstone ( born 7 January 1956 in Glasgow ) is an English swimmer who competed in 1988 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 544, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : use .', 'success_rate': 0.8072916666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 74, 'raw': 128}
{'target': 600, 'success': 90, 'raw': 160}
{'target': 600, 'success': 110, 'raw': 192}
{'target': 600, 'success': 132, 'raw': 224}
{'target': 600, 'success': 155, 'raw': 256}
{'target': 600, 'success': 174, 'raw': 288}
{'target': 600, 'success': 191, 'raw': 320}
{'target': 600, 'success': 209, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 241, 'raw': 416}
{'target': 600, 'success': 260, 'raw': 448}
{'target': 600, 'success': 283, 'raw': 480}
{'target': 600, 'success': 303, 'raw': 512}
{'target': 600, 'success': 322, 'raw': 544}
{'target': 600, 'success': 340, 'raw': 576}
{'target': 600, 'success': 356, 'raw': 608}
{'target': 600, 'success': 376, 'raw': 640}
{'target': 600, 'success': 396, 'raw': 672}
{'target': 600, 'success': 416, 'raw': 704}
{'target': 600, 'success': 433, 'raw': 736}
{'target': 600, 'success': 452, 'raw': 768}
{'target': 600, 'success': 474, 'raw': 800}
{'target': 600, 'success': 493, 'raw': 832}
{'target': 600, 'success': 513, 'raw': 864}
{'target': 600, 'success': 531, 'raw': 896}
{'target': 600, 'success': 546, 'raw': 928}
{'target': 600, 'success': 569, 'raw': 960}
{'target': 600, 'success': 588, 'raw': 992}
{'target': 600, 'success': 608, 'raw': 1024}
{'prompt': 'Relation : voice type .', 'success_rate': 0.59375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/1_ext.jsonl'}}
estimate vocab size: 16397
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16497, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.63it/s]Extractor Estimating: 2it [00:01,  1.67it/s]Extractor Estimating: 3it [00:01,  1.67it/s]Extractor Estimating: 4it [00:02,  1.68it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.62it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:04,  1.61it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.62it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.64it/s]Extractor Estimating: 15it [00:09,  1.63it/s]Extractor Estimating: 16it [00:09,  1.65it/s]Extractor Estimating: 17it [00:10,  1.64it/s]Extractor Estimating: 18it [00:11,  1.66it/s]Extractor Estimating: 19it [00:11,  1.58it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:12,  1.64it/s]Extractor Estimating: 22it [00:13,  1.65it/s]Extractor Estimating: 23it [00:14,  1.69it/s]Extractor Estimating: 24it [00:16,  1.13s/it]Extractor Estimating: 25it [00:17,  1.04it/s]Extractor Estimating: 26it [00:17,  1.14it/s]Extractor Estimating: 27it [00:18,  1.23it/s]Extractor Estimating: 28it [00:19,  1.29it/s]Extractor Estimating: 29it [00:19,  1.34it/s]Extractor Estimating: 30it [00:20,  1.39it/s]Extractor Estimating: 31it [00:21,  1.44it/s]Extractor Estimating: 32it [00:21,  1.53it/s]Extractor Estimating: 33it [00:22,  1.55it/s]Extractor Estimating: 34it [00:22,  1.53it/s]Extractor Estimating: 35it [00:23,  1.54it/s]Extractor Estimating: 36it [00:24,  1.53it/s]Extractor Estimating: 37it [00:24,  1.58it/s]Extractor Estimating: 38it [00:25,  1.61it/s]Extractor Estimating: 39it [00:25,  1.61it/s]Extractor Estimating: 40it [00:26,  1.63it/s]Extractor Estimating: 41it [00:27,  1.58it/s]Extractor Estimating: 42it [00:27,  1.54it/s]Extractor Estimating: 43it [00:28,  1.56it/s]Extractor Estimating: 44it [00:29,  1.58it/s]Extractor Estimating: 45it [00:29,  1.55it/s]Extractor Estimating: 46it [00:30,  1.54it/s]Extractor Estimating: 47it [00:31,  1.54it/s]Extractor Estimating: 48it [00:31,  1.55it/s]Extractor Estimating: 49it [00:32,  1.58it/s]Extractor Estimating: 50it [00:33,  1.59it/s]Extractor Estimating: 51it [00:33,  1.55it/s]Extractor Estimating: 52it [00:34,  1.57it/s]Extractor Estimating: 53it [00:34,  1.54it/s]Extractor Estimating: 54it [00:35,  1.53it/s]Extractor Estimating: 55it [00:36,  1.53it/s]Extractor Estimating: 56it [00:36,  1.54it/s]Extractor Estimating: 57it [00:37,  1.58it/s]Extractor Estimating: 58it [00:38,  1.60it/s]Extractor Estimating: 59it [00:38,  1.58it/s]Extractor Estimating: 60it [00:39,  1.59it/s]Extractor Estimating: 61it [00:40,  1.56it/s]Extractor Estimating: 62it [00:40,  1.52it/s]Extractor Estimating: 63it [00:41,  1.55it/s]Extractor Estimating: 64it [00:42,  1.50it/s]Extractor Estimating: 65it [00:42,  1.50it/s]Extractor Estimating: 66it [00:43,  1.51it/s]Extractor Estimating: 67it [00:44,  1.54it/s]Extractor Estimating: 68it [00:44,  1.55it/s]Extractor Estimating: 69it [00:45,  1.56it/s]Extractor Estimating: 70it [00:46,  1.54it/s]Extractor Estimating: 71it [00:46,  1.54it/s]Extractor Estimating: 72it [00:47,  1.55it/s]Extractor Estimating: 73it [00:47,  1.53it/s]Extractor Estimating: 74it [00:48,  1.54it/s]Extractor Estimating: 75it [00:49,  1.52it/s]Extractor Estimating: 76it [00:49,  1.53it/s]Extractor Estimating: 77it [00:50,  1.54it/s]Extractor Estimating: 78it [00:51,  1.54it/s]Extractor Estimating: 79it [00:51,  1.57it/s]Extractor Estimating: 80it [00:52,  1.58it/s]Extractor Estimating: 81it [00:53,  1.42it/s]Extractor Estimating: 82it [00:53,  1.47it/s]Extractor Estimating: 83it [00:54,  1.52it/s]Extractor Estimating: 84it [00:55,  1.55it/s]Extractor Estimating: 85it [00:55,  1.53it/s]Extractor Estimating: 86it [00:56,  1.51it/s]Extractor Estimating: 87it [00:57,  1.49it/s]Extractor Estimating: 88it [00:57,  1.50it/s]Extractor Estimating: 89it [00:58,  1.51it/s]Extractor Estimating: 90it [00:59,  1.56it/s]Extractor Estimating: 91it [00:59,  1.60it/s]Extractor Estimating: 92it [01:00,  1.59it/s]Extractor Estimating: 93it [01:00,  1.58it/s]Extractor Estimating: 94it [01:01,  1.61it/s]Extractor Estimating: 95it [01:02,  1.58it/s]Extractor Estimating: 96it [01:02,  1.55it/s]Extractor Estimating: 97it [01:03,  1.51it/s]Extractor Estimating: 98it [01:04,  1.56it/s]Extractor Estimating: 99it [01:04,  1.52it/s]Extractor Estimating: 100it [01:05,  1.54it/s]Extractor Estimating: 101it [01:06,  1.49it/s]Extractor Estimating: 102it [01:06,  1.46it/s]Extractor Estimating: 103it [01:07,  1.46it/s]Extractor Estimating: 104it [01:08,  1.48it/s]Extractor Estimating: 105it [01:09,  1.42it/s]Extractor Estimating: 106it [01:09,  1.46it/s]Extractor Estimating: 107it [01:10,  1.49it/s]Extractor Estimating: 108it [01:11,  1.46it/s]Extractor Estimating: 109it [01:11,  1.47it/s]Extractor Estimating: 110it [01:12,  1.47it/s]Extractor Estimating: 111it [01:13,  1.51it/s]Extractor Estimating: 112it [01:13,  1.50it/s]Extractor Estimating: 113it [01:14,  1.49it/s]Extractor Estimating: 114it [01:15,  1.49it/s]Extractor Estimating: 115it [01:15,  1.51it/s]Extractor Estimating: 116it [01:16,  1.53it/s]Extractor Estimating: 117it [01:16,  1.55it/s]Extractor Estimating: 118it [01:17,  1.51it/s]Extractor Estimating: 119it [01:18,  1.50it/s]Extractor Estimating: 120it [01:19,  1.51it/s]Extractor Estimating: 121it [01:19,  1.51it/s]Extractor Estimating: 122it [01:20,  1.42it/s]Extractor Estimating: 123it [01:21,  1.43it/s]Extractor Estimating: 124it [01:21,  1.45it/s]Extractor Estimating: 125it [01:22,  1.45it/s]Extractor Estimating: 126it [01:23,  1.47it/s]Extractor Estimating: 127it [01:23,  1.49it/s]Extractor Estimating: 128it [01:24,  1.51it/s]Extractor Estimating: 129it [01:25,  1.51it/s]Extractor Estimating: 130it [01:25,  1.52it/s]Extractor Estimating: 131it [01:26,  1.53it/s]Extractor Estimating: 132it [01:27,  1.55it/s]Extractor Estimating: 133it [01:27,  1.56it/s]Extractor Estimating: 134it [01:28,  1.55it/s]Extractor Estimating: 135it [01:28,  1.57it/s]Extractor Estimating: 136it [01:29,  1.57it/s]Extractor Estimating: 137it [01:30,  1.50it/s]Extractor Estimating: 138it [01:30,  1.52it/s]Extractor Estimating: 139it [01:31,  1.50it/s]Extractor Estimating: 140it [01:32,  1.54it/s]Extractor Estimating: 141it [01:32,  1.56it/s]Extractor Estimating: 142it [01:33,  1.52it/s]Extractor Estimating: 143it [01:34,  1.55it/s]Extractor Estimating: 144it [01:34,  1.52it/s]Extractor Estimating: 145it [01:35,  1.52it/s]Extractor Estimating: 146it [01:36,  1.55it/s]Extractor Estimating: 147it [01:36,  1.54it/s]Extractor Estimating: 148it [01:37,  1.53it/s]Extractor Estimating: 149it [01:38,  1.54it/s]Extractor Estimating: 150it [01:38,  1.51it/s]Extractor Estimating: 151it [01:39,  1.53it/s]Extractor Estimating: 152it [01:40,  1.15it/s]Extractor Estimating: 153it [01:41,  1.25it/s]Extractor Estimating: 154it [01:42,  1.30it/s]Extractor Estimating: 155it [01:42,  1.34it/s]Extractor Estimating: 156it [01:43,  1.31it/s]Extractor Estimating: 157it [01:44,  1.38it/s]Extractor Estimating: 158it [01:44,  1.45it/s]Extractor Estimating: 159it [01:45,  1.48it/s]Extractor Estimating: 160it [01:46,  1.51it/s]Extractor Estimating: 161it [01:46,  1.47it/s]Extractor Estimating: 162it [01:47,  1.53it/s]Extractor Estimating: 163it [01:48,  1.52it/s]Extractor Estimating: 164it [01:48,  1.54it/s]Extractor Estimating: 165it [01:49,  1.52it/s]Extractor Estimating: 166it [01:50,  1.52it/s]Extractor Estimating: 167it [01:50,  1.51it/s]Extractor Estimating: 168it [01:51,  1.45it/s]Extractor Estimating: 169it [01:52,  1.48it/s]Extractor Estimating: 170it [01:52,  1.52it/s]Extractor Estimating: 171it [01:53,  1.50it/s]Extractor Estimating: 172it [01:54,  1.52it/s]Extractor Estimating: 173it [01:54,  1.58it/s]Extractor Estimating: 174it [01:55,  1.57it/s]Extractor Estimating: 175it [01:55,  1.55it/s]Extractor Estimating: 176it [01:56,  1.55it/s]Extractor Estimating: 177it [01:57,  1.48it/s]Extractor Estimating: 178it [01:58,  1.49it/s]Extractor Estimating: 179it [01:58,  1.47it/s]Extractor Estimating: 180it [01:59,  1.48it/s]Extractor Estimating: 181it [02:00,  1.48it/s]Extractor Estimating: 182it [02:00,  1.51it/s]Extractor Estimating: 183it [02:01,  1.47it/s]Extractor Estimating: 184it [02:02,  1.41it/s]Extractor Estimating: 185it [02:02,  1.45it/s]Extractor Estimating: 186it [02:03,  1.45it/s]Extractor Estimating: 187it [02:04,  1.46it/s]Extractor Estimating: 188it [02:04,  1.51it/s]Extractor Estimating: 189it [02:05,  1.52it/s]Extractor Estimating: 190it [02:06,  1.54it/s]Extractor Estimating: 191it [02:06,  1.49it/s]Extractor Estimating: 192it [02:07,  1.51it/s]Extractor Estimating: 193it [02:08,  1.49it/s]Extractor Estimating: 194it [02:08,  1.51it/s]Extractor Estimating: 195it [02:09,  1.50it/s]Extractor Estimating: 196it [02:10,  1.53it/s]Extractor Estimating: 197it [02:10,  1.55it/s]Extractor Estimating: 198it [02:11,  1.52it/s]Extractor Estimating: 199it [02:12,  1.48it/s]Extractor Estimating: 200it [02:12,  1.45it/s]Extractor Estimating: 201it [02:13,  1.50it/s]Extractor Estimating: 202it [02:14,  1.51it/s]Extractor Estimating: 203it [02:14,  1.47it/s]Extractor Estimating: 204it [02:15,  1.48it/s]Extractor Estimating: 205it [02:16,  1.52it/s]Extractor Estimating: 206it [02:16,  1.53it/s]Extractor Estimating: 207it [02:17,  1.53it/s]Extractor Estimating: 208it [02:18,  1.55it/s]Extractor Estimating: 209it [02:18,  1.56it/s]Extractor Estimating: 210it [02:19,  1.55it/s]Extractor Estimating: 211it [02:20,  1.52it/s]Extractor Estimating: 212it [02:20,  1.52it/s]Extractor Estimating: 213it [02:21,  1.47it/s]Extractor Estimating: 214it [02:22,  1.49it/s]Extractor Estimating: 215it [02:22,  1.51it/s]Extractor Estimating: 216it [02:23,  1.51it/s]Extractor Estimating: 217it [02:24,  1.49it/s]Extractor Estimating: 218it [02:24,  1.50it/s]Extractor Estimating: 219it [02:25,  1.48it/s]Extractor Estimating: 220it [02:26,  1.52it/s]Extractor Estimating: 221it [02:26,  1.53it/s]Extractor Estimating: 222it [02:27,  1.50it/s]Extractor Estimating: 223it [02:28,  1.53it/s]Extractor Estimating: 224it [02:28,  1.51it/s]Extractor Estimating: 225it [02:29,  1.50it/s]Extractor Estimating: 226it [02:30,  1.52it/s]Extractor Estimating: 227it [02:30,  1.57it/s]Extractor Estimating: 228it [02:31,  1.55it/s]Extractor Estimating: 229it [02:31,  1.61it/s]Extractor Estimating: 230it [02:32,  1.65it/s]Extractor Estimating: 231it [02:32,  1.66it/s]Extractor Estimating: 232it [02:33,  1.69it/s]Extractor Estimating: 233it [02:34,  1.65it/s]Extractor Estimating: 234it [02:34,  1.64it/s]Extractor Estimating: 235it [02:35,  1.52it/s]Extractor Estimating: 236it [02:36,  1.59it/s]Extractor Estimating: 237it [02:36,  1.59it/s]Extractor Estimating: 238it [02:37,  1.62it/s]Extractor Estimating: 239it [02:38,  1.53it/s]Extractor Estimating: 240it [02:38,  1.56it/s]Extractor Estimating: 241it [02:39,  1.61it/s]Extractor Estimating: 242it [02:39,  1.61it/s]Extractor Estimating: 243it [02:40,  1.63it/s]Extractor Estimating: 244it [02:41,  1.62it/s]Extractor Estimating: 245it [02:41,  1.61it/s]Extractor Estimating: 246it [02:42,  1.62it/s]Extractor Estimating: 247it [02:43,  1.61it/s]Extractor Estimating: 248it [02:43,  1.63it/s]Extractor Estimating: 249it [02:44,  1.60it/s]Extractor Estimating: 250it [02:44,  1.62it/s]Extractor Estimating: 251it [02:45,  1.60it/s]Extractor Estimating: 252it [02:46,  1.55it/s]Extractor Estimating: 253it [02:46,  1.52it/s]Extractor Estimating: 254it [02:47,  1.46it/s]Extractor Estimating: 255it [02:48,  1.44it/s]Extractor Estimating: 256it [02:48,  1.47it/s]Extractor Estimating: 257it [02:49,  1.47it/s]Extractor Estimating: 258it [02:50,  1.45it/s]Extractor Estimating: 259it [02:51,  1.42it/s]Extractor Estimating: 260it [02:51,  1.46it/s]Extractor Estimating: 261it [02:52,  1.45it/s]Extractor Estimating: 262it [02:53,  1.44it/s]Extractor Estimating: 263it [02:53,  1.42it/s]Extractor Estimating: 264it [02:54,  1.43it/s]Extractor Estimating: 265it [02:55,  1.40it/s]Extractor Estimating: 266it [02:56,  1.35it/s]Extractor Estimating: 267it [02:56,  1.35it/s]Extractor Estimating: 268it [02:57,  1.35it/s]Extractor Estimating: 269it [02:58,  1.43it/s]Extractor Estimating: 270it [02:58,  1.46it/s]Extractor Estimating: 271it [02:59,  1.48it/s]Extractor Estimating: 272it [03:00,  1.44it/s]Extractor Estimating: 273it [03:00,  1.45it/s]Extractor Estimating: 274it [03:01,  1.42it/s]Extractor Estimating: 275it [03:02,  1.47it/s]Extractor Estimating: 276it [03:02,  1.47it/s]Extractor Estimating: 277it [03:03,  1.52it/s]Extractor Estimating: 278it [03:04,  1.51it/s]Extractor Estimating: 279it [03:04,  1.55it/s]Extractor Estimating: 280it [03:05,  1.56it/s]Extractor Estimating: 281it [03:06,  1.56it/s]Extractor Estimating: 282it [03:06,  1.54it/s]Extractor Estimating: 283it [03:07,  1.48it/s]Extractor Estimating: 284it [03:08,  1.45it/s]Extractor Estimating: 285it [03:09,  1.39it/s]Extractor Estimating: 286it [03:09,  1.41it/s]Extractor Estimating: 287it [03:10,  1.44it/s]Extractor Estimating: 288it [03:11,  1.42it/s]Extractor Estimating: 289it [03:11,  1.46it/s]Extractor Estimating: 290it [03:12,  1.48it/s]Extractor Estimating: 291it [03:13,  1.51it/s]Extractor Estimating: 292it [03:13,  1.50it/s]Extractor Estimating: 293it [03:14,  1.51it/s]Extractor Estimating: 294it [03:15,  1.49it/s]Extractor Estimating: 295it [03:15,  1.49it/s]Extractor Estimating: 296it [03:16,  1.51it/s]Extractor Estimating: 297it [03:17,  1.47it/s]Extractor Estimating: 298it [03:17,  1.51it/s]Extractor Estimating: 299it [03:18,  1.57it/s]Extractor Estimating: 300it [03:18,  1.54it/s]Extractor Estimating: 301it [03:19,  1.53it/s]Extractor Estimating: 302it [03:20,  1.57it/s]Extractor Estimating: 303it [03:20,  1.55it/s]Extractor Estimating: 304it [03:21,  1.58it/s]Extractor Estimating: 305it [03:22,  1.58it/s]Extractor Estimating: 306it [03:22,  1.57it/s]Extractor Estimating: 307it [03:23,  1.58it/s]Extractor Estimating: 308it [03:24,  1.60it/s]Extractor Estimating: 309it [03:24,  1.59it/s]Extractor Estimating: 310it [03:25,  1.63it/s]Extractor Estimating: 311it [03:25,  1.62it/s]Extractor Estimating: 312it [03:26,  1.64it/s]Extractor Estimating: 313it [03:27,  1.45it/s]Extractor Estimating: 314it [03:27,  1.51it/s]Extractor Estimating: 315it [03:28,  1.53it/s]Extractor Estimating: 316it [03:29,  1.58it/s]Extractor Estimating: 317it [03:29,  1.56it/s]Extractor Estimating: 318it [03:30,  1.61it/s]Extractor Estimating: 319it [03:30,  1.60it/s]Extractor Estimating: 320it [03:31,  1.60it/s]Extractor Estimating: 321it [03:32,  1.58it/s]Extractor Estimating: 322it [03:32,  1.60it/s]Extractor Estimating: 323it [03:33,  1.64it/s]Extractor Estimating: 324it [03:34,  1.60it/s]Extractor Estimating: 325it [03:34,  1.57it/s]Extractor Estimating: 326it [03:35,  1.59it/s]Extractor Estimating: 327it [03:36,  1.57it/s]Extractor Estimating: 328it [03:36,  1.49it/s]Extractor Estimating: 329it [03:37,  1.49it/s]Extractor Estimating: 330it [03:38,  1.47it/s]Extractor Estimating: 331it [03:38,  1.52it/s]Extractor Estimating: 332it [03:39,  1.54it/s]Extractor Estimating: 333it [03:40,  1.52it/s]Extractor Estimating: 334it [03:40,  1.50it/s]Extractor Estimating: 335it [03:41,  1.49it/s]Extractor Estimating: 336it [03:42,  1.49it/s]Extractor Estimating: 337it [03:42,  1.49it/s]Extractor Estimating: 338it [03:43,  1.51it/s]Extractor Estimating: 339it [03:44,  1.53it/s]Extractor Estimating: 340it [03:44,  1.53it/s]Extractor Estimating: 341it [03:45,  1.53it/s]Extractor Estimating: 342it [03:46,  1.50it/s]Extractor Estimating: 343it [03:46,  1.56it/s]Extractor Estimating: 344it [03:47,  1.57it/s]Extractor Estimating: 345it [03:47,  1.56it/s]Extractor Estimating: 346it [03:48,  1.55it/s]Extractor Estimating: 347it [03:49,  1.56it/s]Extractor Estimating: 348it [03:49,  1.55it/s]Extractor Estimating: 349it [03:50,  1.54it/s]Extractor Estimating: 350it [03:51,  1.53it/s]Extractor Estimating: 351it [03:51,  1.55it/s]Extractor Estimating: 352it [03:52,  1.54it/s]Extractor Estimating: 353it [03:53,  1.55it/s]Extractor Estimating: 354it [03:53,  1.58it/s]Extractor Estimating: 355it [03:54,  1.53it/s]Extractor Estimating: 356it [03:55,  1.47it/s]Extractor Estimating: 357it [03:55,  1.49it/s]Extractor Estimating: 358it [03:56,  1.47it/s]Extractor Estimating: 359it [03:57,  1.47it/s]Extractor Estimating: 360it [03:57,  1.48it/s]Extractor Estimating: 361it [03:58,  1.50it/s]Extractor Estimating: 362it [03:59,  1.52it/s]Extractor Estimating: 363it [03:59,  1.49it/s]Extractor Estimating: 364it [04:00,  1.54it/s]Extractor Estimating: 365it [04:01,  1.55it/s]Extractor Estimating: 366it [04:01,  1.57it/s]Extractor Estimating: 367it [04:02,  1.57it/s]Extractor Estimating: 368it [04:02,  1.59it/s]Extractor Estimating: 369it [04:03,  1.55it/s]Extractor Estimating: 370it [04:04,  1.49it/s]Extractor Estimating: 371it [04:04,  1.55it/s]Extractor Estimating: 372it [04:05,  1.59it/s]Extractor Estimating: 373it [04:06,  1.55it/s]Extractor Estimating: 374it [04:06,  1.51it/s]Extractor Estimating: 375it [04:07,  1.47it/s]Extractor Estimating: 376it [04:08,  1.50it/s]Extractor Estimating: 377it [04:08,  1.54it/s]Extractor Estimating: 378it [04:09,  1.51it/s]Extractor Estimating: 379it [04:10,  1.53it/s]Extractor Estimating: 380it [04:10,  1.51it/s]Extractor Estimating: 381it [04:11,  1.51it/s]Extractor Estimating: 382it [04:12,  1.48it/s]Extractor Estimating: 383it [04:12,  1.45it/s]Extractor Estimating: 384it [04:13,  1.45it/s]Extractor Estimating: 385it [04:14,  1.42it/s]Extractor Estimating: 386it [04:15,  1.45it/s]Extractor Estimating: 387it [04:15,  1.46it/s]Extractor Estimating: 388it [04:16,  1.47it/s]Extractor Estimating: 389it [04:17,  1.36it/s]Extractor Estimating: 390it [04:17,  1.45it/s]Extractor Estimating: 391it [04:18,  1.45it/s]Extractor Estimating: 392it [04:19,  1.50it/s]Extractor Estimating: 393it [04:19,  1.54it/s]Extractor Estimating: 394it [04:20,  1.53it/s]Extractor Estimating: 395it [04:21,  1.49it/s]Extractor Estimating: 396it [04:21,  1.52it/s]Extractor Estimating: 397it [04:22,  1.49it/s]Extractor Estimating: 398it [04:23,  1.52it/s]Extractor Estimating: 399it [04:23,  1.48it/s]Extractor Estimating: 400it [04:24,  1.49it/s]Extractor Estimating: 401it [04:25,  1.53it/s]Extractor Estimating: 402it [04:25,  1.56it/s]Extractor Estimating: 403it [04:26,  1.57it/s]Extractor Estimating: 404it [04:27,  1.56it/s]Extractor Estimating: 405it [04:27,  1.52it/s]Extractor Estimating: 406it [04:28,  1.54it/s]Extractor Estimating: 407it [04:28,  1.56it/s]Extractor Estimating: 408it [04:29,  1.56it/s]Extractor Estimating: 409it [04:30,  1.61it/s]Extractor Estimating: 410it [04:30,  1.62it/s]Extractor Estimating: 411it [04:31,  1.66it/s]Extractor Estimating: 412it [04:31,  1.66it/s]Extractor Estimating: 413it [04:32,  1.63it/s]Extractor Estimating: 414it [04:33,  1.61it/s]Extractor Estimating: 415it [04:33,  1.60it/s]Extractor Estimating: 416it [04:34,  1.60it/s]Extractor Estimating: 417it [04:35,  1.62it/s]Extractor Estimating: 418it [04:35,  1.63it/s]Extractor Estimating: 419it [04:36,  1.62it/s]Extractor Estimating: 420it [04:36,  1.61it/s]Extractor Estimating: 421it [04:37,  1.61it/s]Extractor Estimating: 422it [04:38,  1.62it/s]Extractor Estimating: 423it [04:38,  1.63it/s]Extractor Estimating: 424it [04:39,  1.60it/s]Extractor Estimating: 425it [04:40,  1.58it/s]Extractor Estimating: 426it [04:40,  1.64it/s]Extractor Estimating: 427it [04:41,  1.62it/s]Extractor Estimating: 428it [04:41,  1.62it/s]Extractor Estimating: 429it [04:42,  1.68it/s]Extractor Estimating: 430it [04:43,  1.68it/s]Extractor Estimating: 431it [04:43,  1.69it/s]Extractor Estimating: 432it [04:44,  1.72it/s]Extractor Estimating: 433it [04:44,  1.74it/s]Extractor Estimating: 434it [04:45,  1.76it/s]Extractor Estimating: 435it [04:45,  1.81it/s]Extractor Estimating: 436it [04:46,  1.78it/s]Extractor Estimating: 437it [04:46,  1.75it/s]Extractor Estimating: 438it [04:47,  1.77it/s]Extractor Estimating: 439it [04:48,  1.69it/s]Extractor Estimating: 440it [04:48,  1.64it/s]Extractor Estimating: 441it [04:49,  1.66it/s]Extractor Estimating: 442it [04:50,  1.64it/s]Extractor Estimating: 443it [04:50,  1.62it/s]Extractor Estimating: 444it [04:51,  1.63it/s]Extractor Estimating: 445it [04:51,  1.66it/s]Extractor Estimating: 446it [04:52,  1.66it/s]Extractor Estimating: 447it [04:53,  1.67it/s]Extractor Estimating: 448it [04:53,  1.73it/s]Extractor Estimating: 449it [04:54,  1.76it/s]Extractor Estimating: 450it [04:54,  1.62it/s]Extractor Estimating: 451it [04:55,  1.64it/s]Extractor Estimating: 452it [04:56,  1.60it/s]Extractor Estimating: 453it [04:56,  1.56it/s]Extractor Estimating: 454it [04:57,  1.57it/s]Extractor Estimating: 455it [04:58,  1.55it/s]Extractor Estimating: 456it [04:58,  1.54it/s]Extractor Estimating: 457it [04:59,  1.55it/s]Extractor Estimating: 458it [05:00,  1.52it/s]Extractor Estimating: 459it [05:00,  1.52it/s]Extractor Estimating: 460it [05:01,  1.55it/s]Extractor Estimating: 461it [05:02,  1.51it/s]Extractor Estimating: 462it [05:02,  1.50it/s]Extractor Estimating: 463it [05:03,  1.53it/s]Extractor Estimating: 464it [05:03,  1.56it/s]Extractor Estimating: 465it [05:04,  1.51it/s]Extractor Estimating: 466it [05:05,  1.50it/s]Extractor Estimating: 467it [05:06,  1.40it/s]Extractor Estimating: 468it [05:06,  1.46it/s]Extractor Estimating: 469it [05:07,  1.53it/s]Extractor Estimating: 470it [05:07,  1.54it/s]Extractor Estimating: 471it [05:08,  1.51it/s]Extractor Estimating: 472it [05:09,  1.47it/s]Extractor Estimating: 473it [05:10,  1.52it/s]Extractor Estimating: 474it [05:10,  1.51it/s]Extractor Estimating: 475it [05:11,  1.51it/s]Extractor Estimating: 476it [05:11,  1.54it/s]Extractor Estimating: 477it [05:12,  1.59it/s]Extractor Estimating: 478it [05:13,  1.58it/s]Extractor Estimating: 479it [05:13,  1.59it/s]Extractor Estimating: 480it [05:14,  1.61it/s]Extractor Estimating: 481it [05:15,  1.57it/s]Extractor Estimating: 482it [05:15,  1.54it/s]Extractor Estimating: 483it [05:16,  1.51it/s]Extractor Estimating: 484it [05:17,  1.56it/s]Extractor Estimating: 485it [05:17,  1.59it/s]Extractor Estimating: 486it [05:18,  1.56it/s]Extractor Estimating: 487it [05:18,  1.53it/s]Extractor Estimating: 488it [05:19,  1.55it/s]Extractor Estimating: 489it [05:20,  1.54it/s]Extractor Estimating: 490it [05:20,  1.52it/s]Extractor Estimating: 491it [05:21,  1.56it/s]Extractor Estimating: 492it [05:22,  1.58it/s]Extractor Estimating: 493it [05:22,  1.53it/s]Extractor Estimating: 494it [05:23,  1.58it/s]Extractor Estimating: 495it [05:24,  1.60it/s]Extractor Estimating: 496it [05:24,  1.62it/s]Extractor Estimating: 497it [05:25,  1.57it/s]Extractor Estimating: 498it [05:25,  1.58it/s]Extractor Estimating: 499it [05:26,  1.56it/s]Extractor Estimating: 500it [05:27,  1.78it/s]Extractor Estimating: 500it [05:27,  1.53it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 10029 mean pseudo reward: 0.8867696878495498
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl'}
train vocab size: 31058
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31158, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31158, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.356, loss:1265.8745
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.075, loss:1269.7058
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.095, loss:1242.5870
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.097, loss:1219.9331
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 1.088, loss:1152.3760
>> valid entity prec:0.5911, rec:0.5000, f1:0.5417
>> valid relation prec:0.7085, rec:0.0296, f1:0.0568
>> valid relation with NER prec:0.7085, rec:0.0296, f1:0.0568
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 3.470, loss:1213.8040
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 282, avg_time 1.078, loss:1155.3915
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 382, avg_time 1.093, loss:1091.8526
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 1.097, loss:1038.4050
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 1.089, loss:1063.5016
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6325, rec:0.4886, f1:0.5513
>> valid relation prec:0.5240, rec:0.0623, f1:0.1114
>> valid relation with NER prec:0.5240, rec:0.0623, f1:0.1114
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 264, avg_time 3.441, loss:1030.2269
g_step 1200, step 364, avg_time 1.098, loss:1040.6949
g_step 1300, step 46, avg_time 1.093, loss:960.2475
g_step 1400, step 146, avg_time 1.098, loss:936.1721
g_step 1500, step 246, avg_time 1.086, loss:973.4405
>> valid entity prec:0.5878, rec:0.5607, f1:0.5739
>> valid relation prec:0.5530, rec:0.0805, f1:0.1405
>> valid relation with NER prec:0.5530, rec:0.0805, f1:0.1405
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 346, avg_time 3.452, loss:989.3771
g_step 1700, step 28, avg_time 1.095, loss:897.5442
g_step 1800, step 128, avg_time 1.101, loss:883.3811
g_step 1900, step 228, avg_time 1.089, loss:893.2621
g_step 2000, step 328, avg_time 1.092, loss:912.3832
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5680, rec:0.5604, f1:0.5642
>> valid relation prec:0.4953, rec:0.0885, f1:0.1502
>> valid relation with NER prec:0.4953, rec:0.0885, f1:0.1502
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 10, avg_time 3.440, loss:887.3435
g_step 2200, step 110, avg_time 1.089, loss:847.5424
g_step 2300, step 210, avg_time 1.081, loss:843.3113
g_step 2400, step 310, avg_time 1.097, loss:848.9931
g_step 2500, step 410, avg_time 1.097, loss:867.7159
>> valid entity prec:0.5332, rec:0.4706, f1:0.4999
>> valid relation prec:0.5163, rec:0.0734, f1:0.1285
>> valid relation with NER prec:0.5163, rec:0.0734, f1:0.1285
g_step 2600, step 92, avg_time 3.446, loss:827.7290
g_step 2700, step 192, avg_time 1.086, loss:794.9228
g_step 2800, step 292, avg_time 1.098, loss:819.7697
g_step 2900, step 392, avg_time 1.086, loss:832.0666
g_step 3000, step 74, avg_time 1.087, loss:773.9019
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5500, rec:0.5399, f1:0.5449
>> valid relation prec:0.4305, rec:0.0984, f1:0.1602
>> valid relation with NER prec:0.4305, rec:0.0984, f1:0.1602
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 174, avg_time 3.445, loss:777.1875
g_step 3200, step 274, avg_time 1.077, loss:783.1269
g_step 3300, step 374, avg_time 1.084, loss:815.1679
g_step 3400, step 56, avg_time 1.091, loss:757.2080
g_step 3500, step 156, avg_time 1.094, loss:754.4811
>> valid entity prec:0.5869, rec:0.5362, f1:0.5604
>> valid relation prec:0.4197, rec:0.1019, f1:0.1640
>> valid relation with NER prec:0.4197, rec:0.1019, f1:0.1640
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 256, avg_time 3.436, loss:736.6384
g_step 3700, step 356, avg_time 1.091, loss:760.3891
g_step 3800, step 38, avg_time 1.072, loss:736.3636
g_step 3900, step 138, avg_time 1.087, loss:728.9635
g_step 4000, step 238, avg_time 1.087, loss:713.3058
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5967, rec:0.5087, f1:0.5492
>> valid relation prec:0.4315, rec:0.0791, f1:0.1337
>> valid relation with NER prec:0.4315, rec:0.0791, f1:0.1337
g_step 4100, step 338, avg_time 3.427, loss:735.0386
g_step 4200, step 20, avg_time 1.102, loss:740.2211
g_step 4300, step 120, avg_time 1.098, loss:700.1230
g_step 4400, step 220, avg_time 1.091, loss:699.2273
g_step 4500, step 320, avg_time 1.088, loss:710.8144
>> valid entity prec:0.5162, rec:0.5532, f1:0.5341
>> valid relation prec:0.4650, rec:0.0850, f1:0.1437
>> valid relation with NER prec:0.4650, rec:0.0850, f1:0.1437
g_step 4600, step 2, avg_time 3.443, loss:680.4817
g_step 4700, step 102, avg_time 1.088, loss:645.2207
g_step 4800, step 202, avg_time 1.089, loss:698.5777
g_step 4900, step 302, avg_time 1.097, loss:677.1364
g_step 5000, step 402, avg_time 1.100, loss:681.8864
learning rate was adjusted to 0.0008
>> valid entity prec:0.5808, rec:0.5143, f1:0.5455
>> valid relation prec:0.4179, rec:0.0899, f1:0.1480
>> valid relation with NER prec:0.4179, rec:0.0899, f1:0.1480
g_step 5100, step 84, avg_time 3.442, loss:663.4121
g_step 5200, step 184, avg_time 1.106, loss:653.9091
g_step 5300, step 284, avg_time 1.086, loss:658.5318
g_step 5400, step 384, avg_time 1.079, loss:658.1625
g_step 5500, step 66, avg_time 1.092, loss:617.7435
>> valid entity prec:0.5861, rec:0.5102, f1:0.5455
>> valid relation prec:0.4026, rec:0.0992, f1:0.1591
>> valid relation with NER prec:0.4026, rec:0.0992, f1:0.1591
g_step 5600, step 166, avg_time 3.436, loss:639.7295
g_step 5700, step 266, avg_time 1.089, loss:643.2116
g_step 5800, step 366, avg_time 1.090, loss:625.5266
g_step 5900, step 48, avg_time 1.088, loss:630.6363
g_step 6000, step 148, avg_time 1.090, loss:602.6728
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5765, rec:0.5021, f1:0.5367
>> valid relation prec:0.3777, rec:0.0950, f1:0.1518
>> valid relation with NER prec:0.3777, rec:0.0950, f1:0.1518
g_step 6100, step 248, avg_time 3.436, loss:614.8864
g_step 6200, step 348, avg_time 1.080, loss:620.9441
g_step 6300, step 30, avg_time 1.099, loss:605.5805
g_step 6400, step 130, avg_time 1.101, loss:585.5900
g_step 6500, step 230, avg_time 1.085, loss:575.4332
>> valid entity prec:0.5403, rec:0.5318, f1:0.5360
>> valid relation prec:0.2985, rec:0.0753, f1:0.1202
>> valid relation with NER prec:0.2985, rec:0.0753, f1:0.1202
g_step 6600, step 330, avg_time 3.442, loss:611.5839
g_step 6700, step 12, avg_time 1.078, loss:619.8763
g_step 6800, step 112, avg_time 1.085, loss:545.8699
g_step 6900, step 212, avg_time 1.095, loss:578.4701
g_step 7000, step 312, avg_time 1.082, loss:602.3763
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5507, rec:0.4988, f1:0.5235
>> valid relation prec:0.3132, rec:0.0814, f1:0.1293
>> valid relation with NER prec:0.3132, rec:0.0814, f1:0.1293
g_step 7100, step 412, avg_time 3.451, loss:597.3197
g_step 7200, step 94, avg_time 1.080, loss:552.0744
g_step 7300, step 194, avg_time 1.093, loss:554.4420
g_step 7400, step 294, avg_time 1.085, loss:545.4729
g_step 7500, step 394, avg_time 1.094, loss:584.6473
>> valid entity prec:0.5339, rec:0.4711, f1:0.5006
>> valid relation prec:0.3135, rec:0.0993, f1:0.1509
>> valid relation with NER prec:0.3135, rec:0.0993, f1:0.1509
g_step 7600, step 76, avg_time 3.437, loss:535.3162
g_step 7700, step 176, avg_time 1.081, loss:522.8895
g_step 7800, step 276, avg_time 1.097, loss:541.9778
g_step 7900, step 376, avg_time 1.088, loss:549.3996
g_step 8000, step 58, avg_time 1.090, loss:527.2470
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5777, rec:0.4365, f1:0.4973
>> valid relation prec:0.3043, rec:0.0802, f1:0.1269
>> valid relation with NER prec:0.3043, rec:0.0802, f1:0.1269
g_step 8100, step 158, avg_time 3.437, loss:515.4733
g_step 8200, step 258, avg_time 1.079, loss:529.6059
g_step 8300, step 358, avg_time 1.079, loss:532.7028
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 05:01:00 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 05:01:00 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_05-01-00_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 05:01:01 - WARNING - datasets.builder -   Using custom data configuration default-795c8a865296cbcd
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-795c8a865296cbcd/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 05:01:01,361 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:01:01,362 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:01:01,362 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:01:01,363 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:01:01,375 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:01:01,378 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:01:01,378 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:01:01,378 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:01:01,378 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:01:01,378 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:01:01,378 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 05:01:01,539 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:01:04,674 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 05:01:04,677 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-795c8a865296cbcd/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 05:01:04 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x145bb133b560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.66ba/s] 18%|█▊        | 2/11 [00:00<00:03,  2.73ba/s] 27%|██▋       | 3/11 [00:00<00:02,  3.36ba/s] 36%|███▋      | 4/11 [00:01<00:01,  3.75ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.02ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.18ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.27ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.34ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.42ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.48ba/s]100%|██████████| 11/11 [00:02<00:00,  4.34ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.79ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.29ba/s] 43%|████▎     | 3/7 [00:00<00:01,  3.74ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  4.00ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.15ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.23ba/s]100%|██████████| 7/7 [00:01<00:00,  5.05ba/s]100%|██████████| 7/7 [00:01<00:00,  4.26ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  7.99ba/s] 18%|█▊        | 2/11 [00:00<00:00,  9.04ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.40ba/s] 36%|███▋      | 4/11 [00:00<00:00,  9.61ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.56ba/s] 55%|█████▍    | 6/11 [00:00<00:00,  9.70ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.78ba/s] 73%|███████▎  | 8/11 [00:00<00:00,  9.82ba/s] 82%|████████▏ | 9/11 [00:00<00:00,  9.83ba/s] 91%|█████████ | 10/11 [00:01<00:00,  9.86ba/s]100%|██████████| 11/11 [00:01<00:00, 10.41ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.62ba/s] 29%|██▊       | 2/7 [00:00<00:00,  6.75ba/s] 43%|████▎     | 3/7 [00:00<00:00,  6.78ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  8.30ba/s]100%|██████████| 7/7 [00:00<00:00,  9.98ba/s]100%|██████████| 7/7 [00:00<00:00,  8.60ba/s]
[INFO|trainer.py:414] 2023-08-29 05:01:11,890 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 05:01:11,995 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 05:01:11,995 >>   Num examples = 10180
[INFO|trainer.py:1149] 2023-08-29 05:01:11,995 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 05:01:11,995 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 05:01:11,995 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 05:01:11,995 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 05:01:11,995 >>   Total optimization steps = 795
  0%|          | 0/795 [00:00<?, ?it/s]  0%|          | 1/795 [00:00<04:07,  3.21it/s]  0%|          | 2/795 [00:00<03:56,  3.35it/s]  0%|          | 3/795 [00:00<03:52,  3.41it/s]  1%|          | 4/795 [00:01<03:51,  3.42it/s]  1%|          | 5/795 [00:01<03:49,  3.44it/s]  1%|          | 6/795 [00:01<03:48,  3.45it/s]  1%|          | 7/795 [00:02<03:48,  3.45it/s]  1%|          | 8/795 [00:02<03:48,  3.45it/s]  1%|          | 9/795 [00:02<03:47,  3.45it/s]  1%|▏         | 10/795 [00:02<03:47,  3.46it/s]  1%|▏         | 11/795 [00:03<03:46,  3.46it/s]  2%|▏         | 12/795 [00:03<03:46,  3.46it/s]  2%|▏         | 13/795 [00:03<03:46,  3.46it/s]  2%|▏         | 14/795 [00:04<03:45,  3.46it/s]  2%|▏         | 15/795 [00:04<03:45,  3.46it/s]  2%|▏         | 16/795 [00:04<03:44,  3.46it/s]  2%|▏         | 17/795 [00:04<03:44,  3.46it/s]  2%|▏         | 18/795 [00:05<03:44,  3.46it/s]  2%|▏         | 19/795 [00:05<03:45,  3.44it/s]  3%|▎         | 20/795 [00:05<03:44,  3.45it/s]  3%|▎         | 21/795 [00:06<03:44,  3.45it/s]  3%|▎         | 22/795 [00:06<03:43,  3.45it/s]  3%|▎         | 23/795 [00:06<03:43,  3.45it/s]  3%|▎         | 24/795 [00:06<03:43,  3.46it/s]  3%|▎         | 25/795 [00:07<03:42,  3.46it/s]  3%|▎         | 26/795 [00:07<03:42,  3.46it/s]  3%|▎         | 27/795 [00:07<03:42,  3.46it/s]  4%|▎         | 28/795 [00:08<03:41,  3.46it/s]  4%|▎         | 29/795 [00:08<03:41,  3.46it/s]  4%|▍         | 30/795 [00:08<03:41,  3.45it/s]  4%|▍         | 31/795 [00:08<03:41,  3.45it/s]  4%|▍         | 32/795 [00:09<03:41,  3.45it/s]  4%|▍         | 33/795 [00:09<03:40,  3.45it/s]  4%|▍         | 34/795 [00:09<03:40,  3.45it/s]  4%|▍         | 35/795 [00:10<03:40,  3.45it/s]  5%|▍         | 36/795 [00:10<03:40,  3.45it/s]  5%|▍         | 37/795 [00:10<03:39,  3.45it/s]  5%|▍         | 38/795 [00:11<03:39,  3.45it/s]  5%|▍         | 39/795 [00:11<03:38,  3.45it/s]  5%|▌         | 40/795 [00:11<03:38,  3.45it/s]  5%|▌         | 41/795 [00:11<03:39,  3.44it/s]  5%|▌         | 42/795 [00:12<03:38,  3.44it/s]  5%|▌         | 43/795 [00:12<03:38,  3.45it/s]  6%|▌         | 44/795 [00:12<03:37,  3.45it/s]  6%|▌         | 45/795 [00:13<03:37,  3.45it/s]  6%|▌         | 46/795 [00:13<03:36,  3.45it/s]  6%|▌         | 47/795 [00:13<03:36,  3.45it/s]  6%|▌         | 48/795 [00:13<03:36,  3.45it/s]  6%|▌         | 49/795 [00:14<03:35,  3.45it/s]  6%|▋         | 50/795 [00:14<03:35,  3.45it/s]  6%|▋         | 51/795 [00:14<03:35,  3.46it/s]  7%|▋         | 52/795 [00:15<03:37,  3.42it/s]  7%|▋         | 53/795 [00:15<03:36,  3.43it/s]  7%|▋         | 54/795 [00:15<03:35,  3.44it/s]  7%|▋         | 55/795 [00:15<03:34,  3.44it/s]  7%|▋         | 56/795 [00:16<03:34,  3.45it/s]  7%|▋         | 57/795 [00:16<03:33,  3.45it/s]  7%|▋         | 58/795 [00:16<03:33,  3.45it/s]  7%|▋         | 59/795 [00:17<03:33,  3.45it/s]  8%|▊         | 60/795 [00:17<03:32,  3.45it/s]  8%|▊         | 61/795 [00:17<03:32,  3.45it/s]  8%|▊         | 62/795 [00:17<03:32,  3.45it/s]  8%|▊         | 63/795 [00:18<03:33,  3.44it/s]  8%|▊         | 64/795 [00:18<03:32,  3.44it/s]  8%|▊         | 65/795 [00:18<03:31,  3.44it/s]  8%|▊         | 66/795 [00:19<03:31,  3.45it/s]  8%|▊         | 67/795 [00:19<03:31,  3.45it/s]  9%|▊         | 68/795 [00:19<03:30,  3.45it/s]  9%|▊         | 69/795 [00:20<03:30,  3.45it/s]  9%|▉         | 70/795 [00:20<03:30,  3.45it/s]  9%|▉         | 71/795 [00:20<03:29,  3.45it/s]  9%|▉         | 72/795 [00:20<03:29,  3.45it/s]  9%|▉         | 73/795 [00:21<03:29,  3.45it/s]  9%|▉         | 74/795 [00:21<03:28,  3.45it/s]  9%|▉         | 75/795 [00:21<03:28,  3.45it/s] 10%|▉         | 76/795 [00:22<03:28,  3.45it/s] 10%|▉         | 77/795 [00:22<03:28,  3.45it/s] 10%|▉         | 78/795 [00:22<03:27,  3.45it/s] 10%|▉         | 79/795 [00:22<03:27,  3.45it/s] 10%|█         | 80/795 [00:23<03:28,  3.44it/s] 10%|█         | 81/795 [00:23<03:27,  3.44it/s] 10%|█         | 82/795 [00:23<03:27,  3.44it/s] 10%|█         | 83/795 [00:24<03:26,  3.44it/s] 11%|█         | 84/795 [00:24<03:26,  3.45it/s] 11%|█         | 85/795 [00:24<03:26,  3.45it/s] 11%|█         | 86/795 [00:24<03:25,  3.45it/s] 11%|█         | 87/795 [00:25<03:25,  3.45it/s] 11%|█         | 88/795 [00:25<03:24,  3.45it/s] 11%|█         | 89/795 [00:25<03:24,  3.45it/s] 11%|█▏        | 90/795 [00:26<03:24,  3.45it/s] 11%|█▏        | 91/795 [00:26<03:24,  3.44it/s] 12%|█▏        | 92/795 [00:26<03:24,  3.45it/s] 12%|█▏        | 93/795 [00:26<03:23,  3.45it/s] 12%|█▏        | 94/795 [00:27<03:23,  3.45it/s] 12%|█▏        | 95/795 [00:27<03:23,  3.45it/s] 12%|█▏        | 96/795 [00:27<03:22,  3.45it/s] 12%|█▏        | 97/795 [00:28<03:22,  3.45it/s] 12%|█▏        | 98/795 [00:28<03:22,  3.45it/s] 12%|█▏        | 99/795 [00:28<03:21,  3.45it/s] 13%|█▎        | 100/795 [00:29<03:21,  3.45it/s] 13%|█▎        | 101/795 [00:29<03:21,  3.45it/s] 13%|█▎        | 102/795 [00:29<03:21,  3.44it/s] 13%|█▎        | 103/795 [00:29<03:21,  3.44it/s] 13%|█▎        | 104/795 [00:30<03:20,  3.44it/s] 13%|█▎        | 105/795 [00:30<03:20,  3.44it/s] 13%|█▎        | 106/795 [00:30<03:20,  3.44it/s] 13%|█▎        | 107/795 [00:31<03:19,  3.44it/s] 14%|█▎        | 108/795 [00:31<03:19,  3.44it/s] 14%|█▎        | 109/795 [00:31<03:19,  3.44it/s] 14%|█▍        | 110/795 [00:31<03:18,  3.44it/s] 14%|█▍        | 111/795 [00:32<03:18,  3.44it/s] 14%|█▍        | 112/795 [00:32<03:18,  3.44it/s] 14%|█▍        | 113/795 [00:32<03:18,  3.44it/s] 14%|█▍        | 114/795 [00:33<03:18,  3.44it/s] 14%|█▍        | 115/795 [00:33<03:17,  3.44it/s] 15%|█▍        | 116/795 [00:33<03:17,  3.44it/s] 15%|█▍        | 117/795 [00:33<03:17,  3.44it/s] 15%|█▍        | 118/795 [00:34<03:16,  3.44it/s] 15%|█▍        | 119/795 [00:34<03:16,  3.44it/s] 15%|█▌        | 120/795 [00:34<03:16,  3.44it/s] 15%|█▌        | 121/795 [00:35<03:15,  3.44it/s] 15%|█▌        | 122/795 [00:35<03:15,  3.44it/s] 15%|█▌        | 123/795 [00:35<03:15,  3.44it/s] 16%|█▌        | 124/795 [00:35<03:15,  3.43it/s] 16%|█▌        | 125/795 [00:36<03:15,  3.44it/s] 16%|█▌        | 126/795 [00:36<03:14,  3.44it/s] 16%|█▌        | 127/795 [00:36<03:14,  3.43it/s] 16%|█▌        | 128/795 [00:37<03:14,  3.43it/s] 16%|█▌        | 129/795 [00:37<03:13,  3.44it/s] 16%|█▋        | 130/795 [00:37<03:13,  3.43it/s] 16%|█▋        | 131/795 [00:38<03:13,  3.44it/s] 17%|█▋        | 132/795 [00:38<03:17,  3.36it/s] 17%|█▋        | 133/795 [00:38<03:15,  3.38it/s] 17%|█▋        | 134/795 [00:38<03:14,  3.40it/s] 17%|█▋        | 135/795 [00:39<03:14,  3.39it/s] 17%|█▋        | 136/795 [00:39<03:13,  3.41it/s] 17%|█▋        | 137/795 [00:39<03:12,  3.42it/s] 17%|█▋        | 138/795 [00:40<03:11,  3.43it/s] 17%|█▋        | 139/795 [00:40<03:11,  3.43it/s] 18%|█▊        | 140/795 [00:40<03:10,  3.43it/s] 18%|█▊        | 141/795 [00:40<03:10,  3.43it/s] 18%|█▊        | 142/795 [00:41<03:10,  3.44it/s] 18%|█▊        | 143/795 [00:41<03:09,  3.44it/s] 18%|█▊        | 144/795 [00:41<03:09,  3.44it/s] 18%|█▊        | 145/795 [00:42<03:08,  3.44it/s] 18%|█▊        | 146/795 [00:42<03:08,  3.44it/s] 18%|█▊        | 147/795 [00:42<03:08,  3.44it/s] 19%|█▊        | 148/795 [00:42<03:08,  3.44it/s] 19%|█▊        | 149/795 [00:43<03:07,  3.44it/s] 19%|█▉        | 150/795 [00:43<03:07,  3.44it/s] 19%|█▉        | 151/795 [00:43<03:07,  3.44it/s] 19%|█▉        | 152/795 [00:44<03:06,  3.44it/s] 19%|█▉        | 153/795 [00:44<03:06,  3.44it/s] 19%|█▉        | 154/795 [00:44<03:06,  3.44it/s] 19%|█▉        | 155/795 [00:45<03:06,  3.44it/s] 20%|█▉        | 156/795 [00:45<03:05,  3.44it/s] 20%|█▉        | 157/795 [00:45<03:06,  3.41it/s] 20%|█▉        | 158/795 [00:45<03:06,  3.42it/s] 20%|██        | 159/795 [00:46<03:05,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:01:58,234 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:01:58,234 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 05:01:58,234 >>   Batch size = 8

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.69it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.12it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.44it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.37it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.09it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.89it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.76it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.39it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.31it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.36it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.44it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.38it/s][A
  8%|▊         | 68/811 [00:01<00:15, 46.49it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.35it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.41it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.39it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.33it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 46.27it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.31it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.27it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.26it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.39it/s][A
 15%|█▍        | 118/811 [00:02<00:14, 46.49it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.43it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.33it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.19it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.28it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 46.26it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.34it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.31it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.28it/s][A
 20%|██        | 163/811 [00:03<00:13, 46.37it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.37it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.39it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.38it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.35it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 46.35it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.36it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.29it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.32it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.36it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.43it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.31it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.32it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.31it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 46.32it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.35it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.33it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.04it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.20it/s][A
 32%|███▏      | 258/811 [00:05<00:11, 46.28it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.28it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.30it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.27it/s][A
 34%|███▍      | 278/811 [00:05<00:11, 46.20it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.25it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.35it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.32it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.30it/s][A
 37%|███▋      | 303/811 [00:06<00:10, 46.38it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.31it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.32it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 46.32it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 46.25it/s][A
 40%|████      | 328/811 [00:07<00:10, 46.37it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.34it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.28it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 46.28it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.29it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.40it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.31it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 46.30it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 46.22it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.35it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.33it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.25it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.32it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.28it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.23it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 46.34it/s][A
 50%|█████     | 408/811 [00:08<00:08, 46.38it/s][A
 51%|█████     | 413/811 [00:08<00:08, 46.27it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.29it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.31it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.23it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.30it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.32it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.22it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.23it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 46.22it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 46.27it/s][A
 57%|█████▋    | 463/811 [00:09<00:07, 46.23it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.21it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.19it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.24it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.26it/s][A
 60%|██████    | 488/811 [00:10<00:06, 46.31it/s][A
 61%|██████    | 493/811 [00:10<00:06, 46.26it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 46.38it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.25it/s][A
 63%|██████▎   | 508/811 [00:10<00:06, 46.27it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.20it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 46.23it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.33it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.33it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.32it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.21it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 46.25it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.34it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 46.34it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 46.24it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.23it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.16it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.25it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.27it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.18it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.18it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.25it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.31it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.22it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.19it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.25it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.26it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.31it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.28it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.12it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 46.05it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.07it/s][A
 80%|███████▉  | 648/811 [00:13<00:03, 46.00it/s][A
 81%|████████  | 653/811 [00:14<00:03, 45.90it/s][A
 81%|████████  | 658/811 [00:14<00:03, 45.91it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.04it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 46.16it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.18it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 46.24it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 46.25it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.12it/s][A
 85%|████████▌ | 693/811 [00:14<00:02, 46.13it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 46.18it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 46.22it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 46.26it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.27it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 46.20it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 46.22it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 46.21it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 46.24it/s][A
 91%|█████████ | 738/811 [00:15<00:01, 46.25it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 46.24it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 46.05it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 46.06it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 46.03it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 46.07it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 46.05it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 45.95it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 45.90it/s][A
 97%|█████████▋| 783/811 [00:16<00:00, 45.91it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 45.87it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 45.81it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 45.86it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.01it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 46.08it/s][A                                                 
                                                 [A 20%|██        | 159/795 [01:03<03:05,  3.43it/s]
100%|██████████| 811/811 [00:17<00:00, 46.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:02:16,033 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-159
[INFO|configuration_utils.py:351] 2023-08-29 05:02:16,273 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-159/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:02:20,217 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-159/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:02:20,234 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-159/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:02:20,244 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-159/special_tokens_map.json
 20%|██        | 160/795 [01:12<1:26:55,  8.21s/it] 20%|██        | 161/795 [01:13<1:01:40,  5.84s/it] 20%|██        | 162/795 [01:13<44:01,  4.17s/it]   21%|██        | 163/795 [01:13<31:41,  3.01s/it] 21%|██        | 164/795 [01:14<23:03,  2.19s/it] 21%|██        | 165/795 [01:14<17:01,  1.62s/it] 21%|██        | 166/795 [01:14<12:48,  1.22s/it] 21%|██        | 167/795 [01:14<09:51,  1.06it/s] 21%|██        | 168/795 [01:15<07:48,  1.34it/s] 21%|██▏       | 169/795 [01:15<06:21,  1.64it/s] 21%|██▏       | 170/795 [01:15<05:21,  1.95it/s] 22%|██▏       | 171/795 [01:16<04:38,  2.24it/s] 22%|██▏       | 172/795 [01:16<04:09,  2.50it/s] 22%|██▏       | 173/795 [01:16<03:48,  2.72it/s] 22%|██▏       | 174/795 [01:16<03:33,  2.91it/s] 22%|██▏       | 175/795 [01:17<03:23,  3.05it/s] 22%|██▏       | 176/795 [01:17<03:16,  3.16it/s] 22%|██▏       | 177/795 [01:17<03:10,  3.24it/s] 22%|██▏       | 178/795 [01:18<03:07,  3.30it/s] 23%|██▎       | 179/795 [01:18<03:04,  3.34it/s] 23%|██▎       | 180/795 [01:18<03:02,  3.37it/s] 23%|██▎       | 181/795 [01:18<03:00,  3.39it/s] 23%|██▎       | 182/795 [01:19<02:59,  3.41it/s] 23%|██▎       | 183/795 [01:19<02:59,  3.41it/s] 23%|██▎       | 184/795 [01:19<02:58,  3.42it/s] 23%|██▎       | 185/795 [01:20<02:57,  3.43it/s] 23%|██▎       | 186/795 [01:20<02:57,  3.43it/s] 24%|██▎       | 187/795 [01:20<02:56,  3.44it/s] 24%|██▎       | 188/795 [01:21<02:56,  3.44it/s] 24%|██▍       | 189/795 [01:21<02:56,  3.44it/s] 24%|██▍       | 190/795 [01:21<02:55,  3.44it/s] 24%|██▍       | 191/795 [01:21<02:55,  3.44it/s] 24%|██▍       | 192/795 [01:22<02:55,  3.45it/s] 24%|██▍       | 193/795 [01:22<02:54,  3.44it/s] 24%|██▍       | 194/795 [01:22<02:54,  3.45it/s] 25%|██▍       | 195/795 [01:23<02:54,  3.45it/s] 25%|██▍       | 196/795 [01:23<02:53,  3.45it/s] 25%|██▍       | 197/795 [01:23<02:53,  3.44it/s] 25%|██▍       | 198/795 [01:23<02:53,  3.44it/s] 25%|██▌       | 199/795 [01:24<02:53,  3.44it/s] 25%|██▌       | 200/795 [01:24<02:53,  3.43it/s] 25%|██▌       | 201/795 [01:24<02:53,  3.43it/s] 25%|██▌       | 202/795 [01:25<02:52,  3.43it/s] 26%|██▌       | 203/795 [01:25<02:52,  3.44it/s] 26%|██▌       | 204/795 [01:25<02:51,  3.44it/s] 26%|██▌       | 205/795 [01:25<02:51,  3.44it/s] 26%|██▌       | 206/795 [01:26<02:51,  3.44it/s] 26%|██▌       | 207/795 [01:26<02:50,  3.44it/s] 26%|██▌       | 208/795 [01:26<02:50,  3.44it/s] 26%|██▋       | 209/795 [01:27<02:50,  3.44it/s] 26%|██▋       | 210/795 [01:27<02:50,  3.44it/s] 27%|██▋       | 211/795 [01:27<02:50,  3.43it/s] 27%|██▋       | 212/795 [01:28<02:49,  3.43it/s] 27%|██▋       | 213/795 [01:28<02:49,  3.43it/s] 27%|██▋       | 214/795 [01:28<02:49,  3.44it/s] 27%|██▋       | 215/795 [01:28<02:48,  3.44it/s] 27%|██▋       | 216/795 [01:29<02:48,  3.44it/s] 27%|██▋       | 217/795 [01:29<02:48,  3.43it/s] 27%|██▋       | 218/795 [01:29<02:47,  3.44it/s] 28%|██▊       | 219/795 [01:30<02:47,  3.43it/s] 28%|██▊       | 220/795 [01:30<02:47,  3.43it/s] 28%|██▊       | 221/795 [01:30<02:47,  3.44it/s] 28%|██▊       | 222/795 [01:30<02:47,  3.42it/s] 28%|██▊       | 223/795 [01:31<02:46,  3.43it/s] 28%|██▊       | 224/795 [01:31<02:46,  3.43it/s] 28%|██▊       | 225/795 [01:31<02:45,  3.43it/s] 28%|██▊       | 226/795 [01:32<02:45,  3.43it/s] 29%|██▊       | 227/795 [01:32<02:45,  3.44it/s] 29%|██▊       | 228/795 [01:32<02:44,  3.44it/s] 29%|██▉       | 229/795 [01:32<02:44,  3.44it/s] 29%|██▉       | 230/795 [01:33<02:44,  3.44it/s] 29%|██▉       | 231/795 [01:33<02:44,  3.44it/s] 29%|██▉       | 232/795 [01:33<02:43,  3.44it/s] 29%|██▉       | 233/795 [01:34<02:43,  3.43it/s] 29%|██▉       | 234/795 [01:34<02:43,  3.43it/s] 30%|██▉       | 235/795 [01:34<02:42,  3.44it/s] 30%|██▉       | 236/795 [01:34<02:42,  3.43it/s] 30%|██▉       | 237/795 [01:35<02:42,  3.44it/s] 30%|██▉       | 238/795 [01:35<02:42,  3.43it/s] 30%|███       | 239/795 [01:35<02:41,  3.43it/s] 30%|███       | 240/795 [01:36<02:41,  3.43it/s] 30%|███       | 241/795 [01:36<02:41,  3.43it/s] 30%|███       | 242/795 [01:36<02:40,  3.44it/s] 31%|███       | 243/795 [01:37<02:40,  3.44it/s] 31%|███       | 244/795 [01:37<02:40,  3.43it/s] 31%|███       | 245/795 [01:37<02:40,  3.43it/s] 31%|███       | 246/795 [01:37<02:39,  3.43it/s] 31%|███       | 247/795 [01:38<02:39,  3.43it/s] 31%|███       | 248/795 [01:38<02:39,  3.43it/s] 31%|███▏      | 249/795 [01:38<02:38,  3.44it/s] 31%|███▏      | 250/795 [01:39<02:38,  3.44it/s] 32%|███▏      | 251/795 [01:39<02:38,  3.44it/s] 32%|███▏      | 252/795 [01:39<02:37,  3.44it/s] 32%|███▏      | 253/795 [01:39<02:37,  3.44it/s] 32%|███▏      | 254/795 [01:40<02:37,  3.44it/s] 32%|███▏      | 255/795 [01:40<02:37,  3.43it/s] 32%|███▏      | 256/795 [01:40<02:37,  3.43it/s] 32%|███▏      | 257/795 [01:41<02:36,  3.43it/s] 32%|███▏      | 258/795 [01:41<02:36,  3.43it/s] 33%|███▎      | 259/795 [01:41<02:36,  3.43it/s] 33%|███▎      | 260/795 [01:41<02:35,  3.44it/s] 33%|███▎      | 261/795 [01:42<02:35,  3.43it/s] 33%|███▎      | 262/795 [01:42<02:35,  3.44it/s] 33%|███▎      | 263/795 [01:42<02:34,  3.43it/s] 33%|███▎      | 264/795 [01:43<02:34,  3.44it/s] 33%|███▎      | 265/795 [01:43<02:34,  3.44it/s] 33%|███▎      | 266/795 [01:43<02:34,  3.43it/s] 34%|███▎      | 267/795 [01:44<02:33,  3.43it/s] 34%|███▎      | 268/795 [01:44<02:33,  3.43it/s] 34%|███▍      | 269/795 [01:44<02:33,  3.44it/s] 34%|███▍      | 270/795 [01:44<02:32,  3.44it/s] 34%|███▍      | 271/795 [01:45<02:32,  3.44it/s] 34%|███▍      | 272/795 [01:45<02:32,  3.44it/s] 34%|███▍      | 273/795 [01:45<02:31,  3.44it/s] 34%|███▍      | 274/795 [01:46<02:31,  3.44it/s] 35%|███▍      | 275/795 [01:46<02:31,  3.44it/s] 35%|███▍      | 276/795 [01:46<02:30,  3.44it/s] 35%|███▍      | 277/795 [01:46<02:31,  3.42it/s] 35%|███▍      | 278/795 [01:47<02:31,  3.42it/s] 35%|███▌      | 279/795 [01:47<02:30,  3.43it/s] 35%|███▌      | 280/795 [01:47<02:30,  3.43it/s] 35%|███▌      | 281/795 [01:48<02:29,  3.43it/s] 35%|███▌      | 282/795 [01:48<02:29,  3.43it/s] 36%|███▌      | 283/795 [01:48<02:29,  3.44it/s] 36%|███▌      | 284/795 [01:48<02:28,  3.43it/s] 36%|███▌      | 285/795 [01:49<02:28,  3.44it/s] 36%|███▌      | 286/795 [01:49<02:28,  3.43it/s] 36%|███▌      | 287/795 [01:49<02:27,  3.44it/s] 36%|███▌      | 288/795 [01:50<02:27,  3.43it/s] 36%|███▋      | 289/795 [01:50<02:27,  3.43it/s] 36%|███▋      | 290/795 [01:50<02:26,  3.44it/s] 37%|███▋      | 291/795 [01:51<02:26,  3.44it/s] 37%|███▋      | 292/795 [01:51<02:26,  3.44it/s] 37%|███▋      | 293/795 [01:51<02:26,  3.44it/s] 37%|███▋      | 294/795 [01:51<02:25,  3.44it/s] 37%|███▋      | 295/795 [01:52<02:25,  3.43it/s] 37%|███▋      | 296/795 [01:52<02:25,  3.43it/s] 37%|███▋      | 297/795 [01:52<02:25,  3.43it/s] 37%|███▋      | 298/795 [01:53<02:24,  3.43it/s] 38%|███▊      | 299/795 [01:53<02:24,  3.44it/s] 38%|███▊      | 300/795 [01:53<02:24,  3.44it/s] 38%|███▊      | 301/795 [01:53<02:23,  3.44it/s] 38%|███▊      | 302/795 [01:54<02:23,  3.43it/s] 38%|███▊      | 303/795 [01:54<02:23,  3.44it/s] 38%|███▊      | 304/795 [01:54<02:22,  3.44it/s] 38%|███▊      | 305/795 [01:55<02:22,  3.44it/s] 38%|███▊      | 306/795 [01:55<02:22,  3.43it/s] 39%|███▊      | 307/795 [01:55<02:22,  3.43it/s] 39%|███▊      | 308/795 [01:55<02:21,  3.44it/s] 39%|███▉      | 309/795 [01:56<02:21,  3.43it/s] 39%|███▉      | 310/795 [01:56<02:21,  3.43it/s] 39%|███▉      | 311/795 [01:56<02:20,  3.44it/s] 39%|███▉      | 312/795 [01:57<02:20,  3.44it/s] 39%|███▉      | 313/795 [01:57<02:20,  3.44it/s] 39%|███▉      | 314/795 [01:57<02:19,  3.44it/s] 40%|███▉      | 315/795 [01:57<02:19,  3.44it/s] 40%|███▉      | 316/795 [01:58<02:19,  3.44it/s] 40%|███▉      | 317/795 [01:58<02:19,  3.43it/s] 40%|████      | 318/795 [01:58<02:18,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:03:10,908 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:03:10,908 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 05:03:10,908 >>   Batch size = 8
{'eval_loss': 0.8977049589157104, 'eval_runtime': 17.538, 'eval_samples_per_second': 369.882, 'eval_steps_per_second': 46.242, 'epoch': 1.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.63it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.07it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.42it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.72it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.33it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.98it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.81it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.45it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.26it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.30it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.23it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.22it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.33it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.30it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.39it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.42it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.31it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 46.16it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.21it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.26it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.19it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.25it/s][A
 15%|█▍        | 118/811 [00:02<00:14, 46.37it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.28it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.37it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.25it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.21it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 46.24it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.26it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.12it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.19it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.27it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.34it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.37it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.29it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.22it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 46.27it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.14it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.24it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.16it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.27it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.25it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.30it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.27it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.22it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 46.20it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.24it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.19it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.25it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.20it/s][A
 32%|███▏      | 258/811 [00:05<00:11, 46.27it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.21it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.24it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.26it/s][A
 34%|███▍      | 278/811 [00:05<00:11, 46.13it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.21it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.26it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.25it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.20it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 46.18it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.20it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.27it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 46.20it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 46.24it/s][A
 40%|████      | 328/811 [00:07<00:10, 46.20it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.24it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.18it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 46.05it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.08it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.20it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.28it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 46.27it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 46.20it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.13it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.19it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.27it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.24it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.07it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.00it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 45.96it/s][A
 50%|█████     | 408/811 [00:08<00:08, 45.98it/s][A
 51%|█████     | 413/811 [00:08<00:08, 45.87it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.05it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.07it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.08it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.14it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.20it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.14it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.20it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 46.12it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 46.15it/s][A
 57%|█████▋    | 463/811 [00:09<00:07, 46.22it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.24it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.22it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.31it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.30it/s][A
 60%|██████    | 488/811 [00:10<00:07, 46.14it/s][A
 61%|██████    | 493/811 [00:10<00:06, 46.21it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 46.14it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.22it/s][A
 63%|██████▎   | 508/811 [00:10<00:06, 46.09it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.11it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 45.97it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 45.98it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 45.98it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 45.93it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 45.93it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 45.89it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 45.89it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 45.84it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 45.98it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.03it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.10it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.18it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.05it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.16it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.18it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.23it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.27it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.23it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.18it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.15it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.16it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.20it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.20it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.26it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 46.23it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.14it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 46.14it/s][A
 81%|████████  | 653/811 [00:14<00:03, 46.18it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.22it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.14it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 46.19it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.12it/s][A
 84%|████████▎ | 678/811 [00:14<00:03, 42.65it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 43.69it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 44.55it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 45.11it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 45.53it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 45.62it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 45.83it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 45.93it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 45.65it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 45.69it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 45.85it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 45.89it/s][A
 91%|█████████ | 738/811 [00:15<00:01, 46.05it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 46.17it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 46.27it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 46.25it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 46.18it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 45.94it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 45.84it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 45.86it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 45.96it/s][A
 97%|█████████▋| 783/811 [00:16<00:00, 45.99it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 45.99it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 45.96it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 45.95it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.10it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 45.96it/s][A                                                 
                                                 [A 40%|████      | 318/795 [02:16<02:18,  3.43it/s]
100%|██████████| 811/811 [00:17<00:00, 45.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:03:28,711 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-318
[INFO|configuration_utils.py:351] 2023-08-29 05:03:28,924 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-318/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:03:31,493 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-318/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:03:31,503 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-318/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:03:31,512 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-318/special_tokens_map.json
 40%|████      | 319/795 [02:24<1:02:11,  7.84s/it] 40%|████      | 320/795 [02:24<44:07,  5.57s/it]   40%|████      | 321/795 [02:24<31:30,  3.99s/it] 41%|████      | 322/795 [02:25<22:41,  2.88s/it] 41%|████      | 323/795 [02:25<16:32,  2.10s/it] 41%|████      | 324/795 [02:25<12:14,  1.56s/it] 41%|████      | 325/795 [02:26<09:14,  1.18s/it] 41%|████      | 326/795 [02:26<07:07,  1.10it/s] 41%|████      | 327/795 [02:26<05:39,  1.38it/s] 41%|████▏     | 328/795 [02:26<04:37,  1.68it/s] 41%|████▏     | 329/795 [02:27<03:54,  1.99it/s] 42%|████▏     | 330/795 [02:27<03:24,  2.27it/s] 42%|████▏     | 331/795 [02:27<03:03,  2.53it/s] 42%|████▏     | 332/795 [02:28<02:48,  2.75it/s] 42%|████▏     | 333/795 [02:28<02:37,  2.93it/s] 42%|████▏     | 334/795 [02:28<02:30,  3.07it/s] 42%|████▏     | 335/795 [02:28<02:25,  3.17it/s] 42%|████▏     | 336/795 [02:29<02:21,  3.23it/s] 42%|████▏     | 337/795 [02:29<02:19,  3.29it/s] 43%|████▎     | 338/795 [02:29<02:17,  3.33it/s] 43%|████▎     | 339/795 [02:30<02:15,  3.36it/s] 43%|████▎     | 340/795 [02:30<02:14,  3.39it/s] 43%|████▎     | 341/795 [02:30<02:13,  3.40it/s] 43%|████▎     | 342/795 [02:31<02:12,  3.41it/s] 43%|████▎     | 343/795 [02:31<02:12,  3.42it/s] 43%|████▎     | 344/795 [02:31<02:11,  3.43it/s] 43%|████▎     | 345/795 [02:31<02:11,  3.43it/s] 44%|████▎     | 346/795 [02:32<02:10,  3.43it/s] 44%|████▎     | 347/795 [02:32<02:10,  3.43it/s] 44%|████▍     | 348/795 [02:32<02:10,  3.43it/s] 44%|████▍     | 349/795 [02:33<02:09,  3.43it/s] 44%|████▍     | 350/795 [02:33<02:09,  3.43it/s] 44%|████▍     | 351/795 [02:33<02:09,  3.44it/s] 44%|████▍     | 352/795 [02:33<02:08,  3.44it/s] 44%|████▍     | 353/795 [02:34<02:08,  3.43it/s] 45%|████▍     | 354/795 [02:34<02:08,  3.43it/s] 45%|████▍     | 355/795 [02:34<02:08,  3.43it/s] 45%|████▍     | 356/795 [02:35<02:07,  3.43it/s] 45%|████▍     | 357/795 [02:35<02:07,  3.43it/s] 45%|████▌     | 358/795 [02:35<02:08,  3.40it/s] 45%|████▌     | 359/795 [02:35<02:07,  3.41it/s] 45%|████▌     | 360/795 [02:36<02:07,  3.42it/s] 45%|████▌     | 361/795 [02:36<02:06,  3.43it/s] 46%|████▌     | 362/795 [02:36<02:06,  3.43it/s] 46%|████▌     | 363/795 [02:37<02:06,  3.42it/s] 46%|████▌     | 364/795 [02:37<02:05,  3.43it/s] 46%|████▌     | 365/795 [02:37<02:05,  3.43it/s] 46%|████▌     | 366/795 [02:38<02:04,  3.43it/s] 46%|████▌     | 367/795 [02:38<02:04,  3.43it/s] 46%|████▋     | 368/795 [02:38<02:06,  3.37it/s] 46%|████▋     | 369/795 [02:38<02:06,  3.37it/s] 47%|████▋     | 370/795 [02:39<02:05,  3.39it/s] 47%|████▋     | 371/795 [02:39<02:04,  3.40it/s] 47%|████▋     | 372/795 [02:39<02:03,  3.41it/s] 47%|████▋     | 373/795 [02:40<02:03,  3.42it/s] 47%|████▋     | 374/795 [02:40<02:02,  3.43it/s] 47%|████▋     | 375/795 [02:40<02:02,  3.43it/s] 47%|████▋     | 376/795 [02:40<02:02,  3.43it/s] 47%|████▋     | 377/795 [02:41<02:01,  3.43it/s] 48%|████▊     | 378/795 [02:41<02:01,  3.44it/s] 48%|████▊     | 379/795 [02:41<02:01,  3.44it/s] 48%|████▊     | 380/795 [02:42<02:00,  3.43it/s] 48%|████▊     | 381/795 [02:42<02:00,  3.44it/s] 48%|████▊     | 382/795 [02:42<02:00,  3.44it/s] 48%|████▊     | 383/795 [02:42<01:59,  3.44it/s] 48%|████▊     | 384/795 [02:43<01:59,  3.44it/s] 48%|████▊     | 385/795 [02:43<01:59,  3.44it/s] 49%|████▊     | 386/795 [02:43<01:58,  3.44it/s] 49%|████▊     | 387/795 [02:44<01:58,  3.44it/s] 49%|████▉     | 388/795 [02:44<01:58,  3.44it/s] 49%|████▉     | 389/795 [02:44<01:58,  3.43it/s] 49%|████▉     | 390/795 [02:45<01:58,  3.43it/s] 49%|████▉     | 391/795 [02:45<01:57,  3.43it/s] 49%|████▉     | 392/795 [02:45<01:57,  3.43it/s] 49%|████▉     | 393/795 [02:45<01:57,  3.43it/s] 50%|████▉     | 394/795 [02:46<01:56,  3.43it/s] 50%|████▉     | 395/795 [02:46<01:56,  3.43it/s] 50%|████▉     | 396/795 [02:46<01:56,  3.44it/s] 50%|████▉     | 397/795 [02:47<01:56,  3.42it/s] 50%|█████     | 398/795 [02:47<01:55,  3.43it/s] 50%|█████     | 399/795 [02:47<01:55,  3.43it/s] 50%|█████     | 400/795 [02:47<01:55,  3.43it/s] 50%|█████     | 401/795 [02:48<01:54,  3.44it/s] 51%|█████     | 402/795 [02:48<01:54,  3.42it/s] 51%|█████     | 403/795 [02:48<01:54,  3.42it/s] 51%|█████     | 404/795 [02:49<01:54,  3.43it/s] 51%|█████     | 405/795 [02:49<01:53,  3.43it/s] 51%|█████     | 406/795 [02:49<01:53,  3.43it/s] 51%|█████     | 407/795 [02:49<01:53,  3.43it/s] 51%|█████▏    | 408/795 [02:50<01:52,  3.43it/s] 51%|█████▏    | 409/795 [02:50<01:52,  3.43it/s] 52%|█████▏    | 410/795 [02:50<01:52,  3.43it/s] 52%|█████▏    | 411/795 [02:51<01:51,  3.43it/s] 52%|█████▏    | 412/795 [02:51<01:51,  3.43it/s] 52%|█████▏    | 413/795 [02:51<01:51,  3.43it/s] 52%|█████▏    | 414/795 [02:52<01:51,  3.43it/s] 52%|█████▏    | 415/795 [02:52<01:50,  3.43it/s] 52%|█████▏    | 416/795 [02:52<01:50,  3.43it/s] 52%|█████▏    | 417/795 [02:52<01:50,  3.43it/s] 53%|█████▎    | 418/795 [02:53<01:49,  3.43it/s] 53%|█████▎    | 419/795 [02:53<01:49,  3.44it/s] 53%|█████▎    | 420/795 [02:53<01:49,  3.43it/s] 53%|█████▎    | 421/795 [02:54<01:48,  3.44it/s] 53%|█████▎    | 422/795 [02:54<01:48,  3.43it/s] 53%|█████▎    | 423/795 [02:54<01:48,  3.43it/s] 53%|█████▎    | 424/795 [02:54<01:48,  3.43it/s] 53%|█████▎    | 425/795 [02:55<01:47,  3.43it/s] 54%|█████▎    | 426/795 [02:55<01:47,  3.44it/s] 54%|█████▎    | 427/795 [02:55<01:47,  3.44it/s] 54%|█████▍    | 428/795 [02:56<01:46,  3.44it/s] 54%|█████▍    | 429/795 [02:56<01:46,  3.43it/s] 54%|█████▍    | 430/795 [02:56<01:46,  3.42it/s] 54%|█████▍    | 431/795 [02:56<01:46,  3.42it/s] 54%|█████▍    | 432/795 [02:57<01:45,  3.43it/s] 54%|█████▍    | 433/795 [02:57<01:45,  3.43it/s] 55%|█████▍    | 434/795 [02:57<01:45,  3.43it/s] 55%|█████▍    | 435/795 [02:58<01:44,  3.43it/s] 55%|█████▍    | 436/795 [02:58<01:44,  3.43it/s] 55%|█████▍    | 437/795 [02:58<01:44,  3.43it/s] 55%|█████▌    | 438/795 [02:59<01:43,  3.43it/s] 55%|█████▌    | 439/795 [02:59<01:43,  3.44it/s] 55%|█████▌    | 440/795 [02:59<01:43,  3.43it/s] 55%|█████▌    | 441/795 [02:59<01:43,  3.42it/s] 56%|█████▌    | 442/795 [03:00<01:43,  3.42it/s] 56%|█████▌    | 443/795 [03:00<01:42,  3.43it/s] 56%|█████▌    | 444/795 [03:00<01:42,  3.43it/s] 56%|█████▌    | 445/795 [03:01<01:41,  3.43it/s] 56%|█████▌    | 446/795 [03:01<01:41,  3.43it/s] 56%|█████▌    | 447/795 [03:01<01:41,  3.43it/s] 56%|█████▋    | 448/795 [03:01<01:41,  3.43it/s] 56%|█████▋    | 449/795 [03:02<01:40,  3.43it/s] 57%|█████▋    | 450/795 [03:02<01:40,  3.43it/s] 57%|█████▋    | 451/795 [03:02<01:40,  3.44it/s] 57%|█████▋    | 452/795 [03:03<01:40,  3.43it/s] 57%|█████▋    | 453/795 [03:03<01:39,  3.43it/s] 57%|█████▋    | 454/795 [03:03<01:39,  3.43it/s] 57%|█████▋    | 455/795 [03:03<01:39,  3.43it/s] 57%|█████▋    | 456/795 [03:04<01:38,  3.43it/s] 57%|█████▋    | 457/795 [03:04<01:38,  3.43it/s] 58%|█████▊    | 458/795 [03:04<01:38,  3.43it/s] 58%|█████▊    | 459/795 [03:05<01:37,  3.43it/s] 58%|█████▊    | 460/795 [03:05<01:37,  3.43it/s] 58%|█████▊    | 461/795 [03:05<01:37,  3.43it/s] 58%|█████▊    | 462/795 [03:05<01:36,  3.44it/s] 58%|█████▊    | 463/795 [03:06<01:36,  3.43it/s] 58%|█████▊    | 464/795 [03:06<01:36,  3.43it/s] 58%|█████▊    | 465/795 [03:06<01:36,  3.43it/s] 59%|█████▊    | 466/795 [03:07<01:35,  3.43it/s] 59%|█████▊    | 467/795 [03:07<01:35,  3.43it/s] 59%|█████▉    | 468/795 [03:07<01:35,  3.43it/s] 59%|█████▉    | 469/795 [03:08<01:35,  3.43it/s] 59%|█████▉    | 470/795 [03:08<01:34,  3.43it/s] 59%|█████▉    | 471/795 [03:08<01:34,  3.43it/s] 59%|█████▉    | 472/795 [03:08<01:34,  3.43it/s] 59%|█████▉    | 473/795 [03:09<01:33,  3.43it/s] 60%|█████▉    | 474/795 [03:09<01:33,  3.42it/s] 60%|█████▉    | 475/795 [03:09<01:33,  3.43it/s] 60%|█████▉    | 476/795 [03:10<01:33,  3.43it/s] 60%|██████    | 477/795 [03:10<01:32,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:04:22,410 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:04:22,410 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 05:04:22,410 >>   Batch size = 8
{'eval_loss': 0.9008622765541077, 'eval_runtime': 17.5969, 'eval_samples_per_second': 368.645, 'eval_steps_per_second': 46.088, 'epoch': 2.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.71it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.08it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.20it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.50it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.21it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.99it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.74it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.19it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.11it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.15it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.26it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.26it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.18it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.19it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.20it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.21it/s][A
 11%|█         | 88/811 [00:01<00:15, 45.99it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 45.86it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 45.89it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 45.98it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.17it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.14it/s][A
 15%|█▍        | 118/811 [00:02<00:14, 46.28it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.33it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.13it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.06it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 45.88it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 45.92it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.08it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.20it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.31it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.23it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.29it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.10it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.13it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.04it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 45.97it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.10it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.16it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.20it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.30it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.16it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.29it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.11it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.07it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 45.95it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.16it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.18it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.16it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.28it/s][A
 32%|███▏      | 258/811 [00:05<00:11, 46.28it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.10it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.07it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.13it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.01it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.08it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.12it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.09it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.26it/s][A
 37%|███▋      | 303/811 [00:06<00:10, 46.27it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.19it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.00it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 46.05it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 46.03it/s][A
 40%|████      | 328/811 [00:07<00:10, 46.06it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.21it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.20it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 46.22it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.23it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.02it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.09it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 46.11it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 46.20it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.06it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.08it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.11it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.19it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.28it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.17it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 46.03it/s][A
 50%|█████     | 408/811 [00:08<00:08, 46.11it/s][A
 51%|█████     | 413/811 [00:08<00:08, 46.09it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.10it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.06it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.18it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.16it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.15it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.08it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.10it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 46.15it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 46.13it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.04it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.03it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.02it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.13it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.15it/s][A
 60%|██████    | 488/811 [00:10<00:06, 46.19it/s][A
 61%|██████    | 493/811 [00:10<00:06, 46.13it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 46.07it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.07it/s][A
 63%|██████▎   | 508/811 [00:10<00:06, 46.08it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.10it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 46.06it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.15it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.06it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.04it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.10it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 46.12it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 45.99it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 46.10it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 46.04it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.02it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.06it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.15it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.14it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.15it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.10it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.04it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.08it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.09it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.18it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.09it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.10it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.03it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.09it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.14it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 46.15it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.04it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 46.09it/s][A
 81%|████████  | 653/811 [00:14<00:03, 46.07it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.06it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.16it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 46.15it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.09it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 46.15it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 46.10it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.06it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 46.05it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 46.19it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 46.10it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 46.11it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.13it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 46.07it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 46.18it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 46.18it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 46.06it/s][A
 91%|█████████ | 738/811 [00:15<00:01, 46.09it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 46.09it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 46.15it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 46.14it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 46.11it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 46.08it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 46.13it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 46.05it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 46.04it/s][A
 97%|█████████▋| 783/811 [00:16<00:00, 46.09it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 46.10it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 46.06it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 46.04it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.09it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 46.03it/s][A                                                 
                                                 [A 60%|██████    | 477/795 [03:27<01:32,  3.43it/s]
100%|██████████| 811/811 [00:17<00:00, 46.03it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:04:40,020 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-477
[INFO|configuration_utils.py:351] 2023-08-29 05:04:40,038 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-477/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:04:42,420 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-477/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:04:42,438 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-477/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:04:42,450 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-477/special_tokens_map.json
 60%|██████    | 478/795 [03:35<40:36,  7.68s/it] 60%|██████    | 479/795 [03:35<28:47,  5.47s/it] 60%|██████    | 480/795 [03:35<20:33,  3.91s/it] 61%|██████    | 481/795 [03:36<14:47,  2.83s/it] 61%|██████    | 482/795 [03:36<10:46,  2.07s/it] 61%|██████    | 483/795 [03:36<07:58,  1.53s/it] 61%|██████    | 484/795 [03:37<06:01,  1.16s/it] 61%|██████    | 485/795 [03:37<04:39,  1.11it/s] 61%|██████    | 486/795 [03:37<03:41,  1.39it/s] 61%|██████▏   | 487/795 [03:37<03:01,  1.70it/s] 61%|██████▏   | 488/795 [03:38<02:33,  2.00it/s] 62%|██████▏   | 489/795 [03:38<02:13,  2.29it/s] 62%|██████▏   | 490/795 [03:38<02:02,  2.49it/s] 62%|██████▏   | 491/795 [03:39<01:52,  2.71it/s] 62%|██████▏   | 492/795 [03:39<01:44,  2.89it/s] 62%|██████▏   | 493/795 [03:39<01:39,  3.04it/s] 62%|██████▏   | 494/795 [03:39<01:35,  3.15it/s] 62%|██████▏   | 495/795 [03:40<01:32,  3.23it/s] 62%|██████▏   | 496/795 [03:40<01:30,  3.29it/s] 63%|██████▎   | 497/795 [03:40<01:29,  3.34it/s] 63%|██████▎   | 498/795 [03:41<01:28,  3.37it/s] 63%|██████▎   | 499/795 [03:41<01:27,  3.39it/s] 63%|██████▎   | 500/795 [03:41<01:26,  3.40it/s]                                                  63%|██████▎   | 500/795 [03:41<01:26,  3.40it/s] 63%|██████▎   | 501/795 [03:42<01:26,  3.41it/s] 63%|██████▎   | 502/795 [03:42<01:25,  3.41it/s] 63%|██████▎   | 503/795 [03:42<01:25,  3.42it/s] 63%|██████▎   | 504/795 [03:42<01:24,  3.43it/s] 64%|██████▎   | 505/795 [03:43<01:24,  3.43it/s] 64%|██████▎   | 506/795 [03:43<01:24,  3.43it/s] 64%|██████▍   | 507/795 [03:43<01:23,  3.43it/s] 64%|██████▍   | 508/795 [03:44<01:23,  3.44it/s] 64%|██████▍   | 509/795 [03:44<01:23,  3.44it/s] 64%|██████▍   | 510/795 [03:44<01:22,  3.44it/s] 64%|██████▍   | 511/795 [03:44<01:22,  3.44it/s] 64%|██████▍   | 512/795 [03:45<01:22,  3.43it/s] 65%|██████▍   | 513/795 [03:45<01:22,  3.43it/s] 65%|██████▍   | 514/795 [03:45<01:21,  3.43it/s] 65%|██████▍   | 515/795 [03:46<01:21,  3.43it/s] 65%|██████▍   | 516/795 [03:46<01:21,  3.43it/s] 65%|██████▌   | 517/795 [03:46<01:20,  3.43it/s] 65%|██████▌   | 518/795 [03:46<01:20,  3.44it/s] 65%|██████▌   | 519/795 [03:47<01:20,  3.44it/s] 65%|██████▌   | 520/795 [03:47<01:19,  3.44it/s] 66%|██████▌   | 521/795 [03:47<01:19,  3.44it/s] 66%|██████▌   | 522/795 [03:48<01:19,  3.44it/s] 66%|██████▌   | 523/795 [03:48<01:19,  3.43it/s] 66%|██████▌   | 524/795 [03:48<01:19,  3.43it/s] 66%|██████▌   | 525/795 [03:49<01:18,  3.43it/s] 66%|██████▌   | 526/795 [03:49<01:18,  3.43it/s] 66%|██████▋   | 527/795 [03:49<01:18,  3.43it/s] 66%|██████▋   | 528/795 [03:49<01:17,  3.44it/s] 67%|██████▋   | 529/795 [03:50<01:17,  3.44it/s] 67%|██████▋   | 530/795 [03:50<01:17,  3.44it/s] 67%|██████▋   | 531/795 [03:50<01:16,  3.44it/s] 67%|██████▋   | 532/795 [03:51<01:16,  3.44it/s] 67%|██████▋   | 533/795 [03:51<01:16,  3.43it/s] 67%|██████▋   | 534/795 [03:51<01:16,  3.41it/s] 67%|██████▋   | 535/795 [03:51<01:16,  3.42it/s] 67%|██████▋   | 536/795 [03:52<01:15,  3.42it/s] 68%|██████▊   | 537/795 [03:52<01:15,  3.43it/s] 68%|██████▊   | 538/795 [03:52<01:14,  3.43it/s] 68%|██████▊   | 539/795 [03:53<01:14,  3.43it/s] 68%|██████▊   | 540/795 [03:53<01:14,  3.43it/s] 68%|██████▊   | 541/795 [03:53<01:13,  3.44it/s] 68%|██████▊   | 542/795 [03:53<01:13,  3.43it/s] 68%|██████▊   | 543/795 [03:54<01:13,  3.42it/s] 68%|██████▊   | 544/795 [03:54<01:13,  3.43it/s] 69%|██████▊   | 545/795 [03:54<01:13,  3.42it/s] 69%|██████▊   | 546/795 [03:55<01:12,  3.43it/s] 69%|██████▉   | 547/795 [03:55<01:12,  3.43it/s] 69%|██████▉   | 548/795 [03:55<01:11,  3.43it/s] 69%|██████▉   | 549/795 [03:56<01:11,  3.43it/s] 69%|██████▉   | 550/795 [03:56<01:11,  3.43it/s] 69%|██████▉   | 551/795 [03:56<01:11,  3.44it/s] 69%|██████▉   | 552/795 [03:56<01:10,  3.43it/s] 70%|██████▉   | 553/795 [03:57<01:10,  3.44it/s] 70%|██████▉   | 554/795 [03:57<01:10,  3.43it/s] 70%|██████▉   | 555/795 [03:57<01:09,  3.43it/s] 70%|██████▉   | 556/795 [03:58<01:10,  3.41it/s] 70%|███████   | 557/795 [03:58<01:09,  3.41it/s] 70%|███████   | 558/795 [03:58<01:09,  3.42it/s] 70%|███████   | 559/795 [03:58<01:08,  3.42it/s] 70%|███████   | 560/795 [03:59<01:08,  3.43it/s] 71%|███████   | 561/795 [03:59<01:08,  3.43it/s] 71%|███████   | 562/795 [03:59<01:07,  3.43it/s] 71%|███████   | 563/795 [04:00<01:07,  3.43it/s] 71%|███████   | 564/795 [04:00<01:07,  3.43it/s] 71%|███████   | 565/795 [04:00<01:07,  3.43it/s] 71%|███████   | 566/795 [04:00<01:06,  3.43it/s] 71%|███████▏  | 567/795 [04:01<01:06,  3.43it/s] 71%|███████▏  | 568/795 [04:01<01:06,  3.43it/s] 72%|███████▏  | 569/795 [04:01<01:05,  3.43it/s] 72%|███████▏  | 570/795 [04:02<01:05,  3.43it/s] 72%|███████▏  | 571/795 [04:02<01:05,  3.43it/s] 72%|███████▏  | 572/795 [04:02<01:05,  3.43it/s] 72%|███████▏  | 573/795 [04:03<01:04,  3.43it/s] 72%|███████▏  | 574/795 [04:03<01:04,  3.43it/s] 72%|███████▏  | 575/795 [04:03<01:04,  3.44it/s] 72%|███████▏  | 576/795 [04:03<01:03,  3.43it/s] 73%|███████▎  | 577/795 [04:04<01:03,  3.43it/s] 73%|███████▎  | 578/795 [04:04<01:03,  3.43it/s] 73%|███████▎  | 579/795 [04:04<01:03,  3.43it/s] 73%|███████▎  | 580/795 [04:05<01:02,  3.43it/s] 73%|███████▎  | 581/795 [04:05<01:02,  3.43it/s] 73%|███████▎  | 582/795 [04:05<01:02,  3.43it/s] 73%|███████▎  | 583/795 [04:05<01:01,  3.43it/s] 73%|███████▎  | 584/795 [04:06<01:01,  3.43it/s] 74%|███████▎  | 585/795 [04:06<01:01,  3.43it/s] 74%|███████▎  | 586/795 [04:06<01:00,  3.43it/s] 74%|███████▍  | 587/795 [04:07<01:00,  3.43it/s] 74%|███████▍  | 588/795 [04:07<01:00,  3.43it/s] 74%|███████▍  | 589/795 [04:07<01:00,  3.41it/s] 74%|███████▍  | 590/795 [04:07<00:59,  3.42it/s] 74%|███████▍  | 591/795 [04:08<00:59,  3.42it/s] 74%|███████▍  | 592/795 [04:08<00:59,  3.42it/s] 75%|███████▍  | 593/795 [04:08<00:58,  3.43it/s] 75%|███████▍  | 594/795 [04:09<00:58,  3.43it/s] 75%|███████▍  | 595/795 [04:09<00:58,  3.43it/s] 75%|███████▍  | 596/795 [04:09<00:58,  3.43it/s] 75%|███████▌  | 597/795 [04:10<00:57,  3.43it/s] 75%|███████▌  | 598/795 [04:10<00:57,  3.42it/s] 75%|███████▌  | 599/795 [04:10<00:57,  3.42it/s] 75%|███████▌  | 600/795 [04:10<00:57,  3.42it/s] 76%|███████▌  | 601/795 [04:11<00:56,  3.42it/s] 76%|███████▌  | 602/795 [04:11<00:56,  3.42it/s] 76%|███████▌  | 603/795 [04:11<00:56,  3.43it/s] 76%|███████▌  | 604/795 [04:12<00:55,  3.43it/s] 76%|███████▌  | 605/795 [04:12<00:55,  3.43it/s] 76%|███████▌  | 606/795 [04:12<00:55,  3.43it/s] 76%|███████▋  | 607/795 [04:12<00:54,  3.43it/s] 76%|███████▋  | 608/795 [04:13<00:54,  3.43it/s] 77%|███████▋  | 609/795 [04:13<00:54,  3.43it/s] 77%|███████▋  | 610/795 [04:13<00:53,  3.43it/s] 77%|███████▋  | 611/795 [04:14<00:53,  3.42it/s] 77%|███████▋  | 612/795 [04:14<00:53,  3.42it/s] 77%|███████▋  | 613/795 [04:14<00:53,  3.43it/s] 77%|███████▋  | 614/795 [04:14<00:52,  3.43it/s] 77%|███████▋  | 615/795 [04:15<00:52,  3.43it/s] 77%|███████▋  | 616/795 [04:15<00:52,  3.43it/s] 78%|███████▊  | 617/795 [04:15<00:51,  3.43it/s] 78%|███████▊  | 618/795 [04:16<00:51,  3.43it/s] 78%|███████▊  | 619/795 [04:16<00:51,  3.43it/s] 78%|███████▊  | 620/795 [04:16<00:51,  3.43it/s] 78%|███████▊  | 621/795 [04:17<00:50,  3.43it/s] 78%|███████▊  | 622/795 [04:17<00:50,  3.42it/s] 78%|███████▊  | 623/795 [04:17<00:50,  3.42it/s] 78%|███████▊  | 624/795 [04:17<00:49,  3.43it/s] 79%|███████▊  | 625/795 [04:18<00:49,  3.43it/s] 79%|███████▊  | 626/795 [04:18<00:49,  3.43it/s] 79%|███████▉  | 627/795 [04:18<00:48,  3.43it/s] 79%|███████▉  | 628/795 [04:19<00:48,  3.43it/s] 79%|███████▉  | 629/795 [04:19<00:48,  3.43it/s] 79%|███████▉  | 630/795 [04:19<00:48,  3.43it/s] 79%|███████▉  | 631/795 [04:19<00:47,  3.42it/s] 79%|███████▉  | 632/795 [04:20<00:47,  3.43it/s] 80%|███████▉  | 633/795 [04:20<00:47,  3.42it/s] 80%|███████▉  | 634/795 [04:20<00:47,  3.42it/s] 80%|███████▉  | 635/795 [04:21<00:46,  3.43it/s] 80%|████████  | 636/795 [04:21<00:46,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:05:33,437 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:05:33,437 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 05:05:33,437 >>   Batch size = 8
{'eval_loss': 0.9100190997123718, 'eval_runtime': 17.5864, 'eval_samples_per_second': 368.864, 'eval_steps_per_second': 46.115, 'epoch': 3.0}
{'loss': 0.6841, 'learning_rate': 1.3915094339622643e-05, 'epoch': 3.14}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 57.17it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.14it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.36it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.69it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.23it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.93it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.69it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.13it/s][A
  6%|▌         | 48/811 [00:01<00:16, 45.96it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.13it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.25it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.20it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.30it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.23it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.19it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.07it/s][A
 11%|█         | 88/811 [00:01<00:15, 45.96it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 45.93it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.00it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.02it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.03it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.16it/s][A
 15%|█▍        | 118/811 [00:02<00:15, 46.17it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.19it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.15it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 45.93it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 45.88it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 45.92it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.05it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.22it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.25it/s][A
 20%|██        | 163/811 [00:03<00:13, 46.29it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.05it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.11it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.07it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.04it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 46.06it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.08it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.11it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.06it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.17it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.18it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.01it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.03it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.01it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 45.89it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 45.97it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.09it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.03it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.12it/s][A
 32%|███▏      | 258/811 [00:05<00:11, 46.23it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.04it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.06it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 45.93it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.00it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.01it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.13it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.02it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.13it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 46.12it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.07it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 45.98it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 46.03it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 45.96it/s][A
 40%|████      | 328/811 [00:07<00:10, 45.95it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.03it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.02it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 46.09it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.12it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.00it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.04it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 45.99it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 45.93it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 45.98it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.00it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.00it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.05it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.10it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.02it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 46.11it/s][A
 50%|█████     | 408/811 [00:08<00:08, 46.11it/s][A
 51%|█████     | 413/811 [00:08<00:08, 45.94it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.01it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.00it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 45.98it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.03it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 45.97it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.05it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.10it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 46.16it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 45.99it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.03it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 45.91it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.07it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.06it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.12it/s][A
 60%|██████    | 488/811 [00:10<00:07, 46.01it/s][A
 61%|██████    | 493/811 [00:10<00:06, 45.95it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 45.97it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 45.95it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 46.04it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 45.99it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 45.96it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.02it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 45.98it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.02it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.07it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 46.08it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.03it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 46.09it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 45.94it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 45.96it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.02it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.05it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.09it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.00it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 45.95it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.00it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.07it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.07it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.08it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 45.97it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 45.92it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 45.90it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 45.98it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.03it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 46.03it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.04it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 45.92it/s][A
 81%|████████  | 653/811 [00:14<00:03, 46.01it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.04it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.03it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 46.10it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.06it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 45.90it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 45.95it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.00it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 46.04it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 46.04it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 46.14it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 45.90it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 45.97it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 46.05it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 45.96it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 46.07it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 45.48it/s][A
 91%|█████████ | 738/811 [00:16<00:01, 46.14it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 46.20it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 46.19it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 46.17it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 46.02it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 46.04it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 45.96it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 46.01it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 46.01it/s][A
 97%|█████████▋| 783/811 [00:16<00:00, 46.03it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 46.04it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 46.08it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 46.05it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 44.69it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 45.10it/s][A                                                 
                                                 [A 80%|████████  | 636/795 [04:39<00:46,  3.43it/s]
100%|██████████| 811/811 [00:17<00:00, 45.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:05:51,083 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-636
[INFO|configuration_utils.py:351] 2023-08-29 05:05:51,098 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-636/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:05:53,293 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-636/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:05:53,315 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-636/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:05:53,329 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-636/special_tokens_map.json
 80%|████████  | 637/795 [04:46<19:59,  7.59s/it] 80%|████████  | 638/795 [04:46<14:08,  5.41s/it] 80%|████████  | 639/795 [04:46<10:03,  3.87s/it] 81%|████████  | 640/795 [04:46<07:13,  2.80s/it] 81%|████████  | 641/795 [04:47<05:14,  2.04s/it] 81%|████████  | 642/795 [04:47<03:52,  1.52s/it] 81%|████████  | 643/795 [04:47<02:54,  1.15s/it] 81%|████████  | 644/795 [04:48<02:14,  1.12it/s] 81%|████████  | 645/795 [04:48<01:46,  1.40it/s] 81%|████████▏ | 646/795 [04:48<01:27,  1.71it/s] 81%|████████▏ | 647/795 [04:48<01:13,  2.01it/s] 82%|████████▏ | 648/795 [04:49<01:04,  2.30it/s] 82%|████████▏ | 649/795 [04:49<00:57,  2.55it/s] 82%|████████▏ | 650/795 [04:49<00:52,  2.76it/s] 82%|████████▏ | 651/795 [04:50<00:49,  2.93it/s] 82%|████████▏ | 652/795 [04:50<00:46,  3.07it/s] 82%|████████▏ | 653/795 [04:50<00:44,  3.17it/s] 82%|████████▏ | 654/795 [04:50<00:43,  3.25it/s] 82%|████████▏ | 655/795 [04:51<00:42,  3.30it/s] 83%|████████▎ | 656/795 [04:51<00:41,  3.34it/s] 83%|████████▎ | 657/795 [04:51<00:40,  3.37it/s] 83%|████████▎ | 658/795 [04:52<00:40,  3.39it/s] 83%|████████▎ | 659/795 [04:52<00:39,  3.41it/s] 83%|████████▎ | 660/795 [04:52<00:39,  3.39it/s] 83%|████████▎ | 661/795 [04:53<00:39,  3.40it/s] 83%|████████▎ | 662/795 [04:53<00:38,  3.41it/s] 83%|████████▎ | 663/795 [04:53<00:38,  3.42it/s] 84%|████████▎ | 664/795 [04:53<00:38,  3.43it/s] 84%|████████▎ | 665/795 [04:54<00:37,  3.43it/s] 84%|████████▍ | 666/795 [04:54<00:37,  3.43it/s] 84%|████████▍ | 667/795 [04:54<00:37,  3.44it/s] 84%|████████▍ | 668/795 [04:55<00:36,  3.44it/s] 84%|████████▍ | 669/795 [04:55<00:36,  3.44it/s] 84%|████████▍ | 670/795 [04:55<00:36,  3.44it/s] 84%|████████▍ | 671/795 [04:55<00:36,  3.43it/s] 85%|████████▍ | 672/795 [04:56<00:35,  3.43it/s] 85%|████████▍ | 673/795 [04:56<00:35,  3.44it/s] 85%|████████▍ | 674/795 [04:56<00:35,  3.44it/s] 85%|████████▍ | 675/795 [04:57<00:34,  3.44it/s] 85%|████████▌ | 676/795 [04:57<00:34,  3.44it/s] 85%|████████▌ | 677/795 [04:57<00:34,  3.43it/s] 85%|████████▌ | 678/795 [04:57<00:34,  3.43it/s] 85%|████████▌ | 679/795 [04:58<00:33,  3.43it/s] 86%|████████▌ | 680/795 [04:58<00:33,  3.43it/s] 86%|████████▌ | 681/795 [04:58<00:33,  3.44it/s] 86%|████████▌ | 682/795 [04:59<00:32,  3.44it/s] 86%|████████▌ | 683/795 [04:59<00:32,  3.44it/s] 86%|████████▌ | 684/795 [04:59<00:32,  3.43it/s] 86%|████████▌ | 685/795 [05:00<00:32,  3.43it/s] 86%|████████▋ | 686/795 [05:00<00:31,  3.43it/s] 86%|████████▋ | 687/795 [05:00<00:31,  3.43it/s] 87%|████████▋ | 688/795 [05:00<00:31,  3.43it/s] 87%|████████▋ | 689/795 [05:01<00:30,  3.44it/s] 87%|████████▋ | 690/795 [05:01<00:30,  3.44it/s] 87%|████████▋ | 691/795 [05:01<00:30,  3.44it/s] 87%|████████▋ | 692/795 [05:02<00:29,  3.44it/s] 87%|████████▋ | 693/795 [05:02<00:29,  3.44it/s] 87%|████████▋ | 694/795 [05:02<00:29,  3.44it/s] 87%|████████▋ | 695/795 [05:02<00:29,  3.43it/s] 88%|████████▊ | 696/795 [05:03<00:28,  3.43it/s] 88%|████████▊ | 697/795 [05:03<00:28,  3.43it/s] 88%|████████▊ | 698/795 [05:03<00:28,  3.43it/s] 88%|████████▊ | 699/795 [05:04<00:27,  3.43it/s] 88%|████████▊ | 700/795 [05:04<00:27,  3.43it/s] 88%|████████▊ | 701/795 [05:04<00:27,  3.42it/s] 88%|████████▊ | 702/795 [05:04<00:27,  3.42it/s] 88%|████████▊ | 703/795 [05:05<00:26,  3.42it/s] 89%|████████▊ | 704/795 [05:05<00:26,  3.42it/s] 89%|████████▊ | 705/795 [05:05<00:26,  3.43it/s] 89%|████████▉ | 706/795 [05:06<00:26,  3.42it/s] 89%|████████▉ | 707/795 [05:06<00:25,  3.42it/s] 89%|████████▉ | 708/795 [05:06<00:25,  3.43it/s] 89%|████████▉ | 709/795 [05:07<00:25,  3.43it/s] 89%|████████▉ | 710/795 [05:07<00:24,  3.43it/s] 89%|████████▉ | 711/795 [05:07<00:24,  3.43it/s] 90%|████████▉ | 712/795 [05:07<00:24,  3.43it/s] 90%|████████▉ | 713/795 [05:08<00:23,  3.44it/s] 90%|████████▉ | 714/795 [05:08<00:23,  3.43it/s] 90%|████████▉ | 715/795 [05:08<00:23,  3.43it/s] 90%|█████████ | 716/795 [05:09<00:23,  3.43it/s] 90%|█████████ | 717/795 [05:09<00:22,  3.42it/s] 90%|█████████ | 718/795 [05:09<00:22,  3.42it/s] 90%|█████████ | 719/795 [05:09<00:22,  3.43it/s] 91%|█████████ | 720/795 [05:10<00:21,  3.43it/s] 91%|█████████ | 721/795 [05:10<00:21,  3.43it/s] 91%|█████████ | 722/795 [05:10<00:21,  3.43it/s] 91%|█████████ | 723/795 [05:11<00:20,  3.43it/s] 91%|█████████ | 724/795 [05:11<00:20,  3.43it/s] 91%|█████████ | 725/795 [05:11<00:20,  3.43it/s] 91%|█████████▏| 726/795 [05:11<00:20,  3.43it/s] 91%|█████████▏| 727/795 [05:12<00:19,  3.43it/s] 92%|█████████▏| 728/795 [05:12<00:19,  3.42it/s] 92%|█████████▏| 729/795 [05:12<00:19,  3.42it/s] 92%|█████████▏| 730/795 [05:13<00:18,  3.42it/s] 92%|█████████▏| 731/795 [05:13<00:18,  3.43it/s] 92%|█████████▏| 732/795 [05:13<00:18,  3.43it/s] 92%|█████████▏| 733/795 [05:14<00:18,  3.43it/s] 92%|█████████▏| 734/795 [05:14<00:17,  3.43it/s] 92%|█████████▏| 735/795 [05:14<00:17,  3.43it/s] 93%|█████████▎| 736/795 [05:14<00:17,  3.43it/s] 93%|█████████▎| 737/795 [05:15<00:16,  3.43it/s] 93%|█████████▎| 738/795 [05:15<00:16,  3.43it/s] 93%|█████████▎| 739/795 [05:15<00:16,  3.42it/s] 93%|█████████▎| 740/795 [05:16<00:16,  3.42it/s] 93%|█████████▎| 741/795 [05:16<00:15,  3.42it/s] 93%|█████████▎| 742/795 [05:16<00:15,  3.42it/s] 93%|█████████▎| 743/795 [05:16<00:15,  3.43it/s] 94%|█████████▎| 744/795 [05:17<00:14,  3.43it/s] 94%|█████████▎| 745/795 [05:17<00:14,  3.43it/s] 94%|█████████▍| 746/795 [05:17<00:14,  3.43it/s] 94%|█████████▍| 747/795 [05:18<00:13,  3.43it/s] 94%|█████████▍| 748/795 [05:18<00:13,  3.43it/s] 94%|█████████▍| 749/795 [05:18<00:13,  3.43it/s] 94%|█████████▍| 750/795 [05:18<00:13,  3.40it/s] 94%|█████████▍| 751/795 [05:19<00:12,  3.41it/s] 95%|█████████▍| 752/795 [05:19<00:12,  3.42it/s] 95%|█████████▍| 753/795 [05:19<00:12,  3.42it/s] 95%|█████████▍| 754/795 [05:20<00:11,  3.42it/s] 95%|█████████▍| 755/795 [05:20<00:11,  3.43it/s] 95%|█████████▌| 756/795 [05:20<00:11,  3.43it/s] 95%|█████████▌| 757/795 [05:21<00:11,  3.43it/s] 95%|█████████▌| 758/795 [05:21<00:10,  3.43it/s] 95%|█████████▌| 759/795 [05:21<00:10,  3.43it/s] 96%|█████████▌| 760/795 [05:21<00:10,  3.43it/s] 96%|█████████▌| 761/795 [05:22<00:09,  3.43it/s] 96%|█████████▌| 762/795 [05:22<00:09,  3.43it/s] 96%|█████████▌| 763/795 [05:22<00:09,  3.43it/s] 96%|█████████▌| 764/795 [05:23<00:09,  3.43it/s] 96%|█████████▌| 765/795 [05:23<00:08,  3.43it/s] 96%|█████████▋| 766/795 [05:23<00:08,  3.43it/s] 96%|█████████▋| 767/795 [05:23<00:08,  3.43it/s] 97%|█████████▋| 768/795 [05:24<00:07,  3.43it/s] 97%|█████████▋| 769/795 [05:24<00:07,  3.43it/s] 97%|█████████▋| 770/795 [05:24<00:07,  3.43it/s] 97%|█████████▋| 771/795 [05:25<00:06,  3.43it/s] 97%|█████████▋| 772/795 [05:25<00:06,  3.43it/s] 97%|█████████▋| 773/795 [05:25<00:06,  3.43it/s] 97%|█████████▋| 774/795 [05:25<00:06,  3.43it/s] 97%|█████████▋| 775/795 [05:26<00:05,  3.43it/s] 98%|█████████▊| 776/795 [05:26<00:05,  3.43it/s] 98%|█████████▊| 777/795 [05:26<00:05,  3.43it/s] 98%|█████████▊| 778/795 [05:27<00:04,  3.43it/s] 98%|█████████▊| 779/795 [05:27<00:04,  3.43it/s] 98%|█████████▊| 780/795 [05:27<00:04,  3.43it/s] 98%|█████████▊| 781/795 [05:28<00:04,  3.43it/s] 98%|█████████▊| 782/795 [05:28<00:03,  3.43it/s] 98%|█████████▊| 783/795 [05:28<00:03,  3.43it/s] 99%|█████████▊| 784/795 [05:28<00:03,  3.43it/s] 99%|█████████▊| 785/795 [05:29<00:02,  3.43it/s] 99%|█████████▉| 786/795 [05:29<00:02,  3.43it/s] 99%|█████████▉| 787/795 [05:29<00:02,  3.43it/s] 99%|█████████▉| 788/795 [05:30<00:02,  3.43it/s] 99%|█████████▉| 789/795 [05:30<00:01,  3.40it/s] 99%|█████████▉| 790/795 [05:30<00:01,  3.41it/s] 99%|█████████▉| 791/795 [05:30<00:01,  3.41it/s]100%|█████████▉| 792/795 [05:31<00:00,  3.42it/s]100%|█████████▉| 793/795 [05:31<00:00,  3.42it/s]100%|█████████▉| 794/795 [05:31<00:00,  3.43it/s]100%|██████████| 795/795 [05:32<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:06:44,107 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:06:44,107 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 05:06:44,107 >>   Batch size = 8
{'eval_loss': 0.9146774411201477, 'eval_runtime': 17.6228, 'eval_samples_per_second': 368.102, 'eval_steps_per_second': 46.02, 'epoch': 4.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.53it/s][A
  1%|▏         | 12/811 [00:00<00:15, 49.94it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.25it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.65it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.23it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.73it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.39it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.10it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.14it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.19it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.19it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.21it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.24it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.32it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.17it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.03it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.00it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 45.92it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.00it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.03it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.18it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.22it/s][A
 15%|█▍        | 118/811 [00:02<00:14, 46.30it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.09it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.10it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.03it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 45.92it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 46.00it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.03it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.06it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.18it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.25it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.12it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.07it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.03it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.01it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 46.01it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 45.93it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.08it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.13it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.11it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.14it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 45.95it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.07it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.00it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 46.02it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 45.94it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.07it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.05it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.14it/s][A
 32%|███▏      | 258/811 [00:05<00:11, 46.20it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.02it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.06it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.07it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.08it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 45.96it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.01it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.04it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.17it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 46.06it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.10it/s][A
 39%|███▊      | 313/811 [00:06<00:11, 44.23it/s][A
 39%|███▉      | 318/811 [00:06<00:11, 44.81it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 45.32it/s][A
 40%|████      | 328/811 [00:07<00:10, 45.61it/s][A
 41%|████      | 333/811 [00:07<00:10, 45.71it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 45.80it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 45.92it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.05it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 45.96it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 45.73it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 45.92it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 46.00it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.03it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.17it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.12it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.18it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.09it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 45.89it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 45.90it/s][A
 50%|█████     | 408/811 [00:08<00:08, 45.94it/s][A
 51%|█████     | 413/811 [00:08<00:08, 45.87it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.18it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.16it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.11it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.16it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.10it/s][A
 55%|█████▍    | 443/811 [00:09<00:08, 45.96it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.04it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 45.93it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 46.00it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.07it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.11it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.12it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.13it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.07it/s][A
 60%|██████    | 488/811 [00:10<00:07, 45.99it/s][A
 61%|██████    | 493/811 [00:10<00:06, 45.94it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 45.99it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.03it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 46.09it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.07it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 46.10it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.04it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.14it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.07it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.07it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 45.90it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.00it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 46.12it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 46.12it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.10it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.14it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.15it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.13it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.03it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.04it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.07it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.16it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.00it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.08it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.06it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.09it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.18it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.16it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.06it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 46.01it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.06it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 46.09it/s][A
 81%|████████  | 653/811 [00:14<00:03, 46.03it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.11it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.11it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 46.03it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.01it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 46.02it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 46.11it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.07it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 46.03it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 45.95it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 46.05it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 45.98it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.11it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 46.14it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 45.97it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 45.96it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 46.01it/s][A
 91%|█████████ | 738/811 [00:16<00:01, 46.06it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 46.07it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 46.01it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 45.99it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 45.94it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 46.05it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 46.06it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 46.15it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 46.14it/s][A
 97%|█████████▋| 783/811 [00:16<00:00, 46.03it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 46.00it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 45.99it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 46.10it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.09it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 46.09it/s][A                                                 
                                                 [A100%|██████████| 795/795 [05:49<00:00,  3.43it/s]
100%|██████████| 811/811 [00:17<00:00, 46.09it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:07:01,751 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-795
[INFO|configuration_utils.py:351] 2023-08-29 05:07:01,771 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-795/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:07:03,917 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-795/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:07:03,944 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-795/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:07:03,954 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-795/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 05:07:09,303 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 05:07:09,306 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-159 (score: 0.8977049589157104).
                                                 100%|██████████| 795/795 [05:59<00:00,  3.43it/s]100%|██████████| 795/795 [05:59<00:00,  2.21it/s]
[INFO|trainer.py:1894] 2023-08-29 05:07:11,306 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 05:07:11,335 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:07:13,604 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:07:13,619 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:07:13,632 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:07:13,804 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,805 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,805 >>   train_loss               =     0.6724
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,805 >>   train_runtime            = 0:05:59.30
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,805 >>   train_samples            =      10180
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,805 >>   train_samples_per_second =    141.664
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,805 >>   train_steps_per_second   =      2.213
{'eval_loss': 0.9192439913749695, 'eval_runtime': 17.6189, 'eval_samples_per_second': 368.184, 'eval_steps_per_second': 46.03, 'epoch': 5.0}
{'train_runtime': 359.3018, 'train_samples_per_second': 141.664, 'train_steps_per_second': 2.213, 'train_loss': 0.6724260822032233, 'epoch': 5.0}
08/29/2023 05:07:13 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 05:07:13,846 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:07:13,846 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 05:07:13,846 >>   Batch size = 8
  0%|          | 0/811 [00:00<?, ?it/s]  1%|          | 6/811 [00:00<00:13, 58.06it/s]  1%|▏         | 12/811 [00:00<00:15, 50.86it/s]  2%|▏         | 18/811 [00:00<00:16, 48.68it/s]  3%|▎         | 23/811 [00:00<00:16, 47.90it/s]  3%|▎         | 28/811 [00:00<00:16, 47.44it/s]  4%|▍         | 33/811 [00:00<00:16, 47.19it/s]  5%|▍         | 38/811 [00:00<00:16, 46.89it/s]  5%|▌         | 43/811 [00:00<00:16, 46.61it/s]  6%|▌         | 48/811 [00:01<00:16, 46.28it/s]  7%|▋         | 53/811 [00:01<00:16, 46.20it/s]  7%|▋         | 58/811 [00:01<00:16, 46.24it/s]  8%|▊         | 63/811 [00:01<00:16, 46.22it/s]  8%|▊         | 68/811 [00:01<00:16, 46.34it/s]  9%|▉         | 73/811 [00:01<00:15, 46.34it/s] 10%|▉         | 78/811 [00:01<00:15, 46.33it/s] 10%|█         | 83/811 [00:01<00:15, 46.33it/s] 11%|█         | 88/811 [00:01<00:15, 46.19it/s] 11%|█▏        | 93/811 [00:01<00:15, 46.01it/s] 12%|█▏        | 98/811 [00:02<00:15, 46.12it/s] 13%|█▎        | 103/811 [00:02<00:15, 46.17it/s] 13%|█▎        | 108/811 [00:02<00:15, 46.17it/s] 14%|█▍        | 113/811 [00:02<00:15, 46.12it/s] 15%|█▍        | 118/811 [00:02<00:15, 46.17it/s] 15%|█▌        | 123/811 [00:02<00:14, 46.27it/s] 16%|█▌        | 128/811 [00:02<00:14, 46.30it/s] 16%|█▋        | 133/811 [00:02<00:14, 46.14it/s] 17%|█▋        | 138/811 [00:02<00:14, 46.06it/s] 18%|█▊        | 143/811 [00:03<00:14, 46.13it/s] 18%|█▊        | 148/811 [00:03<00:14, 46.14it/s] 19%|█▉        | 153/811 [00:03<00:14, 46.16it/s] 19%|█▉        | 158/811 [00:03<00:14, 46.19it/s] 20%|██        | 163/811 [00:03<00:14, 46.18it/s] 21%|██        | 168/811 [00:03<00:13, 46.27it/s] 21%|██▏       | 173/811 [00:03<00:13, 46.29it/s] 22%|██▏       | 178/811 [00:03<00:13, 46.14it/s] 23%|██▎       | 183/811 [00:03<00:13, 46.14it/s] 23%|██▎       | 188/811 [00:04<00:13, 46.08it/s] 24%|██▍       | 193/811 [00:04<00:13, 46.08it/s] 24%|██▍       | 198/811 [00:04<00:13, 46.18it/s] 25%|██▌       | 203/811 [00:04<00:13, 46.20it/s] 26%|██▌       | 208/811 [00:04<00:13, 46.25it/s] 26%|██▋       | 213/811 [00:04<00:12, 46.26it/s] 27%|██▋       | 218/811 [00:04<00:12, 46.18it/s] 27%|██▋       | 223/811 [00:04<00:12, 45.97it/s] 28%|██▊       | 228/811 [00:04<00:12, 46.04it/s] 29%|██▊       | 233/811 [00:05<00:12, 46.10it/s] 29%|██▉       | 238/811 [00:05<00:12, 46.12it/s] 30%|██▉       | 243/811 [00:05<00:12, 46.14it/s] 31%|███       | 248/811 [00:05<00:12, 46.07it/s] 31%|███       | 253/811 [00:05<00:12, 46.16it/s] 32%|███▏      | 258/811 [00:05<00:11, 46.27it/s] 32%|███▏      | 263/811 [00:05<00:11, 46.13it/s] 33%|███▎      | 268/811 [00:05<00:11, 46.17it/s] 34%|███▎      | 273/811 [00:05<00:11, 46.05it/s] 34%|███▍      | 278/811 [00:05<00:11, 46.16it/s] 35%|███▍      | 283/811 [00:06<00:11, 46.05it/s] 36%|███▌      | 288/811 [00:06<00:11, 46.04it/s] 36%|███▌      | 293/811 [00:06<00:11, 46.12it/s] 37%|███▋      | 298/811 [00:06<00:11, 46.16it/s] 37%|███▋      | 303/811 [00:06<00:11, 46.08it/s] 38%|███▊      | 308/811 [00:06<00:10, 46.09it/s] 39%|███▊      | 313/811 [00:06<00:10, 46.00it/s] 39%|███▉      | 318/811 [00:06<00:10, 45.98it/s] 40%|███▉      | 323/811 [00:06<00:10, 46.05it/s] 40%|████      | 328/811 [00:07<00:10, 46.10it/s] 41%|████      | 333/811 [00:07<00:10, 45.63it/s] 42%|████▏     | 338/811 [00:07<00:10, 45.96it/s] 42%|████▏     | 343/811 [00:07<00:10, 46.05it/s] 43%|████▎     | 348/811 [00:07<00:10, 46.08it/s] 44%|████▎     | 353/811 [00:07<00:09, 46.10it/s] 44%|████▍     | 358/811 [00:07<00:09, 46.00it/s] 45%|████▍     | 363/811 [00:07<00:09, 45.93it/s] 45%|████▌     | 368/811 [00:07<00:09, 45.97it/s] 46%|████▌     | 373/811 [00:08<00:09, 45.99it/s] 47%|████▋     | 378/811 [00:08<00:09, 45.85it/s] 47%|████▋     | 383/811 [00:08<00:09, 46.02it/s] 48%|████▊     | 388/811 [00:08<00:09, 46.00it/s] 48%|████▊     | 393/811 [00:08<00:09, 46.12it/s] 49%|████▉     | 398/811 [00:08<00:08, 46.15it/s] 50%|████▉     | 403/811 [00:08<00:08, 46.20it/s] 50%|█████     | 408/811 [00:08<00:08, 46.11it/s] 51%|█████     | 413/811 [00:08<00:08, 46.20it/s] 52%|█████▏    | 418/811 [00:09<00:08, 46.12it/s] 52%|█████▏    | 423/811 [00:09<00:08, 46.22it/s] 53%|█████▎    | 428/811 [00:09<00:08, 46.24it/s] 53%|█████▎    | 433/811 [00:09<00:08, 46.31it/s] 54%|█████▍    | 438/811 [00:09<00:08, 46.29it/s] 55%|█████▍    | 443/811 [00:09<00:07, 46.30it/s] 55%|█████▌    | 448/811 [00:09<00:07, 46.35it/s] 56%|█████▌    | 453/811 [00:09<00:07, 46.29it/s] 56%|█████▋    | 458/811 [00:09<00:07, 46.29it/s] 57%|█████▋    | 463/811 [00:10<00:07, 46.30it/s] 58%|█████▊    | 468/811 [00:10<00:07, 46.29it/s] 58%|█████▊    | 473/811 [00:10<00:07, 46.20it/s] 59%|█████▉    | 478/811 [00:10<00:07, 46.12it/s] 60%|█████▉    | 483/811 [00:10<00:07, 46.28it/s] 60%|██████    | 488/811 [00:10<00:06, 46.27it/s] 61%|██████    | 493/811 [00:10<00:06, 46.30it/s] 61%|██████▏   | 498/811 [00:10<00:06, 46.22it/s] 62%|██████▏   | 503/811 [00:10<00:06, 46.29it/s] 63%|██████▎   | 508/811 [00:10<00:06, 46.19it/s] 63%|██████▎   | 513/811 [00:11<00:06, 46.18it/s] 64%|██████▍   | 518/811 [00:11<00:06, 46.18it/s] 64%|██████▍   | 523/811 [00:11<00:06, 46.24it/s] 65%|██████▌   | 528/811 [00:11<00:06, 46.30it/s] 66%|██████▌   | 533/811 [00:11<00:06, 46.33it/s] 66%|██████▋   | 538/811 [00:11<00:05, 46.24it/s] 67%|██████▋   | 543/811 [00:11<00:05, 46.24it/s] 68%|██████▊   | 548/811 [00:11<00:05, 46.30it/s] 68%|██████▊   | 553/811 [00:11<00:05, 46.32it/s] 69%|██████▉   | 558/811 [00:12<00:05, 46.30it/s] 69%|██████▉   | 563/811 [00:12<00:05, 46.28it/s] 70%|███████   | 568/811 [00:12<00:05, 46.01it/s] 71%|███████   | 573/811 [00:12<00:05, 46.17it/s] 71%|███████▏  | 578/811 [00:12<00:05, 46.29it/s] 72%|███████▏  | 583/811 [00:12<00:04, 46.40it/s] 73%|███████▎  | 588/811 [00:12<00:04, 46.32it/s] 73%|███████▎  | 593/811 [00:12<00:04, 46.35it/s] 74%|███████▎  | 598/811 [00:12<00:04, 46.11it/s] 74%|███████▍  | 603/811 [00:13<00:04, 46.23it/s] 75%|███████▍  | 608/811 [00:13<00:04, 46.27it/s] 76%|███████▌  | 613/811 [00:13<00:04, 46.19it/s] 76%|███████▌  | 618/811 [00:13<00:04, 46.26it/s] 77%|███████▋  | 623/811 [00:13<00:04, 46.19it/s] 77%|███████▋  | 628/811 [00:13<00:03, 46.24it/s] 78%|███████▊  | 633/811 [00:13<00:03, 46.31it/s] 79%|███████▊  | 638/811 [00:13<00:03, 46.20it/s] 79%|███████▉  | 643/811 [00:13<00:03, 46.23it/s] 80%|███████▉  | 648/811 [00:14<00:03, 46.26it/s] 81%|████████  | 653/811 [00:14<00:03, 46.29it/s] 81%|████████  | 658/811 [00:14<00:03, 46.15it/s] 82%|████████▏ | 663/811 [00:14<00:03, 46.10it/s] 82%|████████▏ | 668/811 [00:14<00:03, 46.23it/s] 83%|████████▎ | 673/811 [00:14<00:02, 46.23it/s] 84%|████████▎ | 678/811 [00:14<00:02, 46.32it/s] 84%|████████▍ | 683/811 [00:14<00:02, 46.24it/s] 85%|████████▍ | 688/811 [00:14<00:02, 46.31it/s] 85%|████████▌ | 693/811 [00:14<00:02, 46.16it/s] 86%|████████▌ | 698/811 [00:15<00:02, 46.19it/s] 87%|████████▋ | 703/811 [00:15<00:02, 46.18it/s] 87%|████████▋ | 708/811 [00:15<00:02, 46.22it/s] 88%|████████▊ | 713/811 [00:15<00:02, 46.25it/s] 89%|████████▊ | 718/811 [00:15<00:02, 46.31it/s] 89%|████████▉ | 723/811 [00:15<00:01, 46.23it/s] 90%|████████▉ | 728/811 [00:15<00:01, 46.20it/s] 90%|█████████ | 733/811 [00:15<00:01, 46.18it/s] 91%|█████████ | 738/811 [00:15<00:01, 46.27it/s] 92%|█████████▏| 743/811 [00:16<00:01, 46.25it/s] 92%|█████████▏| 748/811 [00:16<00:01, 46.15it/s] 93%|█████████▎| 753/811 [00:16<00:01, 46.24it/s] 93%|█████████▎| 758/811 [00:16<00:01, 46.11it/s] 94%|█████████▍| 763/811 [00:16<00:01, 46.16it/s] 95%|█████████▍| 768/811 [00:16<00:00, 46.16it/s] 95%|█████████▌| 773/811 [00:16<00:00, 46.24it/s] 96%|█████████▌| 778/811 [00:16<00:00, 46.00it/s] 97%|█████████▋| 783/811 [00:16<00:00, 46.22it/s] 97%|█████████▋| 788/811 [00:17<00:00, 46.18it/s] 98%|█████████▊| 793/811 [00:17<00:00, 46.17it/s] 98%|█████████▊| 798/811 [00:17<00:00, 46.30it/s] 99%|█████████▉| 803/811 [00:17<00:00, 46.21it/s]100%|█████████▉| 808/811 [00:17<00:00, 46.20it/s]100%|██████████| 811/811 [00:17<00:00, 46.23it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:07:31,412 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:31,412 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:31,412 >>   eval_loss               =     0.8977
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:31,412 >>   eval_runtime            = 0:00:17.56
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:31,412 >>   eval_samples            =       6487
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:31,412 >>   eval_samples_per_second =    369.299
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:31,412 >>   eval_steps_per_second   =     46.169
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:31,412 >>   perplexity              =      2.454
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:38,603 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:38,608 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:38,608 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:38,608 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:38,608 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:07:39,194 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:07:39,195 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:07:39,758 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:07:40,780 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:07:40,781 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:43,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:43,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:43,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:43,628 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:43,628 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:07:44,245 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:07:44,246 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:07:44,818 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:07:44,957 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:07:44,957 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-318
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-636
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-159
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-795
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/checkpoint-477
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl', 'labels': ['member of political party', 'military branch', 'occupation', 'part of the series', 'place of death'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17184
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17284, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.57it/s]Extractor Predicting: 33it [00:21,  1.57it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:25,  1.56it/s]Extractor Predicting: 40it [00:25,  1.54it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:26,  1.55it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:30,  1.63it/s]Extractor Predicting: 48it [00:30,  1.64it/s]Extractor Predicting: 49it [00:31,  1.68it/s]Extractor Predicting: 50it [00:31,  1.65it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:33,  1.65it/s]Extractor Predicting: 53it [00:33,  1.64it/s]Extractor Predicting: 54it [00:34,  1.59it/s]Extractor Predicting: 55it [00:34,  1.60it/s]Extractor Predicting: 56it [00:35,  1.57it/s]Extractor Predicting: 57it [00:36,  1.61it/s]Extractor Predicting: 58it [00:36,  1.55it/s]Extractor Predicting: 59it [00:37,  1.55it/s]Extractor Predicting: 60it [00:38,  1.59it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:39,  1.60it/s]Extractor Predicting: 63it [00:40,  1.46it/s]Extractor Predicting: 64it [00:40,  1.52it/s]Extractor Predicting: 65it [00:41,  1.56it/s]Extractor Predicting: 66it [00:41,  1.62it/s]Extractor Predicting: 67it [00:42,  1.63it/s]Extractor Predicting: 68it [00:43,  1.61it/s]Extractor Predicting: 69it [00:43,  1.62it/s]Extractor Predicting: 70it [00:44,  1.62it/s]Extractor Predicting: 71it [00:45,  1.63it/s]Extractor Predicting: 72it [00:45,  1.63it/s]Extractor Predicting: 73it [00:46,  1.60it/s]Extractor Predicting: 74it [00:46,  1.61it/s]Extractor Predicting: 75it [00:47,  1.60it/s]Extractor Predicting: 76it [00:48,  1.62it/s]Extractor Predicting: 77it [00:48,  1.62it/s]Extractor Predicting: 78it [00:49,  1.57it/s]Extractor Predicting: 79it [00:50,  1.58it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:51,  1.64it/s]Extractor Predicting: 83it [00:52,  1.69it/s]Extractor Predicting: 84it [00:53,  1.62it/s]Extractor Predicting: 85it [00:53,  1.59it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:55,  1.51it/s]Extractor Predicting: 88it [00:55,  1.48it/s]Extractor Predicting: 89it [00:56,  1.49it/s]Extractor Predicting: 90it [00:57,  1.49it/s]Extractor Predicting: 91it [00:57,  1.48it/s]Extractor Predicting: 92it [00:58,  1.48it/s]Extractor Predicting: 93it [00:59,  1.43it/s]Extractor Predicting: 94it [01:00,  1.41it/s]Extractor Predicting: 95it [01:00,  1.47it/s]Extractor Predicting: 96it [01:01,  1.47it/s]Extractor Predicting: 97it [01:01,  1.50it/s]Extractor Predicting: 98it [01:02,  1.48it/s]Extractor Predicting: 99it [01:03,  1.45it/s]Extractor Predicting: 100it [01:04,  1.49it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.52it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:06,  1.50it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:07,  1.57it/s]Extractor Predicting: 107it [01:08,  1.54it/s]Extractor Predicting: 108it [01:09,  1.57it/s]Extractor Predicting: 109it [01:09,  1.57it/s]Extractor Predicting: 110it [01:10,  1.58it/s]Extractor Predicting: 111it [01:11,  1.55it/s]Extractor Predicting: 112it [01:11,  1.57it/s]Extractor Predicting: 113it [01:12,  1.53it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:13,  1.57it/s]Extractor Predicting: 116it [01:14,  1.54it/s]Extractor Predicting: 117it [01:15,  1.53it/s]Extractor Predicting: 118it [01:15,  1.52it/s]Extractor Predicting: 119it [01:16,  1.52it/s]Extractor Predicting: 120it [01:16,  1.54it/s]Extractor Predicting: 121it [01:17,  1.50it/s]Extractor Predicting: 122it [01:18,  1.52it/s]Extractor Predicting: 123it [01:18,  1.53it/s]Extractor Predicting: 124it [01:19,  1.55it/s]Extractor Predicting: 125it [01:20,  1.54it/s]Extractor Predicting: 126it [01:20,  1.51it/s]Extractor Predicting: 127it [01:21,  1.52it/s]Extractor Predicting: 128it [01:22,  1.51it/s]Extractor Predicting: 129it [01:22,  1.54it/s]Extractor Predicting: 130it [01:23,  1.55it/s]Extractor Predicting: 131it [01:24,  1.56it/s]Extractor Predicting: 132it [01:24,  1.53it/s]Extractor Predicting: 133it [01:25,  1.51it/s]Extractor Predicting: 134it [01:26,  1.49it/s]Extractor Predicting: 135it [01:26,  1.46it/s]Extractor Predicting: 136it [01:27,  1.44it/s]Extractor Predicting: 137it [01:28,  1.43it/s]Extractor Predicting: 138it [01:29,  1.43it/s]Extractor Predicting: 139it [01:29,  1.43it/s]Extractor Predicting: 140it [01:30,  1.43it/s]Extractor Predicting: 141it [01:31,  1.43it/s]Extractor Predicting: 142it [01:31,  1.45it/s]Extractor Predicting: 143it [01:32,  1.44it/s]Extractor Predicting: 144it [01:33,  1.43it/s]Extractor Predicting: 145it [01:33,  1.42it/s]Extractor Predicting: 146it [01:34,  1.44it/s]Extractor Predicting: 147it [01:35,  1.42it/s]Extractor Predicting: 148it [01:36,  1.42it/s]Extractor Predicting: 149it [01:36,  1.43it/s]Extractor Predicting: 150it [01:37,  1.45it/s]Extractor Predicting: 151it [01:38,  1.46it/s]Extractor Predicting: 152it [01:38,  1.47it/s]Extractor Predicting: 153it [01:39,  1.45it/s]Extractor Predicting: 154it [01:40,  1.44it/s]Extractor Predicting: 155it [01:41,  1.30it/s]Extractor Predicting: 156it [01:41,  1.35it/s]Extractor Predicting: 157it [01:42,  1.38it/s]Extractor Predicting: 158it [01:43,  1.38it/s]Extractor Predicting: 159it [01:43,  1.39it/s]Extractor Predicting: 160it [01:44,  1.40it/s]Extractor Predicting: 161it [01:45,  1.42it/s]Extractor Predicting: 162it [01:45,  1.41it/s]Extractor Predicting: 163it [01:46,  1.46it/s]Extractor Predicting: 164it [01:47,  1.46it/s]Extractor Predicting: 165it [01:47,  1.49it/s]Extractor Predicting: 166it [01:48,  1.48it/s]Extractor Predicting: 167it [01:49,  1.48it/s]Extractor Predicting: 168it [01:50,  1.42it/s]Extractor Predicting: 169it [01:50,  1.40it/s]Extractor Predicting: 170it [01:51,  1.42it/s]Extractor Predicting: 171it [01:52,  1.42it/s]Extractor Predicting: 172it [01:52,  1.42it/s]Extractor Predicting: 173it [01:53,  1.42it/s]Extractor Predicting: 174it [01:54,  1.44it/s]Extractor Predicting: 175it [01:55,  1.41it/s]Extractor Predicting: 176it [01:55,  1.38it/s]Extractor Predicting: 177it [01:56,  1.44it/s]Extractor Predicting: 178it [01:57,  1.44it/s]Extractor Predicting: 179it [01:57,  1.47it/s]Extractor Predicting: 180it [01:58,  1.47it/s]Extractor Predicting: 181it [01:59,  1.50it/s]Extractor Predicting: 182it [01:59,  1.50it/s]Extractor Predicting: 183it [02:00,  1.48it/s]Extractor Predicting: 184it [02:01,  1.48it/s]Extractor Predicting: 185it [02:01,  1.49it/s]Extractor Predicting: 186it [02:02,  1.49it/s]Extractor Predicting: 187it [02:03,  1.54it/s]Extractor Predicting: 188it [02:03,  1.50it/s]Extractor Predicting: 189it [02:04,  1.51it/s]Extractor Predicting: 190it [02:05,  1.50it/s]Extractor Predicting: 191it [02:05,  1.52it/s]Extractor Predicting: 192it [02:06,  1.53it/s]Extractor Predicting: 193it [02:07,  1.51it/s]Extractor Predicting: 194it [02:07,  1.53it/s]Extractor Predicting: 195it [02:08,  1.53it/s]Extractor Predicting: 196it [02:08,  1.53it/s]Extractor Predicting: 197it [02:09,  1.53it/s]Extractor Predicting: 198it [02:10,  1.54it/s]Extractor Predicting: 199it [02:10,  1.51it/s]Extractor Predicting: 200it [02:11,  1.54it/s]Extractor Predicting: 201it [02:12,  1.55it/s]Extractor Predicting: 202it [02:12,  1.54it/s]Extractor Predicting: 203it [02:13,  1.52it/s]Extractor Predicting: 204it [02:14,  1.53it/s]Extractor Predicting: 205it [02:14,  1.53it/s]Extractor Predicting: 206it [02:15,  1.52it/s]Extractor Predicting: 207it [02:16,  1.51it/s]Extractor Predicting: 208it [02:16,  1.52it/s]Extractor Predicting: 209it [02:17,  1.54it/s]Extractor Predicting: 210it [02:18,  1.53it/s]Extractor Predicting: 211it [02:18,  1.50it/s]Extractor Predicting: 212it [02:19,  1.52it/s]Extractor Predicting: 213it [02:20,  1.53it/s]Extractor Predicting: 214it [02:20,  1.54it/s]Extractor Predicting: 215it [02:21,  1.53it/s]Extractor Predicting: 216it [02:22,  1.54it/s]Extractor Predicting: 217it [02:22,  1.54it/s]Extractor Predicting: 218it [02:23,  1.52it/s]Extractor Predicting: 219it [02:23,  1.55it/s]Extractor Predicting: 220it [02:24,  1.52it/s]Extractor Predicting: 221it [02:25,  1.49it/s]Extractor Predicting: 222it [02:26,  1.51it/s]Extractor Predicting: 223it [02:26,  1.52it/s]Extractor Predicting: 224it [02:27,  1.50it/s]Extractor Predicting: 225it [02:28,  1.47it/s]Extractor Predicting: 226it [02:28,  1.48it/s]Extractor Predicting: 227it [02:29,  1.50it/s]Extractor Predicting: 228it [02:30,  1.49it/s]Extractor Predicting: 229it [02:30,  1.47it/s]Extractor Predicting: 230it [02:31,  1.49it/s]Extractor Predicting: 231it [02:32,  1.46it/s]Extractor Predicting: 232it [02:32,  1.48it/s]Extractor Predicting: 233it [02:33,  1.41it/s]Extractor Predicting: 234it [02:34,  1.40it/s]Extractor Predicting: 235it [02:35,  1.40it/s]Extractor Predicting: 236it [02:35,  1.42it/s]Extractor Predicting: 237it [02:36,  1.42it/s]Extractor Predicting: 238it [02:37,  1.40it/s]Extractor Predicting: 239it [02:37,  1.45it/s]Extractor Predicting: 240it [02:38,  1.45it/s]Extractor Predicting: 241it [02:39,  1.29it/s]Extractor Predicting: 242it [02:40,  1.33it/s]Extractor Predicting: 243it [02:40,  1.34it/s]Extractor Predicting: 244it [02:41,  1.35it/s]Extractor Predicting: 245it [02:42,  1.41it/s]Extractor Predicting: 246it [02:42,  1.46it/s]Extractor Predicting: 247it [02:43,  1.65it/s]Extractor Predicting: 247it [02:43,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:37,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:37,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:37,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:37,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:37,822 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:10:38,129 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:10:38,130 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:10:38,398 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:10:39,448 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:10:39,448 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:41,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:41,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:41,974 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:41,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:10:41,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:10:42,304 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:10:42,305 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:10:42,977 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:10:43,132 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:10:43,132 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.551417004048583,
  "recall": 0.10497918914752583,
  "score": 0.17637917637917638,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'manufacturer', 'mother', 'occupant', 'organization directed by the office or position', 'owned by', 'part of', 'screenwriter', 'sport', 'sports discipline competed in', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26970
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27070, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.42it/s]Extractor Predicting: 10it [00:06,  1.41it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:12,  1.53it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:14,  1.53it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:16,  1.54it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:17,  1.53it/s]Extractor Predicting: 27it [00:18,  1.57it/s]Extractor Predicting: 28it [00:18,  1.56it/s]Extractor Predicting: 29it [00:19,  1.57it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.54it/s]Extractor Predicting: 32it [00:21,  1.55it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:25,  1.55it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:27,  1.54it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:32,  1.36it/s]Extractor Predicting: 49it [00:32,  1.41it/s]Extractor Predicting: 50it [00:33,  1.44it/s]Extractor Predicting: 51it [00:34,  1.45it/s]Extractor Predicting: 52it [00:34,  1.44it/s]Extractor Predicting: 53it [00:35,  1.45it/s]Extractor Predicting: 54it [00:36,  1.45it/s]Extractor Predicting: 55it [00:36,  1.47it/s]Extractor Predicting: 56it [00:37,  1.47it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:38,  1.47it/s]Extractor Predicting: 59it [00:39,  1.51it/s]Extractor Predicting: 60it [00:40,  1.54it/s]Extractor Predicting: 61it [00:40,  1.52it/s]Extractor Predicting: 62it [00:41,  1.50it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:44,  1.52it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:45,  1.51it/s]Extractor Predicting: 69it [00:46,  1.47it/s]Extractor Predicting: 70it [00:46,  1.49it/s]Extractor Predicting: 71it [00:47,  1.49it/s]Extractor Predicting: 72it [00:48,  1.49it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:49,  1.56it/s]Extractor Predicting: 75it [00:49,  1.55it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:51,  1.50it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:52,  1.55it/s]Extractor Predicting: 80it [00:53,  1.54it/s]Extractor Predicting: 81it [00:53,  1.53it/s]Extractor Predicting: 82it [00:54,  1.56it/s]Extractor Predicting: 83it [00:55,  1.56it/s]Extractor Predicting: 84it [00:55,  1.56it/s]Extractor Predicting: 85it [00:56,  1.59it/s]Extractor Predicting: 86it [00:57,  1.61it/s]Extractor Predicting: 87it [00:57,  1.57it/s]Extractor Predicting: 88it [00:58,  1.58it/s]Extractor Predicting: 89it [00:58,  1.56it/s]Extractor Predicting: 90it [00:59,  1.56it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:00,  1.56it/s]Extractor Predicting: 93it [01:01,  1.55it/s]Extractor Predicting: 94it [01:02,  1.57it/s]Extractor Predicting: 95it [01:02,  1.58it/s]Extractor Predicting: 96it [01:03,  1.57it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:04,  1.52it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:06,  1.58it/s]Extractor Predicting: 101it [01:06,  1.57it/s]Extractor Predicting: 102it [01:07,  1.54it/s]Extractor Predicting: 103it [01:08,  1.52it/s]Extractor Predicting: 104it [01:08,  1.56it/s]Extractor Predicting: 105it [01:09,  1.56it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.55it/s]Extractor Predicting: 108it [01:11,  1.56it/s]Extractor Predicting: 109it [01:11,  1.58it/s]Extractor Predicting: 110it [01:12,  1.59it/s]Extractor Predicting: 111it [01:13,  1.60it/s]Extractor Predicting: 112it [01:13,  1.55it/s]Extractor Predicting: 113it [01:14,  1.56it/s]Extractor Predicting: 114it [01:15,  1.51it/s]Extractor Predicting: 115it [01:15,  1.49it/s]Extractor Predicting: 116it [01:16,  1.51it/s]Extractor Predicting: 117it [01:17,  1.48it/s]Extractor Predicting: 118it [01:17,  1.51it/s]Extractor Predicting: 119it [01:18,  1.48it/s]Extractor Predicting: 120it [01:19,  1.50it/s]Extractor Predicting: 121it [01:19,  1.50it/s]Extractor Predicting: 122it [01:20,  1.49it/s]Extractor Predicting: 123it [01:21,  1.47it/s]Extractor Predicting: 124it [01:21,  1.45it/s]Extractor Predicting: 125it [01:22,  1.45it/s]Extractor Predicting: 126it [01:23,  1.45it/s]Extractor Predicting: 127it [01:23,  1.46it/s]Extractor Predicting: 128it [01:24,  1.43it/s]Extractor Predicting: 129it [01:25,  1.44it/s]Extractor Predicting: 130it [01:25,  1.47it/s]Extractor Predicting: 131it [01:26,  1.49it/s]Extractor Predicting: 132it [01:27,  1.50it/s]Extractor Predicting: 133it [01:27,  1.53it/s]Extractor Predicting: 134it [01:28,  1.55it/s]Extractor Predicting: 135it [01:29,  1.55it/s]Extractor Predicting: 136it [01:29,  1.54it/s]Extractor Predicting: 137it [01:30,  1.58it/s]Extractor Predicting: 138it [01:31,  1.57it/s]Extractor Predicting: 139it [01:31,  1.59it/s]Extractor Predicting: 140it [01:32,  1.58it/s]Extractor Predicting: 141it [01:33,  1.55it/s]Extractor Predicting: 142it [01:33,  1.54it/s]Extractor Predicting: 143it [01:34,  1.56it/s]Extractor Predicting: 144it [01:34,  1.56it/s]Extractor Predicting: 145it [01:35,  1.57it/s]Extractor Predicting: 146it [01:36,  1.54it/s]Extractor Predicting: 147it [01:37,  1.35it/s]Extractor Predicting: 148it [01:37,  1.40it/s]Extractor Predicting: 149it [01:38,  1.43it/s]Extractor Predicting: 150it [01:39,  1.44it/s]Extractor Predicting: 151it [01:39,  1.40it/s]Extractor Predicting: 152it [01:40,  1.42it/s]Extractor Predicting: 153it [01:41,  1.43it/s]Extractor Predicting: 154it [01:41,  1.45it/s]Extractor Predicting: 155it [01:42,  1.41it/s]Extractor Predicting: 156it [01:43,  1.39it/s]Extractor Predicting: 157it [01:44,  1.38it/s]Extractor Predicting: 158it [01:44,  1.40it/s]Extractor Predicting: 159it [01:45,  1.41it/s]Extractor Predicting: 160it [01:46,  1.42it/s]Extractor Predicting: 161it [01:47,  1.42it/s]Extractor Predicting: 162it [01:47,  1.43it/s]Extractor Predicting: 163it [01:48,  1.48it/s]Extractor Predicting: 164it [01:48,  1.58it/s]Extractor Predicting: 165it [01:49,  1.56it/s]Extractor Predicting: 166it [01:50,  1.53it/s]Extractor Predicting: 167it [01:50,  1.52it/s]Extractor Predicting: 168it [01:51,  1.49it/s]Extractor Predicting: 169it [01:52,  1.48it/s]Extractor Predicting: 170it [01:52,  1.52it/s]Extractor Predicting: 171it [01:53,  1.50it/s]Extractor Predicting: 172it [01:54,  1.49it/s]Extractor Predicting: 173it [01:54,  1.51it/s]Extractor Predicting: 174it [01:55,  1.50it/s]Extractor Predicting: 175it [01:56,  1.57it/s]Extractor Predicting: 176it [01:56,  1.61it/s]Extractor Predicting: 177it [01:57,  1.61it/s]Extractor Predicting: 178it [01:57,  1.61it/s]Extractor Predicting: 179it [01:58,  1.58it/s]Extractor Predicting: 180it [01:59,  1.53it/s]Extractor Predicting: 181it [01:59,  1.52it/s]Extractor Predicting: 182it [02:00,  1.55it/s]Extractor Predicting: 183it [02:01,  1.54it/s]Extractor Predicting: 184it [02:01,  1.57it/s]Extractor Predicting: 185it [02:02,  1.59it/s]Extractor Predicting: 186it [02:03,  1.63it/s]Extractor Predicting: 187it [02:03,  1.58it/s]Extractor Predicting: 188it [02:04,  1.57it/s]Extractor Predicting: 189it [02:05,  1.57it/s]Extractor Predicting: 190it [02:05,  1.58it/s]Extractor Predicting: 191it [02:06,  1.56it/s]Extractor Predicting: 192it [02:06,  1.55it/s]Extractor Predicting: 193it [02:07,  1.52it/s]Extractor Predicting: 194it [02:08,  1.51it/s]Extractor Predicting: 195it [02:08,  1.55it/s]Extractor Predicting: 196it [02:09,  1.55it/s]Extractor Predicting: 197it [02:10,  1.57it/s]Extractor Predicting: 198it [02:10,  1.59it/s]Extractor Predicting: 199it [02:11,  1.58it/s]Extractor Predicting: 200it [02:12,  1.57it/s]Extractor Predicting: 201it [02:12,  1.58it/s]Extractor Predicting: 202it [02:13,  1.55it/s]Extractor Predicting: 203it [02:14,  1.54it/s]Extractor Predicting: 204it [02:14,  1.57it/s]Extractor Predicting: 205it [02:15,  1.58it/s]Extractor Predicting: 206it [02:15,  1.56it/s]Extractor Predicting: 207it [02:16,  1.64it/s]Extractor Predicting: 208it [02:17,  1.58it/s]Extractor Predicting: 209it [02:17,  1.59it/s]Extractor Predicting: 210it [02:18,  1.54it/s]Extractor Predicting: 211it [02:19,  1.55it/s]Extractor Predicting: 212it [02:19,  1.51it/s]Extractor Predicting: 213it [02:20,  1.48it/s]Extractor Predicting: 214it [02:21,  1.52it/s]Extractor Predicting: 215it [02:21,  1.53it/s]Extractor Predicting: 216it [02:22,  1.53it/s]Extractor Predicting: 217it [02:23,  1.52it/s]Extractor Predicting: 218it [02:23,  1.53it/s]Extractor Predicting: 219it [02:24,  1.54it/s]Extractor Predicting: 220it [02:24,  1.56it/s]Extractor Predicting: 221it [02:25,  1.59it/s]Extractor Predicting: 222it [02:26,  1.54it/s]Extractor Predicting: 223it [02:26,  1.53it/s]Extractor Predicting: 224it [02:27,  1.53it/s]Extractor Predicting: 225it [02:28,  1.54it/s]Extractor Predicting: 226it [02:28,  1.55it/s]Extractor Predicting: 227it [02:29,  1.56it/s]Extractor Predicting: 228it [02:30,  1.55it/s]Extractor Predicting: 229it [02:30,  1.55it/s]Extractor Predicting: 230it [02:31,  1.53it/s]Extractor Predicting: 231it [02:32,  1.55it/s]Extractor Predicting: 232it [02:32,  1.56it/s]Extractor Predicting: 233it [02:33,  1.58it/s]Extractor Predicting: 234it [02:33,  1.58it/s]Extractor Predicting: 235it [02:34,  1.56it/s]Extractor Predicting: 236it [02:35,  1.55it/s]Extractor Predicting: 237it [02:35,  1.52it/s]Extractor Predicting: 238it [02:36,  1.56it/s]Extractor Predicting: 239it [02:37,  1.54it/s]Extractor Predicting: 240it [02:37,  1.56it/s]Extractor Predicting: 241it [02:38,  1.57it/s]Extractor Predicting: 242it [02:39,  1.56it/s]Extractor Predicting: 243it [02:39,  1.59it/s]Extractor Predicting: 244it [02:40,  1.55it/s]Extractor Predicting: 245it [02:41,  1.57it/s]Extractor Predicting: 246it [02:41,  1.56it/s]Extractor Predicting: 247it [02:42,  1.59it/s]Extractor Predicting: 248it [02:42,  1.57it/s]Extractor Predicting: 249it [02:43,  1.59it/s]Extractor Predicting: 250it [02:44,  1.60it/s]Extractor Predicting: 251it [02:44,  1.55it/s]Extractor Predicting: 252it [02:45,  1.57it/s]Extractor Predicting: 253it [02:46,  1.60it/s]Extractor Predicting: 254it [02:46,  1.60it/s]Extractor Predicting: 255it [02:47,  1.58it/s]Extractor Predicting: 256it [02:48,  1.55it/s]Extractor Predicting: 257it [02:48,  1.49it/s]Extractor Predicting: 258it [02:49,  1.45it/s]Extractor Predicting: 259it [02:50,  1.44it/s]Extractor Predicting: 260it [02:50,  1.47it/s]Extractor Predicting: 261it [02:51,  1.47it/s]Extractor Predicting: 262it [02:52,  1.48it/s]Extractor Predicting: 263it [02:52,  1.51it/s]Extractor Predicting: 264it [02:53,  1.50it/s]Extractor Predicting: 265it [02:54,  1.54it/s]Extractor Predicting: 266it [02:55,  1.37it/s]Extractor Predicting: 267it [02:55,  1.43it/s]Extractor Predicting: 268it [02:56,  1.46it/s]Extractor Predicting: 269it [02:56,  1.48it/s]Extractor Predicting: 270it [02:57,  1.51it/s]Extractor Predicting: 271it [02:58,  1.53it/s]Extractor Predicting: 272it [02:58,  1.51it/s]Extractor Predicting: 273it [02:59,  1.55it/s]Extractor Predicting: 274it [03:00,  1.52it/s]Extractor Predicting: 275it [03:00,  1.55it/s]Extractor Predicting: 276it [03:01,  1.55it/s]Extractor Predicting: 277it [03:02,  1.55it/s]Extractor Predicting: 278it [03:02,  1.53it/s]Extractor Predicting: 279it [03:03,  1.52it/s]Extractor Predicting: 280it [03:04,  1.53it/s]Extractor Predicting: 281it [03:04,  1.54it/s]Extractor Predicting: 282it [03:05,  1.54it/s]Extractor Predicting: 283it [03:06,  1.55it/s]Extractor Predicting: 284it [03:06,  1.53it/s]Extractor Predicting: 285it [03:07,  1.51it/s]Extractor Predicting: 286it [03:08,  1.50it/s]Extractor Predicting: 287it [03:08,  1.53it/s]Extractor Predicting: 288it [03:09,  1.53it/s]Extractor Predicting: 289it [03:09,  1.54it/s]Extractor Predicting: 290it [03:10,  1.53it/s]Extractor Predicting: 291it [03:11,  1.54it/s]Extractor Predicting: 292it [03:11,  1.56it/s]Extractor Predicting: 293it [03:12,  1.56it/s]Extractor Predicting: 294it [03:13,  1.56it/s]Extractor Predicting: 295it [03:13,  1.55it/s]Extractor Predicting: 296it [03:14,  1.52it/s]Extractor Predicting: 297it [03:15,  1.52it/s]Extractor Predicting: 298it [03:15,  1.51it/s]Extractor Predicting: 299it [03:16,  1.50it/s]Extractor Predicting: 300it [03:17,  1.48it/s]Extractor Predicting: 301it [03:17,  1.48it/s]Extractor Predicting: 302it [03:18,  1.49it/s]Extractor Predicting: 303it [03:19,  1.53it/s]Extractor Predicting: 304it [03:19,  1.58it/s]Extractor Predicting: 305it [03:20,  1.58it/s]Extractor Predicting: 306it [03:21,  1.61it/s]Extractor Predicting: 307it [03:21,  1.62it/s]Extractor Predicting: 308it [03:22,  1.63it/s]Extractor Predicting: 309it [03:22,  1.65it/s]Extractor Predicting: 310it [03:23,  1.64it/s]Extractor Predicting: 311it [03:24,  1.64it/s]Extractor Predicting: 312it [03:24,  1.59it/s]Extractor Predicting: 313it [03:25,  1.61it/s]Extractor Predicting: 314it [03:26,  1.54it/s]Extractor Predicting: 315it [03:26,  1.52it/s]Extractor Predicting: 316it [03:27,  1.54it/s]Extractor Predicting: 317it [03:27,  1.58it/s]Extractor Predicting: 318it [03:28,  1.56it/s]Extractor Predicting: 319it [03:29,  1.59it/s]Extractor Predicting: 320it [03:29,  1.58it/s]Extractor Predicting: 321it [03:30,  1.57it/s]Extractor Predicting: 322it [03:31,  1.60it/s]Extractor Predicting: 323it [03:31,  1.60it/s]Extractor Predicting: 324it [03:32,  1.59it/s]Extractor Predicting: 325it [03:33,  1.56it/s]Extractor Predicting: 326it [03:33,  1.57it/s]Extractor Predicting: 327it [03:34,  1.61it/s]Extractor Predicting: 328it [03:34,  1.62it/s]Extractor Predicting: 329it [03:35,  1.65it/s]Extractor Predicting: 330it [03:36,  1.63it/s]Extractor Predicting: 331it [03:36,  1.64it/s]Extractor Predicting: 332it [03:37,  1.61it/s]Extractor Predicting: 333it [03:37,  1.61it/s]Extractor Predicting: 334it [03:38,  1.63it/s]Extractor Predicting: 335it [03:39,  1.59it/s]Extractor Predicting: 336it [03:39,  1.55it/s]Extractor Predicting: 337it [03:40,  1.51it/s]Extractor Predicting: 338it [03:41,  1.54it/s]Extractor Predicting: 339it [03:41,  1.56it/s]Extractor Predicting: 340it [03:42,  1.58it/s]Extractor Predicting: 341it [03:43,  1.56it/s]Extractor Predicting: 342it [03:43,  1.51it/s]Extractor Predicting: 343it [03:44,  1.47it/s]Extractor Predicting: 344it [03:45,  1.49it/s]Extractor Predicting: 345it [03:45,  1.49it/s]Extractor Predicting: 346it [03:46,  1.53it/s]Extractor Predicting: 347it [03:47,  1.55it/s]Extractor Predicting: 348it [03:47,  1.56it/s]Extractor Predicting: 349it [03:48,  1.57it/s]Extractor Predicting: 350it [03:48,  1.55it/s]Extractor Predicting: 351it [03:49,  1.58it/s]Extractor Predicting: 352it [03:50,  1.56it/s]Extractor Predicting: 353it [03:50,  1.60it/s]Extractor Predicting: 354it [03:51,  1.59it/s]Extractor Predicting: 355it [03:52,  1.55it/s]Extractor Predicting: 356it [03:52,  1.56it/s]Extractor Predicting: 357it [03:53,  1.57it/s]Extractor Predicting: 358it [03:54,  1.55it/s]Extractor Predicting: 359it [03:54,  1.55it/s]Extractor Predicting: 360it [03:55,  1.55it/s]Extractor Predicting: 361it [03:56,  1.51it/s]Extractor Predicting: 362it [03:56,  1.53it/s]Extractor Predicting: 363it [03:57,  1.52it/s]Extractor Predicting: 364it [03:58,  1.52it/s]Extractor Predicting: 365it [03:58,  1.49it/s]Extractor Predicting: 366it [03:59,  1.48it/s]Extractor Predicting: 367it [04:00,  1.50it/s]Extractor Predicting: 368it [04:00,  1.56it/s]Extractor Predicting: 369it [04:01,  1.57it/s]Extractor Predicting: 370it [04:01,  1.57it/s]Extractor Predicting: 371it [04:02,  1.58it/s]Extractor Predicting: 372it [04:03,  1.58it/s]Extractor Predicting: 373it [04:03,  1.56it/s]Extractor Predicting: 374it [04:04,  1.58it/s]Extractor Predicting: 375it [04:05,  1.62it/s]Extractor Predicting: 376it [04:05,  1.61it/s]Extractor Predicting: 377it [04:06,  1.53it/s]Extractor Predicting: 378it [04:07,  1.46it/s]Extractor Predicting: 379it [04:07,  1.51it/s]Extractor Predicting: 380it [04:08,  1.50it/s]Extractor Predicting: 381it [04:09,  1.53it/s]Extractor Predicting: 382it [04:09,  1.54it/s]Extractor Predicting: 383it [04:10,  1.53it/s]Extractor Predicting: 384it [04:10,  1.54it/s]Extractor Predicting: 385it [04:11,  1.57it/s]Extractor Predicting: 386it [04:12,  1.55it/s]Extractor Predicting: 387it [04:12,  1.57it/s]Extractor Predicting: 388it [04:13,  1.60it/s]Extractor Predicting: 389it [04:14,  1.59it/s]Extractor Predicting: 390it [04:14,  1.56it/s]Extractor Predicting: 391it [04:15,  1.55it/s]Extractor Predicting: 392it [04:16,  1.38it/s]Extractor Predicting: 393it [04:17,  1.43it/s]Extractor Predicting: 394it [04:17,  1.48it/s]Extractor Predicting: 395it [04:18,  1.50it/s]Extractor Predicting: 396it [04:18,  1.52it/s]Extractor Predicting: 397it [04:19,  1.55it/s]Extractor Predicting: 398it [04:20,  1.60it/s]Extractor Predicting: 399it [04:20,  1.56it/s]Extractor Predicting: 400it [04:21,  1.60it/s]Extractor Predicting: 401it [04:21,  1.60it/s]Extractor Predicting: 402it [04:22,  1.61it/s]Extractor Predicting: 403it [04:23,  1.58it/s]Extractor Predicting: 404it [04:23,  1.56it/s]Extractor Predicting: 405it [04:24,  1.56it/s]Extractor Predicting: 406it [04:25,  1.54it/s]Extractor Predicting: 407it [04:25,  1.53it/s]Extractor Predicting: 408it [04:26,  1.54it/s]Extractor Predicting: 409it [04:27,  1.51it/s]Extractor Predicting: 410it [04:27,  1.47it/s]Extractor Predicting: 411it [04:28,  1.51it/s]Extractor Predicting: 412it [04:29,  1.53it/s]Extractor Predicting: 413it [04:29,  1.48it/s]Extractor Predicting: 414it [04:30,  1.48it/s]Extractor Predicting: 415it [04:31,  1.52it/s]Extractor Predicting: 416it [04:31,  1.50it/s]Extractor Predicting: 417it [04:32,  1.49it/s]Extractor Predicting: 418it [04:33,  1.45it/s]Extractor Predicting: 419it [04:33,  1.48it/s]Extractor Predicting: 420it [04:34,  1.42it/s]Extractor Predicting: 421it [04:35,  1.42it/s]Extractor Predicting: 422it [04:36,  1.44it/s]Extractor Predicting: 423it [04:36,  1.48it/s]Extractor Predicting: 424it [04:37,  1.50it/s]Extractor Predicting: 425it [04:38,  1.52it/s]Extractor Predicting: 426it [04:38,  1.50it/s]Extractor Predicting: 427it [04:39,  1.47it/s]Extractor Predicting: 428it [04:40,  1.48it/s]Extractor Predicting: 429it [04:40,  1.51it/s]Extractor Predicting: 430it [04:41,  1.46it/s]Extractor Predicting: 431it [04:42,  1.46it/s]Extractor Predicting: 432it [04:42,  1.46it/s]Extractor Predicting: 433it [04:43,  1.49it/s]Extractor Predicting: 434it [04:44,  1.51it/s]Extractor Predicting: 435it [04:44,  1.57it/s]Extractor Predicting: 436it [04:45,  1.65it/s]Extractor Predicting: 437it [04:45,  1.72it/s]Extractor Predicting: 438it [04:46,  1.67it/s]Extractor Predicting: 439it [04:46,  1.69it/s]Extractor Predicting: 440it [04:47,  1.78it/s]Extractor Predicting: 441it [04:48,  1.66it/s]Extractor Predicting: 442it [04:48,  1.55it/s]Extractor Predicting: 443it [04:49,  1.49it/s]Extractor Predicting: 444it [04:50,  1.46it/s]Extractor Predicting: 445it [04:51,  1.45it/s]Extractor Predicting: 446it [04:51,  1.43it/s]Extractor Predicting: 447it [04:52,  1.41it/s]Extractor Predicting: 448it [04:53,  1.40it/s]Extractor Predicting: 449it [04:53,  1.39it/s]Extractor Predicting: 450it [04:54,  1.41it/s]Extractor Predicting: 451it [04:55,  1.41it/s]Extractor Predicting: 452it [04:56,  1.40it/s]Extractor Predicting: 453it [04:56,  1.42it/s]Extractor Predicting: 454it [04:57,  1.43it/s]Extractor Predicting: 455it [04:58,  1.41it/s]Extractor Predicting: 456it [04:58,  1.41it/s]Extractor Predicting: 457it [04:59,  1.38it/s]Extractor Predicting: 458it [05:00,  1.40it/s]Extractor Predicting: 459it [05:01,  1.42it/s]Extractor Predicting: 460it [05:01,  1.43it/s]Extractor Predicting: 461it [05:02,  1.45it/s]Extractor Predicting: 462it [05:03,  1.46it/s]Extractor Predicting: 463it [05:03,  1.46it/s]Extractor Predicting: 464it [05:04,  1.44it/s]Extractor Predicting: 465it [05:05,  1.48it/s]Extractor Predicting: 466it [05:05,  1.70it/s]Extractor Predicting: 466it [05:05,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:59,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:59,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:59,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:59,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:59,164 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:15:59,789 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:15:59,790 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:16:00,367 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:16:01,432 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:16:01,432 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:16:04,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:16:04,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:16:04,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:16:04,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:16:04,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:16:04,981 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:16:04,982 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:16:05,549 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:16:05,703 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:16:05,703 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3292307692307692,
  "recall": 0.06706061420001791,
  "score": 0.1114251710800357,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'manufacturer', 'mother', 'occupant', 'organization directed by the office or position', 'owned by', 'part of', 'screenwriter', 'sport', 'sports discipline competed in', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8266
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8366, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.40it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.41it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:09,  1.41it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.41it/s]Extractor Predicting: 17it [00:12,  1.35it/s]Extractor Predicting: 18it [00:12,  1.42it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:14,  1.45it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:16,  1.46it/s]Extractor Predicting: 24it [00:16,  1.46it/s]Extractor Predicting: 25it [00:17,  1.45it/s]Extractor Predicting: 26it [00:18,  1.46it/s]Extractor Predicting: 27it [00:18,  1.41it/s]Extractor Predicting: 28it [00:19,  1.41it/s]Extractor Predicting: 29it [00:20,  1.42it/s]Extractor Predicting: 30it [00:20,  1.45it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:22,  1.44it/s]Extractor Predicting: 33it [00:23,  1.44it/s]Extractor Predicting: 34it [00:23,  1.44it/s]Extractor Predicting: 35it [00:24,  1.36it/s]Extractor Predicting: 36it [00:25,  1.40it/s]Extractor Predicting: 37it [00:25,  1.41it/s]Extractor Predicting: 38it [00:26,  1.43it/s]Extractor Predicting: 39it [00:27,  1.41it/s]Extractor Predicting: 40it [00:28,  1.39it/s]Extractor Predicting: 41it [00:28,  1.38it/s]Extractor Predicting: 42it [00:29,  1.37it/s]Extractor Predicting: 43it [00:30,  1.37it/s]Extractor Predicting: 44it [00:31,  1.36it/s]Extractor Predicting: 45it [00:31,  1.37it/s]Extractor Predicting: 46it [00:32,  1.37it/s]Extractor Predicting: 47it [00:33,  1.36it/s]Extractor Predicting: 48it [00:33,  1.35it/s]Extractor Predicting: 49it [00:34,  1.35it/s]Extractor Predicting: 50it [00:35,  1.36it/s]Extractor Predicting: 51it [00:36,  1.36it/s]Extractor Predicting: 52it [00:36,  1.36it/s]Extractor Predicting: 53it [00:37,  1.40it/s]Extractor Predicting: 54it [00:38,  1.42it/s]Extractor Predicting: 55it [00:38,  1.45it/s]Extractor Predicting: 56it [00:39,  1.43it/s]Extractor Predicting: 57it [00:40,  1.43it/s]Extractor Predicting: 58it [00:41,  1.40it/s]Extractor Predicting: 59it [00:41,  1.39it/s]Extractor Predicting: 60it [00:42,  1.38it/s]Extractor Predicting: 61it [00:43,  1.39it/s]Extractor Predicting: 62it [00:43,  1.42it/s]Extractor Predicting: 63it [00:44,  1.42it/s]Extractor Predicting: 64it [00:45,  1.43it/s]Extractor Predicting: 65it [00:46,  1.42it/s]Extractor Predicting: 66it [00:46,  1.44it/s]Extractor Predicting: 67it [00:47,  1.45it/s]Extractor Predicting: 68it [00:48,  1.47it/s]Extractor Predicting: 69it [00:48,  1.49it/s]Extractor Predicting: 70it [00:49,  1.50it/s]Extractor Predicting: 71it [00:50,  1.47it/s]Extractor Predicting: 72it [00:50,  1.47it/s]Extractor Predicting: 73it [00:51,  1.45it/s]Extractor Predicting: 74it [00:52,  1.43it/s]Extractor Predicting: 75it [00:52,  1.45it/s]Extractor Predicting: 76it [00:53,  1.44it/s]Extractor Predicting: 77it [00:54,  1.41it/s]Extractor Predicting: 78it [00:54,  1.43it/s]Extractor Predicting: 79it [00:55,  1.44it/s]Extractor Predicting: 80it [00:56,  1.43it/s]Extractor Predicting: 81it [00:57,  1.42it/s]Extractor Predicting: 82it [00:57,  1.44it/s]Extractor Predicting: 83it [00:58,  1.44it/s]Extractor Predicting: 84it [00:59,  1.44it/s]Extractor Predicting: 85it [00:59,  1.45it/s]Extractor Predicting: 86it [01:00,  1.44it/s]Extractor Predicting: 87it [01:01,  1.45it/s]Extractor Predicting: 88it [01:01,  1.42it/s]Extractor Predicting: 89it [01:02,  1.45it/s]Extractor Predicting: 90it [01:03,  1.48it/s]Extractor Predicting: 91it [01:03,  1.49it/s]Extractor Predicting: 92it [01:04,  1.62it/s]Extractor Predicting: 92it [01:04,  1.43it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:17:11,531 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:17:11,532 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:17:11,535 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:17:11,536 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:17:11,538 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:17:14,616 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:17:14,618 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:17:14,633 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:17:14,633 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_2/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:17:14,638 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:17:14,641 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:17:14,641 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:17:14,642 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:17:14,642 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:17:14,642 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:17:14,642 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3012232415902141,
  "recall": 0.03865777080062795,
  "score": 0.0685217391304348,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:17:14,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:15,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:16,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:17,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:18,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:18,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:19,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:20,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:20,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:21,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:22,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:22,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:23,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:24,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:24,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:25,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:26,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:26,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:27,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:27,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:28,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:29,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:45, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:29,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:30,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:31,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:32,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:32,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:33,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:34,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:34,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:35,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:36,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:36,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:37,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:38,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:38,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:39,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:40,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:41,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:42,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:42,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:43,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:44,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:45,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:45,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:47, 15.97s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:46,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:47,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:48,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:48,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:49,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:50,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:50,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:51,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:52,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:53,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:53,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:54,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:55,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:56,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:56,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:57,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:58,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:58,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:59,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:00,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:00,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:01,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:02,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:48<04:36, 16.29s/it][WARNING|generation_utils.py:914] 2023-08-29 05:18:03,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:03,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:04,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:05,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:06,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:06,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:07,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:08,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:09,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:09,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:10,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:11,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:11,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:12,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:13,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:14,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:14,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:15,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:16,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:17,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:18,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:18,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:04<04:19, 16.24s/it][WARNING|generation_utils.py:914] 2023-08-29 05:18:19,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:20,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:21,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:21,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:22,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:23,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:24,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:24,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:25,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:26,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:27,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:28,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:28,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:29,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:30,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:31,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:32,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:33,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:33,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:34,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:35,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:36,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:36,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:22<04:14, 16.99s/it][WARNING|generation_utils.py:914] 2023-08-29 05:18:37,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:38,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:39,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:39,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:40,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:41,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:41,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:42,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:43,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:44,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:44,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:45,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:46,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:46,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:47,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:48,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:49,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:49,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:50,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:51,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:52,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:38<03:49, 16.40s/it][WARNING|generation_utils.py:914] 2023-08-29 05:18:52,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:53,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:54,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:55,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:55,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:56,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:57,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:58,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:58,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:59,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:00,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:00,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:01,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:02,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:03,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:03,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:04,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:05,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:05,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:06,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:07,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:53<03:27, 15.94s/it][WARNING|generation_utils.py:914] 2023-08-29 05:19:07,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:08,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:09,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:10,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:10,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:11,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:12,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:12,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:13,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:15,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:15,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:16,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:17,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:17,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:18,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:19,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:20,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:20,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:21,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:22,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:08<03:08, 15.67s/it][WARNING|generation_utils.py:914] 2023-08-29 05:19:23,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:23,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:24,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:25,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:26,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:27,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:27,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:28,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:29,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:30,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:31,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:31,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:32,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:33,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:34,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:35,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:35,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:36,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:37,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:38,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:38,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:39,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:40,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:26<03:01, 16.47s/it][WARNING|generation_utils.py:914] 2023-08-29 05:19:41,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:41,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:42,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:43,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:44,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:45,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:45,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:46,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:47,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:47,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:48,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:49,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:49,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:50,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:51,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:52,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:52,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:53,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:54,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:54,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:40<02:37, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-29 05:19:55,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:56,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:56,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:57,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:58,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:59,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:00,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:00,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:01,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:02,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:03,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:04,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:05,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:06,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:06,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:07,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:08,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:09,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:09,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:10,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:11,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:12,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:58<02:27, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-29 05:20:13,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:14,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:14,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:15,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:16,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:17,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:17,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:18,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:19,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:19,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:20,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:21,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:21,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:22,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:23,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:24,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:24,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:25,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:26,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:27,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:28,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:28,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:14<02:11, 16.40s/it][WARNING|generation_utils.py:914] 2023-08-29 05:20:29,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:30,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:31,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:31,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:32,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:33,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:34,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:34,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:35,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:36,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:37,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:37,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:38,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:39,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:39,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:40,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:41,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:41,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:42,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:43,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:44,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:44,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:30<01:53, 16.28s/it][WARNING|generation_utils.py:914] 2023-08-29 05:20:45,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:46,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:47,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:47,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:48,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:49,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:50,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:51,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:51,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:52,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:53,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:54,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:54,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:55,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:56,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:57,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:57,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:58,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:59,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:00,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:00,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:01,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:47<01:38, 16.39s/it][WARNING|generation_utils.py:914] 2023-08-29 05:21:02,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:03,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:03,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:04,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:05,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:06,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:06,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:07,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:08,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:09,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:09,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:10,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:11,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:11,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:12,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:13,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:13,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:14,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:15,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:16,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:16,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:17,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:18,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:04<01:22, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-29 05:21:19,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:19,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:20,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:21,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:22,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:22,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:23,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:24,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:24,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:25,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:26,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:26,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:27,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:28,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:28,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:29,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:30,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:31,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:31,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:32,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:33,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:18<01:03, 15.95s/it][WARNING|generation_utils.py:914] 2023-08-29 05:21:33,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:34,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:35,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:35,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:36,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:37,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:37,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:38,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:39,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:39,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:40,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:41,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:41,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:42,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:43,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:44,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:45,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:45,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:46,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:47,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:47,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:48,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:49,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:35<00:48, 16.01s/it][WARNING|generation_utils.py:914] 2023-08-29 05:21:49,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:50,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:51,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:51,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:52,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:53,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:54,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:54,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:55,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:56,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:56,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:57,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:58,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:58,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:59,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:00,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:00,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:01,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:02,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:02,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:03,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:04,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:05,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:50<00:31, 15.93s/it][WARNING|generation_utils.py:914] 2023-08-29 05:22:05,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:06,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:07,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:07,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:08,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:09,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:09,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:10,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:11,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:11,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:12,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:13,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:14,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:14,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:15,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:16,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:17,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:17,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:18,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:19,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:20,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:20,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:06<00:15, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-29 05:22:21,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:21,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:22,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:23,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:24,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:24,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:25,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:26,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:26,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:27,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:28,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:28,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:29,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:30,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:31,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:31,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:32,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:33,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:33,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:34,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:35,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:35,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:36,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:37,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:37,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:38,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:39,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:40,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:40,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:26<00:00, 17.13s/it]Generating: 100%|██████████| 20/20 [05:26<00:00, 16.33s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:48,688 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:48,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:48,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:48,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:48,692 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:22:49,326 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:22:49,327 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:22:49,892 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:22:50,972 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:22:50,972 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:53,966 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:53,971 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:53,971 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:53,971 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:53,971 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:22:54,652 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:22:54,653 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:22:55,238 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:22:55,401 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:22:55,401 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8192934782608695, 'errors': {'', "('French colonial government', 'military branch', '', 'It was established in 1859 by the French colonial government under Jacques Dassiné as a military unit during the American Civil War .')"}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : part of the series .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : place of death .', 'success_rate': 0.8383152173913043, 'errors': {'', "('Washington', 'place of death', '', 'The next day he was appointed assistant commissioner of the Washington , DC police force , where he would patrol the city , whereupon he was appointed a captain and first rank .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : director .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.94375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8165760869565217, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8636363636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : occupant .', 'success_rate': 0.8764204545454546, 'errors': {'', "('Roman architect', 'occupant', '', 'It is named after the famous Roman architect ( d.')", 'not enough values to unpack (expected 2, got 1)', "('Harvard University', 'occupant', '', 'D. in Physics from Harvard University , where he served on the faculty of the University of Michigan from 1988 to 1992 and on the faculty of New York University during the American Civil War .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.8835227272727273, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8934659090909091, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : part of .', 'success_rate': 0.8410326086956522, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : sport .', 'success_rate': 0.8505434782608695, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : use .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 81, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 123, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 192, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 234, 'raw': 352}
{'target': 600, 'success': 255, 'raw': 384}
{'target': 600, 'success': 277, 'raw': 416}
{'target': 600, 'success': 299, 'raw': 448}
{'target': 600, 'success': 318, 'raw': 480}
{'target': 600, 'success': 342, 'raw': 512}
{'target': 600, 'success': 362, 'raw': 544}
{'target': 600, 'success': 383, 'raw': 576}
{'target': 600, 'success': 405, 'raw': 608}
{'target': 600, 'success': 426, 'raw': 640}
{'target': 600, 'success': 448, 'raw': 672}
{'target': 600, 'success': 469, 'raw': 704}
{'target': 600, 'success': 490, 'raw': 736}
{'target': 600, 'success': 510, 'raw': 768}
{'target': 600, 'success': 535, 'raw': 800}
{'target': 600, 'success': 556, 'raw': 832}
{'target': 600, 'success': 573, 'raw': 864}
{'target': 600, 'success': 589, 'raw': 896}
{'target': 600, 'success': 610, 'raw': 928}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6573275862068966, 'errors': {'', "('John Lennon', 'voice type', '', 'The Beatles , John Lennon , Paul McCartney , John Lennon and Paul McCartney Jr. sang the praises of their music .')", "('Colin Farrell', 'voice type', '', 'In 2010 , he starred as Kaya in the BBC soap opera EastEnders alongside Paul Rowden and Matt Damon , while in 2011 he appeared in another soap opera starring Colin Farrell .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/2_ext.jsonl'}}
estimate vocab size: 14852
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14952, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.37it/s]Extractor Estimating: 2it [00:01,  1.51it/s]Extractor Estimating: 3it [00:02,  1.44it/s]Extractor Estimating: 4it [00:02,  1.56it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.61it/s]Extractor Estimating: 7it [00:04,  1.68it/s]Extractor Estimating: 8it [00:04,  1.74it/s]Extractor Estimating: 9it [00:05,  1.68it/s]Extractor Estimating: 10it [00:06,  1.68it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.66it/s]Extractor Estimating: 13it [00:07,  1.70it/s]Extractor Estimating: 14it [00:08,  1.72it/s]Extractor Estimating: 15it [00:09,  1.71it/s]Extractor Estimating: 16it [00:09,  1.69it/s]Extractor Estimating: 17it [00:10,  1.69it/s]Extractor Estimating: 18it [00:10,  1.68it/s]Extractor Estimating: 19it [00:11,  1.69it/s]Extractor Estimating: 20it [00:12,  1.72it/s]Extractor Estimating: 21it [00:12,  1.74it/s]Extractor Estimating: 22it [00:13,  1.72it/s]Extractor Estimating: 23it [00:13,  1.75it/s]Extractor Estimating: 24it [00:14,  1.73it/s]Extractor Estimating: 25it [00:14,  1.74it/s]Extractor Estimating: 26it [00:15,  1.69it/s]Extractor Estimating: 27it [00:16,  1.66it/s]Extractor Estimating: 28it [00:16,  1.58it/s]Extractor Estimating: 29it [00:17,  1.55it/s]Extractor Estimating: 30it [00:18,  1.57it/s]Extractor Estimating: 31it [00:18,  1.57it/s]Extractor Estimating: 32it [00:19,  1.62it/s]Extractor Estimating: 33it [00:20,  1.60it/s]Extractor Estimating: 34it [00:20,  1.60it/s]Extractor Estimating: 35it [00:21,  1.59it/s]Extractor Estimating: 36it [00:21,  1.58it/s]Extractor Estimating: 37it [00:22,  1.60it/s]Extractor Estimating: 38it [00:23,  1.57it/s]Extractor Estimating: 39it [00:23,  1.58it/s]Extractor Estimating: 40it [00:24,  1.57it/s]Extractor Estimating: 41it [00:25,  1.54it/s]Extractor Estimating: 42it [00:25,  1.54it/s]Extractor Estimating: 43it [00:26,  1.59it/s]Extractor Estimating: 44it [00:27,  1.49it/s]Extractor Estimating: 45it [00:27,  1.49it/s]Extractor Estimating: 46it [00:28,  1.48it/s]Extractor Estimating: 47it [00:29,  1.49it/s]Extractor Estimating: 48it [00:29,  1.55it/s]Extractor Estimating: 49it [00:30,  1.57it/s]Extractor Estimating: 50it [00:30,  1.59it/s]Extractor Estimating: 51it [00:31,  1.60it/s]Extractor Estimating: 52it [00:32,  1.55it/s]Extractor Estimating: 53it [00:32,  1.53it/s]Extractor Estimating: 54it [00:33,  1.55it/s]Extractor Estimating: 55it [00:34,  1.52it/s]Extractor Estimating: 56it [00:34,  1.54it/s]Extractor Estimating: 57it [00:35,  1.52it/s]Extractor Estimating: 58it [00:36,  1.58it/s]Extractor Estimating: 59it [00:36,  1.54it/s]Extractor Estimating: 60it [00:37,  1.54it/s]Extractor Estimating: 61it [00:38,  1.52it/s]Extractor Estimating: 62it [00:38,  1.49it/s]Extractor Estimating: 63it [00:39,  1.53it/s]Extractor Estimating: 64it [00:40,  1.56it/s]Extractor Estimating: 65it [00:40,  1.56it/s]Extractor Estimating: 66it [00:41,  1.54it/s]Extractor Estimating: 67it [00:42,  1.52it/s]Extractor Estimating: 68it [00:42,  1.52it/s]Extractor Estimating: 69it [00:43,  1.54it/s]Extractor Estimating: 70it [00:43,  1.56it/s]Extractor Estimating: 71it [00:44,  1.52it/s]Extractor Estimating: 72it [00:45,  1.49it/s]Extractor Estimating: 73it [00:46,  1.49it/s]Extractor Estimating: 74it [00:46,  1.52it/s]Extractor Estimating: 75it [00:47,  1.53it/s]Extractor Estimating: 76it [00:47,  1.55it/s]Extractor Estimating: 77it [00:48,  1.52it/s]Extractor Estimating: 78it [00:50,  1.05s/it]Extractor Estimating: 79it [00:51,  1.04it/s]Extractor Estimating: 80it [00:52,  1.15it/s]Extractor Estimating: 81it [00:52,  1.28it/s]Extractor Estimating: 82it [00:53,  1.36it/s]Extractor Estimating: 83it [00:53,  1.44it/s]Extractor Estimating: 84it [00:54,  1.49it/s]Extractor Estimating: 85it [00:55,  1.50it/s]Extractor Estimating: 86it [00:55,  1.52it/s]Extractor Estimating: 87it [00:56,  1.55it/s]Extractor Estimating: 88it [00:56,  1.58it/s]Extractor Estimating: 89it [00:57,  1.61it/s]Extractor Estimating: 90it [00:58,  1.61it/s]Extractor Estimating: 91it [00:58,  1.58it/s]Extractor Estimating: 92it [00:59,  1.60it/s]Extractor Estimating: 93it [01:00,  1.63it/s]Extractor Estimating: 94it [01:00,  1.63it/s]Extractor Estimating: 95it [01:01,  1.59it/s]Extractor Estimating: 96it [01:01,  1.58it/s]Extractor Estimating: 97it [01:02,  1.61it/s]Extractor Estimating: 98it [01:03,  1.63it/s]Extractor Estimating: 99it [01:03,  1.60it/s]Extractor Estimating: 100it [01:04,  1.57it/s]Extractor Estimating: 101it [01:05,  1.55it/s]Extractor Estimating: 102it [01:05,  1.55it/s]Extractor Estimating: 103it [01:06,  1.55it/s]Extractor Estimating: 104it [01:07,  1.50it/s]Extractor Estimating: 105it [01:07,  1.51it/s]Extractor Estimating: 106it [01:08,  1.49it/s]Extractor Estimating: 107it [01:09,  1.50it/s]Extractor Estimating: 108it [01:09,  1.46it/s]Extractor Estimating: 109it [01:10,  1.45it/s]Extractor Estimating: 110it [01:11,  1.46it/s]Extractor Estimating: 111it [01:11,  1.48it/s]Extractor Estimating: 112it [01:12,  1.47it/s]Extractor Estimating: 113it [01:13,  1.48it/s]Extractor Estimating: 114it [01:13,  1.48it/s]Extractor Estimating: 115it [01:14,  1.48it/s]Extractor Estimating: 116it [01:15,  1.52it/s]Extractor Estimating: 117it [01:15,  1.50it/s]Extractor Estimating: 118it [01:16,  1.46it/s]Extractor Estimating: 119it [01:17,  1.43it/s]Extractor Estimating: 120it [01:18,  1.45it/s]Extractor Estimating: 121it [01:18,  1.52it/s]Extractor Estimating: 122it [01:19,  1.52it/s]Extractor Estimating: 123it [01:19,  1.51it/s]Extractor Estimating: 124it [01:20,  1.49it/s]Extractor Estimating: 125it [01:21,  1.52it/s]Extractor Estimating: 126it [01:21,  1.53it/s]Extractor Estimating: 127it [01:22,  1.53it/s]Extractor Estimating: 128it [01:23,  1.57it/s]Extractor Estimating: 129it [01:23,  1.57it/s]Extractor Estimating: 130it [01:24,  1.57it/s]Extractor Estimating: 131it [01:25,  1.55it/s]Extractor Estimating: 132it [01:25,  1.56it/s]Extractor Estimating: 133it [01:26,  1.55it/s]Extractor Estimating: 134it [01:26,  1.61it/s]Extractor Estimating: 135it [01:27,  1.59it/s]Extractor Estimating: 136it [01:28,  1.60it/s]Extractor Estimating: 137it [01:28,  1.53it/s]Extractor Estimating: 138it [01:29,  1.53it/s]Extractor Estimating: 139it [01:30,  1.58it/s]Extractor Estimating: 140it [01:30,  1.59it/s]Extractor Estimating: 141it [01:31,  1.65it/s]Extractor Estimating: 142it [01:31,  1.65it/s]Extractor Estimating: 143it [01:32,  1.66it/s]Extractor Estimating: 144it [01:33,  1.65it/s]Extractor Estimating: 145it [01:33,  1.59it/s]Extractor Estimating: 146it [01:34,  1.63it/s]Extractor Estimating: 147it [01:35,  1.59it/s]Extractor Estimating: 148it [01:35,  1.48it/s]Extractor Estimating: 149it [01:36,  1.49it/s]Extractor Estimating: 150it [01:37,  1.50it/s]Extractor Estimating: 151it [01:37,  1.53it/s]Extractor Estimating: 152it [01:38,  1.52it/s]Extractor Estimating: 153it [01:39,  1.57it/s]Extractor Estimating: 154it [01:39,  1.51it/s]Extractor Estimating: 155it [01:40,  1.48it/s]Extractor Estimating: 156it [01:41,  1.48it/s]Extractor Estimating: 157it [01:41,  1.41it/s]Extractor Estimating: 158it [01:42,  1.45it/s]Extractor Estimating: 159it [01:43,  1.45it/s]Extractor Estimating: 160it [01:43,  1.44it/s]Extractor Estimating: 161it [01:44,  1.47it/s]Extractor Estimating: 162it [01:45,  1.31it/s]Extractor Estimating: 163it [01:46,  1.35it/s]Extractor Estimating: 164it [01:46,  1.40it/s]Extractor Estimating: 165it [01:47,  1.41it/s]Extractor Estimating: 166it [01:48,  1.43it/s]Extractor Estimating: 167it [01:48,  1.48it/s]Extractor Estimating: 168it [01:49,  1.46it/s]Extractor Estimating: 169it [01:50,  1.47it/s]Extractor Estimating: 170it [01:50,  1.49it/s]Extractor Estimating: 171it [01:51,  1.51it/s]Extractor Estimating: 172it [01:52,  1.51it/s]Extractor Estimating: 173it [01:52,  1.51it/s]Extractor Estimating: 174it [01:53,  1.48it/s]Extractor Estimating: 175it [01:54,  1.45it/s]Extractor Estimating: 176it [01:54,  1.49it/s]Extractor Estimating: 177it [01:55,  1.49it/s]Extractor Estimating: 178it [01:56,  1.48it/s]Extractor Estimating: 179it [01:56,  1.48it/s]Extractor Estimating: 180it [01:57,  1.47it/s]Extractor Estimating: 181it [01:58,  1.50it/s]Extractor Estimating: 182it [01:58,  1.52it/s]Extractor Estimating: 183it [01:59,  1.48it/s]Extractor Estimating: 184it [02:00,  1.51it/s]Extractor Estimating: 185it [02:00,  1.53it/s]Extractor Estimating: 186it [02:01,  1.50it/s]Extractor Estimating: 187it [02:02,  1.48it/s]Extractor Estimating: 188it [02:02,  1.49it/s]Extractor Estimating: 189it [02:03,  1.51it/s]Extractor Estimating: 190it [02:04,  1.50it/s]Extractor Estimating: 191it [02:04,  1.49it/s]Extractor Estimating: 192it [02:05,  1.46it/s]Extractor Estimating: 193it [02:06,  1.50it/s]Extractor Estimating: 194it [02:06,  1.50it/s]Extractor Estimating: 195it [02:07,  1.50it/s]Extractor Estimating: 196it [02:08,  1.45it/s]Extractor Estimating: 197it [02:09,  1.44it/s]Extractor Estimating: 198it [02:09,  1.47it/s]Extractor Estimating: 199it [02:10,  1.50it/s]Extractor Estimating: 200it [02:11,  1.50it/s]Extractor Estimating: 201it [02:11,  1.58it/s]Extractor Estimating: 202it [02:12,  1.57it/s]Extractor Estimating: 203it [02:12,  1.58it/s]Extractor Estimating: 204it [02:13,  1.59it/s]Extractor Estimating: 205it [02:14,  1.58it/s]Extractor Estimating: 206it [02:14,  1.60it/s]Extractor Estimating: 207it [02:15,  1.57it/s]Extractor Estimating: 208it [02:16,  1.51it/s]Extractor Estimating: 209it [02:16,  1.52it/s]Extractor Estimating: 210it [02:17,  1.47it/s]Extractor Estimating: 211it [02:18,  1.46it/s]Extractor Estimating: 212it [02:18,  1.48it/s]Extractor Estimating: 213it [02:19,  1.49it/s]Extractor Estimating: 214it [02:20,  1.50it/s]Extractor Estimating: 215it [02:20,  1.53it/s]Extractor Estimating: 216it [02:21,  1.53it/s]Extractor Estimating: 217it [02:22,  1.54it/s]Extractor Estimating: 218it [02:22,  1.47it/s]Extractor Estimating: 219it [02:23,  1.47it/s]Extractor Estimating: 220it [02:24,  1.47it/s]Extractor Estimating: 221it [02:24,  1.45it/s]Extractor Estimating: 222it [02:25,  1.51it/s]Extractor Estimating: 223it [02:26,  1.52it/s]Extractor Estimating: 224it [02:26,  1.50it/s]Extractor Estimating: 225it [02:27,  1.54it/s]Extractor Estimating: 226it [02:28,  1.56it/s]Extractor Estimating: 227it [02:28,  1.59it/s]Extractor Estimating: 228it [02:29,  1.66it/s]Extractor Estimating: 229it [02:29,  1.68it/s]Extractor Estimating: 230it [02:30,  1.65it/s]Extractor Estimating: 231it [02:31,  1.49it/s]Extractor Estimating: 232it [02:31,  1.64it/s]Extractor Estimating: 233it [02:32,  1.57it/s]Extractor Estimating: 234it [02:33,  1.61it/s]Extractor Estimating: 235it [02:33,  1.64it/s]Extractor Estimating: 236it [02:34,  1.69it/s]Extractor Estimating: 237it [02:34,  1.75it/s]Extractor Estimating: 238it [02:35,  1.78it/s]Extractor Estimating: 239it [02:35,  1.73it/s]Extractor Estimating: 240it [02:36,  1.77it/s]Extractor Estimating: 241it [02:36,  1.75it/s]Extractor Estimating: 242it [02:37,  1.75it/s]Extractor Estimating: 243it [02:38,  1.61it/s]Extractor Estimating: 244it [02:38,  1.61it/s]Extractor Estimating: 245it [02:39,  1.69it/s]Extractor Estimating: 246it [02:39,  1.73it/s]Extractor Estimating: 247it [02:40,  1.72it/s]Extractor Estimating: 248it [02:41,  1.55it/s]Extractor Estimating: 249it [02:41,  1.63it/s]Extractor Estimating: 250it [02:42,  1.69it/s]Extractor Estimating: 251it [02:43,  1.61it/s]Extractor Estimating: 252it [02:43,  1.55it/s]Extractor Estimating: 253it [02:44,  1.53it/s]Extractor Estimating: 254it [02:45,  1.51it/s]Extractor Estimating: 255it [02:45,  1.48it/s]Extractor Estimating: 256it [02:46,  1.48it/s]Extractor Estimating: 257it [02:47,  1.49it/s]Extractor Estimating: 258it [02:47,  1.48it/s]Extractor Estimating: 259it [02:48,  1.42it/s]Extractor Estimating: 260it [02:49,  1.40it/s]Extractor Estimating: 261it [02:50,  1.44it/s]Extractor Estimating: 262it [02:50,  1.42it/s]Extractor Estimating: 263it [02:51,  1.41it/s]Extractor Estimating: 264it [02:52,  1.39it/s]Extractor Estimating: 265it [02:52,  1.40it/s]Extractor Estimating: 266it [02:53,  1.46it/s]Extractor Estimating: 267it [02:54,  1.48it/s]Extractor Estimating: 268it [02:54,  1.44it/s]Extractor Estimating: 269it [02:55,  1.43it/s]Extractor Estimating: 270it [02:56,  1.50it/s]Extractor Estimating: 271it [02:56,  1.45it/s]Extractor Estimating: 272it [02:57,  1.48it/s]Extractor Estimating: 273it [02:58,  1.50it/s]Extractor Estimating: 274it [02:58,  1.50it/s]Extractor Estimating: 275it [02:59,  1.51it/s]Extractor Estimating: 276it [03:00,  1.54it/s]Extractor Estimating: 277it [03:00,  1.56it/s]Extractor Estimating: 278it [03:01,  1.58it/s]Extractor Estimating: 279it [03:02,  1.56it/s]Extractor Estimating: 280it [03:02,  1.56it/s]Extractor Estimating: 281it [03:03,  1.53it/s]Extractor Estimating: 282it [03:04,  1.53it/s]Extractor Estimating: 283it [03:04,  1.54it/s]Extractor Estimating: 284it [03:05,  1.56it/s]Extractor Estimating: 285it [03:06,  1.54it/s]Extractor Estimating: 286it [03:06,  1.54it/s]Extractor Estimating: 287it [03:07,  1.52it/s]Extractor Estimating: 288it [03:07,  1.56it/s]Extractor Estimating: 289it [03:08,  1.54it/s]Extractor Estimating: 290it [03:09,  1.55it/s]Extractor Estimating: 291it [03:09,  1.53it/s]Extractor Estimating: 292it [03:10,  1.57it/s]Extractor Estimating: 293it [03:11,  1.54it/s]Extractor Estimating: 294it [03:11,  1.52it/s]Extractor Estimating: 295it [03:12,  1.52it/s]Extractor Estimating: 296it [03:13,  1.52it/s]Extractor Estimating: 297it [03:13,  1.55it/s]Extractor Estimating: 298it [03:14,  1.53it/s]Extractor Estimating: 299it [03:15,  1.53it/s]Extractor Estimating: 300it [03:15,  1.53it/s]Extractor Estimating: 301it [03:16,  1.54it/s]Extractor Estimating: 302it [03:17,  1.53it/s]Extractor Estimating: 303it [03:17,  1.60it/s]Extractor Estimating: 304it [03:18,  1.65it/s]Extractor Estimating: 305it [03:18,  1.63it/s]Extractor Estimating: 306it [03:19,  1.58it/s]Extractor Estimating: 307it [03:20,  1.60it/s]Extractor Estimating: 308it [03:20,  1.65it/s]Extractor Estimating: 309it [03:21,  1.69it/s]Extractor Estimating: 310it [03:21,  1.68it/s]Extractor Estimating: 311it [03:22,  1.63it/s]Extractor Estimating: 312it [03:23,  1.58it/s]Extractor Estimating: 313it [03:23,  1.62it/s]Extractor Estimating: 314it [03:24,  1.61it/s]Extractor Estimating: 315it [03:24,  1.64it/s]Extractor Estimating: 316it [03:25,  1.64it/s]Extractor Estimating: 317it [03:26,  1.62it/s]Extractor Estimating: 318it [03:26,  1.61it/s]Extractor Estimating: 319it [03:27,  1.58it/s]Extractor Estimating: 320it [03:28,  1.57it/s]Extractor Estimating: 321it [03:28,  1.52it/s]Extractor Estimating: 322it [03:29,  1.58it/s]Extractor Estimating: 323it [03:30,  1.60it/s]Extractor Estimating: 324it [03:30,  1.62it/s]Extractor Estimating: 325it [03:31,  1.62it/s]Extractor Estimating: 326it [03:31,  1.63it/s]Extractor Estimating: 327it [03:32,  1.53it/s]Extractor Estimating: 328it [03:33,  1.51it/s]Extractor Estimating: 329it [03:33,  1.51it/s]Extractor Estimating: 330it [03:34,  1.52it/s]Extractor Estimating: 331it [03:35,  1.52it/s]Extractor Estimating: 332it [03:35,  1.48it/s]Extractor Estimating: 333it [03:36,  1.55it/s]Extractor Estimating: 334it [03:37,  1.56it/s]Extractor Estimating: 335it [03:37,  1.54it/s]Extractor Estimating: 336it [03:38,  1.56it/s]Extractor Estimating: 337it [03:39,  1.42it/s]Extractor Estimating: 338it [03:39,  1.48it/s]Extractor Estimating: 339it [03:40,  1.50it/s]Extractor Estimating: 340it [03:41,  1.55it/s]Extractor Estimating: 341it [03:41,  1.54it/s]Extractor Estimating: 342it [03:42,  1.54it/s]Extractor Estimating: 343it [03:43,  1.55it/s]Extractor Estimating: 344it [03:43,  1.58it/s]Extractor Estimating: 345it [03:44,  1.60it/s]Extractor Estimating: 346it [03:45,  1.55it/s]Extractor Estimating: 347it [03:45,  1.57it/s]Extractor Estimating: 348it [03:46,  1.56it/s]Extractor Estimating: 349it [03:46,  1.61it/s]Extractor Estimating: 350it [03:47,  1.58it/s]Extractor Estimating: 351it [03:48,  1.56it/s]Extractor Estimating: 352it [03:48,  1.59it/s]Extractor Estimating: 353it [03:49,  1.60it/s]Extractor Estimating: 354it [03:50,  1.60it/s]Extractor Estimating: 355it [03:50,  1.56it/s]Extractor Estimating: 356it [03:51,  1.56it/s]Extractor Estimating: 357it [03:51,  1.57it/s]Extractor Estimating: 358it [03:52,  1.62it/s]Extractor Estimating: 359it [03:53,  1.60it/s]Extractor Estimating: 360it [03:53,  1.58it/s]Extractor Estimating: 361it [03:54,  1.57it/s]Extractor Estimating: 362it [03:55,  1.54it/s]Extractor Estimating: 363it [03:55,  1.54it/s]Extractor Estimating: 364it [03:56,  1.55it/s]Extractor Estimating: 365it [03:57,  1.58it/s]Extractor Estimating: 366it [03:57,  1.61it/s]Extractor Estimating: 367it [03:58,  1.63it/s]Extractor Estimating: 368it [03:58,  1.61it/s]Extractor Estimating: 369it [03:59,  1.57it/s]Extractor Estimating: 370it [04:00,  1.56it/s]Extractor Estimating: 371it [04:00,  1.58it/s]Extractor Estimating: 372it [04:01,  1.56it/s]Extractor Estimating: 373it [04:02,  1.58it/s]Extractor Estimating: 374it [04:02,  1.58it/s]Extractor Estimating: 375it [04:03,  1.61it/s]Extractor Estimating: 376it [04:03,  1.60it/s]Extractor Estimating: 377it [04:04,  1.56it/s]Extractor Estimating: 378it [04:05,  1.56it/s]Extractor Estimating: 379it [04:05,  1.56it/s]Extractor Estimating: 380it [04:06,  1.59it/s]Extractor Estimating: 381it [04:07,  1.59it/s]Extractor Estimating: 382it [04:07,  1.57it/s]Extractor Estimating: 383it [04:08,  1.58it/s]Extractor Estimating: 384it [04:09,  1.58it/s]Extractor Estimating: 385it [04:09,  1.57it/s]Extractor Estimating: 386it [04:10,  1.61it/s]Extractor Estimating: 387it [04:10,  1.65it/s]Extractor Estimating: 388it [04:11,  1.67it/s]Extractor Estimating: 389it [04:12,  1.64it/s]Extractor Estimating: 390it [04:12,  1.59it/s]Extractor Estimating: 391it [04:13,  1.56it/s]Extractor Estimating: 392it [04:14,  1.57it/s]Extractor Estimating: 393it [04:14,  1.59it/s]Extractor Estimating: 394it [04:15,  1.60it/s]Extractor Estimating: 395it [04:15,  1.57it/s]Extractor Estimating: 396it [04:16,  1.54it/s]Extractor Estimating: 397it [04:17,  1.54it/s]Extractor Estimating: 398it [04:17,  1.58it/s]Extractor Estimating: 399it [04:18,  1.57it/s]Extractor Estimating: 400it [04:19,  1.65it/s]Extractor Estimating: 401it [04:19,  1.67it/s]Extractor Estimating: 402it [04:20,  1.68it/s]Extractor Estimating: 403it [04:20,  1.74it/s]Extractor Estimating: 404it [04:21,  1.70it/s]Extractor Estimating: 405it [04:22,  1.65it/s]Extractor Estimating: 406it [04:22,  1.68it/s]Extractor Estimating: 407it [04:23,  1.65it/s]Extractor Estimating: 408it [04:23,  1.67it/s]Extractor Estimating: 409it [04:24,  1.67it/s]Extractor Estimating: 410it [04:25,  1.63it/s]Extractor Estimating: 411it [04:25,  1.64it/s]Extractor Estimating: 412it [04:26,  1.68it/s]Extractor Estimating: 413it [04:26,  1.64it/s]Extractor Estimating: 414it [04:27,  1.61it/s]Extractor Estimating: 415it [04:28,  1.63it/s]Extractor Estimating: 416it [04:28,  1.66it/s]Extractor Estimating: 417it [04:29,  1.61it/s]Extractor Estimating: 418it [04:29,  1.65it/s]Extractor Estimating: 419it [04:30,  1.68it/s]Extractor Estimating: 420it [04:31,  1.64it/s]Extractor Estimating: 421it [04:31,  1.66it/s]Extractor Estimating: 422it [04:32,  1.62it/s]Extractor Estimating: 423it [04:32,  1.60it/s]Extractor Estimating: 424it [04:33,  1.63it/s]Extractor Estimating: 425it [04:34,  1.49it/s]Extractor Estimating: 426it [04:34,  1.54it/s]Extractor Estimating: 427it [04:35,  1.60it/s]Extractor Estimating: 428it [04:36,  1.66it/s]Extractor Estimating: 429it [04:36,  1.71it/s]Extractor Estimating: 430it [04:37,  1.68it/s]Extractor Estimating: 431it [04:37,  1.66it/s]Extractor Estimating: 432it [04:38,  1.69it/s]Extractor Estimating: 433it [04:39,  1.71it/s]Extractor Estimating: 434it [04:39,  1.73it/s]Extractor Estimating: 435it [04:40,  1.71it/s]Extractor Estimating: 436it [04:40,  1.74it/s]Extractor Estimating: 437it [04:41,  1.75it/s]Extractor Estimating: 438it [04:41,  1.73it/s]Extractor Estimating: 439it [04:42,  1.75it/s]Extractor Estimating: 440it [04:43,  1.76it/s]Extractor Estimating: 441it [04:43,  1.75it/s]Extractor Estimating: 442it [04:44,  1.76it/s]Extractor Estimating: 443it [04:44,  1.70it/s]Extractor Estimating: 444it [04:45,  1.69it/s]Extractor Estimating: 445it [04:45,  1.67it/s]Extractor Estimating: 446it [04:46,  1.69it/s]Extractor Estimating: 447it [04:47,  1.70it/s]Extractor Estimating: 448it [04:47,  1.73it/s]Extractor Estimating: 449it [04:48,  1.75it/s]Extractor Estimating: 450it [04:48,  1.69it/s]Extractor Estimating: 451it [04:49,  1.67it/s]Extractor Estimating: 452it [04:50,  1.67it/s]Extractor Estimating: 453it [04:50,  1.62it/s]Extractor Estimating: 454it [04:51,  1.64it/s]Extractor Estimating: 455it [04:51,  1.66it/s]Extractor Estimating: 456it [04:52,  1.65it/s]Extractor Estimating: 457it [04:53,  1.62it/s]Extractor Estimating: 458it [04:53,  1.61it/s]Extractor Estimating: 459it [04:54,  1.64it/s]Extractor Estimating: 460it [04:55,  1.66it/s]Extractor Estimating: 461it [04:55,  1.63it/s]Extractor Estimating: 462it [04:56,  1.67it/s]Extractor Estimating: 463it [04:56,  1.65it/s]Extractor Estimating: 464it [04:57,  1.58it/s]Extractor Estimating: 465it [04:58,  1.57it/s]Extractor Estimating: 466it [04:58,  1.57it/s]Extractor Estimating: 467it [04:59,  1.56it/s]Extractor Estimating: 468it [05:00,  1.56it/s]Extractor Estimating: 469it [05:00,  1.55it/s]Extractor Estimating: 470it [05:01,  1.56it/s]Extractor Estimating: 471it [05:02,  1.57it/s]Extractor Estimating: 472it [05:02,  1.56it/s]Extractor Estimating: 473it [05:03,  1.63it/s]Extractor Estimating: 474it [05:03,  1.68it/s]Extractor Estimating: 475it [05:04,  1.69it/s]Extractor Estimating: 476it [05:04,  1.68it/s]Extractor Estimating: 477it [05:05,  1.67it/s]Extractor Estimating: 478it [05:06,  1.60it/s]Extractor Estimating: 479it [05:06,  1.62it/s]Extractor Estimating: 480it [05:07,  1.64it/s]Extractor Estimating: 481it [05:08,  1.57it/s]Extractor Estimating: 482it [05:08,  1.57it/s]Extractor Estimating: 483it [05:09,  1.57it/s]Extractor Estimating: 484it [05:09,  1.64it/s]Extractor Estimating: 485it [05:10,  1.68it/s]Extractor Estimating: 486it [05:11,  1.59it/s]Extractor Estimating: 487it [05:11,  1.64it/s]Extractor Estimating: 488it [05:12,  1.59it/s]Extractor Estimating: 489it [05:13,  1.59it/s]Extractor Estimating: 490it [05:13,  1.59it/s]Extractor Estimating: 491it [05:14,  1.54it/s]Extractor Estimating: 492it [05:15,  1.52it/s]Extractor Estimating: 493it [05:15,  1.56it/s]Extractor Estimating: 494it [05:16,  1.58it/s]Extractor Estimating: 495it [05:16,  1.61it/s]Extractor Estimating: 496it [05:17,  1.64it/s]Extractor Estimating: 497it [05:18,  1.62it/s]Extractor Estimating: 498it [05:18,  1.65it/s]Extractor Estimating: 499it [05:19,  1.58it/s]Extractor Estimating: 500it [05:19,  1.73it/s]Extractor Estimating: 500it [05:19,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:37,814 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:37,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:37,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:37,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:37,818 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:28:38,130 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:28:38,131 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:28:38,398 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:28:39,464 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:28:39,464 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:41,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:41,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:41,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:41,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:41,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:28:42,251 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:28:42,252 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:28:42,818 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:28:42,983 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:28:42,983 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 09:04:08,483 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 09:04:08,488 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9993 mean pseudo reward: 0.95881868893052
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl'}
train vocab size: 28524
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28624, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28624, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.114, loss:751.8123
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.095, loss:764.1216
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.089, loss:735.0937
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.115, loss:739.6407
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.098, loss:728.1801
>> valid entity prec:0.5665, rec:0.5520, f1:0.5592
>> valid relation prec:0.4172, rec:0.0921, f1:0.1509
>> valid relation with NER prec:0.4172, rec:0.0921, f1:0.1509
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 3.449, loss:706.1570
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.103, loss:734.8917
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.104, loss:768.3670
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.106, loss:730.3992
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.099, loss:733.7492
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6054, rec:0.4329, f1:0.5049
>> valid relation prec:0.5020, rec:0.0759, f1:0.1318
>> valid relation with NER prec:0.5020, rec:0.0759, f1:0.1318
g_step 1100, step 266, avg_time 3.422, loss:729.2221
g_step 1200, step 366, avg_time 1.096, loss:769.9764
g_step 1300, step 49, avg_time 1.096, loss:712.4537
g_step 1400, step 149, avg_time 1.108, loss:715.9502
g_step 1500, step 249, avg_time 1.107, loss:726.3623
>> valid entity prec:0.6436, rec:0.4837, f1:0.5523
>> valid relation prec:0.5291, rec:0.0813, f1:0.1409
>> valid relation with NER prec:0.5291, rec:0.0813, f1:0.1409
g_step 1600, step 349, avg_time 3.423, loss:708.0724
g_step 1700, step 32, avg_time 1.090, loss:715.0755
g_step 1800, step 132, avg_time 1.084, loss:679.3629
g_step 1900, step 232, avg_time 1.105, loss:699.5852
g_step 2000, step 332, avg_time 1.101, loss:695.8365
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5796, rec:0.5494, f1:0.5641
>> valid relation prec:0.4246, rec:0.0942, f1:0.1542
>> valid relation with NER prec:0.4246, rec:0.0942, f1:0.1542
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 15, avg_time 3.447, loss:680.5850
g_step 2200, step 115, avg_time 1.105, loss:650.8280
g_step 2300, step 215, avg_time 1.100, loss:671.1995
g_step 2400, step 315, avg_time 1.099, loss:665.3138
g_step 2500, step 415, avg_time 1.091, loss:685.6090
>> valid entity prec:0.5348, rec:0.5652, f1:0.5496
>> valid relation prec:0.4133, rec:0.0970, f1:0.1571
>> valid relation with NER prec:0.4133, rec:0.0970, f1:0.1571
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 98, avg_time 3.452, loss:618.6586
g_step 2700, step 198, avg_time 1.079, loss:637.2625
g_step 2800, step 298, avg_time 1.107, loss:659.0445
g_step 2900, step 398, avg_time 1.093, loss:667.4628
g_step 3000, step 81, avg_time 1.102, loss:614.7542
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5860, rec:0.5594, f1:0.5724
>> valid relation prec:0.4229, rec:0.1189, f1:0.1856
>> valid relation with NER prec:0.4229, rec:0.1189, f1:0.1856
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 181, avg_time 3.445, loss:612.9054
g_step 3200, step 281, avg_time 1.099, loss:620.7756
g_step 3300, step 381, avg_time 1.098, loss:647.2945
g_step 3400, step 64, avg_time 1.092, loss:598.6054
g_step 3500, step 164, avg_time 1.098, loss:606.0335
>> valid entity prec:0.5923, rec:0.5351, f1:0.5623
>> valid relation prec:0.4091, rec:0.0774, f1:0.1302
>> valid relation with NER prec:0.4091, rec:0.0774, f1:0.1302
g_step 3600, step 264, avg_time 3.439, loss:593.3458
g_step 3700, step 364, avg_time 1.093, loss:606.7528
g_step 3800, step 47, avg_time 1.097, loss:592.4086
g_step 3900, step 147, avg_time 1.097, loss:579.3010
g_step 4000, step 247, avg_time 1.095, loss:584.3824
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6124, rec:0.5129, f1:0.5582
>> valid relation prec:0.3727, rec:0.1075, f1:0.1669
>> valid relation with NER prec:0.3727, rec:0.1075, f1:0.1669
g_step 4100, step 347, avg_time 3.445, loss:590.5369
g_step 4200, step 30, avg_time 1.099, loss:574.0441
g_step 4300, step 130, avg_time 1.096, loss:558.7211
g_step 4400, step 230, avg_time 1.097, loss:562.3256
g_step 4500, step 330, avg_time 1.088, loss:557.7708
>> valid entity prec:0.5543, rec:0.5102, f1:0.5314
>> valid relation prec:0.4214, rec:0.0802, f1:0.1347
>> valid relation with NER prec:0.4214, rec:0.0802, f1:0.1347
g_step 4600, step 13, avg_time 3.442, loss:579.0957
g_step 4700, step 113, avg_time 1.113, loss:544.3890
g_step 4800, step 213, avg_time 1.089, loss:530.4560
g_step 4900, step 313, avg_time 1.094, loss:570.0453
g_step 5000, step 413, avg_time 1.100, loss:561.1961
learning rate was adjusted to 0.0008
>> valid entity prec:0.5181, rec:0.5474, f1:0.5323
>> valid relation prec:0.3664, rec:0.1066, f1:0.1651
>> valid relation with NER prec:0.3664, rec:0.1066, f1:0.1651
g_step 5100, step 96, avg_time 3.452, loss:515.2520
g_step 5200, step 196, avg_time 1.107, loss:527.7489
g_step 5300, step 296, avg_time 1.092, loss:541.7642
g_step 5400, step 396, avg_time 1.089, loss:533.6505
g_step 5500, step 79, avg_time 1.101, loss:489.4563
>> valid entity prec:0.5482, rec:0.4608, f1:0.5007
>> valid relation prec:0.4050, rec:0.0907, f1:0.1482
>> valid relation with NER prec:0.4050, rec:0.0907, f1:0.1482
g_step 5600, step 179, avg_time 3.435, loss:506.5538
g_step 5700, step 279, avg_time 1.110, loss:545.4513
g_step 5800, step 379, avg_time 1.090, loss:516.1229
g_step 5900, step 62, avg_time 1.108, loss:489.8677
g_step 6000, step 162, avg_time 1.101, loss:490.5345
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5381, rec:0.5217, f1:0.5298
>> valid relation prec:0.3633, rec:0.1027, f1:0.1602
>> valid relation with NER prec:0.3633, rec:0.1027, f1:0.1602
g_step 6100, step 262, avg_time 3.442, loss:504.3295
g_step 6200, step 362, avg_time 1.109, loss:503.3437
g_step 6300, step 45, avg_time 1.100, loss:483.9166
g_step 6400, step 145, avg_time 1.102, loss:451.3239
g_step 6500, step 245, avg_time 1.106, loss:487.8724
>> valid entity prec:0.6147, rec:0.4895, f1:0.5450
>> valid relation prec:0.3577, rec:0.1141, f1:0.1730
>> valid relation with NER prec:0.3577, rec:0.1141, f1:0.1730
g_step 6600, step 345, avg_time 3.450, loss:484.9129
g_step 6700, step 28, avg_time 1.092, loss:491.7698
g_step 6800, step 128, avg_time 1.099, loss:461.1346
g_step 6900, step 228, avg_time 1.106, loss:461.5131
g_step 7000, step 328, avg_time 1.099, loss:497.0895
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5828, rec:0.5235, f1:0.5515
>> valid relation prec:0.3587, rec:0.1229, f1:0.1831
>> valid relation with NER prec:0.3587, rec:0.1229, f1:0.1831
g_step 7100, step 11, avg_time 3.455, loss:478.9004
g_step 7200, step 111, avg_time 1.104, loss:446.5959
g_step 7300, step 211, avg_time 1.104, loss:443.8815
g_step 7400, step 311, avg_time 1.110, loss:476.4752
g_step 7500, step 411, avg_time 1.094, loss:449.7948
>> valid entity prec:0.5726, rec:0.5182, f1:0.5440
>> valid relation prec:0.3507, rec:0.1120, f1:0.1697
>> valid relation with NER prec:0.3507, rec:0.1120, f1:0.1697
g_step 7600, step 94, avg_time 3.443, loss:427.5339
g_step 7700, step 194, avg_time 1.106, loss:422.7604
g_step 7800, step 294, avg_time 1.101, loss:442.4492
g_step 7900, step 394, avg_time 1.107, loss:458.7952
g_step 8000, step 77, avg_time 1.108, loss:434.7925
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5563, rec:0.4964, f1:0.5246
>> valid relation prec:0.3090, rec:0.0979, f1:0.1487
>> valid relation with NER prec:0.3090, rec:0.0979, f1:0.1487
g_step 8100, step 177, avg_time 3.450, loss:426.6751
g_step 8200, step 277, avg_time 1.093, loss:428.0170
g_step 8300, step 377, avg_time 1.115, loss:459.4373
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 09:04:08 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 09:04:08 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_09-04-08_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 09:04:09 - WARNING - datasets.builder -   Using custom data configuration default-6e51259a50044489
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6e51259a50044489/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 09:04:09,743 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:04:09,745 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:04:09,745 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:04:09,746 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:04:09,762 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:09,767 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:09,767 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:09,767 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:09,767 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:09,767 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:04:09,767 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 09:04:09,935 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:04:13,046 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 09:04:13,049 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6e51259a50044489/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  3.23ba/s] 18%|█▊        | 2/11 [00:00<00:02,  4.00ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.30ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.41ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.46ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.52ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.52ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.54ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.56ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.56ba/s]100%|██████████| 11/11 [00:02<00:00,  4.83ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.05ba/s] 29%|██▊       | 2/7 [00:00<00:01,  4.29ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.33ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  3.47ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  3.78ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  3.98ba/s]100%|██████████| 7/7 [00:01<00:00,  4.84ba/s]100%|██████████| 7/7 [00:01<00:00,  4.27ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  8.14ba/s] 18%|█▊        | 2/11 [00:00<00:00,  9.05ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.45ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.85ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.96ba/s] 73%|███████▎  | 8/11 [00:00<00:00,  9.93ba/s] 91%|█████████ | 10/11 [00:01<00:00, 10.03ba/s]100%|██████████| 11/11 [00:01<00:00, 10.74ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  8.24ba/s] 29%|██▊       | 2/7 [00:00<00:00,  9.13ba/s] 43%|████▎     | 3/7 [00:00<00:00,  9.25ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  9.45ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.63ba/s] 86%|████████▌ | 6/7 [00:00<00:00,  9.70ba/s]100%|██████████| 7/7 [00:00<00:00, 10.23ba/s]
[INFO|trainer.py:414] 2023-08-29 09:04:19,083 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 09:04:19,097 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 09:04:19,098 >>   Num examples = 10051
[INFO|trainer.py:1149] 2023-08-29 09:04:19,098 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 09:04:19,098 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 09:04:19,098 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 09:04:19,098 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 09:04:19,098 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:57,  3.30it/s]  0%|          | 2/785 [00:00<03:50,  3.39it/s]  0%|          | 3/785 [00:00<03:48,  3.43it/s]  1%|          | 4/785 [00:01<03:47,  3.44it/s]  1%|          | 5/785 [00:01<03:46,  3.45it/s]  1%|          | 6/785 [00:01<03:45,  3.45it/s]  1%|          | 7/785 [00:02<03:45,  3.45it/s]  1%|          | 8/785 [00:02<03:45,  3.45it/s]  1%|          | 9/785 [00:02<03:44,  3.46it/s]  1%|▏         | 10/785 [00:02<03:44,  3.45it/s]  1%|▏         | 11/785 [00:03<03:44,  3.46it/s]  2%|▏         | 12/785 [00:03<03:44,  3.45it/s]  2%|▏         | 13/785 [00:03<03:43,  3.45it/s]  2%|▏         | 14/785 [00:04<03:43,  3.45it/s]  2%|▏         | 15/785 [00:04<03:42,  3.46it/s]  2%|▏         | 16/785 [00:04<03:43,  3.45it/s]  2%|▏         | 17/785 [00:04<03:42,  3.45it/s]  2%|▏         | 18/785 [00:05<03:42,  3.45it/s]  2%|▏         | 19/785 [00:05<03:44,  3.41it/s]  3%|▎         | 20/785 [00:05<03:43,  3.42it/s]  3%|▎         | 21/785 [00:06<03:42,  3.43it/s]  3%|▎         | 22/785 [00:06<03:41,  3.44it/s]  3%|▎         | 23/785 [00:06<03:42,  3.43it/s]  3%|▎         | 24/785 [00:06<03:41,  3.44it/s]  3%|▎         | 25/785 [00:07<03:40,  3.45it/s]  3%|▎         | 26/785 [00:07<03:40,  3.45it/s]  3%|▎         | 27/785 [00:07<03:39,  3.45it/s]  4%|▎         | 28/785 [00:08<03:39,  3.45it/s]  4%|▎         | 29/785 [00:08<03:39,  3.45it/s]  4%|▍         | 30/785 [00:08<03:38,  3.45it/s]  4%|▍         | 31/785 [00:09<03:38,  3.45it/s]  4%|▍         | 32/785 [00:09<03:37,  3.46it/s]  4%|▍         | 33/785 [00:09<03:37,  3.46it/s]  4%|▍         | 34/785 [00:09<03:37,  3.45it/s]  4%|▍         | 35/785 [00:10<03:37,  3.45it/s]  5%|▍         | 36/785 [00:10<03:37,  3.45it/s]  5%|▍         | 37/785 [00:10<03:36,  3.45it/s]  5%|▍         | 38/785 [00:11<03:36,  3.45it/s]  5%|▍         | 39/785 [00:11<03:36,  3.45it/s]  5%|▌         | 40/785 [00:11<03:36,  3.45it/s]  5%|▌         | 41/785 [00:11<03:35,  3.45it/s]  5%|▌         | 42/785 [00:12<03:35,  3.45it/s]  5%|▌         | 43/785 [00:12<03:35,  3.45it/s]  6%|▌         | 44/785 [00:12<03:34,  3.45it/s]  6%|▌         | 45/785 [00:13<03:36,  3.41it/s]  6%|▌         | 46/785 [00:13<03:35,  3.42it/s]  6%|▌         | 47/785 [00:13<03:34,  3.44it/s]  6%|▌         | 48/785 [00:13<03:34,  3.44it/s]  6%|▌         | 49/785 [00:14<03:33,  3.44it/s]  6%|▋         | 50/785 [00:14<03:33,  3.44it/s]  6%|▋         | 51/785 [00:14<03:32,  3.45it/s]  7%|▋         | 52/785 [00:15<03:32,  3.45it/s]  7%|▋         | 53/785 [00:15<03:32,  3.45it/s]  7%|▋         | 54/785 [00:15<03:32,  3.45it/s]  7%|▋         | 55/785 [00:15<03:31,  3.45it/s]  7%|▋         | 56/785 [00:16<03:31,  3.44it/s]  7%|▋         | 57/785 [00:16<03:31,  3.44it/s]  7%|▋         | 58/785 [00:16<03:30,  3.45it/s]  8%|▊         | 59/785 [00:17<03:30,  3.45it/s]  8%|▊         | 60/785 [00:17<03:30,  3.45it/s]  8%|▊         | 61/785 [00:17<03:29,  3.45it/s]  8%|▊         | 62/785 [00:18<03:29,  3.45it/s]  8%|▊         | 63/785 [00:18<03:29,  3.45it/s]  8%|▊         | 64/785 [00:18<03:29,  3.45it/s]  8%|▊         | 65/785 [00:18<03:28,  3.45it/s]  8%|▊         | 66/785 [00:19<03:28,  3.45it/s]  9%|▊         | 67/785 [00:19<03:29,  3.43it/s]  9%|▊         | 68/785 [00:19<03:28,  3.43it/s]  9%|▉         | 69/785 [00:20<03:28,  3.44it/s]  9%|▉         | 70/785 [00:20<03:27,  3.44it/s]  9%|▉         | 71/785 [00:20<03:27,  3.44it/s]  9%|▉         | 72/785 [00:20<03:27,  3.44it/s]  9%|▉         | 73/785 [00:21<03:26,  3.44it/s]  9%|▉         | 74/785 [00:21<03:26,  3.45it/s] 10%|▉         | 75/785 [00:21<03:25,  3.45it/s] 10%|▉         | 76/785 [00:22<03:25,  3.45it/s] 10%|▉         | 77/785 [00:22<03:25,  3.45it/s] 10%|▉         | 78/785 [00:22<03:25,  3.44it/s] 10%|█         | 79/785 [00:22<03:24,  3.45it/s] 10%|█         | 80/785 [00:23<03:24,  3.44it/s] 10%|█         | 81/785 [00:23<03:24,  3.44it/s] 10%|█         | 82/785 [00:23<03:24,  3.44it/s] 11%|█         | 83/785 [00:24<03:24,  3.43it/s] 11%|█         | 84/785 [00:24<03:24,  3.43it/s] 11%|█         | 85/785 [00:24<03:23,  3.43it/s] 11%|█         | 86/785 [00:24<03:23,  3.44it/s] 11%|█         | 87/785 [00:25<03:22,  3.44it/s] 11%|█         | 88/785 [00:25<03:22,  3.44it/s] 11%|█▏        | 89/785 [00:25<03:22,  3.44it/s] 11%|█▏        | 90/785 [00:26<03:21,  3.45it/s] 12%|█▏        | 91/785 [00:26<03:21,  3.44it/s] 12%|█▏        | 92/785 [00:26<03:21,  3.44it/s] 12%|█▏        | 93/785 [00:27<03:20,  3.44it/s] 12%|█▏        | 94/785 [00:27<03:21,  3.43it/s] 12%|█▏        | 95/785 [00:27<03:20,  3.44it/s] 12%|█▏        | 96/785 [00:27<03:20,  3.44it/s] 12%|█▏        | 97/785 [00:28<03:19,  3.44it/s] 12%|█▏        | 98/785 [00:28<03:19,  3.44it/s] 13%|█▎        | 99/785 [00:28<03:19,  3.44it/s] 13%|█▎        | 100/785 [00:29<03:18,  3.44it/s] 13%|█▎        | 101/785 [00:29<03:18,  3.44it/s] 13%|█▎        | 102/785 [00:29<03:18,  3.44it/s] 13%|█▎        | 103/785 [00:29<03:18,  3.44it/s] 13%|█▎        | 104/785 [00:30<03:17,  3.44it/s] 13%|█▎        | 105/785 [00:30<03:18,  3.43it/s] 14%|█▎        | 106/785 [00:30<03:17,  3.43it/s] 14%|█▎        | 107/785 [00:31<03:17,  3.43it/s] 14%|█▍        | 108/785 [00:31<03:17,  3.43it/s] 14%|█▍        | 109/785 [00:31<03:16,  3.44it/s] 14%|█▍        | 110/785 [00:31<03:16,  3.44it/s] 14%|█▍        | 111/785 [00:32<03:15,  3.44it/s] 14%|█▍        | 112/785 [00:32<03:15,  3.44it/s] 14%|█▍        | 113/785 [00:32<03:15,  3.44it/s] 15%|█▍        | 114/785 [00:33<03:15,  3.44it/s] 15%|█▍        | 115/785 [00:33<03:14,  3.44it/s] 15%|█▍        | 116/785 [00:33<03:15,  3.42it/s] 15%|█▍        | 117/785 [00:33<03:14,  3.43it/s] 15%|█▌        | 118/785 [00:34<03:14,  3.43it/s] 15%|█▌        | 119/785 [00:34<03:14,  3.43it/s] 15%|█▌        | 120/785 [00:34<03:13,  3.44it/s] 15%|█▌        | 121/785 [00:35<03:13,  3.43it/s] 16%|█▌        | 122/785 [00:35<03:12,  3.44it/s] 16%|█▌        | 123/785 [00:35<03:12,  3.44it/s] 16%|█▌        | 124/785 [00:36<03:12,  3.44it/s] 16%|█▌        | 125/785 [00:36<03:11,  3.44it/s] 16%|█▌        | 126/785 [00:36<03:11,  3.44it/s] 16%|█▌        | 127/785 [00:36<03:11,  3.43it/s] 16%|█▋        | 128/785 [00:37<03:11,  3.44it/s] 16%|█▋        | 129/785 [00:37<03:10,  3.44it/s] 17%|█▋        | 130/785 [00:37<03:10,  3.44it/s] 17%|█▋        | 131/785 [00:38<03:10,  3.44it/s] 17%|█▋        | 132/785 [00:38<03:09,  3.44it/s] 17%|█▋        | 133/785 [00:38<03:09,  3.44it/s] 17%|█▋        | 134/785 [00:38<03:09,  3.44it/s] 17%|█▋        | 135/785 [00:39<03:08,  3.44it/s] 17%|█▋        | 136/785 [00:39<03:08,  3.44it/s] 17%|█▋        | 137/785 [00:39<03:08,  3.44it/s] 18%|█▊        | 138/785 [00:40<03:08,  3.43it/s] 18%|█▊        | 139/785 [00:40<03:08,  3.43it/s] 18%|█▊        | 140/785 [00:40<03:08,  3.43it/s] 18%|█▊        | 141/785 [00:40<03:07,  3.43it/s] 18%|█▊        | 142/785 [00:41<03:07,  3.43it/s] 18%|█▊        | 143/785 [00:41<03:06,  3.44it/s] 18%|█▊        | 144/785 [00:41<03:06,  3.44it/s] 18%|█▊        | 145/785 [00:42<03:06,  3.44it/s] 19%|█▊        | 146/785 [00:42<03:05,  3.44it/s] 19%|█▊        | 147/785 [00:42<03:05,  3.44it/s] 19%|█▉        | 148/785 [00:43<03:05,  3.44it/s] 19%|█▉        | 149/785 [00:43<03:05,  3.43it/s] 19%|█▉        | 150/785 [00:43<03:04,  3.44it/s] 19%|█▉        | 151/785 [00:43<03:04,  3.43it/s] 19%|█▉        | 152/785 [00:44<03:04,  3.43it/s] 19%|█▉        | 153/785 [00:44<03:03,  3.43it/s] 20%|█▉        | 154/785 [00:44<03:03,  3.43it/s] 20%|█▉        | 155/785 [00:45<03:03,  3.44it/s] 20%|█▉        | 156/785 [00:45<03:02,  3.44it/s] 20%|██        | 157/785 [00:45<03:02,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 09:05:04,777 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:05:04,777 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 09:05:04,777 >>   Batch size = 8

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.71it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.11it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.43it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.74it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.32it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.91it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.50it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.18it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.13it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.23it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.27it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.22it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.28it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.30it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.32it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.18it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.07it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 45.98it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.00it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.17it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.16it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.24it/s][A
 15%|█▍        | 118/811 [00:02<00:14, 46.25it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.21it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.07it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.07it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.09it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 46.02it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.10it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.13it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.14it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.08it/s][A
 21%|██        | 168/811 [00:03<00:14, 45.78it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.29it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.09it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.20it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 46.12it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 45.94it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.15it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.17it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.11it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.29it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.25it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.09it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.16it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 46.09it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 45.91it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 45.95it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.19it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.25it/s][A
 32%|███▏      | 258/811 [00:05<00:11, 46.27it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.26it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.13it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.07it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.07it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.10it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.08it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.20it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.09it/s][A
 37%|███▋      | 303/811 [00:06<00:10, 46.26it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.16it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.17it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 46.10it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 46.05it/s][A
 40%|████      | 328/811 [00:07<00:10, 45.98it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.00it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.07it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 46.03it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.12it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.12it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.04it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 45.98it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 45.95it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.00it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.09it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.09it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.11it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.03it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.11it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 46.10it/s][A
 50%|█████     | 408/811 [00:08<00:08, 46.08it/s][A
 51%|█████     | 413/811 [00:08<00:08, 46.01it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.06it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.01it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.01it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.02it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.09it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.19it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.08it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 46.09it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 45.96it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.08it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.14it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.11it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.22it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.06it/s][A
 60%|██████    | 488/811 [00:10<00:07, 46.08it/s][A
 61%|██████    | 493/811 [00:10<00:06, 46.09it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 46.11it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.11it/s][A
 63%|██████▎   | 508/811 [00:10<00:06, 46.16it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.07it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 46.09it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.13it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.20it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.11it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.09it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 45.98it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.09it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 46.13it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 46.17it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.17it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.18it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.09it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.01it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.06it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.11it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.14it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.10it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.11it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.15it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.03it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.16it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.05it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.08it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.09it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 46.01it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.10it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 46.08it/s][A
 81%|████████  | 653/811 [00:14<00:03, 46.07it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.14it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.02it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 45.93it/s][A
 83%|████████▎ | 673/811 [00:14<00:03, 45.91it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 45.88it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 46.01it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.08it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 46.07it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 46.06it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 46.01it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 46.09it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.16it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 46.09it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 46.02it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 46.06it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 46.09it/s][A
 91%|█████████ | 738/811 [00:15<00:01, 46.06it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 46.09it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 46.18it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 46.12it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 46.10it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 45.98it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 46.04it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 46.11it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 46.07it/s][A
 97%|█████████▋| 783/811 [00:16<00:00, 46.06it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 46.17it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 46.07it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 46.12it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.11it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 46.01it/s][A
                                                 [A                                                 
100%|██████████| 811/811 [00:17<00:00, 46.01it/s][A 20%|██        | 157/785 [01:03<03:02,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:05:22,389 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-29 09:05:22,405 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:05:24,680 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:05:24,699 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:05:24,713 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:10<1:19:39,  7.62s/it] 20%|██        | 159/785 [01:10<56:34,  5.42s/it]   20%|██        | 160/785 [01:10<40:27,  3.88s/it] 21%|██        | 161/785 [01:11<29:10,  2.81s/it] 21%|██        | 162/785 [01:11<21:17,  2.05s/it] 21%|██        | 163/785 [01:11<15:46,  1.52s/it] 21%|██        | 164/785 [01:12<11:55,  1.15s/it] 21%|██        | 165/785 [01:12<09:14,  1.12it/s] 21%|██        | 166/785 [01:12<07:21,  1.40it/s] 21%|██▏       | 167/785 [01:12<06:02,  1.71it/s] 21%|██▏       | 168/785 [01:13<05:06,  2.01it/s] 22%|██▏       | 169/785 [01:13<04:28,  2.30it/s] 22%|██▏       | 170/785 [01:13<04:01,  2.55it/s] 22%|██▏       | 171/785 [01:14<03:42,  2.76it/s] 22%|██▏       | 172/785 [01:14<03:29,  2.93it/s] 22%|██▏       | 173/785 [01:14<03:19,  3.07it/s] 22%|██▏       | 174/785 [01:15<03:12,  3.17it/s] 22%|██▏       | 175/785 [01:15<03:07,  3.25it/s] 22%|██▏       | 176/785 [01:15<03:04,  3.30it/s] 23%|██▎       | 177/785 [01:15<03:01,  3.34it/s] 23%|██▎       | 178/785 [01:16<02:59,  3.37it/s] 23%|██▎       | 179/785 [01:16<02:58,  3.39it/s] 23%|██▎       | 180/785 [01:16<02:57,  3.41it/s] 23%|██▎       | 181/785 [01:17<02:57,  3.41it/s] 23%|██▎       | 182/785 [01:17<02:56,  3.42it/s] 23%|██▎       | 183/785 [01:17<02:55,  3.43it/s] 23%|██▎       | 184/785 [01:17<02:55,  3.43it/s] 24%|██▎       | 185/785 [01:18<02:54,  3.43it/s] 24%|██▎       | 186/785 [01:18<02:54,  3.43it/s] 24%|██▍       | 187/785 [01:18<02:54,  3.44it/s] 24%|██▍       | 188/785 [01:19<02:53,  3.44it/s] 24%|██▍       | 189/785 [01:19<02:53,  3.44it/s] 24%|██▍       | 190/785 [01:19<02:53,  3.44it/s] 24%|██▍       | 191/785 [01:19<02:52,  3.44it/s] 24%|██▍       | 192/785 [01:20<02:52,  3.43it/s] 25%|██▍       | 193/785 [01:20<02:52,  3.43it/s] 25%|██▍       | 194/785 [01:20<02:52,  3.43it/s] 25%|██▍       | 195/785 [01:21<02:51,  3.43it/s] 25%|██▍       | 196/785 [01:21<02:51,  3.43it/s] 25%|██▌       | 197/785 [01:21<02:51,  3.44it/s] 25%|██▌       | 198/785 [01:21<02:50,  3.44it/s] 25%|██▌       | 199/785 [01:22<02:50,  3.44it/s] 25%|██▌       | 200/785 [01:22<02:50,  3.44it/s] 26%|██▌       | 201/785 [01:22<02:49,  3.44it/s] 26%|██▌       | 202/785 [01:23<02:49,  3.44it/s] 26%|██▌       | 203/785 [01:23<02:49,  3.44it/s] 26%|██▌       | 204/785 [01:23<02:49,  3.44it/s] 26%|██▌       | 205/785 [01:24<02:48,  3.44it/s] 26%|██▌       | 206/785 [01:24<02:48,  3.43it/s] 26%|██▋       | 207/785 [01:24<02:48,  3.43it/s] 26%|██▋       | 208/785 [01:24<02:48,  3.43it/s] 27%|██▋       | 209/785 [01:25<02:47,  3.43it/s] 27%|██▋       | 210/785 [01:25<02:47,  3.43it/s] 27%|██▋       | 211/785 [01:25<02:47,  3.43it/s] 27%|██▋       | 212/785 [01:26<02:46,  3.43it/s] 27%|██▋       | 213/785 [01:26<02:46,  3.43it/s] 27%|██▋       | 214/785 [01:26<02:46,  3.43it/s] 27%|██▋       | 215/785 [01:26<02:46,  3.43it/s] 28%|██▊       | 216/785 [01:27<02:45,  3.43it/s] 28%|██▊       | 217/785 [01:27<02:45,  3.44it/s] 28%|██▊       | 218/785 [01:27<02:45,  3.43it/s] 28%|██▊       | 219/785 [01:28<02:45,  3.43it/s] 28%|██▊       | 220/785 [01:28<02:44,  3.43it/s] 28%|██▊       | 221/785 [01:28<02:44,  3.42it/s] 28%|██▊       | 222/785 [01:28<02:44,  3.43it/s] 28%|██▊       | 223/785 [01:29<02:43,  3.43it/s] 29%|██▊       | 224/785 [01:29<02:43,  3.43it/s] 29%|██▊       | 225/785 [01:29<02:43,  3.43it/s] 29%|██▉       | 226/785 [01:30<02:42,  3.43it/s] 29%|██▉       | 227/785 [01:30<02:42,  3.43it/s] 29%|██▉       | 228/785 [01:30<02:42,  3.43it/s] 29%|██▉       | 229/785 [01:31<02:42,  3.43it/s] 29%|██▉       | 230/785 [01:31<02:41,  3.43it/s] 29%|██▉       | 231/785 [01:31<02:41,  3.43it/s] 30%|██▉       | 232/785 [01:31<02:41,  3.42it/s] 30%|██▉       | 233/785 [01:32<02:41,  3.43it/s] 30%|██▉       | 234/785 [01:32<02:40,  3.43it/s] 30%|██▉       | 235/785 [01:32<02:40,  3.43it/s] 30%|███       | 236/785 [01:33<02:39,  3.43it/s] 30%|███       | 237/785 [01:33<02:39,  3.43it/s] 30%|███       | 238/785 [01:33<02:39,  3.44it/s] 30%|███       | 239/785 [01:33<02:38,  3.44it/s] 31%|███       | 240/785 [01:34<02:38,  3.44it/s] 31%|███       | 241/785 [01:34<02:38,  3.43it/s] 31%|███       | 242/785 [01:34<02:38,  3.44it/s] 31%|███       | 243/785 [01:35<02:38,  3.43it/s] 31%|███       | 244/785 [01:35<02:37,  3.43it/s] 31%|███       | 245/785 [01:35<02:37,  3.43it/s] 31%|███▏      | 246/785 [01:35<02:36,  3.44it/s] 31%|███▏      | 247/785 [01:36<02:36,  3.44it/s] 32%|███▏      | 248/785 [01:36<02:36,  3.44it/s] 32%|███▏      | 249/785 [01:36<02:36,  3.44it/s] 32%|███▏      | 250/785 [01:37<02:35,  3.44it/s] 32%|███▏      | 251/785 [01:37<02:35,  3.44it/s] 32%|███▏      | 252/785 [01:37<02:35,  3.44it/s] 32%|███▏      | 253/785 [01:38<02:34,  3.43it/s] 32%|███▏      | 254/785 [01:38<02:34,  3.43it/s] 32%|███▏      | 255/785 [01:38<02:34,  3.43it/s] 33%|███▎      | 256/785 [01:38<02:34,  3.43it/s] 33%|███▎      | 257/785 [01:39<02:33,  3.43it/s] 33%|███▎      | 258/785 [01:39<02:33,  3.43it/s] 33%|███▎      | 259/785 [01:39<02:33,  3.43it/s] 33%|███▎      | 260/785 [01:40<02:32,  3.43it/s] 33%|███▎      | 261/785 [01:40<02:32,  3.43it/s] 33%|███▎      | 262/785 [01:40<02:32,  3.43it/s] 34%|███▎      | 263/785 [01:40<02:32,  3.43it/s] 34%|███▎      | 264/785 [01:41<02:31,  3.43it/s] 34%|███▍      | 265/785 [01:41<02:32,  3.42it/s] 34%|███▍      | 266/785 [01:41<02:31,  3.42it/s] 34%|███▍      | 267/785 [01:42<02:31,  3.43it/s] 34%|███▍      | 268/785 [01:42<02:30,  3.43it/s] 34%|███▍      | 269/785 [01:42<02:30,  3.43it/s] 34%|███▍      | 270/785 [01:42<02:30,  3.43it/s] 35%|███▍      | 271/785 [01:43<02:29,  3.43it/s] 35%|███▍      | 272/785 [01:43<02:29,  3.43it/s] 35%|███▍      | 273/785 [01:43<02:29,  3.43it/s] 35%|███▍      | 274/785 [01:44<02:29,  3.43it/s] 35%|███▌      | 275/785 [01:44<02:28,  3.43it/s] 35%|███▌      | 276/785 [01:44<02:28,  3.43it/s] 35%|███▌      | 277/785 [01:45<02:28,  3.43it/s] 35%|███▌      | 278/785 [01:45<02:27,  3.43it/s] 36%|███▌      | 279/785 [01:45<02:27,  3.43it/s] 36%|███▌      | 280/785 [01:45<02:27,  3.43it/s] 36%|███▌      | 281/785 [01:46<02:27,  3.42it/s] 36%|███▌      | 282/785 [01:46<02:26,  3.43it/s] 36%|███▌      | 283/785 [01:46<02:26,  3.43it/s] 36%|███▌      | 284/785 [01:47<02:26,  3.43it/s] 36%|███▋      | 285/785 [01:47<02:25,  3.43it/s] 36%|███▋      | 286/785 [01:47<02:25,  3.43it/s] 37%|███▋      | 287/785 [01:47<02:25,  3.43it/s] 37%|███▋      | 288/785 [01:48<02:24,  3.43it/s] 37%|███▋      | 289/785 [01:48<02:24,  3.43it/s] 37%|███▋      | 290/785 [01:48<02:24,  3.43it/s] 37%|███▋      | 291/785 [01:49<02:24,  3.43it/s] 37%|███▋      | 292/785 [01:49<02:23,  3.43it/s] 37%|███▋      | 293/785 [01:49<02:23,  3.43it/s] 37%|███▋      | 294/785 [01:49<02:23,  3.43it/s] 38%|███▊      | 295/785 [01:50<02:22,  3.43it/s] 38%|███▊      | 296/785 [01:50<02:22,  3.43it/s] 38%|███▊      | 297/785 [01:50<02:22,  3.43it/s] 38%|███▊      | 298/785 [01:51<02:22,  3.41it/s] 38%|███▊      | 299/785 [01:51<02:22,  3.41it/s] 38%|███▊      | 300/785 [01:51<02:21,  3.42it/s] 38%|███▊      | 301/785 [01:52<02:21,  3.42it/s] 38%|███▊      | 302/785 [01:52<02:21,  3.42it/s] 39%|███▊      | 303/785 [01:52<02:20,  3.43it/s] 39%|███▊      | 304/785 [01:52<02:20,  3.42it/s] 39%|███▉      | 305/785 [01:53<02:20,  3.43it/s] 39%|███▉      | 306/785 [01:53<02:19,  3.43it/s] 39%|███▉      | 307/785 [01:53<02:19,  3.43it/s] 39%|███▉      | 308/785 [01:54<02:19,  3.43it/s] 39%|███▉      | 309/785 [01:54<02:18,  3.43it/s] 39%|███▉      | 310/785 [01:54<02:18,  3.43it/s] 40%|███▉      | 311/785 [01:54<02:18,  3.43it/s] 40%|███▉      | 312/785 [01:55<02:17,  3.43it/s] 40%|███▉      | 313/785 [01:55<02:17,  3.43it/s] 40%|████      | 314/785 [01:55<02:17,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:06:14,955 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:06:14,955 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 09:06:14,955 >>   Batch size = 8
{'eval_loss': 0.9055418372154236, 'eval_runtime': 17.5925, 'eval_samples_per_second': 368.736, 'eval_steps_per_second': 46.099, 'epoch': 1.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.72it/s][A
  1%|▏         | 12/811 [00:00<00:16, 49.87it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.18it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.51it/s][A
  3%|▎         | 28/811 [00:00<00:16, 46.98it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.62it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.54it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.12it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.04it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.13it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.06it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.17it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.10it/s][A
  9%|▉         | 73/811 [00:01<00:16, 46.02it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.05it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.08it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.09it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 46.04it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 45.93it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.08it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.06it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.05it/s][A
 15%|█▍        | 118/811 [00:02<00:15, 46.06it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.14it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 45.99it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 45.99it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.02it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 45.99it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.00it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 45.99it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 45.98it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.01it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.06it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.03it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.10it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 45.92it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 45.89it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.00it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.09it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.13it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.08it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.01it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 45.98it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 45.95it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.07it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 45.93it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 45.94it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.07it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.09it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.09it/s][A
 32%|███▏      | 258/811 [00:05<00:12, 46.02it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 45.94it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 45.60it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.02it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.01it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.03it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.03it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.01it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.03it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 45.99it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 45.95it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 45.83it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 45.82it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 45.96it/s][A
 40%|████      | 328/811 [00:07<00:10, 46.00it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.02it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.10it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 45.96it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.00it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.01it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 45.93it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 45.94it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 46.00it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 45.95it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 45.92it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 45.99it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 45.99it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.00it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 45.99it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 45.92it/s][A
 50%|█████     | 408/811 [00:08<00:08, 45.98it/s][A
 51%|█████     | 413/811 [00:08<00:08, 45.93it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 45.96it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 45.97it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 45.92it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 45.96it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 45.97it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.03it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.02it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 45.89it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 45.87it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 44.75it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 45.16it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 45.52it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 45.66it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 45.73it/s][A
 60%|██████    | 488/811 [00:10<00:07, 45.88it/s][A
 61%|██████    | 493/811 [00:10<00:06, 45.78it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 45.94it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 45.82it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 45.74it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 45.87it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 45.99it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.02it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 45.98it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.01it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.02it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 46.05it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.04it/s][A
 68%|██████▊   | 553/811 [00:12<00:05, 45.91it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 46.02it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 45.90it/s][A
 70%|███████   | 568/811 [00:12<00:05, 45.93it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.04it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.03it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.11it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.06it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.00it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 45.84it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 45.89it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.02it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 45.92it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 45.98it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 45.99it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.05it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 45.97it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 45.95it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.00it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 45.86it/s][A
 81%|████████  | 653/811 [00:14<00:03, 45.97it/s][A
 81%|████████  | 658/811 [00:14<00:03, 45.96it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.04it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 46.02it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.05it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 45.97it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 45.90it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.02it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 46.01it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 46.00it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 45.89it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 45.98it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.00it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 46.05it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 45.46it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 45.72it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 45.80it/s][A
 91%|█████████ | 738/811 [00:16<00:01, 45.88it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 45.80it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 45.84it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 45.96it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 45.92it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 46.02it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 46.00it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 45.86it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 45.88it/s][A
 97%|█████████▋| 783/811 [00:17<00:00, 45.94it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 45.91it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 45.93it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 46.01it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.02it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 46.01it/s][A
                                                 [A                                                 
100%|██████████| 811/811 [00:17<00:00, 46.01it/s][A 40%|████      | 314/785 [02:13<02:17,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:06:32,625 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-29 09:06:32,642 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:06:34,780 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:06:34,796 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:06:34,807 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:20<59:13,  7.56s/it] 40%|████      | 316/785 [02:20<42:03,  5.38s/it] 40%|████      | 317/785 [02:20<30:03,  3.85s/it] 41%|████      | 318/785 [02:21<21:40,  2.78s/it] 41%|████      | 319/785 [02:21<15:49,  2.04s/it] 41%|████      | 320/785 [02:21<11:43,  1.51s/it] 41%|████      | 321/785 [02:22<08:51,  1.15s/it] 41%|████      | 322/785 [02:22<06:51,  1.12it/s] 41%|████      | 323/785 [02:22<05:27,  1.41it/s] 41%|████▏     | 324/785 [02:22<04:29,  1.71it/s] 41%|████▏     | 325/785 [02:23<03:48,  2.02it/s] 42%|████▏     | 326/785 [02:23<03:19,  2.30it/s] 42%|████▏     | 327/785 [02:23<02:59,  2.55it/s] 42%|████▏     | 328/785 [02:24<02:45,  2.77it/s] 42%|████▏     | 329/785 [02:24<02:35,  2.94it/s] 42%|████▏     | 330/785 [02:24<02:28,  3.07it/s] 42%|████▏     | 331/785 [02:24<02:23,  3.17it/s] 42%|████▏     | 332/785 [02:25<02:19,  3.25it/s] 42%|████▏     | 333/785 [02:25<02:16,  3.30it/s] 43%|████▎     | 334/785 [02:25<02:14,  3.34it/s] 43%|████▎     | 335/785 [02:26<02:13,  3.37it/s] 43%|████▎     | 336/785 [02:26<02:12,  3.39it/s] 43%|████▎     | 337/785 [02:26<02:11,  3.41it/s] 43%|████▎     | 338/785 [02:27<02:11,  3.41it/s] 43%|████▎     | 339/785 [02:27<02:10,  3.42it/s] 43%|████▎     | 340/785 [02:27<02:09,  3.42it/s] 43%|████▎     | 341/785 [02:27<02:09,  3.43it/s] 44%|████▎     | 342/785 [02:28<02:09,  3.43it/s] 44%|████▎     | 343/785 [02:28<02:08,  3.43it/s] 44%|████▍     | 344/785 [02:28<02:08,  3.44it/s] 44%|████▍     | 345/785 [02:29<02:08,  3.44it/s] 44%|████▍     | 346/785 [02:29<02:07,  3.44it/s] 44%|████▍     | 347/785 [02:29<02:07,  3.44it/s] 44%|████▍     | 348/785 [02:29<02:07,  3.44it/s] 44%|████▍     | 349/785 [02:30<02:07,  3.43it/s] 45%|████▍     | 350/785 [02:30<02:06,  3.43it/s] 45%|████▍     | 351/785 [02:30<02:06,  3.43it/s] 45%|████▍     | 352/785 [02:31<02:05,  3.44it/s] 45%|████▍     | 353/785 [02:31<02:05,  3.44it/s] 45%|████▌     | 354/785 [02:31<02:05,  3.44it/s] 45%|████▌     | 355/785 [02:31<02:05,  3.44it/s] 45%|████▌     | 356/785 [02:32<02:04,  3.44it/s] 45%|████▌     | 357/785 [02:32<02:04,  3.44it/s] 46%|████▌     | 358/785 [02:32<02:04,  3.44it/s] 46%|████▌     | 359/785 [02:33<02:03,  3.44it/s] 46%|████▌     | 360/785 [02:33<02:04,  3.42it/s] 46%|████▌     | 361/785 [02:33<02:03,  3.43it/s] 46%|████▌     | 362/785 [02:34<02:03,  3.43it/s] 46%|████▌     | 363/785 [02:34<02:03,  3.43it/s] 46%|████▋     | 364/785 [02:34<02:02,  3.43it/s] 46%|████▋     | 365/785 [02:34<02:02,  3.43it/s] 47%|████▋     | 366/785 [02:35<02:01,  3.44it/s] 47%|████▋     | 367/785 [02:35<02:01,  3.44it/s] 47%|████▋     | 368/785 [02:35<02:01,  3.44it/s] 47%|████▋     | 369/785 [02:36<02:01,  3.44it/s] 47%|████▋     | 370/785 [02:36<02:00,  3.44it/s] 47%|████▋     | 371/785 [02:36<02:00,  3.43it/s] 47%|████▋     | 372/785 [02:36<02:00,  3.43it/s] 48%|████▊     | 373/785 [02:37<02:00,  3.43it/s] 48%|████▊     | 374/785 [02:37<01:59,  3.43it/s] 48%|████▊     | 375/785 [02:37<01:59,  3.43it/s] 48%|████▊     | 376/785 [02:38<01:59,  3.44it/s] 48%|████▊     | 377/785 [02:38<01:58,  3.44it/s] 48%|████▊     | 378/785 [02:38<01:58,  3.44it/s] 48%|████▊     | 379/785 [02:38<01:58,  3.44it/s] 48%|████▊     | 380/785 [02:39<01:57,  3.44it/s] 49%|████▊     | 381/785 [02:39<01:57,  3.44it/s] 49%|████▊     | 382/785 [02:39<01:57,  3.43it/s] 49%|████▉     | 383/785 [02:40<01:57,  3.43it/s] 49%|████▉     | 384/785 [02:40<01:56,  3.43it/s] 49%|████▉     | 385/785 [02:40<01:56,  3.43it/s] 49%|████▉     | 386/785 [02:41<01:56,  3.43it/s] 49%|████▉     | 387/785 [02:41<01:55,  3.43it/s] 49%|████▉     | 388/785 [02:41<01:55,  3.44it/s] 50%|████▉     | 389/785 [02:41<01:55,  3.44it/s] 50%|████▉     | 390/785 [02:42<01:54,  3.44it/s] 50%|████▉     | 391/785 [02:42<01:54,  3.44it/s] 50%|████▉     | 392/785 [02:42<01:54,  3.44it/s] 50%|█████     | 393/785 [02:43<01:54,  3.43it/s] 50%|█████     | 394/785 [02:43<01:53,  3.43it/s] 50%|█████     | 395/785 [02:43<01:53,  3.43it/s] 50%|█████     | 396/785 [02:43<01:53,  3.43it/s] 51%|█████     | 397/785 [02:44<01:52,  3.44it/s] 51%|█████     | 398/785 [02:44<01:52,  3.43it/s] 51%|█████     | 399/785 [02:44<01:52,  3.44it/s] 51%|█████     | 400/785 [02:45<01:52,  3.43it/s] 51%|█████     | 401/785 [02:45<01:51,  3.43it/s] 51%|█████     | 402/785 [02:45<01:51,  3.43it/s] 51%|█████▏    | 403/785 [02:45<01:51,  3.43it/s] 51%|█████▏    | 404/785 [02:46<01:50,  3.43it/s] 52%|█████▏    | 405/785 [02:46<01:50,  3.43it/s] 52%|█████▏    | 406/785 [02:46<01:50,  3.43it/s] 52%|█████▏    | 407/785 [02:47<01:50,  3.43it/s] 52%|█████▏    | 408/785 [02:47<01:50,  3.41it/s] 52%|█████▏    | 409/785 [02:47<01:50,  3.42it/s] 52%|█████▏    | 410/785 [02:48<01:49,  3.42it/s] 52%|█████▏    | 411/785 [02:48<01:49,  3.43it/s] 52%|█████▏    | 412/785 [02:48<01:48,  3.43it/s] 53%|█████▎    | 413/785 [02:48<01:48,  3.43it/s] 53%|█████▎    | 414/785 [02:49<01:48,  3.43it/s] 53%|█████▎    | 415/785 [02:49<01:47,  3.43it/s] 53%|█████▎    | 416/785 [02:49<01:47,  3.43it/s] 53%|█████▎    | 417/785 [02:50<01:47,  3.43it/s] 53%|█████▎    | 418/785 [02:50<01:47,  3.43it/s] 53%|█████▎    | 419/785 [02:50<01:46,  3.42it/s] 54%|█████▎    | 420/785 [02:50<01:46,  3.43it/s] 54%|█████▎    | 421/785 [02:51<01:46,  3.43it/s] 54%|█████▍    | 422/785 [02:51<01:45,  3.43it/s] 54%|█████▍    | 423/785 [02:51<01:45,  3.43it/s] 54%|█████▍    | 424/785 [02:52<01:45,  3.43it/s] 54%|█████▍    | 425/785 [02:52<01:44,  3.43it/s] 54%|█████▍    | 426/785 [02:52<01:44,  3.43it/s] 54%|█████▍    | 427/785 [02:52<01:44,  3.43it/s] 55%|█████▍    | 428/785 [02:53<01:44,  3.43it/s] 55%|█████▍    | 429/785 [02:53<01:43,  3.43it/s] 55%|█████▍    | 430/785 [02:53<01:43,  3.42it/s] 55%|█████▍    | 431/785 [02:54<01:43,  3.42it/s] 55%|█████▌    | 432/785 [02:54<01:43,  3.43it/s] 55%|█████▌    | 433/785 [02:54<01:42,  3.43it/s] 55%|█████▌    | 434/785 [02:55<01:42,  3.43it/s] 55%|█████▌    | 435/785 [02:55<01:42,  3.43it/s] 56%|█████▌    | 436/785 [02:55<01:41,  3.43it/s] 56%|█████▌    | 437/785 [02:55<01:41,  3.42it/s] 56%|█████▌    | 438/785 [02:56<01:41,  3.43it/s] 56%|█████▌    | 439/785 [02:56<01:40,  3.43it/s] 56%|█████▌    | 440/785 [02:56<01:40,  3.43it/s] 56%|█████▌    | 441/785 [02:57<01:40,  3.43it/s] 56%|█████▋    | 442/785 [02:57<01:39,  3.43it/s] 56%|█████▋    | 443/785 [02:57<01:39,  3.43it/s] 57%|█████▋    | 444/785 [02:57<01:40,  3.41it/s] 57%|█████▋    | 445/785 [02:58<01:39,  3.41it/s] 57%|█████▋    | 446/785 [02:58<01:39,  3.42it/s] 57%|█████▋    | 447/785 [02:58<01:38,  3.42it/s] 57%|█████▋    | 448/785 [02:59<01:38,  3.42it/s] 57%|█████▋    | 449/785 [02:59<01:38,  3.43it/s] 57%|█████▋    | 450/785 [02:59<01:37,  3.42it/s] 57%|█████▋    | 451/785 [02:59<01:37,  3.43it/s] 58%|█████▊    | 452/785 [03:00<01:37,  3.43it/s] 58%|█████▊    | 453/785 [03:00<01:36,  3.43it/s] 58%|█████▊    | 454/785 [03:00<01:36,  3.43it/s] 58%|█████▊    | 455/785 [03:01<01:36,  3.42it/s] 58%|█████▊    | 456/785 [03:01<01:36,  3.43it/s] 58%|█████▊    | 457/785 [03:01<01:35,  3.43it/s] 58%|█████▊    | 458/785 [03:02<01:35,  3.43it/s] 58%|█████▊    | 459/785 [03:02<01:35,  3.43it/s] 59%|█████▊    | 460/785 [03:02<01:34,  3.43it/s] 59%|█████▊    | 461/785 [03:02<01:34,  3.41it/s] 59%|█████▉    | 462/785 [03:03<01:34,  3.42it/s] 59%|█████▉    | 463/785 [03:03<01:34,  3.41it/s] 59%|█████▉    | 464/785 [03:03<01:33,  3.42it/s] 59%|█████▉    | 465/785 [03:04<01:33,  3.42it/s] 59%|█████▉    | 466/785 [03:04<01:33,  3.42it/s] 59%|█████▉    | 467/785 [03:04<01:32,  3.42it/s] 60%|█████▉    | 468/785 [03:04<01:33,  3.40it/s] 60%|█████▉    | 469/785 [03:05<01:32,  3.41it/s] 60%|█████▉    | 470/785 [03:05<01:32,  3.42it/s] 60%|██████    | 471/785 [03:05<01:31,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 09:07:24,979 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:07:24,979 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 09:07:24,979 >>   Batch size = 8
{'eval_loss': 0.9171075224876404, 'eval_runtime': 17.6461, 'eval_samples_per_second': 367.617, 'eval_steps_per_second': 45.959, 'epoch': 2.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:13, 57.91it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.57it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.40it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.54it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.09it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.80it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.64it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.14it/s][A
  6%|▌         | 48/811 [00:01<00:16, 45.99it/s][A
  7%|▋         | 53/811 [00:01<00:16, 45.92it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.00it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.05it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.12it/s][A
  9%|▉         | 73/811 [00:01<00:16, 46.03it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.06it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.07it/s][A
 11%|█         | 88/811 [00:01<00:15, 45.96it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 45.86it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 45.82it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.01it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.07it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.13it/s][A
 15%|█▍        | 118/811 [00:02<00:15, 46.03it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.01it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.12it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 45.98it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 45.97it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 45.95it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 45.97it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 45.98it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 45.98it/s][A
 20%|██        | 163/811 [00:03<00:14, 45.98it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.03it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.05it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 45.97it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.03it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 45.93it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 45.97it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.03it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.00it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.00it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.03it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.08it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 45.98it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.01it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 46.03it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.05it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.13it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.08it/s][A
 31%|███       | 253/811 [00:05<00:12, 45.92it/s][A
 32%|███▏      | 258/811 [00:05<00:12, 46.01it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.05it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.04it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.01it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.04it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 45.93it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 45.93it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 45.98it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.00it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 46.01it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.02it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.07it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 45.94it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 45.96it/s][A
 40%|████      | 328/811 [00:07<00:10, 45.97it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.05it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 45.97it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 46.07it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.08it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 45.92it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.01it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 45.97it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 45.91it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.12it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.05it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.07it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.00it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.00it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.05it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 46.12it/s][A
 50%|█████     | 408/811 [00:08<00:08, 46.03it/s][A
 51%|█████     | 413/811 [00:08<00:08, 46.12it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 45.91it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.05it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 45.97it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.00it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.02it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.04it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.11it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 45.86it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 45.99it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.04it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 45.94it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.05it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 45.95it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 45.97it/s][A
 60%|██████    | 488/811 [00:10<00:07, 46.02it/s][A
 61%|██████    | 493/811 [00:10<00:06, 45.94it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 45.80it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 45.93it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 45.99it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.00it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 45.99it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 45.81it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 45.77it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 45.77it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 45.81it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 45.83it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 45.94it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 45.95it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 45.94it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 45.95it/s][A
 70%|███████   | 568/811 [00:12<00:05, 45.96it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.07it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 45.94it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 45.98it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.05it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.11it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.03it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.01it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.07it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 45.97it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.11it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.02it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 45.95it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.02it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 45.97it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.06it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 45.87it/s][A
 81%|████████  | 653/811 [00:14<00:03, 45.97it/s][A
 81%|████████  | 658/811 [00:14<00:03, 45.97it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 45.97it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 45.98it/s][A
 83%|████████▎ | 673/811 [00:14<00:03, 45.93it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 46.00it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 46.02it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.01it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 45.74it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 45.69it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 45.63it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 45.62it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 45.58it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 45.67it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 45.66it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 45.66it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 45.65it/s][A
 91%|█████████ | 738/811 [00:16<00:01, 45.79it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 45.85it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 45.91it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 45.95it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 45.86it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 45.94it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 45.89it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 46.00it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 45.90it/s][A
 97%|█████████▋| 783/811 [00:17<00:00, 45.92it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 46.03it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 46.01it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 46.02it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.00it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 45.93it/s][A
                                                 [A                                                 
100%|██████████| 811/811 [00:17<00:00, 45.93it/s][A 60%|██████    | 471/785 [03:23<01:31,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:07:42,652 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-29 09:07:42,670 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:07:45,225 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:07:45,251 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:07:45,262 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:30<40:20,  7.73s/it] 60%|██████    | 473/785 [03:31<28:36,  5.50s/it] 60%|██████    | 474/785 [03:31<20:25,  3.94s/it] 61%|██████    | 475/785 [03:31<14:41,  2.84s/it] 61%|██████    | 476/785 [03:32<10:42,  2.08s/it] 61%|██████    | 477/785 [03:32<07:54,  1.54s/it] 61%|██████    | 478/785 [03:32<05:58,  1.17s/it] 61%|██████    | 479/785 [03:32<04:36,  1.11it/s] 61%|██████    | 480/785 [03:33<03:39,  1.39it/s] 61%|██████▏   | 481/785 [03:33<02:59,  1.69it/s] 61%|██████▏   | 482/785 [03:33<02:31,  2.00it/s] 62%|██████▏   | 483/785 [03:34<02:12,  2.28it/s] 62%|██████▏   | 484/785 [03:34<01:58,  2.53it/s] 62%|██████▏   | 485/785 [03:34<01:49,  2.75it/s] 62%|██████▏   | 486/785 [03:34<01:42,  2.92it/s] 62%|██████▏   | 487/785 [03:35<01:37,  3.06it/s] 62%|██████▏   | 488/785 [03:35<01:33,  3.17it/s] 62%|██████▏   | 489/785 [03:35<01:31,  3.24it/s] 62%|██████▏   | 490/785 [03:36<01:29,  3.30it/s] 63%|██████▎   | 491/785 [03:36<01:28,  3.34it/s] 63%|██████▎   | 492/785 [03:36<01:27,  3.37it/s] 63%|██████▎   | 493/785 [03:37<01:26,  3.39it/s] 63%|██████▎   | 494/785 [03:37<01:25,  3.40it/s] 63%|██████▎   | 495/785 [03:37<01:25,  3.40it/s] 63%|██████▎   | 496/785 [03:37<01:24,  3.41it/s] 63%|██████▎   | 497/785 [03:38<01:24,  3.42it/s] 63%|██████▎   | 498/785 [03:38<01:23,  3.43it/s] 64%|██████▎   | 499/785 [03:38<01:23,  3.43it/s] 64%|██████▎   | 500/785 [03:39<01:23,  3.43it/s]                                                  64%|██████▎   | 500/785 [03:39<01:23,  3.43it/s] 64%|██████▍   | 501/785 [03:39<01:22,  3.43it/s] 64%|██████▍   | 502/785 [03:39<01:22,  3.43it/s] 64%|██████▍   | 503/785 [03:39<01:22,  3.44it/s] 64%|██████▍   | 504/785 [03:40<01:21,  3.44it/s] 64%|██████▍   | 505/785 [03:40<01:21,  3.44it/s] 64%|██████▍   | 506/785 [03:40<01:21,  3.44it/s] 65%|██████▍   | 507/785 [03:41<01:20,  3.44it/s] 65%|██████▍   | 508/785 [03:41<01:20,  3.44it/s] 65%|██████▍   | 509/785 [03:41<01:20,  3.44it/s] 65%|██████▍   | 510/785 [03:41<01:19,  3.44it/s] 65%|██████▌   | 511/785 [03:42<01:19,  3.44it/s] 65%|██████▌   | 512/785 [03:42<01:19,  3.44it/s] 65%|██████▌   | 513/785 [03:42<01:19,  3.44it/s] 65%|██████▌   | 514/785 [03:43<01:18,  3.44it/s] 66%|██████▌   | 515/785 [03:43<01:18,  3.44it/s] 66%|██████▌   | 516/785 [03:43<01:18,  3.44it/s] 66%|██████▌   | 517/785 [03:44<01:18,  3.43it/s] 66%|██████▌   | 518/785 [03:44<01:17,  3.43it/s] 66%|██████▌   | 519/785 [03:44<01:17,  3.43it/s] 66%|██████▌   | 520/785 [03:44<01:17,  3.43it/s] 66%|██████▋   | 521/785 [03:45<01:16,  3.44it/s] 66%|██████▋   | 522/785 [03:45<01:16,  3.44it/s] 67%|██████▋   | 523/785 [03:45<01:16,  3.44it/s] 67%|██████▋   | 524/785 [03:46<01:15,  3.44it/s] 67%|██████▋   | 525/785 [03:46<01:15,  3.44it/s] 67%|██████▋   | 526/785 [03:46<01:15,  3.44it/s] 67%|██████▋   | 527/785 [03:46<01:15,  3.44it/s] 67%|██████▋   | 528/785 [03:47<01:15,  3.42it/s] 67%|██████▋   | 529/785 [03:47<01:14,  3.43it/s] 68%|██████▊   | 530/785 [03:47<01:14,  3.43it/s] 68%|██████▊   | 531/785 [03:48<01:13,  3.43it/s] 68%|██████▊   | 532/785 [03:48<01:13,  3.43it/s] 68%|██████▊   | 533/785 [03:48<01:13,  3.44it/s] 68%|██████▊   | 534/785 [03:48<01:13,  3.44it/s] 68%|██████▊   | 535/785 [03:49<01:12,  3.44it/s] 68%|██████▊   | 536/785 [03:49<01:12,  3.44it/s] 68%|██████▊   | 537/785 [03:49<01:12,  3.44it/s] 69%|██████▊   | 538/785 [03:50<01:12,  3.43it/s] 69%|██████▊   | 539/785 [03:50<01:11,  3.42it/s] 69%|██████▉   | 540/785 [03:50<01:11,  3.43it/s] 69%|██████▉   | 541/785 [03:50<01:11,  3.43it/s] 69%|██████▉   | 542/785 [03:51<01:10,  3.43it/s] 69%|██████▉   | 543/785 [03:51<01:10,  3.43it/s] 69%|██████▉   | 544/785 [03:51<01:10,  3.43it/s] 69%|██████▉   | 545/785 [03:52<01:09,  3.43it/s] 70%|██████▉   | 546/785 [03:52<01:09,  3.43it/s] 70%|██████▉   | 547/785 [03:52<01:09,  3.43it/s] 70%|██████▉   | 548/785 [03:53<01:08,  3.44it/s] 70%|██████▉   | 549/785 [03:53<01:08,  3.44it/s] 70%|███████   | 550/785 [03:53<01:08,  3.42it/s] 70%|███████   | 551/785 [03:53<01:08,  3.42it/s] 70%|███████   | 552/785 [03:54<01:07,  3.43it/s] 70%|███████   | 553/785 [03:54<01:07,  3.43it/s] 71%|███████   | 554/785 [03:54<01:07,  3.43it/s] 71%|███████   | 555/785 [03:55<01:06,  3.43it/s] 71%|███████   | 556/785 [03:55<01:06,  3.44it/s] 71%|███████   | 557/785 [03:55<01:06,  3.43it/s] 71%|███████   | 558/785 [03:55<01:06,  3.44it/s] 71%|███████   | 559/785 [03:56<01:05,  3.44it/s] 71%|███████▏  | 560/785 [03:56<01:05,  3.44it/s] 71%|███████▏  | 561/785 [03:56<01:05,  3.44it/s] 72%|███████▏  | 562/785 [03:57<01:04,  3.44it/s] 72%|███████▏  | 563/785 [03:57<01:04,  3.44it/s] 72%|███████▏  | 564/785 [03:57<01:04,  3.44it/s] 72%|███████▏  | 565/785 [03:57<01:03,  3.44it/s] 72%|███████▏  | 566/785 [03:58<01:03,  3.44it/s] 72%|███████▏  | 567/785 [03:58<01:03,  3.44it/s] 72%|███████▏  | 568/785 [03:58<01:03,  3.44it/s] 72%|███████▏  | 569/785 [03:59<01:02,  3.43it/s] 73%|███████▎  | 570/785 [03:59<01:02,  3.43it/s] 73%|███████▎  | 571/785 [03:59<01:02,  3.43it/s] 73%|███████▎  | 572/785 [04:00<01:02,  3.43it/s] 73%|███████▎  | 573/785 [04:00<01:01,  3.43it/s] 73%|███████▎  | 574/785 [04:00<01:01,  3.44it/s] 73%|███████▎  | 575/785 [04:00<01:01,  3.44it/s] 73%|███████▎  | 576/785 [04:01<01:00,  3.44it/s] 74%|███████▎  | 577/785 [04:01<01:00,  3.44it/s] 74%|███████▎  | 578/785 [04:01<01:00,  3.43it/s] 74%|███████▍  | 579/785 [04:02<00:59,  3.43it/s] 74%|███████▍  | 580/785 [04:02<00:59,  3.43it/s] 74%|███████▍  | 581/785 [04:02<00:59,  3.43it/s] 74%|███████▍  | 582/785 [04:02<00:59,  3.43it/s] 74%|███████▍  | 583/785 [04:03<00:58,  3.43it/s] 74%|███████▍  | 584/785 [04:03<00:58,  3.43it/s] 75%|███████▍  | 585/785 [04:03<00:58,  3.43it/s] 75%|███████▍  | 586/785 [04:04<00:58,  3.43it/s] 75%|███████▍  | 587/785 [04:04<00:57,  3.42it/s] 75%|███████▍  | 588/785 [04:04<00:57,  3.42it/s] 75%|███████▌  | 589/785 [04:04<00:57,  3.42it/s] 75%|███████▌  | 590/785 [04:05<00:57,  3.42it/s] 75%|███████▌  | 591/785 [04:05<00:56,  3.42it/s] 75%|███████▌  | 592/785 [04:05<00:56,  3.42it/s] 76%|███████▌  | 593/785 [04:06<00:57,  3.35it/s] 76%|███████▌  | 594/785 [04:06<00:56,  3.37it/s] 76%|███████▌  | 595/785 [04:06<00:56,  3.39it/s] 76%|███████▌  | 596/785 [04:07<00:55,  3.40it/s] 76%|███████▌  | 597/785 [04:07<00:55,  3.41it/s] 76%|███████▌  | 598/785 [04:07<00:54,  3.42it/s] 76%|███████▋  | 599/785 [04:07<00:54,  3.42it/s] 76%|███████▋  | 600/785 [04:08<00:54,  3.42it/s] 77%|███████▋  | 601/785 [04:08<00:53,  3.43it/s] 77%|███████▋  | 602/785 [04:08<00:53,  3.43it/s] 77%|███████▋  | 603/785 [04:09<00:53,  3.43it/s] 77%|███████▋  | 604/785 [04:09<00:52,  3.42it/s] 77%|███████▋  | 605/785 [04:09<00:52,  3.42it/s] 77%|███████▋  | 606/785 [04:09<00:52,  3.42it/s] 77%|███████▋  | 607/785 [04:10<00:52,  3.42it/s] 77%|███████▋  | 608/785 [04:10<00:51,  3.42it/s] 78%|███████▊  | 609/785 [04:10<00:51,  3.43it/s] 78%|███████▊  | 610/785 [04:11<00:51,  3.43it/s] 78%|███████▊  | 611/785 [04:11<00:50,  3.43it/s] 78%|███████▊  | 612/785 [04:11<00:50,  3.43it/s] 78%|███████▊  | 613/785 [04:12<00:50,  3.43it/s] 78%|███████▊  | 614/785 [04:12<00:49,  3.43it/s] 78%|███████▊  | 615/785 [04:12<00:49,  3.43it/s] 78%|███████▊  | 616/785 [04:12<00:49,  3.43it/s] 79%|███████▊  | 617/785 [04:13<00:49,  3.43it/s] 79%|███████▊  | 618/785 [04:13<00:48,  3.43it/s] 79%|███████▉  | 619/785 [04:13<00:48,  3.43it/s] 79%|███████▉  | 620/785 [04:14<00:48,  3.43it/s] 79%|███████▉  | 621/785 [04:14<00:47,  3.43it/s] 79%|███████▉  | 622/785 [04:14<00:47,  3.42it/s] 79%|███████▉  | 623/785 [04:14<00:47,  3.43it/s] 79%|███████▉  | 624/785 [04:15<00:46,  3.43it/s] 80%|███████▉  | 625/785 [04:15<00:46,  3.43it/s] 80%|███████▉  | 626/785 [04:15<00:46,  3.43it/s] 80%|███████▉  | 627/785 [04:16<00:46,  3.43it/s] 80%|████████  | 628/785 [04:16<00:45,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 09:08:35,529 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:08:35,529 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 09:08:35,529 >>   Batch size = 8
{'eval_loss': 0.9251283407211304, 'eval_runtime': 17.6469, 'eval_samples_per_second': 367.601, 'eval_steps_per_second': 45.957, 'epoch': 3.0}
{'loss': 0.6052, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 57.25it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.18it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.27it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.52it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.13it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.76it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.50it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.09it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.01it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.04it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.13it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.20it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.13it/s][A
  9%|▉         | 73/811 [00:01<00:16, 46.12it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.15it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.12it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.09it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 45.96it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.07it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.09it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.02it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.07it/s][A
 15%|█▍        | 118/811 [00:02<00:15, 46.15it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.16it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.04it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.01it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.02it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 45.93it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 45.94it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 45.98it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.02it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.11it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.16it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.13it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.02it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 45.98it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 46.01it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.04it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.04it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 45.99it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.07it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.11it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.16it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 45.95it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 45.98it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 45.94it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.04it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.08it/s][A
 31%|███       | 248/811 [00:05<00:12, 45.95it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.04it/s][A
 32%|███▏      | 258/811 [00:05<00:12, 45.99it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.06it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 45.99it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.05it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.06it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.02it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.06it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 45.93it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 45.98it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 46.03it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.08it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.03it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 46.06it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 46.02it/s][A
 40%|████      | 328/811 [00:07<00:10, 46.00it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.02it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 45.94it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 45.96it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.03it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.07it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.10it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 45.90it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 45.99it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 45.97it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.06it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.05it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.03it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.00it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.05it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 46.08it/s][A
 50%|█████     | 408/811 [00:08<00:08, 45.99it/s][A
 51%|█████     | 413/811 [00:08<00:08, 45.99it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 45.97it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.03it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.07it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.07it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.04it/s][A
 55%|█████▍    | 443/811 [00:09<00:08, 45.97it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 45.98it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 46.02it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 46.07it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.03it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.02it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.08it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.00it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.07it/s][A
 60%|██████    | 488/811 [00:10<00:07, 45.93it/s][A
 61%|██████    | 493/811 [00:10<00:06, 46.04it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 46.00it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.06it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 46.07it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 45.98it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 45.99it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 45.99it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.09it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 45.91it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 45.98it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 46.01it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.02it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 46.01it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 45.98it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.01it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.01it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.10it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.08it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.01it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 45.99it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 45.98it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.04it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 45.93it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 45.95it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.01it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.09it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.13it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.02it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 45.95it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 45.99it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.08it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 46.07it/s][A
 81%|████████  | 653/811 [00:14<00:03, 45.96it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.01it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 45.92it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 45.96it/s][A
 83%|████████▎ | 673/811 [00:14<00:03, 45.94it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 46.01it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 45.88it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 45.99it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 45.98it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 45.96it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 45.96it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 46.01it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.06it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 46.03it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 45.96it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 46.03it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 46.04it/s][A
 91%|█████████ | 738/811 [00:16<00:01, 46.02it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 45.97it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 46.01it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 46.01it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 46.06it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 45.92it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 45.94it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 45.97it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 46.02it/s][A
 97%|█████████▋| 783/811 [00:16<00:00, 46.03it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 45.98it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 46.01it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 45.99it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.03it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 45.96it/s][A
                                                 [A                                                 
100%|██████████| 811/811 [00:17<00:00, 45.96it/s][A 80%|████████  | 628/785 [04:34<00:45,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:08:53,170 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-29 09:08:53,187 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:08:55,558 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:08:55,575 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:08:55,585 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:41<20:01,  7.70s/it] 80%|████████  | 630/785 [04:41<14:09,  5.48s/it] 80%|████████  | 631/785 [04:41<10:04,  3.92s/it] 81%|████████  | 632/785 [04:42<07:13,  2.83s/it] 81%|████████  | 633/785 [04:42<05:14,  2.07s/it] 81%|████████  | 634/785 [04:42<03:52,  1.54s/it] 81%|████████  | 635/785 [04:43<02:54,  1.16s/it] 81%|████████  | 636/785 [04:43<02:14,  1.11it/s] 81%|████████  | 637/785 [04:43<01:46,  1.39it/s] 81%|████████▏ | 638/785 [04:44<01:26,  1.70it/s] 81%|████████▏ | 639/785 [04:44<01:12,  2.00it/s] 82%|████████▏ | 640/785 [04:44<01:03,  2.29it/s] 82%|████████▏ | 641/785 [04:44<00:56,  2.54it/s] 82%|████████▏ | 642/785 [04:45<00:51,  2.76it/s] 82%|████████▏ | 643/785 [04:45<00:48,  2.93it/s] 82%|████████▏ | 644/785 [04:45<00:45,  3.07it/s] 82%|████████▏ | 645/785 [04:46<00:44,  3.17it/s] 82%|████████▏ | 646/785 [04:46<00:42,  3.25it/s] 82%|████████▏ | 647/785 [04:46<00:41,  3.30it/s] 83%|████████▎ | 648/785 [04:46<00:40,  3.34it/s] 83%|████████▎ | 649/785 [04:47<00:40,  3.37it/s] 83%|████████▎ | 650/785 [04:47<00:39,  3.39it/s] 83%|████████▎ | 651/785 [04:47<00:39,  3.40it/s] 83%|████████▎ | 652/785 [04:48<00:39,  3.41it/s] 83%|████████▎ | 653/785 [04:48<00:38,  3.42it/s] 83%|████████▎ | 654/785 [04:48<00:38,  3.42it/s] 83%|████████▎ | 655/785 [04:48<00:37,  3.43it/s] 84%|████████▎ | 656/785 [04:49<00:37,  3.43it/s] 84%|████████▎ | 657/785 [04:49<00:37,  3.44it/s] 84%|████████▍ | 658/785 [04:49<00:36,  3.44it/s] 84%|████████▍ | 659/785 [04:50<00:36,  3.44it/s] 84%|████████▍ | 660/785 [04:50<00:36,  3.44it/s] 84%|████████▍ | 661/785 [04:50<00:36,  3.44it/s] 84%|████████▍ | 662/785 [04:50<00:35,  3.44it/s] 84%|████████▍ | 663/785 [04:51<00:35,  3.41it/s] 85%|████████▍ | 664/785 [04:51<00:35,  3.42it/s] 85%|████████▍ | 665/785 [04:51<00:35,  3.42it/s] 85%|████████▍ | 666/785 [04:52<00:34,  3.43it/s] 85%|████████▍ | 667/785 [04:52<00:34,  3.40it/s] 85%|████████▌ | 668/785 [04:52<00:34,  3.41it/s] 85%|████████▌ | 669/785 [04:53<00:33,  3.42it/s] 85%|████████▌ | 670/785 [04:53<00:33,  3.43it/s] 85%|████████▌ | 671/785 [04:53<00:33,  3.43it/s] 86%|████████▌ | 672/785 [04:53<00:32,  3.43it/s] 86%|████████▌ | 673/785 [04:54<00:32,  3.43it/s] 86%|████████▌ | 674/785 [04:54<00:32,  3.42it/s] 86%|████████▌ | 675/785 [04:54<00:32,  3.43it/s] 86%|████████▌ | 676/785 [04:55<00:31,  3.43it/s] 86%|████████▌ | 677/785 [04:55<00:31,  3.44it/s] 86%|████████▋ | 678/785 [04:55<00:31,  3.44it/s] 86%|████████▋ | 679/785 [04:55<00:30,  3.44it/s] 87%|████████▋ | 680/785 [04:56<00:30,  3.44it/s] 87%|████████▋ | 681/785 [04:56<00:30,  3.44it/s] 87%|████████▋ | 682/785 [04:56<00:29,  3.44it/s] 87%|████████▋ | 683/785 [04:57<00:29,  3.44it/s] 87%|████████▋ | 684/785 [04:57<00:29,  3.44it/s] 87%|████████▋ | 685/785 [04:57<00:29,  3.43it/s] 87%|████████▋ | 686/785 [04:57<00:28,  3.43it/s] 88%|████████▊ | 687/785 [04:58<00:28,  3.43it/s] 88%|████████▊ | 688/785 [04:58<00:28,  3.44it/s] 88%|████████▊ | 689/785 [04:58<00:27,  3.44it/s] 88%|████████▊ | 690/785 [04:59<00:27,  3.44it/s] 88%|████████▊ | 691/785 [04:59<00:27,  3.44it/s] 88%|████████▊ | 692/785 [04:59<00:27,  3.44it/s] 88%|████████▊ | 693/785 [05:00<00:26,  3.44it/s] 88%|████████▊ | 694/785 [05:00<00:26,  3.44it/s] 89%|████████▊ | 695/785 [05:00<00:26,  3.44it/s] 89%|████████▊ | 696/785 [05:00<00:25,  3.43it/s] 89%|████████▉ | 697/785 [05:01<00:25,  3.43it/s] 89%|████████▉ | 698/785 [05:01<00:25,  3.44it/s] 89%|████████▉ | 699/785 [05:01<00:25,  3.44it/s] 89%|████████▉ | 700/785 [05:02<00:24,  3.44it/s] 89%|████████▉ | 701/785 [05:02<00:24,  3.44it/s] 89%|████████▉ | 702/785 [05:02<00:24,  3.44it/s] 90%|████████▉ | 703/785 [05:02<00:23,  3.44it/s] 90%|████████▉ | 704/785 [05:03<00:23,  3.44it/s] 90%|████████▉ | 705/785 [05:03<00:23,  3.44it/s] 90%|████████▉ | 706/785 [05:03<00:22,  3.44it/s] 90%|█████████ | 707/785 [05:04<00:22,  3.43it/s] 90%|█████████ | 708/785 [05:04<00:22,  3.44it/s] 90%|█████████ | 709/785 [05:04<00:22,  3.44it/s] 90%|█████████ | 710/785 [05:04<00:21,  3.44it/s] 91%|█████████ | 711/785 [05:05<00:21,  3.43it/s] 91%|█████████ | 712/785 [05:05<00:21,  3.44it/s] 91%|█████████ | 713/785 [05:05<00:21,  3.43it/s] 91%|█████████ | 714/785 [05:06<00:20,  3.43it/s] 91%|█████████ | 715/785 [05:06<00:20,  3.36it/s] 91%|█████████ | 716/785 [05:06<00:20,  3.38it/s] 91%|█████████▏| 717/785 [05:07<00:20,  3.39it/s] 91%|█████████▏| 718/785 [05:07<00:19,  3.41it/s] 92%|█████████▏| 719/785 [05:07<00:19,  3.42it/s] 92%|█████████▏| 720/785 [05:07<00:19,  3.42it/s] 92%|█████████▏| 721/785 [05:08<00:18,  3.42it/s] 92%|█████████▏| 722/785 [05:08<00:18,  3.43it/s] 92%|█████████▏| 723/785 [05:08<00:18,  3.43it/s] 92%|█████████▏| 724/785 [05:09<00:17,  3.40it/s] 92%|█████████▏| 725/785 [05:09<00:17,  3.42it/s] 92%|█████████▏| 726/785 [05:09<00:17,  3.42it/s] 93%|█████████▎| 727/785 [05:09<00:16,  3.42it/s] 93%|█████████▎| 728/785 [05:10<00:16,  3.42it/s] 93%|█████████▎| 729/785 [05:10<00:16,  3.43it/s] 93%|█████████▎| 730/785 [05:10<00:16,  3.43it/s] 93%|█████████▎| 731/785 [05:11<00:15,  3.43it/s] 93%|█████████▎| 732/785 [05:13<00:41,  1.27it/s] 93%|█████████▎| 733/785 [05:14<00:44,  1.17it/s] 94%|█████████▎| 734/785 [05:14<00:34,  1.46it/s] 94%|█████████▎| 735/785 [05:14<00:28,  1.77it/s] 94%|█████████▍| 736/785 [05:14<00:23,  2.07it/s] 94%|█████████▍| 737/785 [05:15<00:20,  2.36it/s] 94%|█████████▍| 738/785 [05:15<00:18,  2.60it/s] 94%|█████████▍| 739/785 [05:15<00:16,  2.81it/s] 94%|█████████▍| 740/785 [05:16<00:15,  2.97it/s] 94%|█████████▍| 741/785 [05:16<00:14,  3.10it/s] 95%|█████████▍| 742/785 [05:16<00:13,  3.19it/s] 95%|█████████▍| 743/785 [05:16<00:12,  3.26it/s] 95%|█████████▍| 744/785 [05:17<00:12,  3.32it/s] 95%|█████████▍| 745/785 [05:17<00:11,  3.35it/s] 95%|█████████▌| 746/785 [05:17<00:11,  3.38it/s] 95%|█████████▌| 747/785 [05:18<00:11,  3.40it/s] 95%|█████████▌| 748/785 [05:18<00:10,  3.41it/s] 95%|█████████▌| 749/785 [05:18<00:10,  3.42it/s] 96%|█████████▌| 750/785 [05:18<00:10,  3.43it/s] 96%|█████████▌| 751/785 [05:19<00:09,  3.44it/s] 96%|█████████▌| 752/785 [05:19<00:09,  3.44it/s] 96%|█████████▌| 753/785 [05:19<00:09,  3.43it/s] 96%|█████████▌| 754/785 [05:20<00:09,  3.43it/s] 96%|█████████▌| 755/785 [05:20<00:08,  3.44it/s] 96%|█████████▋| 756/785 [05:20<00:08,  3.44it/s] 96%|█████████▋| 757/785 [05:21<00:08,  3.44it/s] 97%|█████████▋| 758/785 [05:21<00:07,  3.44it/s] 97%|█████████▋| 759/785 [05:21<00:07,  3.44it/s] 97%|█████████▋| 760/785 [05:21<00:07,  3.44it/s] 97%|█████████▋| 761/785 [05:22<00:06,  3.44it/s] 97%|█████████▋| 762/785 [05:22<00:06,  3.44it/s] 97%|█████████▋| 763/785 [05:22<00:06,  3.44it/s] 97%|█████████▋| 764/785 [05:23<00:06,  3.43it/s] 97%|█████████▋| 765/785 [05:23<00:05,  3.43it/s] 98%|█████████▊| 766/785 [05:23<00:05,  3.43it/s] 98%|█████████▊| 767/785 [05:23<00:05,  3.43it/s] 98%|█████████▊| 768/785 [05:24<00:04,  3.44it/s] 98%|█████████▊| 769/785 [05:24<00:04,  3.44it/s] 98%|█████████▊| 770/785 [05:24<00:04,  3.44it/s] 98%|█████████▊| 771/785 [05:25<00:04,  3.44it/s] 98%|█████████▊| 772/785 [05:25<00:03,  3.44it/s] 98%|█████████▊| 773/785 [05:25<00:03,  3.44it/s] 99%|█████████▊| 774/785 [05:25<00:03,  3.44it/s] 99%|█████████▊| 775/785 [05:26<00:02,  3.42it/s] 99%|█████████▉| 776/785 [05:26<00:02,  3.43it/s] 99%|█████████▉| 777/785 [05:26<00:02,  3.43it/s] 99%|█████████▉| 778/785 [05:27<00:02,  3.44it/s] 99%|█████████▉| 779/785 [05:27<00:01,  3.44it/s] 99%|█████████▉| 780/785 [05:27<00:01,  3.44it/s] 99%|█████████▉| 781/785 [05:28<00:01,  3.44it/s]100%|█████████▉| 782/785 [05:28<00:00,  3.44it/s]100%|█████████▉| 783/785 [05:28<00:00,  3.44it/s]100%|█████████▉| 784/785 [05:28<00:00,  3.44it/s]100%|██████████| 785/785 [05:29<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 09:09:48,375 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:09:48,375 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 09:09:48,376 >>   Batch size = 8
{'eval_loss': 0.9314392805099487, 'eval_runtime': 17.6215, 'eval_samples_per_second': 368.129, 'eval_steps_per_second': 46.023, 'epoch': 4.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:13, 58.82it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.72it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.58it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.77it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.30it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.89it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.70it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.33it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.21it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.24it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.31it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.30it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.21it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.31it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.34it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.26it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.08it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 46.11it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.09it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.17it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.25it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.20it/s][A
 15%|█▍        | 118/811 [00:02<00:15, 46.18it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.26it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.30it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.09it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.12it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 46.11it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.11it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 45.62it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 45.78it/s][A
 20%|██        | 163/811 [00:03<00:14, 45.79it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.01it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 45.95it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 45.90it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 45.97it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 45.99it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.01it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.06it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.00it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.09it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.14it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.11it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.02it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 45.96it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 45.94it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 45.98it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.10it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.09it/s][A
 31%|███       | 253/811 [00:05<00:12, 45.99it/s][A
 32%|███▏      | 258/811 [00:05<00:11, 46.10it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.08it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.04it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 45.95it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.02it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 45.86it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.10it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.08it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.03it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 46.08it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.05it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.08it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 45.91it/s][A
 40%|███▉      | 323/811 [00:06<00:10, 45.95it/s][A
 40%|████      | 328/811 [00:07<00:10, 46.06it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.07it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.14it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 46.07it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 46.14it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.14it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.15it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 46.03it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 46.07it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.05it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.08it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.10it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.17it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.13it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.18it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 46.12it/s][A
 50%|█████     | 408/811 [00:08<00:08, 46.09it/s][A
 51%|█████     | 413/811 [00:08<00:08, 46.00it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.11it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.08it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.10it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.13it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.14it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.12it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.10it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 46.06it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 46.07it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.09it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.10it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.10it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.10it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.10it/s][A
 60%|██████    | 488/811 [00:10<00:06, 46.16it/s][A
 61%|██████    | 493/811 [00:10<00:06, 46.06it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 46.08it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.06it/s][A
 63%|██████▎   | 508/811 [00:10<00:06, 46.09it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.06it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 46.10it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.09it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.11it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.12it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.09it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 45.98it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.06it/s][A
 68%|██████▊   | 553/811 [00:11<00:05, 46.06it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 46.10it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.13it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.08it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.15it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 46.12it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 46.10it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.06it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.06it/s][A
 74%|███████▎  | 598/811 [00:12<00:04, 46.09it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.15it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.16it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.07it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.07it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.13it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.16it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.14it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 45.97it/s][A
 79%|███████▉  | 643/811 [00:13<00:03, 46.06it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 46.09it/s][A
 81%|████████  | 653/811 [00:14<00:03, 46.11it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.05it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.06it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 46.04it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.12it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 46.14it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 46.07it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.07it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 46.02it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 46.09it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 46.04it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 46.08it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.08it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 46.02it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 46.12it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 46.08it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 46.04it/s][A
 91%|█████████ | 738/811 [00:15<00:01, 46.12it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 46.06it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 46.13it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 46.07it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 46.11it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 46.10it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 45.96it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 46.05it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 46.04it/s][A
 97%|█████████▋| 783/811 [00:16<00:00, 46.07it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 46.06it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 46.08it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 45.87it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 45.93it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 45.99it/s][A
                                                 [A                                                 
100%|██████████| 811/811 [00:17<00:00, 45.99it/s][A100%|██████████| 785/785 [05:46<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:10:06,019 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-29 09:10:06,037 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:10:08,597 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:10:08,612 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:10:08,620 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 09:10:13,344 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 09:10:13,348 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-157 (score: 0.9055418372154236).
                                                 100%|██████████| 785/785 [05:55<00:00,  3.44it/s]100%|██████████| 785/785 [05:55<00:00,  2.21it/s]
[INFO|trainer.py:1894] 2023-08-29 09:10:15,086 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 09:10:15,108 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:10:17,268 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:10:17,288 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:10:17,299 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:10:17,486 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:17,486 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:17,486 >>   train_loss               =     0.5943
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:17,486 >>   train_runtime            = 0:05:55.98
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:17,486 >>   train_samples            =      10051
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:17,486 >>   train_samples_per_second =    141.172
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:17,486 >>   train_steps_per_second   =      2.205
{'eval_loss': 0.938694417476654, 'eval_runtime': 17.6173, 'eval_samples_per_second': 368.217, 'eval_steps_per_second': 46.034, 'epoch': 5.0}
{'train_runtime': 355.9842, 'train_samples_per_second': 141.172, 'train_steps_per_second': 2.205, 'train_loss': 0.5942734250597134, 'epoch': 5.0}
08/29/2023 09:10:17 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 09:10:17,535 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:10:17,535 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 09:10:17,535 >>   Batch size = 8
  0%|          | 0/811 [00:00<?, ?it/s]  1%|          | 6/811 [00:00<00:13, 58.25it/s]  1%|▏         | 12/811 [00:00<00:15, 50.84it/s]  2%|▏         | 18/811 [00:00<00:16, 48.79it/s]  3%|▎         | 23/811 [00:00<00:16, 48.06it/s]  3%|▎         | 28/811 [00:00<00:16, 47.56it/s]  4%|▍         | 33/811 [00:00<00:16, 47.34it/s]  5%|▍         | 38/811 [00:00<00:16, 47.15it/s]  5%|▌         | 43/811 [00:00<00:16, 46.89it/s]  6%|▌         | 48/811 [00:01<00:16, 46.51it/s]  7%|▋         | 53/811 [00:01<00:16, 46.49it/s]  7%|▋         | 58/811 [00:01<00:16, 46.56it/s]  8%|▊         | 63/811 [00:01<00:16, 46.50it/s]  8%|▊         | 68/811 [00:01<00:15, 46.49it/s]  9%|▉         | 73/811 [00:01<00:15, 46.56it/s] 10%|▉         | 78/811 [00:01<00:15, 46.60it/s] 10%|█         | 83/811 [00:01<00:15, 46.67it/s] 11%|█         | 88/811 [00:01<00:15, 46.60it/s] 11%|█▏        | 93/811 [00:01<00:15, 46.42it/s] 12%|█▏        | 98/811 [00:02<00:15, 46.41it/s] 13%|█▎        | 103/811 [00:02<00:15, 46.45it/s] 13%|█▎        | 108/811 [00:02<00:15, 46.45it/s] 14%|█▍        | 113/811 [00:02<00:15, 46.44it/s] 15%|█▍        | 118/811 [00:02<00:14, 46.41it/s] 15%|█▌        | 123/811 [00:02<00:14, 46.50it/s] 16%|█▌        | 128/811 [00:02<00:14, 46.58it/s] 16%|█▋        | 133/811 [00:02<00:14, 46.52it/s] 17%|█▋        | 138/811 [00:02<00:14, 46.42it/s] 18%|█▊        | 143/811 [00:03<00:14, 46.40it/s] 18%|█▊        | 148/811 [00:03<00:14, 46.41it/s] 19%|█▉        | 153/811 [00:03<00:14, 46.43it/s] 19%|█▉        | 158/811 [00:03<00:14, 46.30it/s] 20%|██        | 163/811 [00:03<00:13, 46.29it/s] 21%|██        | 168/811 [00:03<00:13, 46.39it/s] 21%|██▏       | 173/811 [00:03<00:13, 46.43it/s] 22%|██▏       | 178/811 [00:03<00:13, 46.47it/s] 23%|██▎       | 183/811 [00:03<00:13, 46.35it/s] 23%|██▎       | 188/811 [00:04<00:13, 46.38it/s] 24%|██▍       | 193/811 [00:04<00:13, 46.42it/s] 24%|██▍       | 198/811 [00:04<00:13, 46.45it/s] 25%|██▌       | 203/811 [00:04<00:13, 46.44it/s] 26%|██▌       | 208/811 [00:04<00:12, 46.39it/s] 26%|██▋       | 213/811 [00:04<00:12, 46.48it/s] 27%|██▋       | 218/811 [00:04<00:12, 46.48it/s] 27%|██▋       | 223/811 [00:04<00:12, 46.47it/s] 28%|██▊       | 228/811 [00:04<00:12, 46.31it/s] 29%|██▊       | 233/811 [00:04<00:12, 46.29it/s] 29%|██▉       | 238/811 [00:05<00:12, 46.36it/s] 30%|██▉       | 243/811 [00:05<00:12, 46.41it/s] 31%|███       | 248/811 [00:05<00:12, 46.42it/s] 31%|███       | 253/811 [00:05<00:12, 46.50it/s] 32%|███▏      | 258/811 [00:05<00:11, 46.42it/s] 32%|███▏      | 263/811 [00:05<00:11, 46.37it/s] 33%|███▎      | 268/811 [00:05<00:11, 46.34it/s] 34%|███▎      | 273/811 [00:05<00:11, 46.34it/s] 34%|███▍      | 278/811 [00:05<00:11, 46.36it/s] 35%|███▍      | 283/811 [00:06<00:11, 46.38it/s] 36%|███▌      | 288/811 [00:06<00:11, 46.41it/s] 36%|███▌      | 293/811 [00:06<00:11, 46.42it/s] 37%|███▋      | 298/811 [00:06<00:11, 46.42it/s] 37%|███▋      | 303/811 [00:06<00:10, 46.45it/s] 38%|███▊      | 308/811 [00:06<00:10, 46.43it/s] 39%|███▊      | 313/811 [00:06<00:10, 46.43it/s] 39%|███▉      | 318/811 [00:06<00:10, 46.38it/s] 40%|███▉      | 323/811 [00:06<00:10, 46.39it/s] 40%|████      | 328/811 [00:07<00:10, 46.45it/s] 41%|████      | 333/811 [00:07<00:10, 46.41it/s] 42%|████▏     | 338/811 [00:07<00:10, 46.43it/s] 42%|████▏     | 343/811 [00:07<00:10, 46.42it/s] 43%|████▎     | 348/811 [00:07<00:09, 46.40it/s] 44%|████▎     | 353/811 [00:07<00:09, 46.45it/s] 44%|████▍     | 358/811 [00:07<00:09, 46.44it/s] 45%|████▍     | 363/811 [00:07<00:09, 46.30it/s] 45%|████▌     | 368/811 [00:07<00:09, 46.39it/s] 46%|████▌     | 373/811 [00:08<00:10, 41.93it/s] 47%|████▋     | 378/811 [00:08<00:10, 43.27it/s] 47%|████▋     | 383/811 [00:08<00:09, 44.22it/s] 48%|████▊     | 388/811 [00:08<00:09, 44.93it/s] 48%|████▊     | 393/811 [00:08<00:09, 45.45it/s] 49%|████▉     | 398/811 [00:08<00:09, 45.85it/s] 50%|████▉     | 403/811 [00:08<00:08, 45.92it/s] 50%|█████     | 408/811 [00:08<00:08, 46.03it/s] 51%|█████     | 413/811 [00:08<00:08, 45.99it/s] 52%|█████▏    | 418/811 [00:09<00:08, 45.91it/s] 52%|█████▏    | 423/811 [00:09<00:08, 46.03it/s] 53%|█████▎    | 428/811 [00:09<00:08, 46.03it/s] 53%|█████▎    | 433/811 [00:09<00:08, 46.26it/s] 54%|█████▍    | 438/811 [00:09<00:08, 46.40it/s] 55%|█████▍    | 443/811 [00:09<00:07, 46.44it/s] 55%|█████▌    | 448/811 [00:09<00:07, 46.49it/s] 56%|█████▌    | 453/811 [00:09<00:07, 46.49it/s] 56%|█████▋    | 458/811 [00:09<00:07, 46.39it/s] 57%|█████▋    | 463/811 [00:09<00:07, 46.18it/s] 58%|█████▊    | 468/811 [00:10<00:07, 46.27it/s] 58%|█████▊    | 473/811 [00:10<00:07, 46.21it/s] 59%|█████▉    | 478/811 [00:10<00:07, 46.27it/s] 60%|█████▉    | 483/811 [00:10<00:07, 46.39it/s] 60%|██████    | 488/811 [00:10<00:06, 46.43it/s] 61%|██████    | 493/811 [00:10<00:06, 46.45it/s] 61%|██████▏   | 498/811 [00:10<00:06, 46.46it/s] 62%|██████▏   | 503/811 [00:10<00:06, 46.39it/s] 63%|██████▎   | 508/811 [00:10<00:06, 46.32it/s] 63%|██████▎   | 513/811 [00:11<00:06, 46.23it/s] 64%|██████▍   | 518/811 [00:11<00:06, 46.21it/s] 64%|██████▍   | 523/811 [00:11<00:06, 46.32it/s] 65%|██████▌   | 528/811 [00:11<00:06, 46.30it/s] 66%|██████▌   | 533/811 [00:11<00:05, 46.38it/s] 66%|██████▋   | 538/811 [00:11<00:05, 46.44it/s] 67%|██████▋   | 543/811 [00:11<00:05, 46.38it/s] 68%|██████▊   | 548/811 [00:11<00:05, 46.43it/s] 68%|██████▊   | 553/811 [00:11<00:05, 46.30it/s] 69%|██████▉   | 558/811 [00:12<00:05, 46.27it/s] 69%|██████▉   | 563/811 [00:12<00:05, 46.16it/s] 70%|███████   | 568/811 [00:12<00:05, 46.27it/s] 71%|███████   | 573/811 [00:12<00:05, 46.37it/s] 71%|███████▏  | 578/811 [00:12<00:05, 46.36it/s] 72%|███████▏  | 583/811 [00:12<00:04, 46.42it/s] 73%|███████▎  | 588/811 [00:12<00:04, 46.36it/s] 73%|███████▎  | 593/811 [00:12<00:04, 46.33it/s] 74%|███████▎  | 598/811 [00:12<00:04, 46.39it/s] 74%|███████▍  | 603/811 [00:13<00:04, 46.24it/s] 75%|███████▍  | 608/811 [00:13<00:04, 46.19it/s] 76%|███████▌  | 613/811 [00:13<00:04, 46.19it/s] 76%|███████▌  | 618/811 [00:13<00:04, 46.28it/s] 77%|███████▋  | 623/811 [00:13<00:04, 46.36it/s] 77%|███████▋  | 628/811 [00:13<00:03, 46.35it/s] 78%|███████▊  | 633/811 [00:13<00:03, 46.38it/s] 79%|███████▊  | 638/811 [00:13<00:03, 46.32it/s] 79%|███████▉  | 643/811 [00:13<00:03, 46.19it/s] 80%|███████▉  | 648/811 [00:13<00:03, 46.16it/s] 81%|████████  | 653/811 [00:14<00:03, 46.11it/s] 81%|████████  | 658/811 [00:14<00:03, 46.12it/s] 82%|████████▏ | 663/811 [00:14<00:03, 46.15it/s] 82%|████████▏ | 668/811 [00:14<00:03, 46.29it/s] 83%|████████▎ | 673/811 [00:14<00:02, 46.32it/s] 84%|████████▎ | 678/811 [00:14<00:02, 46.33it/s] 84%|████████▍ | 683/811 [00:14<00:02, 46.27it/s] 85%|████████▍ | 688/811 [00:14<00:02, 46.29it/s] 85%|████████▌ | 693/811 [00:14<00:02, 46.24it/s] 86%|████████▌ | 698/811 [00:15<00:02, 46.28it/s] 87%|████████▋ | 703/811 [00:15<00:02, 46.19it/s] 87%|████████▋ | 708/811 [00:15<00:02, 46.17it/s] 88%|████████▊ | 713/811 [00:15<00:02, 46.18it/s] 89%|████████▊ | 718/811 [00:15<00:02, 46.30it/s] 89%|████████▉ | 723/811 [00:15<00:01, 46.35it/s] 90%|████████▉ | 728/811 [00:15<00:01, 46.32it/s] 90%|█████████ | 733/811 [00:15<00:01, 46.29it/s] 91%|█████████ | 738/811 [00:15<00:01, 46.29it/s] 92%|█████████▏| 743/811 [00:16<00:01, 46.24it/s] 92%|█████████▏| 748/811 [00:16<00:01, 46.24it/s] 93%|█████████▎| 753/811 [00:16<00:01, 46.20it/s] 93%|█████████▎| 758/811 [00:16<00:01, 46.20it/s] 94%|█████████▍| 763/811 [00:16<00:01, 46.15it/s] 95%|█████████▍| 768/811 [00:16<00:00, 45.73it/s] 95%|█████████▌| 773/811 [00:16<00:00, 45.94it/s] 96%|█████████▌| 778/811 [00:16<00:00, 46.07it/s] 97%|█████████▋| 783/811 [00:16<00:00, 46.14it/s] 97%|█████████▋| 788/811 [00:17<00:00, 46.15it/s] 98%|█████████▊| 793/811 [00:17<00:00, 46.12it/s] 98%|█████████▊| 798/811 [00:17<00:00, 46.05it/s] 99%|█████████▉| 803/811 [00:17<00:00, 46.07it/s]100%|█████████▉| 808/811 [00:17<00:00, 45.99it/s]100%|██████████| 811/811 [00:17<00:00, 46.29it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:10:35,078 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:35,078 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:35,078 >>   eval_loss               =     0.9055
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:35,078 >>   eval_runtime            = 0:00:17.54
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:35,078 >>   eval_samples            =       6487
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:35,078 >>   eval_samples_per_second =    369.791
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:35,078 >>   eval_steps_per_second   =     46.231
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:10:35,078 >>   perplexity              =     2.4733
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:40,825 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:40,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:40,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:40,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:40,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:10:41,125 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:10:41,126 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:10:41,388 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:10:42,473 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:10:42,473 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:43,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:43,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:43,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:43,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:10:43,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:10:44,243 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:10:44,244 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:10:44,505 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:10:44,676 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:10:44,676 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-157
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-471
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-628
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-314
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/checkpoint-785
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl', 'labels': ['member of political party', 'military branch', 'occupation', 'part of the series', 'place of death'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17184
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17284, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.52it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:09,  1.62it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:12,  1.51it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.42it/s]Extractor Predicting: 27it [00:17,  1.45it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:18,  1.49it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.56it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:30,  1.63it/s]Extractor Predicting: 48it [00:31,  1.63it/s]Extractor Predicting: 49it [00:31,  1.68it/s]Extractor Predicting: 50it [00:32,  1.65it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:33,  1.64it/s]Extractor Predicting: 53it [00:34,  1.63it/s]Extractor Predicting: 54it [00:34,  1.58it/s]Extractor Predicting: 55it [00:35,  1.59it/s]Extractor Predicting: 56it [00:36,  1.57it/s]Extractor Predicting: 57it [00:36,  1.60it/s]Extractor Predicting: 58it [00:37,  1.54it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:38,  1.58it/s]Extractor Predicting: 61it [00:39,  1.60it/s]Extractor Predicting: 62it [00:39,  1.59it/s]Extractor Predicting: 63it [00:40,  1.61it/s]Extractor Predicting: 64it [00:41,  1.62it/s]Extractor Predicting: 65it [00:41,  1.64it/s]Extractor Predicting: 66it [00:42,  1.67it/s]Extractor Predicting: 67it [00:42,  1.66it/s]Extractor Predicting: 68it [00:43,  1.63it/s]Extractor Predicting: 69it [00:44,  1.63it/s]Extractor Predicting: 70it [00:44,  1.63it/s]Extractor Predicting: 71it [00:45,  1.63it/s]Extractor Predicting: 72it [00:45,  1.63it/s]Extractor Predicting: 73it [00:46,  1.61it/s]Extractor Predicting: 74it [00:47,  1.61it/s]Extractor Predicting: 75it [00:47,  1.61it/s]Extractor Predicting: 76it [00:48,  1.62it/s]Extractor Predicting: 77it [00:49,  1.62it/s]Extractor Predicting: 78it [00:49,  1.57it/s]Extractor Predicting: 79it [00:50,  1.58it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.61it/s]Extractor Predicting: 82it [00:52,  1.63it/s]Extractor Predicting: 83it [00:52,  1.68it/s]Extractor Predicting: 84it [00:53,  1.62it/s]Extractor Predicting: 85it [00:54,  1.58it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:55,  1.51it/s]Extractor Predicting: 88it [00:56,  1.48it/s]Extractor Predicting: 89it [00:56,  1.49it/s]Extractor Predicting: 90it [00:57,  1.49it/s]Extractor Predicting: 91it [00:58,  1.47it/s]Extractor Predicting: 92it [00:58,  1.49it/s]Extractor Predicting: 93it [00:59,  1.43it/s]Extractor Predicting: 94it [01:00,  1.41it/s]Extractor Predicting: 95it [01:00,  1.47it/s]Extractor Predicting: 96it [01:01,  1.48it/s]Extractor Predicting: 97it [01:02,  1.51it/s]Extractor Predicting: 98it [01:02,  1.48it/s]Extractor Predicting: 99it [01:03,  1.45it/s]Extractor Predicting: 100it [01:04,  1.49it/s]Extractor Predicting: 101it [01:04,  1.51it/s]Extractor Predicting: 102it [01:05,  1.53it/s]Extractor Predicting: 103it [01:06,  1.55it/s]Extractor Predicting: 104it [01:06,  1.50it/s]Extractor Predicting: 105it [01:07,  1.54it/s]Extractor Predicting: 106it [01:08,  1.57it/s]Extractor Predicting: 107it [01:08,  1.54it/s]Extractor Predicting: 108it [01:09,  1.57it/s]Extractor Predicting: 109it [01:10,  1.56it/s]Extractor Predicting: 110it [01:10,  1.58it/s]Extractor Predicting: 111it [01:11,  1.55it/s]Extractor Predicting: 112it [01:11,  1.57it/s]Extractor Predicting: 113it [01:12,  1.53it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:14,  1.41it/s]Extractor Predicting: 116it [01:14,  1.43it/s]Extractor Predicting: 117it [01:15,  1.46it/s]Extractor Predicting: 118it [01:16,  1.47it/s]Extractor Predicting: 119it [01:16,  1.48it/s]Extractor Predicting: 120it [01:17,  1.52it/s]Extractor Predicting: 121it [01:18,  1.48it/s]Extractor Predicting: 122it [01:18,  1.51it/s]Extractor Predicting: 123it [01:19,  1.53it/s]Extractor Predicting: 124it [01:20,  1.55it/s]Extractor Predicting: 125it [01:20,  1.54it/s]Extractor Predicting: 126it [01:21,  1.51it/s]Extractor Predicting: 127it [01:22,  1.51it/s]Extractor Predicting: 128it [01:22,  1.52it/s]Extractor Predicting: 129it [01:23,  1.54it/s]Extractor Predicting: 130it [01:23,  1.55it/s]Extractor Predicting: 131it [01:24,  1.56it/s]Extractor Predicting: 132it [01:25,  1.53it/s]Extractor Predicting: 133it [01:25,  1.51it/s]Extractor Predicting: 134it [01:26,  1.49it/s]Extractor Predicting: 135it [01:27,  1.46it/s]Extractor Predicting: 136it [01:28,  1.44it/s]Extractor Predicting: 137it [01:28,  1.42it/s]Extractor Predicting: 138it [01:29,  1.42it/s]Extractor Predicting: 139it [01:30,  1.42it/s]Extractor Predicting: 140it [01:30,  1.43it/s]Extractor Predicting: 141it [01:31,  1.43it/s]Extractor Predicting: 142it [01:32,  1.44it/s]Extractor Predicting: 143it [01:32,  1.44it/s]Extractor Predicting: 144it [01:33,  1.42it/s]Extractor Predicting: 145it [01:34,  1.42it/s]Extractor Predicting: 146it [01:35,  1.43it/s]Extractor Predicting: 147it [01:35,  1.41it/s]Extractor Predicting: 148it [01:36,  1.42it/s]Extractor Predicting: 149it [01:37,  1.42it/s]Extractor Predicting: 150it [01:37,  1.44it/s]Extractor Predicting: 151it [01:38,  1.45it/s]Extractor Predicting: 152it [01:39,  1.45it/s]Extractor Predicting: 153it [01:39,  1.44it/s]Extractor Predicting: 154it [01:40,  1.43it/s]Extractor Predicting: 155it [01:41,  1.40it/s]Extractor Predicting: 156it [01:42,  1.42it/s]Extractor Predicting: 157it [01:42,  1.43it/s]Extractor Predicting: 158it [01:43,  1.41it/s]Extractor Predicting: 159it [01:44,  1.41it/s]Extractor Predicting: 160it [01:44,  1.41it/s]Extractor Predicting: 161it [01:45,  1.42it/s]Extractor Predicting: 162it [01:46,  1.41it/s]Extractor Predicting: 163it [01:46,  1.45it/s]Extractor Predicting: 164it [01:47,  1.46it/s]Extractor Predicting: 165it [01:48,  1.49it/s]Extractor Predicting: 166it [01:48,  1.48it/s]Extractor Predicting: 167it [01:49,  1.47it/s]Extractor Predicting: 168it [01:50,  1.42it/s]Extractor Predicting: 169it [01:51,  1.41it/s]Extractor Predicting: 170it [01:51,  1.42it/s]Extractor Predicting: 171it [01:52,  1.42it/s]Extractor Predicting: 172it [01:53,  1.42it/s]Extractor Predicting: 173it [01:53,  1.41it/s]Extractor Predicting: 174it [01:54,  1.45it/s]Extractor Predicting: 175it [01:55,  1.40it/s]Extractor Predicting: 176it [01:56,  1.37it/s]Extractor Predicting: 177it [01:56,  1.43it/s]Extractor Predicting: 178it [01:57,  1.43it/s]Extractor Predicting: 179it [01:58,  1.47it/s]Extractor Predicting: 180it [01:58,  1.46it/s]Extractor Predicting: 181it [01:59,  1.49it/s]Extractor Predicting: 182it [02:00,  1.49it/s]Extractor Predicting: 183it [02:00,  1.47it/s]Extractor Predicting: 184it [02:01,  1.47it/s]Extractor Predicting: 185it [02:02,  1.49it/s]Extractor Predicting: 186it [02:02,  1.49it/s]Extractor Predicting: 187it [02:03,  1.55it/s]Extractor Predicting: 188it [02:04,  1.50it/s]Extractor Predicting: 189it [02:04,  1.51it/s]Extractor Predicting: 190it [02:05,  1.50it/s]Extractor Predicting: 191it [02:06,  1.52it/s]Extractor Predicting: 192it [02:06,  1.52it/s]Extractor Predicting: 193it [02:07,  1.50it/s]Extractor Predicting: 194it [02:08,  1.53it/s]Extractor Predicting: 195it [02:08,  1.53it/s]Extractor Predicting: 196it [02:09,  1.53it/s]Extractor Predicting: 197it [02:10,  1.53it/s]Extractor Predicting: 198it [02:10,  1.53it/s]Extractor Predicting: 199it [02:11,  1.51it/s]Extractor Predicting: 200it [02:11,  1.54it/s]Extractor Predicting: 201it [02:12,  1.55it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:13,  1.52it/s]Extractor Predicting: 204it [02:14,  1.52it/s]Extractor Predicting: 205it [02:15,  1.53it/s]Extractor Predicting: 206it [02:15,  1.53it/s]Extractor Predicting: 207it [02:16,  1.51it/s]Extractor Predicting: 208it [02:17,  1.36it/s]Extractor Predicting: 209it [02:18,  1.42it/s]Extractor Predicting: 210it [02:18,  1.45it/s]Extractor Predicting: 211it [02:19,  1.44it/s]Extractor Predicting: 212it [02:20,  1.47it/s]Extractor Predicting: 213it [02:20,  1.49it/s]Extractor Predicting: 214it [02:21,  1.52it/s]Extractor Predicting: 215it [02:22,  1.52it/s]Extractor Predicting: 216it [02:22,  1.53it/s]Extractor Predicting: 217it [02:23,  1.53it/s]Extractor Predicting: 218it [02:24,  1.50it/s]Extractor Predicting: 219it [02:24,  1.55it/s]Extractor Predicting: 220it [02:25,  1.52it/s]Extractor Predicting: 221it [02:26,  1.48it/s]Extractor Predicting: 222it [02:26,  1.51it/s]Extractor Predicting: 223it [02:27,  1.51it/s]Extractor Predicting: 224it [02:28,  1.48it/s]Extractor Predicting: 225it [02:28,  1.47it/s]Extractor Predicting: 226it [02:29,  1.47it/s]Extractor Predicting: 227it [02:30,  1.49it/s]Extractor Predicting: 228it [02:30,  1.49it/s]Extractor Predicting: 229it [02:31,  1.46it/s]Extractor Predicting: 230it [02:32,  1.49it/s]Extractor Predicting: 231it [02:32,  1.45it/s]Extractor Predicting: 232it [02:33,  1.47it/s]Extractor Predicting: 233it [02:34,  1.41it/s]Extractor Predicting: 234it [02:35,  1.40it/s]Extractor Predicting: 235it [02:35,  1.39it/s]Extractor Predicting: 236it [02:36,  1.41it/s]Extractor Predicting: 237it [02:37,  1.42it/s]Extractor Predicting: 238it [02:37,  1.40it/s]Extractor Predicting: 239it [02:38,  1.45it/s]Extractor Predicting: 240it [02:39,  1.44it/s]Extractor Predicting: 241it [02:39,  1.42it/s]Extractor Predicting: 242it [02:40,  1.42it/s]Extractor Predicting: 243it [02:41,  1.40it/s]Extractor Predicting: 244it [02:42,  1.40it/s]Extractor Predicting: 245it [02:42,  1.45it/s]Extractor Predicting: 246it [02:43,  1.48it/s]Extractor Predicting: 247it [02:43,  1.67it/s]Extractor Predicting: 247it [02:43,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:36,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:36,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:36,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:36,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:36,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:13:37,172 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:13:37,173 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:13:37,443 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:13:38,475 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:13:38,476 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:39,792 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:39,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:39,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:39,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:13:39,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:13:40,114 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:13:40,122 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:13:40,785 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:13:40,936 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:13:40,937 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5220539828834759,
  "recall": 0.12224448897795591,
  "score": 0.19810142393205096,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'manufacturer', 'mother', 'occupant', 'organization directed by the office or position', 'owned by', 'part of', 'screenwriter', 'sport', 'sports discipline competed in', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26970
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27070, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.42it/s]Extractor Predicting: 10it [00:06,  1.42it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:12,  1.53it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:14,  1.52it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:16,  1.53it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:18,  1.56it/s]Extractor Predicting: 28it [00:18,  1.56it/s]Extractor Predicting: 29it [00:19,  1.55it/s]Extractor Predicting: 30it [00:20,  1.53it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:22,  1.51it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.50it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:29,  1.48it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:34,  1.47it/s]Extractor Predicting: 53it [00:35,  1.46it/s]Extractor Predicting: 54it [00:36,  1.46it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:37,  1.47it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:38,  1.47it/s]Extractor Predicting: 59it [00:39,  1.49it/s]Extractor Predicting: 60it [00:40,  1.52it/s]Extractor Predicting: 61it [00:40,  1.51it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 64it [00:42,  1.48it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.51it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:45,  1.50it/s]Extractor Predicting: 69it [00:46,  1.47it/s]Extractor Predicting: 70it [00:46,  1.48it/s]Extractor Predicting: 71it [00:47,  1.49it/s]Extractor Predicting: 72it [00:48,  1.49it/s]Extractor Predicting: 73it [00:48,  1.51it/s]Extractor Predicting: 74it [00:49,  1.55it/s]Extractor Predicting: 75it [00:49,  1.55it/s]Extractor Predicting: 76it [00:50,  1.54it/s]Extractor Predicting: 77it [00:51,  1.52it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:52,  1.56it/s]Extractor Predicting: 80it [00:53,  1.37it/s]Extractor Predicting: 81it [00:54,  1.42it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.51it/s]Extractor Predicting: 84it [00:56,  1.51it/s]Extractor Predicting: 85it [00:56,  1.56it/s]Extractor Predicting: 86it [00:57,  1.59it/s]Extractor Predicting: 87it [00:57,  1.56it/s]Extractor Predicting: 88it [00:58,  1.58it/s]Extractor Predicting: 89it [00:59,  1.56it/s]Extractor Predicting: 90it [00:59,  1.57it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:01,  1.57it/s]Extractor Predicting: 93it [01:01,  1.55it/s]Extractor Predicting: 94it [01:02,  1.57it/s]Extractor Predicting: 95it [01:03,  1.58it/s]Extractor Predicting: 96it [01:03,  1.57it/s]Extractor Predicting: 97it [01:04,  1.56it/s]Extractor Predicting: 98it [01:05,  1.53it/s]Extractor Predicting: 99it [01:05,  1.54it/s]Extractor Predicting: 100it [01:06,  1.59it/s]Extractor Predicting: 101it [01:06,  1.58it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:08,  1.53it/s]Extractor Predicting: 104it [01:08,  1.56it/s]Extractor Predicting: 105it [01:09,  1.56it/s]Extractor Predicting: 106it [01:10,  1.53it/s]Extractor Predicting: 107it [01:10,  1.55it/s]Extractor Predicting: 108it [01:11,  1.56it/s]Extractor Predicting: 109it [01:12,  1.58it/s]Extractor Predicting: 110it [01:12,  1.58it/s]Extractor Predicting: 111it [01:13,  1.59it/s]Extractor Predicting: 112it [01:13,  1.55it/s]Extractor Predicting: 113it [01:14,  1.56it/s]Extractor Predicting: 114it [01:15,  1.50it/s]Extractor Predicting: 115it [01:16,  1.49it/s]Extractor Predicting: 116it [01:16,  1.50it/s]Extractor Predicting: 117it [01:17,  1.47it/s]Extractor Predicting: 118it [01:18,  1.50it/s]Extractor Predicting: 119it [01:18,  1.47it/s]Extractor Predicting: 120it [01:19,  1.49it/s]Extractor Predicting: 121it [01:20,  1.49it/s]Extractor Predicting: 122it [01:20,  1.48it/s]Extractor Predicting: 123it [01:21,  1.46it/s]Extractor Predicting: 124it [01:22,  1.44it/s]Extractor Predicting: 125it [01:22,  1.43it/s]Extractor Predicting: 126it [01:23,  1.44it/s]Extractor Predicting: 127it [01:24,  1.45it/s]Extractor Predicting: 128it [01:24,  1.43it/s]Extractor Predicting: 129it [01:25,  1.43it/s]Extractor Predicting: 130it [01:26,  1.46it/s]Extractor Predicting: 131it [01:26,  1.49it/s]Extractor Predicting: 132it [01:27,  1.50it/s]Extractor Predicting: 133it [01:28,  1.52it/s]Extractor Predicting: 134it [01:28,  1.55it/s]Extractor Predicting: 135it [01:29,  1.55it/s]Extractor Predicting: 136it [01:30,  1.54it/s]Extractor Predicting: 137it [01:30,  1.58it/s]Extractor Predicting: 138it [01:31,  1.57it/s]Extractor Predicting: 139it [01:31,  1.59it/s]Extractor Predicting: 140it [01:32,  1.58it/s]Extractor Predicting: 141it [01:33,  1.54it/s]Extractor Predicting: 142it [01:33,  1.53it/s]Extractor Predicting: 143it [01:34,  1.56it/s]Extractor Predicting: 144it [01:35,  1.55it/s]Extractor Predicting: 145it [01:35,  1.57it/s]Extractor Predicting: 146it [01:36,  1.54it/s]Extractor Predicting: 147it [01:37,  1.53it/s]Extractor Predicting: 148it [01:37,  1.52it/s]Extractor Predicting: 149it [01:38,  1.52it/s]Extractor Predicting: 150it [01:39,  1.51it/s]Extractor Predicting: 151it [01:39,  1.45it/s]Extractor Predicting: 152it [01:40,  1.45it/s]Extractor Predicting: 153it [01:41,  1.45it/s]Extractor Predicting: 154it [01:41,  1.47it/s]Extractor Predicting: 155it [01:42,  1.42it/s]Extractor Predicting: 156it [01:43,  1.40it/s]Extractor Predicting: 157it [01:44,  1.39it/s]Extractor Predicting: 158it [01:44,  1.41it/s]Extractor Predicting: 159it [01:45,  1.41it/s]Extractor Predicting: 160it [01:46,  1.42it/s]Extractor Predicting: 161it [01:47,  1.42it/s]Extractor Predicting: 162it [01:47,  1.42it/s]Extractor Predicting: 163it [01:48,  1.48it/s]Extractor Predicting: 164it [01:48,  1.58it/s]Extractor Predicting: 165it [01:49,  1.56it/s]Extractor Predicting: 166it [01:50,  1.53it/s]Extractor Predicting: 167it [01:50,  1.51it/s]Extractor Predicting: 168it [01:51,  1.49it/s]Extractor Predicting: 169it [01:52,  1.47it/s]Extractor Predicting: 170it [01:52,  1.51it/s]Extractor Predicting: 171it [01:53,  1.50it/s]Extractor Predicting: 172it [01:54,  1.48it/s]Extractor Predicting: 173it [01:54,  1.50it/s]Extractor Predicting: 174it [01:55,  1.49it/s]Extractor Predicting: 175it [01:56,  1.56it/s]Extractor Predicting: 176it [01:56,  1.60it/s]Extractor Predicting: 177it [01:57,  1.60it/s]Extractor Predicting: 178it [01:58,  1.59it/s]Extractor Predicting: 179it [01:58,  1.57it/s]Extractor Predicting: 180it [01:59,  1.52it/s]Extractor Predicting: 181it [02:00,  1.52it/s]Extractor Predicting: 182it [02:00,  1.55it/s]Extractor Predicting: 183it [02:01,  1.54it/s]Extractor Predicting: 184it [02:01,  1.56it/s]Extractor Predicting: 185it [02:02,  1.59it/s]Extractor Predicting: 186it [02:03,  1.63it/s]Extractor Predicting: 187it [02:03,  1.58it/s]Extractor Predicting: 188it [02:04,  1.57it/s]Extractor Predicting: 189it [02:05,  1.57it/s]Extractor Predicting: 190it [02:05,  1.58it/s]Extractor Predicting: 191it [02:06,  1.56it/s]Extractor Predicting: 192it [02:07,  1.55it/s]Extractor Predicting: 193it [02:07,  1.52it/s]Extractor Predicting: 194it [02:08,  1.51it/s]Extractor Predicting: 195it [02:09,  1.54it/s]Extractor Predicting: 196it [02:09,  1.55it/s]Extractor Predicting: 197it [02:10,  1.39it/s]Extractor Predicting: 198it [02:11,  1.45it/s]Extractor Predicting: 199it [02:11,  1.48it/s]Extractor Predicting: 200it [02:12,  1.49it/s]Extractor Predicting: 201it [02:13,  1.52it/s]Extractor Predicting: 202it [02:13,  1.51it/s]Extractor Predicting: 203it [02:14,  1.51it/s]Extractor Predicting: 204it [02:15,  1.54it/s]Extractor Predicting: 205it [02:15,  1.55it/s]Extractor Predicting: 206it [02:16,  1.55it/s]Extractor Predicting: 207it [02:16,  1.62it/s]Extractor Predicting: 208it [02:17,  1.57it/s]Extractor Predicting: 209it [02:18,  1.58it/s]Extractor Predicting: 210it [02:18,  1.53it/s]Extractor Predicting: 211it [02:19,  1.55it/s]Extractor Predicting: 212it [02:20,  1.51it/s]Extractor Predicting: 213it [02:20,  1.47it/s]Extractor Predicting: 214it [02:21,  1.51it/s]Extractor Predicting: 215it [02:22,  1.53it/s]Extractor Predicting: 216it [02:22,  1.52it/s]Extractor Predicting: 217it [02:23,  1.50it/s]Extractor Predicting: 218it [02:24,  1.52it/s]Extractor Predicting: 219it [02:24,  1.53it/s]Extractor Predicting: 220it [02:25,  1.55it/s]Extractor Predicting: 221it [02:26,  1.58it/s]Extractor Predicting: 222it [02:26,  1.53it/s]Extractor Predicting: 223it [02:27,  1.51it/s]Extractor Predicting: 224it [02:28,  1.52it/s]Extractor Predicting: 225it [02:28,  1.53it/s]Extractor Predicting: 226it [02:29,  1.54it/s]Extractor Predicting: 227it [02:29,  1.56it/s]Extractor Predicting: 228it [02:30,  1.54it/s]Extractor Predicting: 229it [02:31,  1.54it/s]Extractor Predicting: 230it [02:32,  1.51it/s]Extractor Predicting: 231it [02:32,  1.55it/s]Extractor Predicting: 232it [02:33,  1.55it/s]Extractor Predicting: 233it [02:33,  1.57it/s]Extractor Predicting: 234it [02:34,  1.56it/s]Extractor Predicting: 235it [02:35,  1.54it/s]Extractor Predicting: 236it [02:35,  1.54it/s]Extractor Predicting: 237it [02:36,  1.51it/s]Extractor Predicting: 238it [02:37,  1.54it/s]Extractor Predicting: 239it [02:37,  1.53it/s]Extractor Predicting: 240it [02:38,  1.55it/s]Extractor Predicting: 241it [02:39,  1.56it/s]Extractor Predicting: 242it [02:39,  1.55it/s]Extractor Predicting: 243it [02:40,  1.58it/s]Extractor Predicting: 244it [02:41,  1.55it/s]Extractor Predicting: 245it [02:41,  1.57it/s]Extractor Predicting: 246it [02:42,  1.56it/s]Extractor Predicting: 247it [02:42,  1.58it/s]Extractor Predicting: 248it [02:43,  1.57it/s]Extractor Predicting: 249it [02:44,  1.59it/s]Extractor Predicting: 250it [02:44,  1.60it/s]Extractor Predicting: 251it [02:45,  1.55it/s]Extractor Predicting: 252it [02:46,  1.57it/s]Extractor Predicting: 253it [02:46,  1.60it/s]Extractor Predicting: 254it [02:47,  1.60it/s]Extractor Predicting: 255it [02:47,  1.58it/s]Extractor Predicting: 256it [02:48,  1.54it/s]Extractor Predicting: 257it [02:49,  1.49it/s]Extractor Predicting: 258it [02:50,  1.44it/s]Extractor Predicting: 259it [02:50,  1.43it/s]Extractor Predicting: 260it [02:51,  1.46it/s]Extractor Predicting: 261it [02:52,  1.47it/s]Extractor Predicting: 262it [02:52,  1.48it/s]Extractor Predicting: 263it [02:53,  1.50it/s]Extractor Predicting: 264it [02:54,  1.50it/s]Extractor Predicting: 265it [02:54,  1.53it/s]Extractor Predicting: 266it [02:55,  1.53it/s]Extractor Predicting: 267it [02:56,  1.54it/s]Extractor Predicting: 268it [02:56,  1.54it/s]Extractor Predicting: 269it [02:57,  1.54it/s]Extractor Predicting: 270it [02:57,  1.55it/s]Extractor Predicting: 271it [02:58,  1.55it/s]Extractor Predicting: 272it [02:59,  1.53it/s]Extractor Predicting: 273it [02:59,  1.56it/s]Extractor Predicting: 274it [03:00,  1.53it/s]Extractor Predicting: 275it [03:01,  1.55it/s]Extractor Predicting: 276it [03:01,  1.55it/s]Extractor Predicting: 277it [03:02,  1.55it/s]Extractor Predicting: 278it [03:03,  1.52it/s]Extractor Predicting: 279it [03:03,  1.51it/s]Extractor Predicting: 280it [03:04,  1.52it/s]Extractor Predicting: 281it [03:05,  1.54it/s]Extractor Predicting: 282it [03:05,  1.53it/s]Extractor Predicting: 283it [03:06,  1.55it/s]Extractor Predicting: 284it [03:07,  1.52it/s]Extractor Predicting: 285it [03:07,  1.50it/s]Extractor Predicting: 286it [03:08,  1.50it/s]Extractor Predicting: 287it [03:09,  1.53it/s]Extractor Predicting: 288it [03:09,  1.52it/s]Extractor Predicting: 289it [03:10,  1.54it/s]Extractor Predicting: 290it [03:11,  1.52it/s]Extractor Predicting: 291it [03:11,  1.54it/s]Extractor Predicting: 292it [03:12,  1.56it/s]Extractor Predicting: 293it [03:12,  1.56it/s]Extractor Predicting: 294it [03:13,  1.56it/s]Extractor Predicting: 295it [03:14,  1.56it/s]Extractor Predicting: 296it [03:14,  1.52it/s]Extractor Predicting: 297it [03:15,  1.52it/s]Extractor Predicting: 298it [03:16,  1.51it/s]Extractor Predicting: 299it [03:16,  1.50it/s]Extractor Predicting: 300it [03:17,  1.48it/s]Extractor Predicting: 301it [03:18,  1.48it/s]Extractor Predicting: 302it [03:18,  1.49it/s]Extractor Predicting: 303it [03:19,  1.53it/s]Extractor Predicting: 304it [03:20,  1.57it/s]Extractor Predicting: 305it [03:20,  1.58it/s]Extractor Predicting: 306it [03:21,  1.61it/s]Extractor Predicting: 307it [03:22,  1.61it/s]Extractor Predicting: 308it [03:22,  1.63it/s]Extractor Predicting: 309it [03:23,  1.64it/s]Extractor Predicting: 310it [03:23,  1.64it/s]Extractor Predicting: 311it [03:24,  1.64it/s]Extractor Predicting: 312it [03:25,  1.59it/s]Extractor Predicting: 313it [03:25,  1.61it/s]Extractor Predicting: 314it [03:26,  1.55it/s]Extractor Predicting: 315it [03:27,  1.52it/s]Extractor Predicting: 316it [03:27,  1.55it/s]Extractor Predicting: 317it [03:28,  1.58it/s]Extractor Predicting: 318it [03:28,  1.55it/s]Extractor Predicting: 319it [03:29,  1.59it/s]Extractor Predicting: 320it [03:30,  1.58it/s]Extractor Predicting: 321it [03:30,  1.57it/s]Extractor Predicting: 322it [03:31,  1.60it/s]Extractor Predicting: 323it [03:32,  1.40it/s]Extractor Predicting: 324it [03:33,  1.45it/s]Extractor Predicting: 325it [03:33,  1.45it/s]Extractor Predicting: 326it [03:34,  1.49it/s]Extractor Predicting: 327it [03:34,  1.55it/s]Extractor Predicting: 328it [03:35,  1.57it/s]Extractor Predicting: 329it [03:36,  1.61it/s]Extractor Predicting: 330it [03:36,  1.59it/s]Extractor Predicting: 331it [03:37,  1.62it/s]Extractor Predicting: 332it [03:38,  1.59it/s]Extractor Predicting: 333it [03:38,  1.59it/s]Extractor Predicting: 334it [03:39,  1.62it/s]Extractor Predicting: 335it [03:39,  1.58it/s]Extractor Predicting: 336it [03:40,  1.54it/s]Extractor Predicting: 337it [03:41,  1.50it/s]Extractor Predicting: 338it [03:41,  1.53it/s]Extractor Predicting: 339it [03:42,  1.56it/s]Extractor Predicting: 340it [03:43,  1.57it/s]Extractor Predicting: 341it [03:43,  1.56it/s]Extractor Predicting: 342it [03:44,  1.51it/s]Extractor Predicting: 343it [03:45,  1.48it/s]Extractor Predicting: 344it [03:45,  1.49it/s]Extractor Predicting: 345it [03:46,  1.49it/s]Extractor Predicting: 346it [03:47,  1.53it/s]Extractor Predicting: 347it [03:47,  1.54it/s]Extractor Predicting: 348it [03:48,  1.56it/s]Extractor Predicting: 349it [03:49,  1.57it/s]Extractor Predicting: 350it [03:49,  1.54it/s]Extractor Predicting: 351it [03:50,  1.58it/s]Extractor Predicting: 352it [03:51,  1.55it/s]Extractor Predicting: 353it [03:51,  1.59it/s]Extractor Predicting: 354it [03:52,  1.58it/s]Extractor Predicting: 355it [03:52,  1.55it/s]Extractor Predicting: 356it [03:53,  1.56it/s]Extractor Predicting: 357it [03:54,  1.55it/s]Extractor Predicting: 358it [03:54,  1.54it/s]Extractor Predicting: 359it [03:55,  1.54it/s]Extractor Predicting: 360it [03:56,  1.54it/s]Extractor Predicting: 361it [03:56,  1.51it/s]Extractor Predicting: 362it [03:57,  1.53it/s]Extractor Predicting: 363it [03:58,  1.52it/s]Extractor Predicting: 364it [03:58,  1.53it/s]Extractor Predicting: 365it [03:59,  1.50it/s]Extractor Predicting: 366it [04:00,  1.48it/s]Extractor Predicting: 367it [04:00,  1.50it/s]Extractor Predicting: 368it [04:01,  1.56it/s]Extractor Predicting: 369it [04:02,  1.57it/s]Extractor Predicting: 370it [04:02,  1.58it/s]Extractor Predicting: 371it [04:03,  1.58it/s]Extractor Predicting: 372it [04:03,  1.58it/s]Extractor Predicting: 373it [04:04,  1.56it/s]Extractor Predicting: 374it [04:05,  1.58it/s]Extractor Predicting: 375it [04:05,  1.62it/s]Extractor Predicting: 376it [04:06,  1.61it/s]Extractor Predicting: 377it [04:07,  1.53it/s]Extractor Predicting: 378it [04:07,  1.46it/s]Extractor Predicting: 379it [04:08,  1.51it/s]Extractor Predicting: 380it [04:09,  1.50it/s]Extractor Predicting: 381it [04:09,  1.53it/s]Extractor Predicting: 382it [04:10,  1.53it/s]Extractor Predicting: 383it [04:11,  1.52it/s]Extractor Predicting: 384it [04:11,  1.54it/s]Extractor Predicting: 385it [04:12,  1.57it/s]Extractor Predicting: 386it [04:13,  1.55it/s]Extractor Predicting: 387it [04:13,  1.57it/s]Extractor Predicting: 388it [04:14,  1.60it/s]Extractor Predicting: 389it [04:14,  1.58it/s]Extractor Predicting: 390it [04:15,  1.55it/s]Extractor Predicting: 391it [04:16,  1.55it/s]Extractor Predicting: 392it [04:16,  1.57it/s]Extractor Predicting: 393it [04:17,  1.57it/s]Extractor Predicting: 394it [04:18,  1.58it/s]Extractor Predicting: 395it [04:18,  1.57it/s]Extractor Predicting: 396it [04:19,  1.56it/s]Extractor Predicting: 397it [04:19,  1.59it/s]Extractor Predicting: 398it [04:20,  1.62it/s]Extractor Predicting: 399it [04:21,  1.59it/s]Extractor Predicting: 400it [04:21,  1.62it/s]Extractor Predicting: 401it [04:22,  1.61it/s]Extractor Predicting: 402it [04:23,  1.61it/s]Extractor Predicting: 403it [04:23,  1.60it/s]Extractor Predicting: 404it [04:24,  1.56it/s]Extractor Predicting: 405it [04:25,  1.56it/s]Extractor Predicting: 406it [04:25,  1.54it/s]Extractor Predicting: 407it [04:26,  1.52it/s]Extractor Predicting: 408it [04:27,  1.52it/s]Extractor Predicting: 409it [04:27,  1.50it/s]Extractor Predicting: 410it [04:28,  1.46it/s]Extractor Predicting: 411it [04:29,  1.50it/s]Extractor Predicting: 412it [04:29,  1.52it/s]Extractor Predicting: 413it [04:30,  1.47it/s]Extractor Predicting: 414it [04:31,  1.47it/s]Extractor Predicting: 415it [04:31,  1.51it/s]Extractor Predicting: 416it [04:32,  1.49it/s]Extractor Predicting: 417it [04:33,  1.49it/s]Extractor Predicting: 418it [04:33,  1.45it/s]Extractor Predicting: 419it [04:34,  1.47it/s]Extractor Predicting: 420it [04:35,  1.41it/s]Extractor Predicting: 421it [04:35,  1.42it/s]Extractor Predicting: 422it [04:36,  1.44it/s]Extractor Predicting: 423it [04:37,  1.48it/s]Extractor Predicting: 424it [04:37,  1.50it/s]Extractor Predicting: 425it [04:38,  1.52it/s]Extractor Predicting: 426it [04:39,  1.49it/s]Extractor Predicting: 427it [04:39,  1.47it/s]Extractor Predicting: 428it [04:40,  1.47it/s]Extractor Predicting: 429it [04:41,  1.50it/s]Extractor Predicting: 430it [04:42,  1.46it/s]Extractor Predicting: 431it [04:42,  1.46it/s]Extractor Predicting: 432it [04:43,  1.46it/s]Extractor Predicting: 433it [04:44,  1.49it/s]Extractor Predicting: 434it [04:44,  1.51it/s]Extractor Predicting: 435it [04:45,  1.58it/s]Extractor Predicting: 436it [04:45,  1.66it/s]Extractor Predicting: 437it [04:46,  1.73it/s]Extractor Predicting: 438it [04:46,  1.67it/s]Extractor Predicting: 439it [04:47,  1.69it/s]Extractor Predicting: 440it [04:47,  1.78it/s]Extractor Predicting: 441it [04:48,  1.66it/s]Extractor Predicting: 442it [04:49,  1.55it/s]Extractor Predicting: 443it [04:50,  1.49it/s]Extractor Predicting: 444it [04:51,  1.29it/s]Extractor Predicting: 445it [04:51,  1.32it/s]Extractor Predicting: 446it [04:52,  1.34it/s]Extractor Predicting: 447it [04:53,  1.34it/s]Extractor Predicting: 448it [04:54,  1.35it/s]Extractor Predicting: 449it [04:54,  1.36it/s]Extractor Predicting: 450it [04:55,  1.39it/s]Extractor Predicting: 451it [04:56,  1.39it/s]Extractor Predicting: 452it [04:56,  1.39it/s]Extractor Predicting: 453it [04:57,  1.41it/s]Extractor Predicting: 454it [04:58,  1.42it/s]Extractor Predicting: 455it [04:59,  1.41it/s]Extractor Predicting: 456it [04:59,  1.40it/s]Extractor Predicting: 457it [05:00,  1.38it/s]Extractor Predicting: 458it [05:01,  1.39it/s]Extractor Predicting: 459it [05:01,  1.42it/s]Extractor Predicting: 460it [05:02,  1.43it/s]Extractor Predicting: 461it [05:03,  1.44it/s]Extractor Predicting: 462it [05:03,  1.46it/s]Extractor Predicting: 463it [05:04,  1.45it/s]Extractor Predicting: 464it [05:05,  1.44it/s]Extractor Predicting: 465it [05:05,  1.47it/s]Extractor Predicting: 466it [05:06,  1.68it/s]Extractor Predicting: 466it [05:06,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:57,369 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:57,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:57,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:57,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:18:57,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:18:58,012 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:18:58,014 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:18:58,600 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:18:59,687 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:18:59,687 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:02,540 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:02,544 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:02,544 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:02,544 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:19:02,544 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:19:03,183 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:19:03,184 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:19:03,764 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:19:03,928 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:19:03,928 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3435072142064373,
  "recall": 0.05542125526009491,
  "score": 0.09544368205997997,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'manufacturer', 'mother', 'occupant', 'organization directed by the office or position', 'owned by', 'part of', 'screenwriter', 'sport', 'sports discipline competed in', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8266
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8366, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.47it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.40it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:09,  1.40it/s]Extractor Predicting: 15it [00:10,  1.42it/s]Extractor Predicting: 16it [00:11,  1.41it/s]Extractor Predicting: 17it [00:12,  1.35it/s]Extractor Predicting: 18it [00:12,  1.42it/s]Extractor Predicting: 19it [00:13,  1.44it/s]Extractor Predicting: 20it [00:14,  1.45it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:16,  1.46it/s]Extractor Predicting: 24it [00:16,  1.46it/s]Extractor Predicting: 25it [00:17,  1.45it/s]Extractor Predicting: 26it [00:18,  1.46it/s]Extractor Predicting: 27it [00:19,  1.31it/s]Extractor Predicting: 28it [00:19,  1.33it/s]Extractor Predicting: 29it [00:20,  1.36it/s]Extractor Predicting: 30it [00:21,  1.41it/s]Extractor Predicting: 31it [00:21,  1.41it/s]Extractor Predicting: 32it [00:22,  1.42it/s]Extractor Predicting: 33it [00:23,  1.43it/s]Extractor Predicting: 34it [00:23,  1.44it/s]Extractor Predicting: 35it [00:24,  1.45it/s]Extractor Predicting: 36it [00:25,  1.46it/s]Extractor Predicting: 37it [00:25,  1.46it/s]Extractor Predicting: 38it [00:26,  1.46it/s]Extractor Predicting: 39it [00:27,  1.43it/s]Extractor Predicting: 40it [00:28,  1.40it/s]Extractor Predicting: 41it [00:28,  1.39it/s]Extractor Predicting: 42it [00:29,  1.38it/s]Extractor Predicting: 43it [00:30,  1.37it/s]Extractor Predicting: 44it [00:31,  1.36it/s]Extractor Predicting: 45it [00:31,  1.37it/s]Extractor Predicting: 46it [00:32,  1.37it/s]Extractor Predicting: 47it [00:33,  1.35it/s]Extractor Predicting: 48it [00:34,  1.34it/s]Extractor Predicting: 49it [00:34,  1.35it/s]Extractor Predicting: 50it [00:35,  1.36it/s]Extractor Predicting: 51it [00:36,  1.36it/s]Extractor Predicting: 52it [00:36,  1.35it/s]Extractor Predicting: 53it [00:37,  1.40it/s]Extractor Predicting: 54it [00:38,  1.42it/s]Extractor Predicting: 55it [00:38,  1.45it/s]Extractor Predicting: 56it [00:39,  1.43it/s]Extractor Predicting: 57it [00:40,  1.43it/s]Extractor Predicting: 58it [00:41,  1.41it/s]Extractor Predicting: 59it [00:41,  1.40it/s]Extractor Predicting: 60it [00:42,  1.39it/s]Extractor Predicting: 61it [00:43,  1.40it/s]Extractor Predicting: 62it [00:43,  1.43it/s]Extractor Predicting: 63it [00:44,  1.42it/s]Extractor Predicting: 64it [00:45,  1.43it/s]Extractor Predicting: 65it [00:46,  1.42it/s]Extractor Predicting: 66it [00:46,  1.44it/s]Extractor Predicting: 67it [00:47,  1.46it/s]Extractor Predicting: 68it [00:48,  1.47it/s]Extractor Predicting: 69it [00:48,  1.49it/s]Extractor Predicting: 70it [00:49,  1.50it/s]Extractor Predicting: 71it [00:50,  1.47it/s]Extractor Predicting: 72it [00:50,  1.47it/s]Extractor Predicting: 73it [00:51,  1.46it/s]Extractor Predicting: 74it [00:52,  1.43it/s]Extractor Predicting: 75it [00:52,  1.45it/s]Extractor Predicting: 76it [00:53,  1.44it/s]Extractor Predicting: 77it [00:54,  1.41it/s]Extractor Predicting: 78it [00:55,  1.42it/s]Extractor Predicting: 79it [00:55,  1.43it/s]Extractor Predicting: 80it [00:56,  1.43it/s]Extractor Predicting: 81it [00:57,  1.42it/s]Extractor Predicting: 82it [00:57,  1.43it/s]Extractor Predicting: 83it [00:58,  1.44it/s]Extractor Predicting: 84it [00:59,  1.44it/s]Extractor Predicting: 85it [00:59,  1.45it/s]Extractor Predicting: 86it [01:00,  1.44it/s]Extractor Predicting: 87it [01:01,  1.45it/s]Extractor Predicting: 88it [01:02,  1.42it/s]Extractor Predicting: 89it [01:02,  1.45it/s]Extractor Predicting: 90it [01:03,  1.48it/s]Extractor Predicting: 91it [01:03,  1.49it/s]Extractor Predicting: 92it [01:04,  1.62it/s]Extractor Predicting: 92it [01:04,  1.43it/s]
[INFO|configuration_utils.py:515] 2023-08-29 09:20:09,821 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:20:09,822 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:20:09,824 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:20:09,825 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 09:20:09,828 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:20:12,900 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 09:20:12,900 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 09:20:12,914 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:20:12,915 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:20:12,922 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:20:12,925 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:20:12,925 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:20:12,925 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:20:12,925 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:20:12,925 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:20:12,925 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.19718309859154928,
  "recall": 0.016483516483516484,
  "score": 0.03042375950742485,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 09:20:13,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:13,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:14,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:15,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:15,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:16,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:16,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:17,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:18,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:18,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:19,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:20,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:20,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:21,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:21,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:22,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:23,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:23,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:24,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:25,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:25,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:13<04:07, 13.03s/it][WARNING|generation_utils.py:914] 2023-08-29 09:20:26,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:26,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:27,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:28,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:29,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:29,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:30,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:31,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:31,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:32,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:33,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:34,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:34,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:35,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:36,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:37,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:37,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:38,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:39,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:39,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:40,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:41,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:28<04:23, 14.64s/it][WARNING|generation_utils.py:914] 2023-08-29 09:20:41,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:42,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:43,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:44,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:44,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:45,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:45,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:46,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:47,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:48,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:49,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:49,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:50,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:51,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:51,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:52,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:53,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:54,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:54,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:55,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:56,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:43<04:12, 14.86s/it][WARNING|generation_utils.py:914] 2023-08-29 09:20:57,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:57,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:58,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:59,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:20:59,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:00,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:01,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:02,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:02,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:03,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:04,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:04,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:05,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:06,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:07,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:07,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:08,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:09,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:09,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:10,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:11,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:12,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:59<04:03, 15.22s/it][WARNING|generation_utils.py:914] 2023-08-29 09:21:12,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:13,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:14,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:15,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:15,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:16,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:17,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:18,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:18,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:19,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:20,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:21,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:22,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:22,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:23,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:24,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:25,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:26,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:26,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:27,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:28,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:29,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:17<03:59, 16.00s/it][WARNING|generation_utils.py:914] 2023-08-29 09:21:30,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:30,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:31,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:32,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:33,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:33,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:34,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:35,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:36,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:37,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:37,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:38,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:39,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:39,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:40,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:41,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:41,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:42,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:43,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:44,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:32<03:39, 15.65s/it][WARNING|generation_utils.py:914] 2023-08-29 09:21:45,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:45,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:46,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:47,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:48,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:48,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:49,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:50,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:51,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:52,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:52,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:53,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:54,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:54,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:55,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:56,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:56,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:57,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:58,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:21:59,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:46<03:19, 15.34s/it][WARNING|generation_utils.py:914] 2023-08-29 09:21:59,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:00,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:01,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:02,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:02,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:03,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:04,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:05,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:05,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:06,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:07,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:08,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:08,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:09,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:10,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:11,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:11,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:12,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:13,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:14,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:01<03:02, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-29 09:22:14,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:15,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:16,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:17,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:17,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:18,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:19,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:20,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:20,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:21,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:22,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:23,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:23,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:24,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:25,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:25,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:26,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:27,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:28,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:28,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:29,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:30,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:17<02:51, 15.55s/it][WARNING|generation_utils.py:914] 2023-08-29 09:22:31,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:31,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:32,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:33,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:33,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:34,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:34,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:35,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:36,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:36,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:37,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:38,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:38,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:39,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:40,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:41,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:41,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:42,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:43,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:43,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:31<02:28, 14.82s/it][WARNING|generation_utils.py:914] 2023-08-29 09:22:44,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:45,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:45,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:46,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:47,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:48,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:49,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:49,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:50,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:51,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:52,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:52,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:53,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:54,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:55,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:56,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:56,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:57,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:58,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:22:59,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:00,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:47<02:18, 15.35s/it][WARNING|generation_utils.py:914] 2023-08-29 09:23:00,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:01,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:02,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:03,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:03,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:04,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:05,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:05,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:06,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:07,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:07,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:08,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:09,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:10,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:10,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:11,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:12,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:13,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:13,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:14,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:15,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:02<02:02, 15.26s/it][WARNING|generation_utils.py:914] 2023-08-29 09:23:15,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:16,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:17,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:18,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:18,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:19,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:20,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:20,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:21,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:22,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:23,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:23,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:24,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:25,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:26,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:27,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:28,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:28,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:29,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:30,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:30,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:18<01:47, 15.33s/it][WARNING|generation_utils.py:914] 2023-08-29 09:23:31,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:32,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:32,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:33,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:34,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:35,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:35,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:36,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:37,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:38,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:38,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:39,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:40,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:41,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:41,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:42,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:43,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:44,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:44,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:45,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:46,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:33<01:32, 15.44s/it][WARNING|generation_utils.py:914] 2023-08-29 09:23:47,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:47,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:48,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:49,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:50,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:51,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:51,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:52,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:53,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:53,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:54,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:55,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:56,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:57,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:57,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:58,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:59,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:23:59,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:00,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:01,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:02,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:02,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:50<01:18, 15.74s/it][WARNING|generation_utils.py:914] 2023-08-29 09:24:03,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:04,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:04,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:05,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:06,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:07,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:08,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:08,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:09,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:10,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:10,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:11,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:12,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:13,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:13,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:14,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:15,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:15,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:16,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:17,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:04<01:01, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-29 09:24:18,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:18,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:19,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:20,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:20,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:21,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:22,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:22,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:23,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:24,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:25,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:25,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:26,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:27,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:28,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:28,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:29,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:30,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:31,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:32,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:32,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:33,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:20<00:46, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-29 09:24:34,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:34,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:35,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:36,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:37,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:37,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:38,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:39,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:39,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:40,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:41,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:41,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:42,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:43,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:43,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:44,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:44,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:45,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:46,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:46,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:47,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:35<00:30, 15.15s/it][WARNING|generation_utils.py:914] 2023-08-29 09:24:48,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:48,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:50,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:50,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:51,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:52,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:52,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:53,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:54,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:54,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:55,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:56,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:57,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:57,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:58,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:24:59,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:00,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:00,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:01,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:02,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:03,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:03,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:51<00:15, 15.44s/it][WARNING|generation_utils.py:914] 2023-08-29 09:25:04,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:05,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:05,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:06,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:07,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:07,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:08,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:09,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:10,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:10,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:11,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:12,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:12,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:13,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:14,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:14,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:15,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:16,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:16,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:17,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:18,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:19,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:19,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:20,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 09:25:21,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:08<00:00, 16.05s/it]Generating: 100%|██████████| 20/20 [05:08<00:00, 15.44s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:28,981 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:28,985 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:28,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:28,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:28,986 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:25:29,613 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:25:29,614 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:25:30,189 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:25:31,264 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:25:31,264 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:34,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:34,129 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:34,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:34,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:25:34,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:25:34,768 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:25:34,769 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:25:35,343 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:25:35,509 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:25:35,510 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : member of political party . Context : Following his leadership in the 2010 parliamentary election , he served as parliamentary secretary to the Conservative Party of Canada from 2006 to 2010 from 2009 until 2010 , when he resigned . Head Entity : Tory Party of Canada , Tail Entity : Conservative Party .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9047619047619048, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : part of the series .', 'success_rate': 0.8536931818181818, 'errors': {'', "('Johan', 'part of the series', '', 'It features eight children s books on the famous Swedish writer Johan .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : place of death .', 'success_rate': 0.8721590909090909, 'errors': {'', "('Thomas', 'place of death', '', 'Thomas ( 1582 1618 ) , German schwarzmer ( 1497 &ndash; 1513 ) , b. 12th Baron von Humboldt ( 1495 1570 ) , was a German physician of great influence during the times of the Hanseatic Wars .')"}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : director .', 'success_rate': 0.9546875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.9625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.8988095238095238, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : owned by . Context : Later in 1853 , the railroad built the first branch of the railway between Little Bessarabia , near Manchester , and Manchester , the county seat of Manchester , then part of Yorkshire . Head Entity : Manchester , Tail Entity : Manchester State Government .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : part of .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : sport .', 'success_rate': 0.8764204545454546, 'errors': {'', "('Kojima', 'sport', '', 'After a brief spell in the Japan national team and promotion to the FIFA 16 Under 20 Football League , Kojima left for Club Atlético Madrid to work as a scout for the Swiss national team .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.8988095238095238, 'errors': {''}}
['Relation : use . Context : Later in Life , he came to love the songs of Stephenie Meyer , whom he later met in Oxford , Oxfordshire , after she had been ill . Head Entity : Stephenie Meyer , Tail Entity : The songs of Stephenie Meyer .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : use .', 'success_rate': 0.8863636363636364, 'errors': {'', "('format', 'use', '', 'The command format is used so that it automates the formatting of the .')", "('Java', 'use', '', 'The LJK module can automatically update hardware , software and applications using Java or C.')", "('Visual Studio', 'use', '', 'Vyten is open source software package manager for the Linux distribution of Microsoft Windows and Mac OS , and is bundled with the Visual Studio and Visual Basic interfaces .')"}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 400, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : voice type .', 'success_rate': 0.775, 'errors': {'', "('John Malkovich', 'voice type', '', 'She sang the lead role in the musicals 1996 film Get a Job , starring John Travolta , Mark Ruffalo , and John Malkovich .')", "('Michael Piller', 'voice type', '', 'In January 2015 , Michael Piller wrote in an interview with TVA TV , that the voice talent of the BBC hit ITV soap opera , One Life to Live had to change their voice .')", "('Katy Perry', 'voice type', '', 'The group have performed several recordings of Katy Perry s 1989 single ( In the Name of Love ) .')", "('David Ayoob', 'voice type', '', 'David Ayoob is an American composer who has produced most of the country s biggest hits , including All My Children .')", "('Goodbye', 'voice type', '', 'His other hits include the 1991 album You Must Make Me Feel Better , the 1995 album Goodbye to New York City , the 1997 album The Lonely Island and the 1999 album My Life and Love .')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/3_ext.jsonl'}}
estimate vocab size: 13251
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13351, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.62it/s]Extractor Estimating: 2it [00:01,  1.55it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.68it/s]Extractor Estimating: 5it [00:03,  1.70it/s]Extractor Estimating: 6it [00:03,  1.74it/s]Extractor Estimating: 7it [00:04,  1.76it/s]Extractor Estimating: 8it [00:04,  1.72it/s]Extractor Estimating: 9it [00:05,  1.73it/s]Extractor Estimating: 10it [00:05,  1.71it/s]Extractor Estimating: 11it [00:06,  1.71it/s]Extractor Estimating: 12it [00:07,  1.72it/s]Extractor Estimating: 13it [00:07,  1.68it/s]Extractor Estimating: 14it [00:08,  1.67it/s]Extractor Estimating: 15it [00:08,  1.66it/s]Extractor Estimating: 16it [00:09,  1.68it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:10,  1.61it/s]Extractor Estimating: 19it [00:11,  1.63it/s]Extractor Estimating: 20it [00:11,  1.65it/s]Extractor Estimating: 21it [00:12,  1.63it/s]Extractor Estimating: 22it [00:13,  1.66it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.70it/s]Extractor Estimating: 25it [00:14,  1.70it/s]Extractor Estimating: 26it [00:15,  1.63it/s]Extractor Estimating: 27it [00:16,  1.56it/s]Extractor Estimating: 28it [00:17,  1.49it/s]Extractor Estimating: 29it [00:17,  1.52it/s]Extractor Estimating: 30it [00:18,  1.53it/s]Extractor Estimating: 31it [00:18,  1.55it/s]Extractor Estimating: 32it [00:19,  1.58it/s]Extractor Estimating: 33it [00:20,  1.59it/s]Extractor Estimating: 34it [00:20,  1.55it/s]Extractor Estimating: 35it [00:21,  1.56it/s]Extractor Estimating: 36it [00:22,  1.56it/s]Extractor Estimating: 37it [00:22,  1.55it/s]Extractor Estimating: 38it [00:23,  1.54it/s]Extractor Estimating: 39it [00:24,  1.53it/s]Extractor Estimating: 40it [00:24,  1.49it/s]Extractor Estimating: 41it [00:25,  1.53it/s]Extractor Estimating: 42it [00:26,  1.55it/s]Extractor Estimating: 43it [00:26,  1.54it/s]Extractor Estimating: 44it [00:27,  1.51it/s]Extractor Estimating: 45it [00:28,  1.50it/s]Extractor Estimating: 46it [00:28,  1.52it/s]Extractor Estimating: 47it [00:29,  1.53it/s]Extractor Estimating: 48it [00:30,  1.52it/s]Extractor Estimating: 49it [00:30,  1.57it/s]Extractor Estimating: 50it [00:31,  1.61it/s]Extractor Estimating: 51it [00:31,  1.57it/s]Extractor Estimating: 52it [00:32,  1.56it/s]Extractor Estimating: 53it [00:33,  1.53it/s]Extractor Estimating: 54it [00:33,  1.57it/s]Extractor Estimating: 55it [00:34,  1.56it/s]Extractor Estimating: 56it [00:35,  1.56it/s]Extractor Estimating: 57it [00:35,  1.58it/s]Extractor Estimating: 58it [00:36,  1.51it/s]Extractor Estimating: 59it [00:37,  1.51it/s]Extractor Estimating: 60it [00:37,  1.54it/s]Extractor Estimating: 61it [00:38,  1.52it/s]Extractor Estimating: 62it [00:39,  1.53it/s]Extractor Estimating: 63it [00:39,  1.52it/s]Extractor Estimating: 64it [00:40,  1.55it/s]Extractor Estimating: 65it [00:41,  1.50it/s]Extractor Estimating: 66it [00:41,  1.41it/s]Extractor Estimating: 67it [00:42,  1.44it/s]Extractor Estimating: 68it [00:43,  1.45it/s]Extractor Estimating: 69it [00:43,  1.51it/s]Extractor Estimating: 70it [00:44,  1.52it/s]Extractor Estimating: 71it [00:45,  1.54it/s]Extractor Estimating: 72it [00:45,  1.58it/s]Extractor Estimating: 73it [00:46,  1.55it/s]Extractor Estimating: 74it [00:46,  1.59it/s]Extractor Estimating: 75it [00:47,  1.58it/s]Extractor Estimating: 76it [00:48,  1.56it/s]Extractor Estimating: 77it [00:48,  1.55it/s]Extractor Estimating: 78it [00:49,  1.52it/s]Extractor Estimating: 79it [00:50,  1.53it/s]Extractor Estimating: 80it [00:50,  1.54it/s]Extractor Estimating: 81it [00:51,  1.54it/s]Extractor Estimating: 82it [00:52,  1.53it/s]Extractor Estimating: 83it [00:52,  1.57it/s]Extractor Estimating: 84it [00:53,  1.58it/s]Extractor Estimating: 85it [00:54,  1.55it/s]Extractor Estimating: 86it [00:54,  1.60it/s]Extractor Estimating: 87it [00:55,  1.56it/s]Extractor Estimating: 88it [00:55,  1.60it/s]Extractor Estimating: 89it [00:56,  1.59it/s]Extractor Estimating: 90it [00:57,  1.56it/s]Extractor Estimating: 91it [00:57,  1.58it/s]Extractor Estimating: 92it [00:58,  1.51it/s]Extractor Estimating: 93it [00:59,  1.55it/s]Extractor Estimating: 94it [00:59,  1.56it/s]Extractor Estimating: 95it [01:00,  1.59it/s]Extractor Estimating: 96it [01:01,  1.58it/s]Extractor Estimating: 97it [01:01,  1.60it/s]Extractor Estimating: 98it [01:02,  1.60it/s]Extractor Estimating: 99it [01:02,  1.61it/s]Extractor Estimating: 100it [01:03,  1.58it/s]Extractor Estimating: 101it [01:04,  1.57it/s]Extractor Estimating: 102it [01:04,  1.55it/s]Extractor Estimating: 103it [01:05,  1.56it/s]Extractor Estimating: 104it [01:06,  1.55it/s]Extractor Estimating: 105it [01:06,  1.54it/s]Extractor Estimating: 106it [01:07,  1.52it/s]Extractor Estimating: 107it [01:08,  1.51it/s]Extractor Estimating: 108it [01:08,  1.49it/s]Extractor Estimating: 109it [01:09,  1.49it/s]Extractor Estimating: 110it [01:10,  1.47it/s]Extractor Estimating: 111it [01:10,  1.50it/s]Extractor Estimating: 112it [01:11,  1.49it/s]Extractor Estimating: 113it [01:12,  1.50it/s]Extractor Estimating: 114it [01:12,  1.48it/s]Extractor Estimating: 115it [01:13,  1.46it/s]Extractor Estimating: 116it [01:14,  1.49it/s]Extractor Estimating: 117it [01:14,  1.50it/s]Extractor Estimating: 118it [01:15,  1.51it/s]Extractor Estimating: 119it [01:16,  1.48it/s]Extractor Estimating: 120it [01:16,  1.46it/s]Extractor Estimating: 121it [01:17,  1.44it/s]Extractor Estimating: 122it [01:18,  1.47it/s]Extractor Estimating: 123it [01:18,  1.49it/s]Extractor Estimating: 124it [01:19,  1.51it/s]Extractor Estimating: 125it [01:20,  1.44it/s]Extractor Estimating: 126it [01:21,  1.47it/s]Extractor Estimating: 127it [01:21,  1.49it/s]Extractor Estimating: 128it [01:22,  1.47it/s]Extractor Estimating: 129it [01:23,  1.50it/s]Extractor Estimating: 130it [01:23,  1.48it/s]Extractor Estimating: 131it [01:24,  1.51it/s]Extractor Estimating: 132it [01:24,  1.55it/s]Extractor Estimating: 133it [01:25,  1.54it/s]Extractor Estimating: 134it [01:26,  1.59it/s]Extractor Estimating: 135it [01:28,  1.06s/it]Extractor Estimating: 136it [01:28,  1.07it/s]Extractor Estimating: 137it [01:29,  1.19it/s]Extractor Estimating: 138it [01:30,  1.29it/s]Extractor Estimating: 139it [01:30,  1.36it/s]Extractor Estimating: 140it [01:31,  1.40it/s]Extractor Estimating: 141it [01:32,  1.45it/s]Extractor Estimating: 142it [01:32,  1.50it/s]Extractor Estimating: 143it [01:33,  1.52it/s]Extractor Estimating: 144it [01:33,  1.57it/s]Extractor Estimating: 145it [01:34,  1.55it/s]Extractor Estimating: 146it [01:35,  1.51it/s]Extractor Estimating: 147it [01:35,  1.52it/s]Extractor Estimating: 148it [01:36,  1.52it/s]Extractor Estimating: 149it [01:37,  1.52it/s]Extractor Estimating: 150it [01:37,  1.54it/s]Extractor Estimating: 151it [01:38,  1.53it/s]Extractor Estimating: 152it [01:39,  1.47it/s]Extractor Estimating: 153it [01:39,  1.46it/s]Extractor Estimating: 154it [01:40,  1.44it/s]Extractor Estimating: 155it [01:41,  1.43it/s]Extractor Estimating: 156it [01:42,  1.42it/s]Extractor Estimating: 157it [01:42,  1.34it/s]Extractor Estimating: 158it [01:43,  1.28it/s]Extractor Estimating: 159it [01:44,  1.31it/s]Extractor Estimating: 160it [01:45,  1.34it/s]Extractor Estimating: 161it [01:45,  1.40it/s]Extractor Estimating: 162it [01:46,  1.42it/s]Extractor Estimating: 163it [01:47,  1.46it/s]Extractor Estimating: 164it [01:47,  1.47it/s]Extractor Estimating: 165it [01:48,  1.43it/s]Extractor Estimating: 166it [01:49,  1.40it/s]Extractor Estimating: 167it [01:50,  1.40it/s]Extractor Estimating: 168it [01:50,  1.43it/s]Extractor Estimating: 169it [01:51,  1.45it/s]Extractor Estimating: 170it [01:52,  1.43it/s]Extractor Estimating: 171it [01:52,  1.46it/s]Extractor Estimating: 172it [01:53,  1.45it/s]Extractor Estimating: 173it [01:54,  1.45it/s]Extractor Estimating: 174it [01:54,  1.44it/s]Extractor Estimating: 175it [01:55,  1.43it/s]Extractor Estimating: 176it [01:56,  1.40it/s]Extractor Estimating: 177it [01:57,  1.41it/s]Extractor Estimating: 178it [01:57,  1.42it/s]Extractor Estimating: 179it [01:58,  1.43it/s]Extractor Estimating: 180it [01:59,  1.45it/s]Extractor Estimating: 181it [01:59,  1.45it/s]Extractor Estimating: 182it [02:00,  1.47it/s]Extractor Estimating: 183it [02:01,  1.48it/s]Extractor Estimating: 184it [02:01,  1.49it/s]Extractor Estimating: 185it [02:02,  1.46it/s]Extractor Estimating: 186it [02:03,  1.42it/s]Extractor Estimating: 187it [02:03,  1.41it/s]Extractor Estimating: 188it [02:04,  1.43it/s]Extractor Estimating: 189it [02:05,  1.44it/s]Extractor Estimating: 190it [02:05,  1.45it/s]Extractor Estimating: 191it [02:06,  1.48it/s]Extractor Estimating: 192it [02:07,  1.46it/s]Extractor Estimating: 193it [02:08,  1.47it/s]Extractor Estimating: 194it [02:08,  1.48it/s]Extractor Estimating: 195it [02:09,  1.49it/s]Extractor Estimating: 196it [02:10,  1.47it/s]Extractor Estimating: 197it [02:10,  1.49it/s]Extractor Estimating: 198it [02:11,  1.49it/s]Extractor Estimating: 199it [02:12,  1.45it/s]Extractor Estimating: 200it [02:12,  1.47it/s]Extractor Estimating: 201it [02:13,  1.49it/s]Extractor Estimating: 202it [02:14,  1.49it/s]Extractor Estimating: 203it [02:14,  1.50it/s]Extractor Estimating: 204it [02:15,  1.55it/s]Extractor Estimating: 205it [02:15,  1.54it/s]Extractor Estimating: 206it [02:16,  1.48it/s]Extractor Estimating: 207it [02:17,  1.52it/s]Extractor Estimating: 208it [02:17,  1.53it/s]Extractor Estimating: 209it [02:18,  1.54it/s]Extractor Estimating: 210it [02:19,  1.56it/s]Extractor Estimating: 211it [02:19,  1.60it/s]Extractor Estimating: 212it [02:20,  1.60it/s]Extractor Estimating: 213it [02:21,  1.55it/s]Extractor Estimating: 214it [02:21,  1.56it/s]Extractor Estimating: 215it [02:22,  1.55it/s]Extractor Estimating: 216it [02:23,  1.55it/s]Extractor Estimating: 217it [02:23,  1.53it/s]Extractor Estimating: 218it [02:24,  1.51it/s]Extractor Estimating: 219it [02:25,  1.56it/s]Extractor Estimating: 220it [02:25,  1.60it/s]Extractor Estimating: 221it [02:26,  1.58it/s]Extractor Estimating: 222it [02:26,  1.57it/s]Extractor Estimating: 223it [02:27,  1.52it/s]Extractor Estimating: 224it [02:28,  1.49it/s]Extractor Estimating: 225it [02:29,  1.47it/s]Extractor Estimating: 226it [02:29,  1.54it/s]Extractor Estimating: 227it [02:30,  1.60it/s]Extractor Estimating: 228it [02:30,  1.65it/s]Extractor Estimating: 229it [02:31,  1.66it/s]Extractor Estimating: 230it [02:31,  1.73it/s]Extractor Estimating: 231it [02:32,  1.72it/s]Extractor Estimating: 232it [02:32,  1.80it/s]Extractor Estimating: 233it [02:33,  1.80it/s]Extractor Estimating: 234it [02:34,  1.62it/s]Extractor Estimating: 235it [02:34,  1.69it/s]Extractor Estimating: 236it [02:35,  1.74it/s]Extractor Estimating: 237it [02:35,  1.78it/s]Extractor Estimating: 238it [02:36,  1.75it/s]Extractor Estimating: 239it [02:36,  1.78it/s]Extractor Estimating: 240it [02:37,  1.75it/s]Extractor Estimating: 241it [02:38,  1.70it/s]Extractor Estimating: 242it [02:38,  1.69it/s]Extractor Estimating: 243it [02:39,  1.73it/s]Extractor Estimating: 244it [02:39,  1.74it/s]Extractor Estimating: 245it [02:40,  1.75it/s]Extractor Estimating: 246it [02:40,  1.81it/s]Extractor Estimating: 247it [02:41,  1.85it/s]Extractor Estimating: 248it [02:42,  1.85it/s]Extractor Estimating: 249it [02:42,  1.84it/s]Extractor Estimating: 250it [02:43,  1.78it/s]Extractor Estimating: 251it [02:43,  1.70it/s]Extractor Estimating: 252it [02:44,  1.59it/s]Extractor Estimating: 253it [02:45,  1.59it/s]Extractor Estimating: 254it [02:45,  1.57it/s]Extractor Estimating: 255it [02:46,  1.51it/s]Extractor Estimating: 256it [02:47,  1.51it/s]Extractor Estimating: 257it [02:47,  1.48it/s]Extractor Estimating: 258it [02:48,  1.49it/s]Extractor Estimating: 259it [02:49,  1.44it/s]Extractor Estimating: 260it [02:50,  1.41it/s]Extractor Estimating: 261it [02:50,  1.40it/s]Extractor Estimating: 262it [02:51,  1.47it/s]Extractor Estimating: 263it [02:52,  1.52it/s]Extractor Estimating: 264it [02:52,  1.50it/s]Extractor Estimating: 265it [02:53,  1.52it/s]Extractor Estimating: 266it [02:53,  1.54it/s]Extractor Estimating: 267it [02:54,  1.55it/s]Extractor Estimating: 268it [02:55,  1.53it/s]Extractor Estimating: 269it [02:55,  1.54it/s]Extractor Estimating: 270it [02:56,  1.47it/s]Extractor Estimating: 271it [02:57,  1.45it/s]Extractor Estimating: 272it [02:58,  1.47it/s]Extractor Estimating: 273it [02:58,  1.48it/s]Extractor Estimating: 274it [02:59,  1.48it/s]Extractor Estimating: 275it [03:00,  1.46it/s]Extractor Estimating: 276it [03:00,  1.52it/s]Extractor Estimating: 277it [03:01,  1.49it/s]Extractor Estimating: 278it [03:02,  1.51it/s]Extractor Estimating: 279it [03:02,  1.50it/s]Extractor Estimating: 280it [03:03,  1.50it/s]Extractor Estimating: 281it [03:04,  1.53it/s]Extractor Estimating: 282it [03:04,  1.51it/s]Extractor Estimating: 283it [03:05,  1.53it/s]Extractor Estimating: 284it [03:05,  1.55it/s]Extractor Estimating: 285it [03:06,  1.53it/s]Extractor Estimating: 286it [03:07,  1.53it/s]Extractor Estimating: 287it [03:07,  1.54it/s]Extractor Estimating: 288it [03:08,  1.51it/s]Extractor Estimating: 289it [03:09,  1.53it/s]Extractor Estimating: 290it [03:09,  1.51it/s]Extractor Estimating: 291it [03:10,  1.48it/s]Extractor Estimating: 292it [03:11,  1.50it/s]Extractor Estimating: 293it [03:11,  1.49it/s]Extractor Estimating: 294it [03:12,  1.52it/s]Extractor Estimating: 295it [03:13,  1.54it/s]Extractor Estimating: 296it [03:13,  1.51it/s]Extractor Estimating: 297it [03:14,  1.51it/s]Extractor Estimating: 298it [03:15,  1.47it/s]Extractor Estimating: 299it [03:15,  1.53it/s]Extractor Estimating: 300it [03:16,  1.56it/s]Extractor Estimating: 301it [03:17,  1.56it/s]Extractor Estimating: 302it [03:17,  1.60it/s]Extractor Estimating: 303it [03:18,  1.61it/s]Extractor Estimating: 304it [03:18,  1.60it/s]Extractor Estimating: 305it [03:19,  1.57it/s]Extractor Estimating: 306it [03:20,  1.61it/s]Extractor Estimating: 307it [03:20,  1.68it/s]Extractor Estimating: 308it [03:21,  1.65it/s]Extractor Estimating: 309it [03:21,  1.67it/s]Extractor Estimating: 310it [03:22,  1.67it/s]Extractor Estimating: 311it [03:23,  1.67it/s]Extractor Estimating: 312it [03:23,  1.65it/s]Extractor Estimating: 313it [03:24,  1.70it/s]Extractor Estimating: 314it [03:25,  1.45it/s]Extractor Estimating: 315it [03:25,  1.49it/s]Extractor Estimating: 316it [03:26,  1.47it/s]Extractor Estimating: 317it [03:27,  1.50it/s]Extractor Estimating: 318it [03:27,  1.52it/s]Extractor Estimating: 319it [03:28,  1.56it/s]Extractor Estimating: 320it [03:29,  1.58it/s]Extractor Estimating: 321it [03:29,  1.59it/s]Extractor Estimating: 322it [03:30,  1.58it/s]Extractor Estimating: 323it [03:30,  1.59it/s]Extractor Estimating: 324it [03:31,  1.61it/s]Extractor Estimating: 325it [03:32,  1.62it/s]Extractor Estimating: 326it [03:32,  1.61it/s]Extractor Estimating: 327it [03:33,  1.59it/s]Extractor Estimating: 328it [03:34,  1.59it/s]Extractor Estimating: 329it [03:34,  1.60it/s]Extractor Estimating: 330it [03:35,  1.59it/s]Extractor Estimating: 331it [03:36,  1.56it/s]Extractor Estimating: 332it [03:36,  1.50it/s]Extractor Estimating: 333it [03:37,  1.53it/s]Extractor Estimating: 334it [03:37,  1.55it/s]Extractor Estimating: 335it [03:38,  1.57it/s]Extractor Estimating: 336it [03:39,  1.55it/s]Extractor Estimating: 337it [03:39,  1.59it/s]Extractor Estimating: 338it [03:40,  1.59it/s]Extractor Estimating: 339it [03:41,  1.56it/s]Extractor Estimating: 340it [03:41,  1.58it/s]Extractor Estimating: 341it [03:42,  1.59it/s]Extractor Estimating: 342it [03:43,  1.57it/s]Extractor Estimating: 343it [03:43,  1.54it/s]Extractor Estimating: 344it [03:44,  1.54it/s]Extractor Estimating: 345it [03:45,  1.55it/s]Extractor Estimating: 346it [03:45,  1.57it/s]Extractor Estimating: 347it [03:46,  1.56it/s]Extractor Estimating: 348it [03:46,  1.55it/s]Extractor Estimating: 349it [03:47,  1.55it/s]Extractor Estimating: 350it [03:48,  1.56it/s]Extractor Estimating: 351it [03:48,  1.59it/s]Extractor Estimating: 352it [03:49,  1.59it/s]Extractor Estimating: 353it [03:50,  1.59it/s]Extractor Estimating: 354it [03:50,  1.60it/s]Extractor Estimating: 355it [03:51,  1.59it/s]Extractor Estimating: 356it [03:51,  1.57it/s]Extractor Estimating: 357it [03:52,  1.60it/s]Extractor Estimating: 358it [03:53,  1.60it/s]Extractor Estimating: 359it [03:53,  1.60it/s]Extractor Estimating: 360it [03:54,  1.62it/s]Extractor Estimating: 361it [03:55,  1.63it/s]Extractor Estimating: 362it [03:55,  1.62it/s]Extractor Estimating: 363it [03:56,  1.57it/s]Extractor Estimating: 364it [03:57,  1.51it/s]Extractor Estimating: 365it [03:57,  1.51it/s]Extractor Estimating: 366it [03:58,  1.53it/s]Extractor Estimating: 367it [03:59,  1.54it/s]Extractor Estimating: 368it [03:59,  1.58it/s]Extractor Estimating: 369it [04:00,  1.58it/s]Extractor Estimating: 370it [04:00,  1.60it/s]Extractor Estimating: 371it [04:01,  1.61it/s]Extractor Estimating: 372it [04:02,  1.63it/s]Extractor Estimating: 373it [04:02,  1.59it/s]Extractor Estimating: 374it [04:03,  1.58it/s]Extractor Estimating: 375it [04:04,  1.57it/s]Extractor Estimating: 376it [04:04,  1.59it/s]Extractor Estimating: 377it [04:05,  1.59it/s]Extractor Estimating: 378it [04:05,  1.51it/s]Extractor Estimating: 379it [04:06,  1.55it/s]Extractor Estimating: 380it [04:07,  1.55it/s]Extractor Estimating: 381it [04:07,  1.50it/s]Extractor Estimating: 382it [04:08,  1.50it/s]Extractor Estimating: 383it [04:09,  1.52it/s]Extractor Estimating: 384it [04:09,  1.57it/s]Extractor Estimating: 385it [04:10,  1.57it/s]Extractor Estimating: 386it [04:11,  1.58it/s]Extractor Estimating: 387it [04:11,  1.55it/s]Extractor Estimating: 388it [04:12,  1.55it/s]Extractor Estimating: 389it [04:13,  1.34it/s]Extractor Estimating: 390it [04:14,  1.39it/s]Extractor Estimating: 391it [04:14,  1.44it/s]Extractor Estimating: 392it [04:15,  1.48it/s]Extractor Estimating: 393it [04:15,  1.49it/s]Extractor Estimating: 394it [04:16,  1.52it/s]Extractor Estimating: 395it [04:17,  1.54it/s]Extractor Estimating: 396it [04:17,  1.52it/s]Extractor Estimating: 397it [04:18,  1.53it/s]Extractor Estimating: 398it [04:19,  1.53it/s]Extractor Estimating: 399it [04:19,  1.53it/s]Extractor Estimating: 400it [04:20,  1.55it/s]Extractor Estimating: 401it [04:21,  1.61it/s]Extractor Estimating: 402it [04:21,  1.58it/s]Extractor Estimating: 403it [04:22,  1.57it/s]Extractor Estimating: 404it [04:23,  1.54it/s]Extractor Estimating: 405it [04:23,  1.58it/s]Extractor Estimating: 406it [04:24,  1.58it/s]Extractor Estimating: 407it [04:24,  1.56it/s]Extractor Estimating: 408it [04:25,  1.57it/s]Extractor Estimating: 409it [04:26,  1.54it/s]Extractor Estimating: 410it [04:26,  1.60it/s]Extractor Estimating: 411it [04:27,  1.61it/s]Extractor Estimating: 412it [04:28,  1.59it/s]Extractor Estimating: 413it [04:28,  1.53it/s]Extractor Estimating: 414it [04:29,  1.56it/s]Extractor Estimating: 415it [04:30,  1.58it/s]Extractor Estimating: 416it [04:30,  1.59it/s]Extractor Estimating: 417it [04:31,  1.51it/s]Extractor Estimating: 418it [04:32,  1.50it/s]Extractor Estimating: 419it [04:32,  1.54it/s]Extractor Estimating: 420it [04:33,  1.48it/s]Extractor Estimating: 421it [04:34,  1.50it/s]Extractor Estimating: 422it [04:34,  1.49it/s]Extractor Estimating: 423it [04:35,  1.50it/s]Extractor Estimating: 424it [04:36,  1.52it/s]Extractor Estimating: 425it [04:36,  1.57it/s]Extractor Estimating: 426it [04:37,  1.58it/s]Extractor Estimating: 427it [04:37,  1.60it/s]Extractor Estimating: 428it [04:38,  1.65it/s]Extractor Estimating: 429it [04:38,  1.65it/s]Extractor Estimating: 430it [04:39,  1.65it/s]Extractor Estimating: 431it [04:40,  1.72it/s]Extractor Estimating: 432it [04:40,  1.75it/s]Extractor Estimating: 433it [04:41,  1.72it/s]Extractor Estimating: 434it [04:41,  1.73it/s]Extractor Estimating: 435it [04:42,  1.73it/s]Extractor Estimating: 436it [04:43,  1.70it/s]Extractor Estimating: 437it [04:43,  1.74it/s]Extractor Estimating: 438it [04:44,  1.78it/s]Extractor Estimating: 439it [04:44,  1.77it/s]Extractor Estimating: 440it [04:45,  1.78it/s]Extractor Estimating: 441it [04:45,  1.79it/s]Extractor Estimating: 442it [04:46,  1.83it/s]Extractor Estimating: 443it [04:46,  1.80it/s]Extractor Estimating: 444it [04:47,  1.80it/s]Extractor Estimating: 445it [04:47,  1.80it/s]Extractor Estimating: 446it [04:48,  1.75it/s]Extractor Estimating: 447it [04:49,  1.73it/s]Extractor Estimating: 448it [04:49,  1.75it/s]Extractor Estimating: 449it [04:50,  1.74it/s]Extractor Estimating: 450it [04:50,  1.76it/s]Extractor Estimating: 451it [04:51,  1.74it/s]Extractor Estimating: 452it [04:52,  1.72it/s]Extractor Estimating: 453it [04:52,  1.72it/s]Extractor Estimating: 454it [04:53,  1.73it/s]Extractor Estimating: 455it [04:53,  1.73it/s]Extractor Estimating: 456it [04:54,  1.67it/s]Extractor Estimating: 457it [04:55,  1.67it/s]Extractor Estimating: 458it [04:55,  1.71it/s]Extractor Estimating: 459it [04:56,  1.71it/s]Extractor Estimating: 460it [04:56,  1.68it/s]Extractor Estimating: 461it [04:57,  1.70it/s]Extractor Estimating: 462it [04:57,  1.73it/s]Extractor Estimating: 463it [04:58,  1.65it/s]Extractor Estimating: 464it [04:59,  1.68it/s]Extractor Estimating: 465it [04:59,  1.67it/s]Extractor Estimating: 466it [05:00,  1.53it/s]Extractor Estimating: 467it [05:01,  1.56it/s]Extractor Estimating: 468it [05:01,  1.60it/s]Extractor Estimating: 469it [05:02,  1.59it/s]Extractor Estimating: 470it [05:03,  1.51it/s]Extractor Estimating: 471it [05:03,  1.54it/s]Extractor Estimating: 472it [05:04,  1.62it/s]Extractor Estimating: 473it [05:04,  1.63it/s]Extractor Estimating: 474it [05:05,  1.59it/s]Extractor Estimating: 475it [05:06,  1.61it/s]Extractor Estimating: 476it [05:06,  1.68it/s]Extractor Estimating: 477it [05:07,  1.56it/s]Extractor Estimating: 478it [05:08,  1.53it/s]Extractor Estimating: 479it [05:08,  1.54it/s]Extractor Estimating: 480it [05:09,  1.54it/s]Extractor Estimating: 481it [05:10,  1.51it/s]Extractor Estimating: 482it [05:10,  1.52it/s]Extractor Estimating: 483it [05:11,  1.56it/s]Extractor Estimating: 484it [05:12,  1.38it/s]Extractor Estimating: 485it [05:12,  1.40it/s]Extractor Estimating: 486it [05:13,  1.46it/s]Extractor Estimating: 487it [05:14,  1.47it/s]Extractor Estimating: 488it [05:14,  1.50it/s]Extractor Estimating: 489it [05:15,  1.54it/s]Extractor Estimating: 490it [05:16,  1.56it/s]Extractor Estimating: 491it [05:16,  1.61it/s]Extractor Estimating: 492it [05:17,  1.59it/s]Extractor Estimating: 493it [05:18,  1.56it/s]Extractor Estimating: 494it [05:18,  1.59it/s]Extractor Estimating: 495it [05:19,  1.59it/s]Extractor Estimating: 496it [05:19,  1.62it/s]Extractor Estimating: 497it [05:20,  1.56it/s]Extractor Estimating: 498it [05:21,  1.57it/s]Extractor Estimating: 499it [05:21,  1.58it/s]Extractor Estimating: 500it [05:22,  1.64it/s]Extractor Estimating: 500it [05:22,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:17,846 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:17,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:17,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:17,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:17,851 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:31:18,167 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:31:18,169 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:31:18,437 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:31:19,505 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:31:19,505 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:20,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:20,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:20,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:20,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:31:20,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:31:21,272 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:31:21,273 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:31:21,541 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:31:21,705 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:31:21,705 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 13:09:21,406 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 13:09:21,682 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_2/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9979 mean pseudo reward: 0.9406060318751246
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl'}
train vocab size: 25538
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25638, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25638, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.113, loss:700.7771
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.127, loss:697.8941
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.118, loss:642.9860
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.118, loss:669.0304
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.123, loss:650.1935
>> valid entity prec:0.5857, rec:0.5495, f1:0.5670
>> valid relation prec:0.4420, rec:0.1211, f1:0.1901
>> valid relation with NER prec:0.4420, rec:0.1211, f1:0.1901
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 3.478, loss:676.8198
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.113, loss:655.3313
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.122, loss:657.4867
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.117, loss:637.5601
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.118, loss:646.1012
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6536, rec:0.4786, f1:0.5526
>> valid relation prec:0.4545, rec:0.0916, f1:0.1525
>> valid relation with NER prec:0.4545, rec:0.0916, f1:0.1525
g_step 1100, step 268, avg_time 3.458, loss:655.2434
g_step 1200, step 368, avg_time 1.123, loss:677.1839
g_step 1300, step 52, avg_time 1.123, loss:651.6843
g_step 1400, step 152, avg_time 1.111, loss:599.8764
g_step 1500, step 252, avg_time 1.107, loss:647.7399
>> valid entity prec:0.6183, rec:0.5289, f1:0.5701
>> valid relation prec:0.4292, rec:0.1029, f1:0.1660
>> valid relation with NER prec:0.4292, rec:0.1029, f1:0.1660
new max entity f1 on valid!
g_step 1600, step 352, avg_time 3.480, loss:676.4732
g_step 1700, step 36, avg_time 1.120, loss:635.7114
g_step 1800, step 136, avg_time 1.116, loss:587.9569
g_step 1900, step 236, avg_time 1.115, loss:611.9367
g_step 2000, step 336, avg_time 1.126, loss:612.4310
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6156, rec:0.5339, f1:0.5718
>> valid relation prec:0.4113, rec:0.1101, f1:0.1737
>> valid relation with NER prec:0.4113, rec:0.1101, f1:0.1737
new max entity f1 on valid!
g_step 2100, step 20, avg_time 3.470, loss:608.6453
g_step 2200, step 120, avg_time 1.118, loss:586.5362
g_step 2300, step 220, avg_time 1.119, loss:580.7185
g_step 2400, step 320, avg_time 1.116, loss:587.2461
g_step 2500, step 4, avg_time 1.114, loss:602.8564
>> valid entity prec:0.5945, rec:0.4938, f1:0.5395
>> valid relation prec:0.4447, rec:0.0961, f1:0.1580
>> valid relation with NER prec:0.4447, rec:0.0961, f1:0.1580
g_step 2600, step 104, avg_time 3.461, loss:544.1716
g_step 2700, step 204, avg_time 1.119, loss:561.7486
g_step 2800, step 304, avg_time 1.123, loss:584.4962
g_step 2900, step 404, avg_time 1.128, loss:598.5000
g_step 3000, step 88, avg_time 1.111, loss:558.3716
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6093, rec:0.5121, f1:0.5565
>> valid relation prec:0.4427, rec:0.0972, f1:0.1594
>> valid relation with NER prec:0.4427, rec:0.0972, f1:0.1594
g_step 3100, step 188, avg_time 3.500, loss:555.2797
g_step 3200, step 288, avg_time 1.125, loss:554.7988
g_step 3300, step 388, avg_time 1.122, loss:564.6263
g_step 3400, step 72, avg_time 1.118, loss:525.6758
g_step 3500, step 172, avg_time 1.108, loss:542.5221
>> valid entity prec:0.5654, rec:0.5106, f1:0.5366
>> valid relation prec:0.3873, rec:0.1060, f1:0.1664
>> valid relation with NER prec:0.3873, rec:0.1060, f1:0.1664
g_step 3600, step 272, avg_time 3.485, loss:547.9187
g_step 3700, step 372, avg_time 1.179, loss:533.0088
g_step 3800, step 56, avg_time 1.110, loss:515.9302
g_step 3900, step 156, avg_time 1.115, loss:510.6005
g_step 4000, step 256, avg_time 1.113, loss:526.9024
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6177, rec:0.4799, f1:0.5402
>> valid relation prec:0.4374, rec:0.0910, f1:0.1506
>> valid relation with NER prec:0.4374, rec:0.0910, f1:0.1506
g_step 4100, step 356, avg_time 3.452, loss:535.9449
g_step 4200, step 40, avg_time 1.121, loss:525.9194
g_step 4300, step 140, avg_time 1.121, loss:501.4954
g_step 4400, step 240, avg_time 1.108, loss:507.9857
g_step 4500, step 340, avg_time 1.111, loss:509.6644
>> valid entity prec:0.6334, rec:0.4569, f1:0.5309
>> valid relation prec:0.4499, rec:0.1026, f1:0.1670
>> valid relation with NER prec:0.4499, rec:0.1026, f1:0.1670
g_step 4600, step 24, avg_time 3.470, loss:505.8676
g_step 4700, step 124, avg_time 1.111, loss:473.7732
g_step 4800, step 224, avg_time 1.114, loss:486.4547
g_step 4900, step 324, avg_time 1.119, loss:507.4985
g_step 5000, step 8, avg_time 1.122, loss:500.4385
learning rate was adjusted to 0.0008
>> valid entity prec:0.5881, rec:0.5290, f1:0.5570
>> valid relation prec:0.3762, rec:0.1263, f1:0.1891
>> valid relation with NER prec:0.3762, rec:0.1263, f1:0.1891
g_step 5100, step 108, avg_time 3.469, loss:458.0889
g_step 5200, step 208, avg_time 1.118, loss:474.0486
g_step 5300, step 308, avg_time 1.115, loss:480.1516
g_step 5400, step 408, avg_time 1.127, loss:470.6260
g_step 5500, step 92, avg_time 1.116, loss:446.0760
>> valid entity prec:0.5983, rec:0.5096, f1:0.5504
>> valid relation prec:0.3444, rec:0.1294, f1:0.1881
>> valid relation with NER prec:0.3444, rec:0.1294, f1:0.1881
g_step 5600, step 192, avg_time 3.473, loss:462.8839
g_step 5700, step 292, avg_time 1.117, loss:448.0989
g_step 5800, step 392, avg_time 1.114, loss:478.7852
g_step 5900, step 76, avg_time 1.114, loss:446.7352
g_step 6000, step 176, avg_time 1.108, loss:442.2203
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6057, rec:0.4766, f1:0.5335
>> valid relation prec:0.3293, rec:0.0970, f1:0.1499
>> valid relation with NER prec:0.3293, rec:0.0970, f1:0.1499
g_step 6100, step 276, avg_time 3.464, loss:467.0410
g_step 6200, step 376, avg_time 1.114, loss:449.2867
g_step 6300, step 60, avg_time 1.133, loss:428.1005
g_step 6400, step 160, avg_time 1.118, loss:430.9560
g_step 6500, step 260, avg_time 1.122, loss:429.6979
>> valid entity prec:0.5899, rec:0.5071, f1:0.5454
>> valid relation prec:0.3583, rec:0.1061, f1:0.1637
>> valid relation with NER prec:0.3583, rec:0.1061, f1:0.1637
g_step 6600, step 360, avg_time 3.492, loss:433.4738
g_step 6700, step 44, avg_time 1.119, loss:428.6520
g_step 6800, step 144, avg_time 1.126, loss:412.2787
g_step 6900, step 244, avg_time 1.121, loss:427.8868
g_step 7000, step 344, avg_time 1.111, loss:424.1959
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6278, rec:0.4826, f1:0.5457
>> valid relation prec:0.4240, rec:0.1101, f1:0.1748
>> valid relation with NER prec:0.4240, rec:0.1101, f1:0.1748
g_step 7100, step 28, avg_time 3.470, loss:461.8169
g_step 7200, step 128, avg_time 1.132, loss:410.6433
g_step 7300, step 228, avg_time 1.120, loss:407.1210
g_step 7400, step 328, avg_time 1.110, loss:428.2408
g_step 7500, step 12, avg_time 1.113, loss:411.0134
>> valid entity prec:0.6012, rec:0.5269, f1:0.5616
>> valid relation prec:0.3423, rec:0.1373, f1:0.1959
>> valid relation with NER prec:0.3423, rec:0.1373, f1:0.1959
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 7600, step 112, avg_time 3.483, loss:396.7077
g_step 7700, step 212, avg_time 1.120, loss:395.3160
g_step 7800, step 312, avg_time 1.110, loss:412.0410
g_step 7900, step 412, avg_time 1.114, loss:417.4442
g_step 8000, step 96, avg_time 1.125, loss:382.6361
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5892, rec:0.4817, f1:0.5300
>> valid relation prec:0.3636, rec:0.1093, f1:0.1681
>> valid relation with NER prec:0.3636, rec:0.1093, f1:0.1681
g_step 8100, step 196, avg_time 3.465, loss:386.9717
g_step 8200, step 296, avg_time 1.120, loss:394.4713
g_step 8300, step 396, avg_time 1.123, loss:396.1506
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 13:09:21 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 13:09:21 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_13-09-21_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 13:09:23 - WARNING - datasets.builder -   Using custom data configuration default-6dfc84f4dcb66c80
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6dfc84f4dcb66c80/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 13:09:25,755 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 13:09:25,756 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 13:09:25,757 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 13:09:25,758 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 13:09:25,905 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:09:25,973 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:09:25,973 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:09:25,973 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:09:25,973 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:09:25,973 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:09:25,973 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 13:09:26,502 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 13:09:31,421 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 13:09:31,442 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6dfc84f4dcb66c80/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.59ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.53ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.01ba/s] 36%|███▋      | 4/11 [00:01<00:01,  4.25ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.39ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.47ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.53ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.59ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.61ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.62ba/s]100%|██████████| 11/11 [00:02<00:00,  4.75ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  3.24ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.87ba/s] 43%|████▎     | 3/7 [00:00<00:01,  3.06ba/s] 57%|█████▋    | 4/7 [00:01<00:00,  3.50ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  3.81ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.00ba/s]100%|██████████| 7/7 [00:01<00:00,  4.87ba/s]100%|██████████| 7/7 [00:01<00:00,  4.06ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:02,  4.54ba/s] 18%|█▊        | 2/11 [00:00<00:01,  6.61ba/s] 27%|██▋       | 3/11 [00:00<00:01,  7.80ba/s] 45%|████▌     | 5/11 [00:00<00:00,  8.94ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.40ba/s] 73%|███████▎  | 8/11 [00:00<00:00,  9.51ba/s] 91%|█████████ | 10/11 [00:01<00:00,  9.72ba/s]100%|██████████| 11/11 [00:01<00:00,  9.78ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:01,  4.21ba/s] 29%|██▊       | 2/7 [00:00<00:00,  5.43ba/s] 43%|████▎     | 3/7 [00:00<00:00,  6.76ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  8.31ba/s]100%|██████████| 7/7 [00:00<00:00,  9.99ba/s]100%|██████████| 7/7 [00:00<00:00,  8.35ba/s]
[INFO|trainer.py:414] 2023-08-29 13:09:38,787 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 13:09:38,885 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 13:09:38,885 >>   Num examples = 10003
[INFO|trainer.py:1149] 2023-08-29 13:09:38,885 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 13:09:38,885 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 13:09:38,885 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 13:09:38,886 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 13:09:38,886 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<04:02,  3.21it/s]  0%|          | 2/780 [00:00<03:52,  3.35it/s]  0%|          | 3/780 [00:00<03:48,  3.39it/s]  1%|          | 4/780 [00:01<03:47,  3.42it/s]  1%|          | 5/780 [00:01<03:46,  3.43it/s]  1%|          | 6/780 [00:01<03:45,  3.44it/s]  1%|          | 7/780 [00:02<03:44,  3.44it/s]  1%|          | 8/780 [00:02<04:04,  3.16it/s]  1%|          | 9/780 [00:02<03:57,  3.25it/s]  1%|▏         | 10/780 [00:03<03:52,  3.30it/s]  1%|▏         | 11/780 [00:03<03:49,  3.35it/s]  2%|▏         | 12/780 [00:03<03:47,  3.38it/s]  2%|▏         | 13/780 [00:03<03:45,  3.40it/s]  2%|▏         | 14/780 [00:04<03:44,  3.42it/s]  2%|▏         | 15/780 [00:04<03:43,  3.43it/s]  2%|▏         | 16/780 [00:04<03:42,  3.44it/s]  2%|▏         | 17/780 [00:05<03:41,  3.44it/s]  2%|▏         | 18/780 [00:05<03:41,  3.44it/s]  2%|▏         | 19/780 [00:05<03:53,  3.26it/s]  3%|▎         | 20/780 [00:05<03:49,  3.31it/s]  3%|▎         | 21/780 [00:06<03:46,  3.35it/s]  3%|▎         | 22/780 [00:06<03:44,  3.38it/s]  3%|▎         | 23/780 [00:06<03:42,  3.40it/s]  3%|▎         | 24/780 [00:07<03:42,  3.40it/s]  3%|▎         | 25/780 [00:07<03:40,  3.42it/s]  3%|▎         | 26/780 [00:07<03:40,  3.42it/s]  3%|▎         | 27/780 [00:07<03:39,  3.43it/s]  4%|▎         | 28/780 [00:08<03:38,  3.44it/s]  4%|▎         | 29/780 [00:08<03:38,  3.44it/s]  4%|▍         | 30/780 [00:08<03:46,  3.31it/s]  4%|▍         | 31/780 [00:09<03:43,  3.34it/s]  4%|▍         | 32/780 [00:09<03:41,  3.37it/s]  4%|▍         | 33/780 [00:09<03:40,  3.40it/s]  4%|▍         | 34/780 [00:10<03:38,  3.41it/s]  4%|▍         | 35/780 [00:10<03:37,  3.42it/s]  5%|▍         | 36/780 [00:10<03:37,  3.43it/s]  5%|▍         | 37/780 [00:10<03:36,  3.43it/s]  5%|▍         | 38/780 [00:11<03:35,  3.44it/s]  5%|▌         | 39/780 [00:11<03:35,  3.44it/s]  5%|▌         | 40/780 [00:11<03:35,  3.44it/s]  5%|▌         | 41/780 [00:12<03:54,  3.15it/s]  5%|▌         | 42/780 [00:12<03:48,  3.23it/s]  6%|▌         | 43/780 [00:12<03:44,  3.29it/s]  6%|▌         | 44/780 [00:13<03:40,  3.33it/s]  6%|▌         | 45/780 [00:13<04:02,  3.04it/s]  6%|▌         | 46/780 [00:13<03:53,  3.14it/s]  6%|▌         | 47/780 [00:14<03:47,  3.23it/s]  6%|▌         | 48/780 [00:14<03:42,  3.29it/s]  6%|▋         | 49/780 [00:14<03:39,  3.33it/s]  6%|▋         | 50/780 [00:14<03:36,  3.37it/s]  7%|▋         | 51/780 [00:15<03:35,  3.39it/s]  7%|▋         | 52/780 [00:15<03:33,  3.41it/s]  7%|▋         | 53/780 [00:15<03:32,  3.42it/s]  7%|▋         | 54/780 [00:16<03:31,  3.43it/s]  7%|▋         | 55/780 [00:16<03:37,  3.34it/s]  7%|▋         | 56/780 [00:16<03:34,  3.37it/s]  7%|▋         | 57/780 [00:16<03:33,  3.39it/s]  7%|▋         | 58/780 [00:17<03:31,  3.41it/s]  8%|▊         | 59/780 [00:17<03:30,  3.42it/s]  8%|▊         | 60/780 [00:17<03:30,  3.43it/s]  8%|▊         | 61/780 [00:18<03:29,  3.43it/s]  8%|▊         | 62/780 [00:18<03:28,  3.44it/s]  8%|▊         | 63/780 [00:18<03:28,  3.44it/s]  8%|▊         | 64/780 [00:18<03:28,  3.44it/s]  8%|▊         | 65/780 [00:19<03:27,  3.44it/s]  8%|▊         | 66/780 [00:19<03:35,  3.31it/s]  9%|▊         | 67/780 [00:19<03:32,  3.35it/s]  9%|▊         | 68/780 [00:20<03:31,  3.37it/s]  9%|▉         | 69/780 [00:20<03:29,  3.40it/s]  9%|▉         | 70/780 [00:20<03:28,  3.41it/s]  9%|▉         | 71/780 [00:21<03:27,  3.42it/s]  9%|▉         | 72/780 [00:21<03:26,  3.42it/s]  9%|▉         | 73/780 [00:21<03:26,  3.42it/s]  9%|▉         | 74/780 [00:21<03:26,  3.43it/s] 10%|▉         | 75/780 [00:22<03:25,  3.43it/s] 10%|▉         | 76/780 [00:22<03:24,  3.44it/s] 10%|▉         | 77/780 [00:22<03:30,  3.34it/s] 10%|█         | 78/780 [00:23<03:28,  3.37it/s] 10%|█         | 79/780 [00:23<03:26,  3.39it/s] 10%|█         | 80/780 [00:23<03:25,  3.41it/s] 10%|█         | 81/780 [00:24<03:24,  3.42it/s] 11%|█         | 82/780 [00:24<03:23,  3.43it/s] 11%|█         | 83/780 [00:24<03:23,  3.43it/s] 11%|█         | 84/780 [00:24<03:22,  3.43it/s] 11%|█         | 85/780 [00:25<03:22,  3.44it/s] 11%|█         | 86/780 [00:25<03:21,  3.44it/s] 11%|█         | 87/780 [00:25<03:21,  3.44it/s] 11%|█▏        | 88/780 [00:26<03:29,  3.30it/s] 11%|█▏        | 89/780 [00:26<03:26,  3.34it/s] 12%|█▏        | 90/780 [00:26<03:24,  3.37it/s] 12%|█▏        | 91/780 [00:26<03:23,  3.39it/s] 12%|█▏        | 92/780 [00:27<03:22,  3.40it/s] 12%|█▏        | 93/780 [00:27<03:21,  3.42it/s] 12%|█▏        | 94/780 [00:27<03:20,  3.42it/s] 12%|█▏        | 95/780 [00:28<03:19,  3.43it/s] 12%|█▏        | 96/780 [00:28<03:19,  3.43it/s] 12%|█▏        | 97/780 [00:28<03:18,  3.43it/s] 13%|█▎        | 98/780 [00:28<03:18,  3.44it/s] 13%|█▎        | 99/780 [00:29<03:29,  3.25it/s] 13%|█▎        | 100/780 [00:29<03:25,  3.30it/s] 13%|█▎        | 101/780 [00:29<03:23,  3.34it/s] 13%|█▎        | 102/780 [00:30<03:21,  3.37it/s] 13%|█▎        | 103/780 [00:30<03:19,  3.39it/s] 13%|█▎        | 104/780 [00:30<03:18,  3.40it/s] 13%|█▎        | 105/780 [00:31<03:17,  3.41it/s] 14%|█▎        | 106/780 [00:31<03:17,  3.42it/s] 14%|█▎        | 107/780 [00:31<03:16,  3.42it/s] 14%|█▍        | 108/780 [00:31<03:16,  3.43it/s] 14%|█▍        | 109/780 [00:32<03:15,  3.43it/s] 14%|█▍        | 110/780 [00:32<03:20,  3.34it/s] 14%|█▍        | 111/780 [00:32<03:19,  3.36it/s] 14%|█▍        | 112/780 [00:33<03:17,  3.38it/s] 14%|█▍        | 113/780 [00:33<03:16,  3.40it/s] 15%|█▍        | 114/780 [00:33<03:15,  3.41it/s] 15%|█▍        | 115/780 [00:34<03:14,  3.41it/s] 15%|█▍        | 116/780 [00:34<03:14,  3.42it/s] 15%|█▌        | 117/780 [00:34<03:13,  3.43it/s] 15%|█▌        | 118/780 [00:34<03:13,  3.42it/s] 15%|█▌        | 119/780 [00:35<03:13,  3.42it/s] 15%|█▌        | 120/780 [00:35<03:12,  3.43it/s] 16%|█▌        | 121/780 [00:35<03:19,  3.30it/s] 16%|█▌        | 122/780 [00:36<03:17,  3.33it/s] 16%|█▌        | 123/780 [00:36<03:15,  3.36it/s] 16%|█▌        | 124/780 [00:36<03:13,  3.38it/s] 16%|█▌        | 125/780 [00:36<03:12,  3.40it/s] 16%|█▌        | 126/780 [00:37<03:11,  3.41it/s] 16%|█▋        | 127/780 [00:37<03:10,  3.42it/s] 16%|█▋        | 128/780 [00:37<03:10,  3.42it/s] 17%|█▋        | 129/780 [00:38<03:09,  3.43it/s] 17%|█▋        | 130/780 [00:38<03:09,  3.43it/s] 17%|█▋        | 131/780 [00:38<03:09,  3.43it/s] 17%|█▋        | 132/780 [00:39<03:12,  3.37it/s] 17%|█▋        | 133/780 [00:39<03:10,  3.39it/s] 17%|█▋        | 134/780 [00:39<03:09,  3.40it/s] 17%|█▋        | 135/780 [00:39<03:08,  3.42it/s] 17%|█▋        | 136/780 [00:40<03:08,  3.42it/s] 18%|█▊        | 137/780 [00:40<03:14,  3.31it/s] 18%|█▊        | 138/780 [00:40<03:12,  3.34it/s] 18%|█▊        | 139/780 [00:41<03:10,  3.36it/s] 18%|█▊        | 140/780 [00:41<03:09,  3.38it/s] 18%|█▊        | 141/780 [00:41<03:08,  3.40it/s] 18%|█▊        | 142/780 [00:41<03:07,  3.41it/s] 18%|█▊        | 143/780 [00:42<03:15,  3.26it/s] 18%|█▊        | 144/780 [00:42<03:55,  2.71it/s] 19%|█▊        | 145/780 [00:43<03:40,  2.88it/s] 19%|█▊        | 146/780 [00:43<03:29,  3.03it/s] 19%|█▉        | 147/780 [00:43<03:21,  3.14it/s] 19%|█▉        | 148/780 [00:44<03:16,  3.22it/s] 19%|█▉        | 149/780 [00:44<03:12,  3.28it/s] 19%|█▉        | 150/780 [00:44<03:09,  3.32it/s] 19%|█▉        | 151/780 [00:44<03:07,  3.35it/s] 19%|█▉        | 152/780 [00:45<03:05,  3.38it/s] 20%|█▉        | 153/780 [00:45<03:04,  3.39it/s] 20%|█▉        | 154/780 [00:45<03:03,  3.40it/s] 20%|█▉        | 155/780 [00:46<03:03,  3.41it/s] 20%|██        | 156/780 [00:46<03:02,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 13:10:25,271 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:10:25,271 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 13:10:25,271 >>   Batch size = 8

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.94it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.47it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.51it/s][A
  3%|▎         | 23/811 [00:00<00:17, 46.29it/s][A
  3%|▎         | 28/811 [00:00<00:16, 46.40it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.37it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.10it/s][A
  5%|▌         | 43/811 [00:00<00:16, 45.85it/s][A
  6%|▌         | 48/811 [00:01<00:16, 45.99it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.07it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.25it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.28it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.26it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.32it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.30it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.26it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.02it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 46.20it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.16it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.25it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.24it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.21it/s][A
 15%|█▍        | 118/811 [00:02<00:14, 46.27it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.18it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.16it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.04it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.01it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 46.08it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.18it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.28it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.21it/s][A
 20%|██        | 163/811 [00:03<00:15, 41.71it/s][A
 21%|██        | 168/811 [00:03<00:14, 43.02it/s][A
 21%|██▏       | 173/811 [00:03<00:14, 44.06it/s][A
 22%|██▏       | 178/811 [00:03<00:14, 44.67it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 45.13it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 45.57it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 45.88it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.03it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 45.50it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 45.55it/s][A
 26%|██▋       | 213/811 [00:04<00:13, 45.83it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 45.98it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.06it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.14it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 46.23it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.30it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.36it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.17it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.08it/s][A
 32%|███▏      | 258/811 [00:05<00:12, 46.04it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.22it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.28it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.21it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.25it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.29it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.29it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.20it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.14it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 42.47it/s][A
 38%|███▊      | 308/811 [00:06<00:11, 43.61it/s][A
 39%|███▊      | 313/811 [00:06<00:11, 44.37it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 44.91it/s][A
 40%|███▉      | 323/811 [00:07<00:10, 45.41it/s][A
 40%|████      | 328/811 [00:07<00:10, 45.69it/s][A
 41%|████      | 333/811 [00:07<00:10, 45.87it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.05it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 45.81it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 45.82it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 46.04it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.15it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 46.08it/s][A
 45%|████▌     | 368/811 [00:08<00:09, 46.23it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.33it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.23it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.26it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.10it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.07it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.12it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 46.21it/s][A
 50%|█████     | 408/811 [00:08<00:08, 46.14it/s][A
 51%|█████     | 413/811 [00:08<00:08, 46.21it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.22it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.27it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.20it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.12it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.13it/s][A
 55%|█████▍    | 443/811 [00:09<00:08, 44.67it/s][A
 55%|█████▌    | 448/811 [00:09<00:08, 45.12it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 45.46it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 45.70it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 45.94it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.12it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.04it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 45.98it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 45.90it/s][A
 60%|██████    | 488/811 [00:10<00:07, 45.98it/s][A
 61%|██████    | 493/811 [00:10<00:06, 46.09it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 46.15it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.14it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 46.26it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.28it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 46.17it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.05it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.06it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 45.99it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.06it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 46.15it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.25it/s][A
 68%|██████▊   | 553/811 [00:12<00:05, 46.25it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 46.25it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.16it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.11it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.06it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 45.97it/s][A
 72%|███████▏  | 583/811 [00:12<00:05, 43.86it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 44.61it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 45.13it/s][A
 74%|███████▎  | 598/811 [00:13<00:04, 45.48it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 45.78it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 45.86it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 45.96it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.11it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 45.84it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 45.82it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 45.90it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 46.10it/s][A
 79%|███████▉  | 643/811 [00:14<00:03, 46.11it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 45.85it/s][A
 81%|████████  | 653/811 [00:14<00:03, 46.08it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.15it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.15it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 46.02it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.02it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 45.92it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 46.10it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 46.19it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 46.12it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 46.00it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 46.10it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 46.05it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.02it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 45.96it/s][A
 89%|████████▉ | 723/811 [00:15<00:02, 43.66it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 44.48it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 44.97it/s][A
 91%|█████████ | 738/811 [00:16<00:01, 45.39it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 45.57it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 45.79it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 45.86it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 45.91it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 45.62it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 45.33it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 45.63it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 45.85it/s][A
 97%|█████████▋| 783/811 [00:17<00:00, 45.89it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 46.00it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 46.02it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 46.16it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.03it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 45.89it/s][A                                                 
                                                 [A 20%|██        | 156/780 [01:04<03:02,  3.42it/s]
100%|██████████| 811/811 [00:17<00:00, 45.89it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:10:43,611 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 13:10:43,941 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:10:48,438 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:10:48,727 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:10:48,884 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:20<1:49:44, 10.57s/it] 20%|██        | 158/780 [01:21<1:17:41,  7.49s/it] 20%|██        | 159/780 [01:21<55:11,  5.33s/it]   21%|██        | 160/780 [01:21<39:28,  3.82s/it] 21%|██        | 161/780 [01:22<28:29,  2.76s/it] 21%|██        | 162/780 [01:22<20:48,  2.02s/it] 21%|██        | 163/780 [01:22<15:26,  1.50s/it] 21%|██        | 164/780 [01:22<11:41,  1.14s/it] 21%|██        | 165/780 [01:23<09:03,  1.13it/s] 21%|██▏       | 166/780 [01:23<07:13,  1.42it/s] 21%|██▏       | 167/780 [01:23<05:56,  1.72it/s] 22%|██▏       | 168/780 [01:24<05:02,  2.02it/s] 22%|██▏       | 169/780 [01:24<04:30,  2.26it/s] 22%|██▏       | 170/780 [01:24<04:01,  2.52it/s] 22%|██▏       | 171/780 [01:25<03:42,  2.74it/s] 22%|██▏       | 172/780 [01:25<03:28,  2.92it/s] 22%|██▏       | 173/780 [01:25<03:18,  3.06it/s] 22%|██▏       | 174/780 [01:25<03:11,  3.16it/s] 22%|██▏       | 175/780 [01:26<03:06,  3.24it/s] 23%|██▎       | 176/780 [01:26<03:03,  3.30it/s] 23%|██▎       | 177/780 [01:26<03:00,  3.34it/s] 23%|██▎       | 178/780 [01:27<02:58,  3.37it/s] 23%|██▎       | 179/780 [01:27<02:57,  3.39it/s] 23%|██▎       | 180/780 [01:27<03:00,  3.33it/s] 23%|██▎       | 181/780 [01:27<02:58,  3.36it/s] 23%|██▎       | 182/780 [01:28<02:56,  3.38it/s] 23%|██▎       | 183/780 [01:28<02:55,  3.40it/s] 24%|██▎       | 184/780 [01:28<02:54,  3.41it/s] 24%|██▎       | 185/780 [01:29<02:53,  3.42it/s] 24%|██▍       | 186/780 [01:29<02:53,  3.43it/s] 24%|██▍       | 187/780 [01:29<02:52,  3.43it/s] 24%|██▍       | 188/780 [01:29<02:52,  3.43it/s] 24%|██▍       | 189/780 [01:30<02:52,  3.43it/s] 24%|██▍       | 190/780 [01:30<02:51,  3.44it/s] 24%|██▍       | 191/780 [01:30<03:00,  3.26it/s] 25%|██▍       | 192/780 [01:31<02:57,  3.31it/s] 25%|██▍       | 193/780 [01:31<02:55,  3.35it/s] 25%|██▍       | 194/780 [01:31<02:53,  3.37it/s] 25%|██▌       | 195/780 [01:32<02:52,  3.39it/s] 25%|██▌       | 196/780 [01:32<02:51,  3.40it/s] 25%|██▌       | 197/780 [01:32<02:50,  3.41it/s] 25%|██▌       | 198/780 [01:32<02:50,  3.41it/s] 26%|██▌       | 199/780 [01:33<02:50,  3.41it/s] 26%|██▌       | 200/780 [01:33<02:49,  3.41it/s] 26%|██▌       | 201/780 [01:33<02:49,  3.42it/s] 26%|██▌       | 202/780 [01:34<02:51,  3.37it/s] 26%|██▌       | 203/780 [01:34<02:50,  3.39it/s] 26%|██▌       | 204/780 [01:34<02:49,  3.40it/s] 26%|██▋       | 205/780 [01:34<02:48,  3.41it/s] 26%|██▋       | 206/780 [01:35<02:47,  3.42it/s] 27%|██▋       | 207/780 [01:35<02:47,  3.42it/s] 27%|██▋       | 208/780 [01:35<02:46,  3.43it/s] 27%|██▋       | 209/780 [01:36<02:46,  3.43it/s] 27%|██▋       | 210/780 [01:36<02:46,  3.43it/s] 27%|██▋       | 211/780 [01:36<02:45,  3.43it/s] 27%|██▋       | 212/780 [01:37<02:45,  3.43it/s] 27%|██▋       | 213/780 [01:37<02:51,  3.30it/s] 27%|██▋       | 214/780 [01:37<02:49,  3.34it/s] 28%|██▊       | 215/780 [01:37<02:47,  3.37it/s] 28%|██▊       | 216/780 [01:38<02:46,  3.39it/s] 28%|██▊       | 217/780 [01:38<02:45,  3.40it/s] 28%|██▊       | 218/780 [01:38<02:44,  3.41it/s] 28%|██▊       | 219/780 [01:39<02:44,  3.42it/s] 28%|██▊       | 220/780 [01:39<02:43,  3.42it/s] 28%|██▊       | 221/780 [01:39<02:43,  3.42it/s] 28%|██▊       | 222/780 [01:39<02:42,  3.43it/s] 29%|██▊       | 223/780 [01:40<02:42,  3.43it/s] 29%|██▊       | 224/780 [01:40<02:55,  3.17it/s] 29%|██▉       | 225/780 [01:40<02:51,  3.24it/s] 29%|██▉       | 226/780 [01:41<02:48,  3.29it/s] 29%|██▉       | 227/780 [01:41<02:45,  3.33it/s] 29%|██▉       | 228/780 [01:41<02:44,  3.36it/s] 29%|██▉       | 229/780 [01:42<02:42,  3.38it/s] 29%|██▉       | 230/780 [01:42<02:41,  3.40it/s] 30%|██▉       | 231/780 [01:42<03:22,  2.72it/s] 30%|██▉       | 232/780 [01:43<03:09,  2.89it/s] 30%|██▉       | 233/780 [01:43<03:00,  3.03it/s] 30%|███       | 234/780 [01:43<02:56,  3.09it/s] 30%|███       | 235/780 [01:44<02:51,  3.19it/s] 30%|███       | 236/780 [01:44<02:47,  3.26it/s] 30%|███       | 237/780 [01:44<02:44,  3.31it/s] 31%|███       | 238/780 [01:45<02:42,  3.34it/s] 31%|███       | 239/780 [01:45<02:40,  3.37it/s] 31%|███       | 240/780 [01:45<02:39,  3.39it/s] 31%|███       | 241/780 [01:45<02:38,  3.40it/s] 31%|███       | 242/780 [01:46<02:37,  3.41it/s] 31%|███       | 243/780 [01:46<02:37,  3.42it/s] 31%|███▏      | 244/780 [01:46<02:36,  3.42it/s] 31%|███▏      | 245/780 [01:47<02:36,  3.42it/s] 32%|███▏      | 246/780 [01:47<02:36,  3.42it/s] 32%|███▏      | 247/780 [01:47<02:35,  3.42it/s] 32%|███▏      | 248/780 [01:47<02:35,  3.42it/s] 32%|███▏      | 249/780 [01:48<02:34,  3.43it/s] 32%|███▏      | 250/780 [01:48<02:37,  3.37it/s] 32%|███▏      | 251/780 [01:48<02:36,  3.38it/s] 32%|███▏      | 252/780 [01:49<02:35,  3.40it/s] 32%|███▏      | 253/780 [01:49<02:34,  3.41it/s] 33%|███▎      | 254/780 [01:49<02:33,  3.42it/s] 33%|███▎      | 255/780 [01:49<02:33,  3.42it/s] 33%|███▎      | 256/780 [01:50<02:33,  3.42it/s] 33%|███▎      | 257/780 [01:50<02:32,  3.42it/s] 33%|███▎      | 258/780 [01:50<02:32,  3.43it/s] 33%|███▎      | 259/780 [01:51<02:32,  3.42it/s] 33%|███▎      | 260/780 [01:51<02:31,  3.43it/s] 33%|███▎      | 261/780 [01:51<02:34,  3.35it/s] 34%|███▎      | 262/780 [01:52<02:33,  3.38it/s] 34%|███▎      | 263/780 [01:52<02:32,  3.39it/s] 34%|███▍      | 264/780 [01:52<02:31,  3.41it/s] 34%|███▍      | 265/780 [01:52<02:30,  3.41it/s] 34%|███▍      | 266/780 [01:53<02:30,  3.42it/s] 34%|███▍      | 267/780 [01:53<02:29,  3.43it/s] 34%|███▍      | 268/780 [01:53<02:29,  3.43it/s] 34%|███▍      | 269/780 [01:54<02:28,  3.43it/s] 35%|███▍      | 270/780 [01:54<02:28,  3.43it/s] 35%|███▍      | 271/780 [01:54<02:28,  3.43it/s] 35%|███▍      | 272/780 [01:54<02:33,  3.31it/s] 35%|███▌      | 273/780 [01:55<02:31,  3.35it/s] 35%|███▌      | 274/780 [01:55<02:29,  3.37it/s] 35%|███▌      | 275/780 [01:55<02:28,  3.39it/s] 35%|███▌      | 276/780 [01:56<02:28,  3.40it/s] 36%|███▌      | 277/780 [01:56<02:27,  3.41it/s] 36%|███▌      | 278/780 [01:56<02:26,  3.42it/s] 36%|███▌      | 279/780 [01:57<02:26,  3.42it/s] 36%|███▌      | 280/780 [01:57<02:26,  3.42it/s] 36%|███▌      | 281/780 [01:57<02:25,  3.42it/s] 36%|███▌      | 282/780 [01:57<02:25,  3.43it/s] 36%|███▋      | 283/780 [01:58<02:32,  3.26it/s] 36%|███▋      | 284/780 [01:58<02:29,  3.31it/s] 37%|███▋      | 285/780 [01:58<02:27,  3.35it/s] 37%|███▋      | 286/780 [01:59<02:26,  3.37it/s] 37%|███▋      | 287/780 [01:59<02:25,  3.39it/s] 37%|███▋      | 288/780 [01:59<02:24,  3.40it/s] 37%|███▋      | 289/780 [01:59<02:23,  3.41it/s] 37%|███▋      | 290/780 [02:00<02:23,  3.41it/s] 37%|███▋      | 291/780 [02:00<02:23,  3.42it/s] 37%|███▋      | 292/780 [02:00<02:22,  3.42it/s] 38%|███▊      | 293/780 [02:01<02:22,  3.42it/s] 38%|███▊      | 294/780 [02:01<02:26,  3.31it/s] 38%|███▊      | 295/780 [02:01<02:24,  3.35it/s] 38%|███▊      | 296/780 [02:02<02:23,  3.37it/s] 38%|███▊      | 297/780 [02:02<02:22,  3.39it/s] 38%|███▊      | 298/780 [02:02<02:21,  3.40it/s] 38%|███▊      | 299/780 [02:02<02:20,  3.41it/s] 38%|███▊      | 300/780 [02:03<02:20,  3.42it/s] 39%|███▊      | 301/780 [02:03<02:19,  3.42it/s] 39%|███▊      | 302/780 [02:03<02:19,  3.43it/s] 39%|███▉      | 303/780 [02:04<02:19,  3.43it/s] 39%|███▉      | 304/780 [02:04<02:18,  3.43it/s] 39%|███▉      | 305/780 [02:04<02:23,  3.32it/s] 39%|███▉      | 306/780 [02:05<02:21,  3.35it/s] 39%|███▉      | 307/780 [02:05<02:20,  3.37it/s] 39%|███▉      | 308/780 [02:05<02:19,  3.39it/s] 40%|███▉      | 309/780 [02:05<02:18,  3.40it/s] 40%|███▉      | 310/780 [02:06<02:17,  3.41it/s] 40%|███▉      | 311/780 [02:06<02:17,  3.42it/s] 40%|████      | 312/780 [02:06<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 13:11:45,701 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:11:45,702 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 13:11:45,702 >>   Batch size = 8
{'eval_loss': 0.9258166551589966, 'eval_runtime': 17.7374, 'eval_samples_per_second': 365.724, 'eval_steps_per_second': 45.723, 'epoch': 1.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 57.24it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.45it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.48it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.70it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.17it/s][A
  4%|▍         | 33/811 [00:00<00:18, 42.47it/s][A
  5%|▍         | 38/811 [00:00<00:17, 43.59it/s][A
  5%|▌         | 43/811 [00:00<00:17, 44.43it/s][A
  6%|▌         | 48/811 [00:01<00:16, 45.02it/s][A
  7%|▋         | 53/811 [00:01<00:16, 45.36it/s][A
  7%|▋         | 58/811 [00:01<00:16, 45.69it/s][A
  8%|▊         | 63/811 [00:01<00:16, 45.94it/s][A
  8%|▊         | 68/811 [00:01<00:16, 45.88it/s][A
  9%|▉         | 73/811 [00:01<00:16, 45.51it/s][A
 10%|▉         | 78/811 [00:01<00:16, 45.76it/s][A
 10%|█         | 83/811 [00:01<00:15, 45.95it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.02it/s][A
 11%|█▏        | 93/811 [00:02<00:15, 46.05it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.12it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.11it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.21it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 46.12it/s][A
 15%|█▍        | 118/811 [00:02<00:15, 45.96it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 45.92it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.06it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.03it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.15it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 46.29it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.35it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.30it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.22it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.15it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.10it/s][A
 21%|██▏       | 173/811 [00:03<00:16, 39.38it/s][A
 22%|██▏       | 178/811 [00:03<00:15, 41.27it/s][A
 23%|██▎       | 183/811 [00:04<00:14, 42.69it/s][A
 23%|██▎       | 188/811 [00:04<00:14, 43.76it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 44.57it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 45.04it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 45.46it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 45.71it/s][A
 26%|██▋       | 213/811 [00:04<00:13, 45.52it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 45.65it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 45.74it/s][A
 28%|██▊       | 228/811 [00:05<00:12, 46.00it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 45.94it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.19it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.22it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.28it/s][A
 31%|███       | 253/811 [00:05<00:12, 46.09it/s][A
 32%|███▏      | 258/811 [00:05<00:12, 46.00it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.02it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.07it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.15it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.14it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.13it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.22it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.25it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.28it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 46.14it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.12it/s][A
 39%|███▊      | 313/811 [00:06<00:12, 40.73it/s][A
 39%|███▉      | 318/811 [00:07<00:11, 42.31it/s][A
 40%|███▉      | 323/811 [00:07<00:11, 43.37it/s][A
 40%|████      | 328/811 [00:07<00:10, 44.25it/s][A
 41%|████      | 333/811 [00:07<00:10, 44.82it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 45.31it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 45.59it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 45.81it/s][A
 44%|████▎     | 353/811 [00:07<00:10, 45.60it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 45.67it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 45.88it/s][A
 45%|████▌     | 368/811 [00:08<00:09, 44.48it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 45.08it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 45.45it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 45.71it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 45.91it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 46.08it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 45.96it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 45.95it/s][A
 50%|█████     | 408/811 [00:08<00:08, 45.98it/s][A
 51%|█████     | 413/811 [00:09<00:08, 45.92it/s][A
 52%|█████▏    | 418/811 [00:09<00:09, 43.66it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 44.47it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 44.89it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 45.30it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 45.62it/s][A
 55%|█████▍    | 443/811 [00:09<00:08, 45.86it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 46.04it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 45.99it/s][A
 56%|█████▋    | 458/811 [00:10<00:07, 45.77it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 45.89it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 45.96it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 45.99it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 45.96it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.07it/s][A
 60%|██████    | 488/811 [00:10<00:07, 46.13it/s][A
 61%|██████    | 493/811 [00:10<00:06, 46.26it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 46.26it/s][A
 62%|██████▏   | 503/811 [00:11<00:06, 46.04it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 45.93it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 45.99it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 46.11it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.05it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.07it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.21it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.24it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 46.15it/s][A
 68%|██████▊   | 548/811 [00:12<00:05, 46.09it/s][A
 68%|██████▊   | 553/811 [00:12<00:05, 46.06it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 44.55it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 45.04it/s][A
 70%|███████   | 568/811 [00:12<00:05, 45.38it/s][A
 71%|███████   | 573/811 [00:12<00:05, 45.64it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 45.72it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 45.84it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 45.95it/s][A
 73%|███████▎  | 593/811 [00:13<00:04, 45.95it/s][A
 74%|███████▎  | 598/811 [00:13<00:04, 45.76it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 45.80it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 45.98it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.05it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 45.95it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.05it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 46.02it/s][A
 78%|███████▊  | 633/811 [00:13<00:03, 46.08it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 46.06it/s][A
 79%|███████▉  | 643/811 [00:14<00:03, 45.90it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 45.93it/s][A
 81%|████████  | 653/811 [00:14<00:03, 46.02it/s][A
 81%|████████  | 658/811 [00:14<00:03, 46.08it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 46.00it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 45.90it/s][A
 83%|████████▎ | 673/811 [00:14<00:02, 46.07it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 46.04it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 46.06it/s][A
 85%|████████▍ | 688/811 [00:15<00:02, 46.01it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 45.91it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 45.04it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 45.35it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 45.61it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 45.68it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 45.85it/s][A
 89%|████████▉ | 723/811 [00:15<00:01, 45.90it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 45.87it/s][A
 90%|█████████ | 733/811 [00:16<00:01, 45.95it/s][A
 91%|█████████ | 738/811 [00:16<00:01, 45.87it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 45.88it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 45.93it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 46.06it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 46.06it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 45.99it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 46.11it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 46.04it/s][A
 96%|█████████▌| 778/811 [00:17<00:00, 45.97it/s][A
 97%|█████████▋| 783/811 [00:17<00:00, 45.99it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 46.02it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 45.92it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 45.97it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 46.10it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 46.12it/s][A                                                 
                                                 [A 40%|████      | 312/780 [02:24<02:16,  3.42it/s]
100%|██████████| 811/811 [00:17<00:00, 46.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:12:03,717 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 13:12:03,884 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:12:08,794 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:12:09,027 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:12:09,128 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:38<1:16:48,  9.87s/it] 40%|████      | 314/780 [02:39<54:24,  7.00s/it]   40%|████      | 315/780 [02:39<38:40,  4.99s/it] 41%|████      | 316/780 [02:39<27:41,  3.58s/it] 41%|████      | 317/780 [02:40<20:00,  2.59s/it] 41%|████      | 318/780 [02:40<14:38,  1.90s/it] 41%|████      | 319/780 [02:40<10:57,  1.43s/it] 41%|████      | 320/780 [02:41<08:19,  1.09s/it] 41%|████      | 321/780 [02:41<06:29,  1.18it/s] 41%|████▏     | 322/780 [02:41<05:11,  1.47it/s] 41%|████▏     | 323/780 [02:41<04:17,  1.77it/s] 42%|████▏     | 324/780 [02:42<03:39,  2.07it/s] 42%|████▏     | 325/780 [02:42<03:17,  2.30it/s] 42%|████▏     | 326/780 [02:43<03:37,  2.08it/s] 42%|████▏     | 327/780 [02:43<03:12,  2.36it/s] 42%|████▏     | 328/780 [02:43<02:53,  2.60it/s] 42%|████▏     | 329/780 [02:44<02:40,  2.81it/s] 42%|████▏     | 330/780 [02:44<02:31,  2.97it/s] 42%|████▏     | 331/780 [02:44<02:24,  3.10it/s] 43%|████▎     | 332/780 [02:44<02:20,  3.19it/s] 43%|████▎     | 333/780 [02:45<02:17,  3.26it/s] 43%|████▎     | 334/780 [02:45<02:14,  3.31it/s] 43%|████▎     | 335/780 [02:45<02:18,  3.22it/s] 43%|████▎     | 336/780 [02:46<02:15,  3.28it/s] 43%|████▎     | 337/780 [02:46<02:13,  3.33it/s] 43%|████▎     | 338/780 [02:46<02:11,  3.36it/s] 43%|████▎     | 339/780 [02:46<02:11,  3.36it/s] 44%|████▎     | 340/780 [02:47<02:10,  3.38it/s] 44%|████▎     | 341/780 [02:47<02:09,  3.40it/s] 44%|████▍     | 342/780 [02:47<02:08,  3.41it/s] 44%|████▍     | 343/780 [02:48<02:07,  3.41it/s] 44%|████▍     | 344/780 [02:48<02:07,  3.42it/s] 44%|████▍     | 345/780 [02:48<02:06,  3.43it/s] 44%|████▍     | 346/780 [02:49<02:06,  3.42it/s] 44%|████▍     | 347/780 [02:49<02:06,  3.43it/s] 45%|████▍     | 348/780 [02:49<02:05,  3.43it/s] 45%|████▍     | 349/780 [02:49<02:05,  3.43it/s] 45%|████▍     | 350/780 [02:50<02:09,  3.32it/s] 45%|████▌     | 351/780 [02:50<02:07,  3.35it/s] 45%|████▌     | 352/780 [02:50<02:06,  3.38it/s] 45%|████▌     | 353/780 [02:51<02:05,  3.39it/s] 45%|████▌     | 354/780 [02:51<02:05,  3.41it/s] 46%|████▌     | 355/780 [02:51<02:04,  3.41it/s] 46%|████▌     | 356/780 [02:51<02:04,  3.42it/s] 46%|████▌     | 357/780 [02:52<02:03,  3.42it/s] 46%|████▌     | 358/780 [02:52<02:03,  3.43it/s] 46%|████▌     | 359/780 [02:52<02:02,  3.43it/s] 46%|████▌     | 360/780 [02:53<02:02,  3.43it/s] 46%|████▋     | 361/780 [02:53<02:06,  3.32it/s] 46%|████▋     | 362/780 [02:53<02:04,  3.36it/s] 47%|████▋     | 363/780 [02:54<02:03,  3.37it/s] 47%|████▋     | 364/780 [02:54<02:02,  3.39it/s] 47%|████▋     | 365/780 [02:54<02:01,  3.40it/s] 47%|████▋     | 366/780 [02:54<02:01,  3.41it/s] 47%|████▋     | 367/780 [02:55<02:01,  3.41it/s] 47%|████▋     | 368/780 [02:55<02:00,  3.42it/s] 47%|████▋     | 369/780 [02:55<01:59,  3.43it/s] 47%|████▋     | 370/780 [02:56<01:59,  3.43it/s] 48%|████▊     | 371/780 [02:56<01:59,  3.43it/s] 48%|████▊     | 372/780 [02:56<02:04,  3.28it/s] 48%|████▊     | 373/780 [02:56<02:02,  3.33it/s] 48%|████▊     | 374/780 [02:57<02:00,  3.36it/s] 48%|████▊     | 375/780 [02:57<01:59,  3.38it/s] 48%|████▊     | 376/780 [02:57<01:59,  3.39it/s] 48%|████▊     | 377/780 [02:58<01:58,  3.40it/s] 48%|████▊     | 378/780 [02:58<01:57,  3.41it/s] 49%|████▊     | 379/780 [02:58<01:57,  3.42it/s] 49%|████▊     | 380/780 [02:59<01:56,  3.42it/s] 49%|████▉     | 381/780 [02:59<01:56,  3.43it/s] 49%|████▉     | 382/780 [02:59<01:56,  3.43it/s] 49%|████▉     | 383/780 [02:59<02:01,  3.27it/s] 49%|████▉     | 384/780 [03:00<01:59,  3.31it/s] 49%|████▉     | 385/780 [03:00<01:57,  3.35it/s] 49%|████▉     | 386/780 [03:00<01:56,  3.37it/s] 50%|████▉     | 387/780 [03:01<01:55,  3.39it/s] 50%|████▉     | 388/780 [03:01<01:55,  3.40it/s] 50%|████▉     | 389/780 [03:01<01:54,  3.41it/s] 50%|█████     | 390/780 [03:01<01:54,  3.42it/s] 50%|█████     | 391/780 [03:02<01:53,  3.42it/s] 50%|█████     | 392/780 [03:02<01:53,  3.43it/s] 50%|█████     | 393/780 [03:02<01:52,  3.43it/s] 51%|█████     | 394/780 [03:03<01:57,  3.29it/s] 51%|█████     | 395/780 [03:03<01:55,  3.33it/s] 51%|█████     | 396/780 [03:03<01:54,  3.36it/s] 51%|█████     | 397/780 [03:04<01:53,  3.38it/s] 51%|█████     | 398/780 [03:04<01:52,  3.39it/s] 51%|█████     | 399/780 [03:04<01:51,  3.40it/s] 51%|█████▏    | 400/780 [03:04<01:51,  3.41it/s] 51%|█████▏    | 401/780 [03:05<01:50,  3.42it/s] 52%|█████▏    | 402/780 [03:05<01:50,  3.42it/s] 52%|█████▏    | 403/780 [03:05<01:50,  3.42it/s] 52%|█████▏    | 404/780 [03:06<01:49,  3.43it/s] 52%|█████▏    | 405/780 [03:06<01:57,  3.19it/s] 52%|█████▏    | 406/780 [03:06<01:54,  3.26it/s] 52%|█████▏    | 407/780 [03:07<01:52,  3.31it/s] 52%|█████▏    | 408/780 [03:07<01:51,  3.35it/s] 52%|█████▏    | 409/780 [03:07<01:49,  3.37it/s] 53%|█████▎    | 410/780 [03:07<01:49,  3.39it/s] 53%|█████▎    | 411/780 [03:08<01:48,  3.41it/s] 53%|█████▎    | 412/780 [03:08<01:47,  3.42it/s] 53%|█████▎    | 413/780 [03:08<01:47,  3.42it/s] 53%|█████▎    | 414/780 [03:09<01:46,  3.43it/s] 53%|█████▎    | 415/780 [03:09<01:46,  3.43it/s] 53%|█████▎    | 416/780 [03:09<01:53,  3.20it/s] 53%|█████▎    | 417/780 [03:10<01:51,  3.27it/s] 54%|█████▎    | 418/780 [03:10<01:49,  3.32it/s] 54%|█████▎    | 419/780 [03:10<01:47,  3.35it/s] 54%|█████▍    | 420/780 [03:10<01:46,  3.37it/s] 54%|█████▍    | 421/780 [03:11<01:45,  3.39it/s] 54%|█████▍    | 422/780 [03:11<01:45,  3.40it/s] 54%|█████▍    | 423/780 [03:11<01:44,  3.41it/s] 54%|█████▍    | 424/780 [03:12<01:44,  3.42it/s] 54%|█████▍    | 425/780 [03:12<01:43,  3.42it/s] 55%|█████▍    | 426/780 [03:12<01:43,  3.42it/s] 55%|█████▍    | 427/780 [03:13<01:50,  3.19it/s] 55%|█████▍    | 428/780 [03:13<01:47,  3.26it/s] 55%|█████▌    | 429/780 [03:13<01:46,  3.31it/s] 55%|█████▌    | 430/780 [03:13<01:44,  3.35it/s] 55%|█████▌    | 431/780 [03:14<01:43,  3.37it/s] 55%|█████▌    | 432/780 [03:14<01:42,  3.39it/s] 56%|█████▌    | 433/780 [03:14<01:42,  3.40it/s] 56%|█████▌    | 434/780 [03:15<01:41,  3.41it/s] 56%|█████▌    | 435/780 [03:15<01:40,  3.42it/s] 56%|█████▌    | 436/780 [03:15<01:40,  3.42it/s] 56%|█████▌    | 437/780 [03:15<01:40,  3.42it/s] 56%|█████▌    | 438/780 [03:16<01:46,  3.20it/s] 56%|█████▋    | 439/780 [03:16<01:44,  3.27it/s] 56%|█████▋    | 440/780 [03:16<01:42,  3.31it/s] 57%|█████▋    | 441/780 [03:17<01:41,  3.35it/s] 57%|█████▋    | 442/780 [03:17<01:40,  3.37it/s] 57%|█████▋    | 443/780 [03:17<01:39,  3.39it/s] 57%|█████▋    | 444/780 [03:18<01:38,  3.40it/s] 57%|█████▋    | 445/780 [03:18<01:38,  3.41it/s] 57%|█████▋    | 446/780 [03:18<01:37,  3.42it/s] 57%|█████▋    | 447/780 [03:18<01:37,  3.42it/s] 57%|█████▋    | 448/780 [03:19<01:36,  3.43it/s] 58%|█████▊    | 449/780 [03:19<01:36,  3.43it/s] 58%|█████▊    | 450/780 [03:19<01:36,  3.43it/s] 58%|█████▊    | 451/780 [03:20<01:35,  3.44it/s] 58%|█████▊    | 452/780 [03:20<01:35,  3.44it/s] 58%|█████▊    | 453/780 [03:20<01:37,  3.37it/s] 58%|█████▊    | 454/780 [03:20<01:36,  3.39it/s] 58%|█████▊    | 455/780 [03:21<01:35,  3.40it/s] 58%|█████▊    | 456/780 [03:21<01:34,  3.41it/s] 59%|█████▊    | 457/780 [03:21<01:34,  3.42it/s] 59%|█████▊    | 458/780 [03:22<01:34,  3.42it/s] 59%|█████▉    | 459/780 [03:22<01:33,  3.43it/s] 59%|█████▉    | 460/780 [03:22<01:33,  3.43it/s] 59%|█████▉    | 461/780 [03:23<01:32,  3.43it/s] 59%|█████▉    | 462/780 [03:23<01:32,  3.43it/s] 59%|█████▉    | 463/780 [03:23<01:32,  3.43it/s] 59%|█████▉    | 464/780 [03:23<01:34,  3.34it/s] 60%|█████▉    | 465/780 [03:24<01:33,  3.37it/s] 60%|█████▉    | 466/780 [03:24<01:32,  3.39it/s] 60%|█████▉    | 467/780 [03:24<01:32,  3.40it/s] 60%|██████    | 468/780 [03:25<01:31,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 13:13:04,018 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:13:04,018 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 13:13:04,018 >>   Batch size = 8
{'eval_loss': 0.9432083964347839, 'eval_runtime': 17.8143, 'eval_samples_per_second': 364.145, 'eval_steps_per_second': 45.525, 'epoch': 2.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.98it/s][A
  1%|▏         | 12/811 [00:00<00:15, 50.37it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.40it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.66it/s][A
  3%|▎         | 28/811 [00:00<00:16, 46.95it/s][A
  4%|▍         | 33/811 [00:00<00:16, 46.60it/s][A
  5%|▍         | 38/811 [00:00<00:16, 46.23it/s][A
  5%|▌         | 43/811 [00:00<00:16, 45.89it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.03it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.10it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.14it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.21it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.27it/s][A
  9%|▉         | 73/811 [00:01<00:16, 43.95it/s][A
 10%|▉         | 78/811 [00:01<00:16, 44.71it/s][A
 10%|█         | 83/811 [00:01<00:16, 45.16it/s][A
 11%|█         | 88/811 [00:01<00:15, 45.49it/s][A
 11%|█▏        | 93/811 [00:02<00:15, 45.79it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 45.87it/s][A
 13%|█▎        | 103/811 [00:02<00:15, 46.10it/s][A
 13%|█▎        | 108/811 [00:02<00:15, 46.12it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 45.92it/s][A
 15%|█▍        | 118/811 [00:02<00:15, 45.95it/s][A
 15%|█▌        | 123/811 [00:02<00:14, 46.04it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 46.14it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 46.15it/s][A
 17%|█▋        | 138/811 [00:02<00:14, 46.20it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 46.16it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 46.25it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 46.26it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.07it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.11it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.04it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.05it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.12it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.15it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 46.21it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.25it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.26it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.13it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.00it/s][A
 26%|██▋       | 213/811 [00:04<00:13, 44.78it/s][A
 27%|██▋       | 218/811 [00:04<00:13, 45.33it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 45.62it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 45.76it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 45.96it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.04it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 46.06it/s][A
 31%|███       | 248/811 [00:05<00:12, 46.05it/s][A
 31%|███       | 253/811 [00:05<00:12, 45.91it/s][A
 32%|███▏      | 258/811 [00:05<00:12, 45.97it/s][A
 32%|███▏      | 263/811 [00:05<00:11, 46.03it/s][A
 33%|███▎      | 268/811 [00:05<00:11, 46.19it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 46.29it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 46.18it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 46.25it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 46.27it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 46.12it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 46.05it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 45.97it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 46.04it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.16it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 46.20it/s][A
 40%|███▉      | 323/811 [00:07<00:10, 46.27it/s][A
 40%|████      | 328/811 [00:07<00:10, 46.19it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.22it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.16it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 46.06it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 45.99it/s][A
 44%|████▎     | 353/811 [00:07<00:10, 43.53it/s][A
 44%|████▍     | 358/811 [00:07<00:10, 44.33it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 44.85it/s][A
 45%|████▌     | 368/811 [00:07<00:09, 45.34it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 45.63it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 45.82it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 45.98it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.12it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 45.67it/s][A
 49%|████▉     | 398/811 [00:08<00:09, 45.82it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 45.96it/s][A
 50%|█████     | 408/811 [00:08<00:08, 45.99it/s][A
 51%|█████     | 413/811 [00:08<00:08, 46.10it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.23it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.19it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.25it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.18it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 46.00it/s][A
 55%|█████▍    | 443/811 [00:09<00:08, 45.94it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 45.98it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 46.07it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 46.19it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.16it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.19it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 46.24it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 46.24it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 46.06it/s][A
 60%|██████    | 488/811 [00:10<00:07, 46.08it/s][A
 61%|██████    | 493/811 [00:10<00:07, 43.20it/s][A
 61%|██████▏   | 498/811 [00:10<00:07, 44.10it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 44.81it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 45.26it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 45.52it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 45.83it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 45.87it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 46.04it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 45.77it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 45.69it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 45.88it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.03it/s][A
 68%|██████▊   | 553/811 [00:12<00:05, 46.02it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 46.19it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 46.17it/s][A
 70%|███████   | 568/811 [00:12<00:05, 46.28it/s][A
 71%|███████   | 573/811 [00:12<00:05, 46.19it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 45.98it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 45.91it/s][A
 73%|███████▎  | 588/811 [00:12<00:04, 46.00it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 46.11it/s][A
 74%|███████▎  | 598/811 [00:13<00:04, 46.13it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 46.11it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 46.25it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 46.21it/s][A
 76%|███████▌  | 618/811 [00:13<00:04, 46.17it/s][A
 77%|███████▋  | 623/811 [00:13<00:04, 46.10it/s][A
 77%|███████▋  | 628/811 [00:13<00:03, 45.97it/s][A
 78%|███████▊  | 633/811 [00:13<00:04, 43.85it/s][A
 79%|███████▊  | 638/811 [00:13<00:03, 44.51it/s][A
 79%|███████▉  | 643/811 [00:14<00:03, 45.06it/s][A
 80%|███████▉  | 648/811 [00:14<00:03, 45.38it/s][A
 81%|████████  | 653/811 [00:14<00:03, 45.68it/s][A
 81%|████████  | 658/811 [00:14<00:03, 45.87it/s][A
 82%|████████▏ | 663/811 [00:14<00:03, 45.97it/s][A
 82%|████████▏ | 668/811 [00:14<00:03, 45.91it/s][A
 83%|████████▎ | 673/811 [00:14<00:03, 45.74it/s][A
 84%|████████▎ | 678/811 [00:14<00:02, 45.81it/s][A
 84%|████████▍ | 683/811 [00:14<00:02, 45.85it/s][A
 85%|████████▍ | 688/811 [00:14<00:02, 45.95it/s][A
 85%|████████▌ | 693/811 [00:15<00:02, 45.94it/s][A
 86%|████████▌ | 698/811 [00:15<00:02, 46.12it/s][A
 87%|████████▋ | 703/811 [00:15<00:02, 46.16it/s][A
 87%|████████▋ | 708/811 [00:15<00:02, 46.10it/s][A
 88%|████████▊ | 713/811 [00:15<00:02, 46.05it/s][A
 89%|████████▊ | 718/811 [00:15<00:02, 42.58it/s][A
 89%|████████▉ | 723/811 [00:15<00:02, 43.62it/s][A
 90%|████████▉ | 728/811 [00:15<00:01, 44.40it/s][A
 90%|█████████ | 733/811 [00:15<00:01, 44.92it/s][A
 91%|█████████ | 738/811 [00:16<00:01, 45.20it/s][A
 92%|█████████▏| 743/811 [00:16<00:01, 45.46it/s][A
 92%|█████████▏| 748/811 [00:16<00:01, 45.64it/s][A
 93%|█████████▎| 753/811 [00:16<00:01, 45.89it/s][A
 93%|█████████▎| 758/811 [00:16<00:01, 45.66it/s][A
 94%|█████████▍| 763/811 [00:16<00:01, 45.63it/s][A
 95%|█████████▍| 768/811 [00:16<00:00, 45.71it/s][A
 95%|█████████▌| 773/811 [00:16<00:00, 44.30it/s][A
 96%|█████████▌| 778/811 [00:16<00:00, 44.85it/s][A
 97%|█████████▋| 783/811 [00:17<00:00, 45.33it/s][A
 97%|█████████▋| 788/811 [00:17<00:00, 45.55it/s][A
 98%|█████████▊| 793/811 [00:17<00:00, 45.74it/s][A
 98%|█████████▊| 798/811 [00:17<00:00, 45.90it/s][A
 99%|█████████▉| 803/811 [00:17<00:00, 45.97it/s][A
100%|█████████▉| 808/811 [00:17<00:00, 45.88it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [03:43<01:31,  3.41it/s]
100%|██████████| 811/811 [00:17<00:00, 45.88it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:13:22,206 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 13:13:22,377 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:13:26,106 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:13:26,261 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:13:26,330 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:56<50:04,  9.66s/it] 60%|██████    | 470/780 [03:56<35:26,  6.86s/it] 60%|██████    | 471/780 [03:57<25:10,  4.89s/it] 61%|██████    | 472/780 [03:57<18:01,  3.51s/it] 61%|██████    | 473/780 [03:57<13:01,  2.54s/it] 61%|██████    | 474/780 [03:58<09:31,  1.87s/it] 61%|██████    | 475/780 [03:58<07:05,  1.40s/it] 61%|██████    | 476/780 [03:58<05:23,  1.06s/it] 61%|██████    | 477/780 [03:58<04:12,  1.20it/s] 61%|██████▏   | 478/780 [03:59<03:22,  1.49it/s] 61%|██████▏   | 479/780 [03:59<02:47,  1.80it/s] 62%|██████▏   | 480/780 [03:59<02:22,  2.10it/s] 62%|██████▏   | 481/780 [04:00<02:09,  2.31it/s] 62%|██████▏   | 482/780 [04:00<01:56,  2.57it/s] 62%|██████▏   | 483/780 [04:00<01:46,  2.78it/s] 62%|██████▏   | 484/780 [04:01<01:40,  2.95it/s] 62%|██████▏   | 485/780 [04:01<01:35,  3.08it/s] 62%|██████▏   | 486/780 [04:01<01:32,  3.18it/s] 62%|██████▏   | 487/780 [04:01<01:30,  3.25it/s] 63%|██████▎   | 488/780 [04:02<01:28,  3.30it/s] 63%|██████▎   | 489/780 [04:02<01:27,  3.34it/s] 63%|██████▎   | 490/780 [04:02<01:26,  3.37it/s] 63%|██████▎   | 491/780 [04:03<01:25,  3.39it/s] 63%|██████▎   | 492/780 [04:03<01:27,  3.28it/s] 63%|██████▎   | 493/780 [04:03<01:26,  3.32it/s] 63%|██████▎   | 494/780 [04:03<01:25,  3.35it/s] 63%|██████▎   | 495/780 [04:04<01:24,  3.38it/s] 64%|██████▎   | 496/780 [04:04<01:23,  3.39it/s] 64%|██████▎   | 497/780 [04:04<01:23,  3.40it/s] 64%|██████▍   | 498/780 [04:05<01:22,  3.41it/s] 64%|██████▍   | 499/780 [04:05<01:22,  3.42it/s] 64%|██████▍   | 500/780 [04:05<01:21,  3.42it/s]                                                  64%|██████▍   | 500/780 [04:05<01:21,  3.42it/s] 64%|██████▍   | 501/780 [04:06<01:21,  3.42it/s] 64%|██████▍   | 502/780 [04:06<01:21,  3.42it/s] 64%|██████▍   | 503/780 [04:06<01:22,  3.35it/s] 65%|██████▍   | 504/780 [04:06<01:21,  3.37it/s] 65%|██████▍   | 505/780 [04:07<01:21,  3.39it/s] 65%|██████▍   | 506/780 [04:07<01:20,  3.40it/s] 65%|██████▌   | 507/780 [04:07<01:20,  3.41it/s] 65%|██████▌   | 508/780 [04:08<01:19,  3.42it/s] 65%|██████▌   | 509/780 [04:08<01:19,  3.42it/s] 65%|██████▌   | 510/780 [04:08<01:18,  3.43it/s] 66%|██████▌   | 511/780 [04:08<01:18,  3.43it/s] 66%|██████▌   | 512/780 [04:09<01:18,  3.43it/s] 66%|██████▌   | 513/780 [04:09<01:17,  3.43it/s] 66%|██████▌   | 514/780 [04:09<01:22,  3.21it/s] 66%|██████▌   | 515/780 [04:10<01:20,  3.28it/s] 66%|██████▌   | 516/780 [04:10<01:19,  3.32it/s] 66%|██████▋   | 517/780 [04:10<01:18,  3.36it/s] 66%|██████▋   | 518/780 [04:11<01:17,  3.38it/s] 67%|██████▋   | 519/780 [04:11<01:16,  3.39it/s] 67%|██████▋   | 520/780 [04:11<01:16,  3.40it/s] 67%|██████▋   | 521/780 [04:11<01:15,  3.41it/s] 67%|██████▋   | 522/780 [04:12<01:15,  3.42it/s] 67%|██████▋   | 523/780 [04:12<01:15,  3.42it/s] 67%|██████▋   | 524/780 [04:12<01:14,  3.43it/s] 67%|██████▋   | 525/780 [04:13<01:19,  3.21it/s] 67%|██████▋   | 526/780 [04:13<01:17,  3.27it/s] 68%|██████▊   | 527/780 [04:13<01:16,  3.32it/s] 68%|██████▊   | 528/780 [04:14<01:15,  3.35it/s] 68%|██████▊   | 529/780 [04:14<01:14,  3.38it/s] 68%|██████▊   | 530/780 [04:14<01:13,  3.39it/s] 68%|██████▊   | 531/780 [04:14<01:13,  3.40it/s] 68%|██████▊   | 532/780 [04:15<01:12,  3.41it/s] 68%|██████▊   | 533/780 [04:15<01:12,  3.42it/s] 68%|██████▊   | 534/780 [04:15<01:11,  3.42it/s] 69%|██████▊   | 535/780 [04:16<01:11,  3.43it/s] 69%|██████▊   | 536/780 [04:16<01:16,  3.20it/s] 69%|██████▉   | 537/780 [04:16<01:14,  3.26it/s] 69%|██████▉   | 538/780 [04:17<01:13,  3.31it/s] 69%|██████▉   | 539/780 [04:17<01:12,  3.35it/s] 69%|██████▉   | 540/780 [04:17<01:11,  3.37it/s] 69%|██████▉   | 541/780 [04:17<01:10,  3.39it/s] 69%|██████▉   | 542/780 [04:18<01:09,  3.40it/s] 70%|██████▉   | 543/780 [04:18<01:09,  3.41it/s] 70%|██████▉   | 544/780 [04:18<01:09,  3.42it/s] 70%|██████▉   | 545/780 [04:19<01:08,  3.42it/s] 70%|███████   | 546/780 [04:19<01:08,  3.42it/s] 70%|███████   | 547/780 [04:19<01:07,  3.43it/s] 70%|███████   | 548/780 [04:19<01:07,  3.43it/s] 70%|███████   | 549/780 [04:20<01:07,  3.42it/s] 71%|███████   | 550/780 [04:20<01:07,  3.43it/s] 71%|███████   | 551/780 [04:20<01:06,  3.43it/s] 71%|███████   | 552/780 [04:21<01:06,  3.43it/s] 71%|███████   | 553/780 [04:21<01:06,  3.43it/s] 71%|███████   | 554/780 [04:21<01:05,  3.43it/s] 71%|███████   | 555/780 [04:21<01:05,  3.43it/s] 71%|███████▏  | 556/780 [04:22<01:07,  3.30it/s] 71%|███████▏  | 557/780 [04:22<01:06,  3.34it/s] 72%|███████▏  | 558/780 [04:22<01:05,  3.36it/s] 72%|███████▏  | 559/780 [04:23<01:05,  3.39it/s] 72%|███████▏  | 560/780 [04:23<01:04,  3.40it/s] 72%|███████▏  | 561/780 [04:23<01:04,  3.41it/s] 72%|███████▏  | 562/780 [04:24<01:03,  3.41it/s] 72%|███████▏  | 563/780 [04:24<01:03,  3.42it/s] 72%|███████▏  | 564/780 [04:24<01:03,  3.42it/s] 72%|███████▏  | 565/780 [04:24<01:02,  3.42it/s] 73%|███████▎  | 566/780 [04:25<01:02,  3.42it/s] 73%|███████▎  | 567/780 [04:25<01:03,  3.36it/s] 73%|███████▎  | 568/780 [04:25<01:02,  3.38it/s] 73%|███████▎  | 569/780 [04:26<01:02,  3.39it/s] 73%|███████▎  | 570/780 [04:26<01:01,  3.40it/s] 73%|███████▎  | 571/780 [04:26<01:01,  3.41it/s] 73%|███████▎  | 572/780 [04:27<01:00,  3.42it/s] 73%|███████▎  | 573/780 [04:27<01:00,  3.42it/s] 74%|███████▎  | 574/780 [04:27<01:00,  3.42it/s] 74%|███████▎  | 575/780 [04:27<00:59,  3.42it/s] 74%|███████▍  | 576/780 [04:28<00:59,  3.43it/s] 74%|███████▍  | 577/780 [04:28<00:59,  3.43it/s] 74%|███████▍  | 578/780 [04:28<01:00,  3.34it/s] 74%|███████▍  | 579/780 [04:29<00:59,  3.36it/s] 74%|███████▍  | 580/780 [04:29<00:59,  3.38it/s] 74%|███████▍  | 581/780 [04:29<00:58,  3.40it/s] 75%|███████▍  | 582/780 [04:29<00:58,  3.41it/s] 75%|███████▍  | 583/780 [04:30<00:57,  3.42it/s] 75%|███████▍  | 584/780 [04:30<00:57,  3.42it/s] 75%|███████▌  | 585/780 [04:30<00:56,  3.42it/s] 75%|███████▌  | 586/780 [04:31<00:56,  3.43it/s] 75%|███████▌  | 587/780 [04:31<00:56,  3.43it/s] 75%|███████▌  | 588/780 [04:31<00:55,  3.43it/s] 76%|███████▌  | 589/780 [04:32<00:57,  3.34it/s] 76%|███████▌  | 590/780 [04:32<00:56,  3.37it/s] 76%|███████▌  | 591/780 [04:32<00:55,  3.39it/s] 76%|███████▌  | 592/780 [04:32<00:55,  3.40it/s] 76%|███████▌  | 593/780 [04:33<00:54,  3.41it/s] 76%|███████▌  | 594/780 [04:33<00:54,  3.42it/s] 76%|███████▋  | 595/780 [04:33<00:54,  3.42it/s] 76%|███████▋  | 596/780 [04:34<00:53,  3.43it/s] 77%|███████▋  | 597/780 [04:34<00:53,  3.43it/s] 77%|███████▋  | 598/780 [04:34<00:53,  3.43it/s] 77%|███████▋  | 599/780 [04:34<00:52,  3.43it/s] 77%|███████▋  | 600/780 [04:35<00:53,  3.35it/s] 77%|███████▋  | 601/780 [04:35<00:53,  3.37it/s] 77%|███████▋  | 602/780 [04:35<00:52,  3.39it/s] 77%|███████▋  | 603/780 [04:36<00:51,  3.41it/s] 77%|███████▋  | 604/780 [04:36<00:51,  3.41it/s] 78%|███████▊  | 605/780 [04:36<00:51,  3.42it/s] 78%|███████▊  | 606/780 [04:36<00:50,  3.42it/s] 78%|███████▊  | 607/780 [04:37<00:50,  3.43it/s] 78%|███████▊  | 608/780 [04:37<00:50,  3.43it/s] 78%|███████▊  | 609/780 [04:37<00:49,  3.43it/s] 78%|███████▊  | 610/780 [04:38<00:49,  3.43it/s] 78%|███████▊  | 611/780 [04:38<00:50,  3.32it/s] 78%|███████▊  | 612/780 [04:38<00:50,  3.35it/s] 79%|███████▊  | 613/780 [04:39<00:49,  3.38it/s] 79%|███████▊  | 614/780 [04:39<00:48,  3.39it/s] 79%|███████▉  | 615/780 [04:39<00:48,  3.40it/s] 79%|███████▉  | 616/780 [04:39<00:48,  3.41it/s] 79%|███████▉  | 617/780 [04:40<00:47,  3.42it/s] 79%|███████▉  | 618/780 [04:40<00:47,  3.42it/s] 79%|███████▉  | 619/780 [04:40<00:47,  3.42it/s] 79%|███████▉  | 620/780 [04:41<00:46,  3.43it/s] 80%|███████▉  | 621/780 [04:41<00:46,  3.43it/s] 80%|███████▉  | 622/780 [04:41<00:47,  3.36it/s] 80%|███████▉  | 623/780 [04:41<00:46,  3.38it/s] 80%|████████  | 624/780 [04:42<00:46,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 13:14:21,219 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:14:21,219 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 13:14:21,219 >>   Batch size = 8
{'eval_loss': 0.9542800784111023, 'eval_runtime': 17.9546, 'eval_samples_per_second': 361.3, 'eval_steps_per_second': 45.169, 'epoch': 3.0}
{'loss': 0.5276, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.43it/s][A
  1%|▏         | 12/811 [00:00<00:16, 49.93it/s][A
  2%|▏         | 18/811 [00:00<00:16, 48.26it/s][A
  3%|▎         | 23/811 [00:00<00:16, 47.43it/s][A
  3%|▎         | 28/811 [00:00<00:16, 47.06it/s][A
  4%|▍         | 33/811 [00:00<00:17, 44.38it/s][A
  5%|▍         | 38/811 [00:00<00:16, 45.93it/s][A
  5%|▌         | 43/811 [00:00<00:16, 46.02it/s][A
  6%|▌         | 48/811 [00:01<00:16, 46.06it/s][A
  7%|▋         | 53/811 [00:01<00:16, 46.23it/s][A
  7%|▋         | 58/811 [00:01<00:16, 46.17it/s][A
  8%|▊         | 63/811 [00:01<00:16, 46.24it/s][A
  8%|▊         | 68/811 [00:01<00:16, 46.20it/s][A
  9%|▉         | 73/811 [00:01<00:15, 46.25it/s][A
 10%|▉         | 78/811 [00:01<00:15, 46.20it/s][A
 10%|█         | 83/811 [00:01<00:15, 46.14it/s][A
 11%|█         | 88/811 [00:01<00:15, 46.14it/s][A
 11%|█▏        | 93/811 [00:01<00:15, 46.10it/s][A
 12%|█▏        | 98/811 [00:02<00:15, 46.10it/s][A
 13%|█▎        | 103/811 [00:02<00:16, 42.62it/s][A
 13%|█▎        | 108/811 [00:02<00:16, 43.67it/s][A
 14%|█▍        | 113/811 [00:02<00:15, 44.47it/s][A
 15%|█▍        | 118/811 [00:02<00:15, 45.00it/s][A
 15%|█▌        | 123/811 [00:02<00:15, 45.34it/s][A
 16%|█▌        | 128/811 [00:02<00:14, 45.66it/s][A
 16%|█▋        | 133/811 [00:02<00:14, 45.84it/s][A
 17%|█▋        | 138/811 [00:03<00:14, 45.91it/s][A
 18%|█▊        | 143/811 [00:03<00:14, 45.92it/s][A
 18%|█▊        | 148/811 [00:03<00:14, 45.87it/s][A
 19%|█▉        | 153/811 [00:03<00:14, 45.94it/s][A
 19%|█▉        | 158/811 [00:03<00:14, 46.13it/s][A
 20%|██        | 163/811 [00:03<00:14, 46.17it/s][A
 21%|██        | 168/811 [00:03<00:13, 46.15it/s][A
 21%|██▏       | 173/811 [00:03<00:13, 46.26it/s][A
 22%|██▏       | 178/811 [00:03<00:13, 46.21it/s][A
 23%|██▎       | 183/811 [00:03<00:13, 46.15it/s][A
 23%|██▎       | 188/811 [00:04<00:13, 46.13it/s][A
 24%|██▍       | 193/811 [00:04<00:13, 46.21it/s][A
 24%|██▍       | 198/811 [00:04<00:13, 46.09it/s][A
 25%|██▌       | 203/811 [00:04<00:13, 46.06it/s][A
 26%|██▌       | 208/811 [00:04<00:13, 46.12it/s][A
 26%|██▋       | 213/811 [00:04<00:12, 46.02it/s][A
 27%|██▋       | 218/811 [00:04<00:12, 46.13it/s][A
 27%|██▋       | 223/811 [00:04<00:12, 46.20it/s][A
 28%|██▊       | 228/811 [00:04<00:12, 46.07it/s][A
 29%|██▊       | 233/811 [00:05<00:12, 46.00it/s][A
 29%|██▉       | 238/811 [00:05<00:12, 46.07it/s][A
 30%|██▉       | 243/811 [00:05<00:12, 44.49it/s][A
 31%|███       | 248/811 [00:05<00:12, 45.02it/s][A
 31%|███       | 253/811 [00:05<00:12, 45.39it/s][A
 32%|███▏      | 258/811 [00:05<00:12, 42.90it/s][A
 32%|███▏      | 263/811 [00:05<00:12, 43.95it/s][A
 33%|███▎      | 268/811 [00:05<00:12, 44.70it/s][A
 34%|███▎      | 273/811 [00:05<00:11, 45.18it/s][A
 34%|███▍      | 278/811 [00:06<00:11, 45.45it/s][A
 35%|███▍      | 283/811 [00:06<00:11, 45.54it/s][A
 36%|███▌      | 288/811 [00:06<00:11, 45.65it/s][A
 36%|███▌      | 293/811 [00:06<00:11, 45.83it/s][A
 37%|███▋      | 298/811 [00:06<00:11, 45.75it/s][A
 37%|███▋      | 303/811 [00:06<00:11, 45.76it/s][A
 38%|███▊      | 308/811 [00:06<00:10, 45.84it/s][A
 39%|███▊      | 313/811 [00:06<00:10, 46.05it/s][A
 39%|███▉      | 318/811 [00:06<00:10, 46.03it/s][A
 40%|███▉      | 323/811 [00:07<00:10, 46.06it/s][A
 40%|████      | 328/811 [00:07<00:10, 45.96it/s][A
 41%|████      | 333/811 [00:07<00:10, 46.11it/s][A
 42%|████▏     | 338/811 [00:07<00:10, 46.11it/s][A
 42%|████▏     | 343/811 [00:07<00:10, 45.96it/s][A
 43%|████▎     | 348/811 [00:07<00:10, 45.95it/s][A
 44%|████▎     | 353/811 [00:07<00:09, 45.92it/s][A
 44%|████▍     | 358/811 [00:07<00:09, 46.09it/s][A
 45%|████▍     | 363/811 [00:07<00:09, 46.14it/s][A
 45%|████▌     | 368/811 [00:08<00:09, 46.12it/s][A
 46%|████▌     | 373/811 [00:08<00:09, 46.23it/s][A
 47%|████▋     | 378/811 [00:08<00:09, 46.10it/s][A
 47%|████▋     | 383/811 [00:08<00:09, 46.11it/s][A
 48%|████▊     | 388/811 [00:08<00:09, 46.01it/s][A
 48%|████▊     | 393/811 [00:08<00:09, 45.99it/s][A
 49%|████▉     | 398/811 [00:08<00:08, 46.02it/s][A
 50%|████▉     | 403/811 [00:08<00:08, 45.98it/s][A
 50%|█████     | 408/811 [00:08<00:08, 46.10it/s][A
 51%|█████     | 413/811 [00:09<00:08, 46.13it/s][A
 52%|█████▏    | 418/811 [00:09<00:08, 46.13it/s][A
 52%|█████▏    | 423/811 [00:09<00:08, 46.07it/s][A
 53%|█████▎    | 428/811 [00:09<00:08, 46.11it/s][A
 53%|█████▎    | 433/811 [00:09<00:08, 46.03it/s][A
 54%|█████▍    | 438/811 [00:09<00:08, 45.92it/s][A
 55%|█████▍    | 443/811 [00:09<00:07, 46.02it/s][A
 55%|█████▌    | 448/811 [00:09<00:07, 45.91it/s][A
 56%|█████▌    | 453/811 [00:09<00:07, 45.98it/s][A
 56%|█████▋    | 458/811 [00:09<00:07, 46.12it/s][A
 57%|█████▋    | 463/811 [00:10<00:07, 46.13it/s][A
 58%|█████▊    | 468/811 [00:10<00:07, 46.05it/s][A
 58%|█████▊    | 473/811 [00:10<00:07, 45.32it/s][A
 59%|█████▉    | 478/811 [00:10<00:07, 45.58it/s][A
 60%|█████▉    | 483/811 [00:10<00:07, 45.72it/s][A
 60%|██████    | 488/811 [00:10<00:07, 45.83it/s][A
 61%|██████    | 493/811 [00:10<00:06, 45.84it/s][A
 61%|██████▏   | 498/811 [00:10<00:06, 45.97it/s][A
 62%|██████▏   | 503/811 [00:10<00:06, 46.04it/s][A
 63%|██████▎   | 508/811 [00:11<00:06, 46.06it/s][A
 63%|██████▎   | 513/811 [00:11<00:06, 46.01it/s][A
 64%|██████▍   | 518/811 [00:11<00:06, 45.98it/s][A
 64%|██████▍   | 523/811 [00:11<00:06, 46.00it/s][A
 65%|██████▌   | 528/811 [00:11<00:06, 45.95it/s][A
 66%|██████▌   | 533/811 [00:11<00:06, 46.05it/s][A
 66%|██████▋   | 538/811 [00:11<00:05, 46.02it/s][A
 67%|██████▋   | 543/811 [00:11<00:05, 46.02it/s][A
 68%|██████▊   | 548/811 [00:11<00:05, 46.10it/s][A
 68%|██████▊   | 553/811 [00:12<00:05, 46.10it/s][A
 69%|██████▉   | 558/811 [00:12<00:05, 45.99it/s][A
 69%|██████▉   | 563/811 [00:12<00:05, 45.90it/s][A
 70%|███████   | 568/811 [00:12<00:05, 45.86it/s][A
 71%|███████   | 573/811 [00:12<00:05, 45.87it/s][A
 71%|███████▏  | 578/811 [00:12<00:05, 45.92it/s][A
 72%|███████▏  | 583/811 [00:12<00:04, 45.95it/s][A
 73%|███████▎  | 588/811 [00:12<00:05, 43.90it/s][A
 73%|███████▎  | 593/811 [00:12<00:04, 44.70it/s][A
 74%|███████▎  | 598/811 [00:13<00:04, 45.21it/s][A
 74%|███████▍  | 603/811 [00:13<00:04, 45.46it/s][A
 75%|███████▍  | 608/811 [00:13<00:04, 45.56it/s][A
 76%|███████▌  | 613/811 [00:13<00:04, 44.64it/s][A
 76%|███████▌  | 618/811 [00:13<00:05, 35.58it/s][A
 77%|███████▋  | 622/811 [00:13<00:05, 35.25it/s][A
 77%|███████▋  | 627/811 [00:13<00:04, 38.15it/s][A
 78%|███████▊  | 632/811 [00:13<00:04, 40.33it/s][A
 79%|███████▊  | 637/811 [00:14<00:04, 42.02it/s][A
 79%|███████▉  | 642/811 [00:14<00:03, 43.22it/s][A
 80%|███████▉  | 647/811 [00:14<00:03, 44.12it/s][A
 80%|████████  | 652/811 [00:14<00:03, 44.72it/s][A
 81%|████████  | 657/811 [00:14<00:03, 45.19it/s][A
 82%|████████▏ | 662/811 [00:14<00:03, 45.20it/s][A
 82%|████████▏ | 667/811 [00:14<00:03, 45.22it/s][A
 83%|████████▎ | 672/811 [00:14<00:03, 45.36it/s][A
 83%|████████▎ | 677/811 [00:14<00:02, 45.69it/s][A
 84%|████████▍ | 682/811 [00:15<00:02, 45.91it/s][A
 85%|████████▍ | 687/811 [00:15<00:02, 46.05it/s][A
 85%|████████▌ | 692/811 [00:15<00:02, 46.11it/s][A
 86%|████████▌ | 697/811 [00:15<00:02, 46.13it/s][A
 87%|████████▋ | 702/811 [00:15<00:02, 46.15it/s][A
 87%|████████▋ | 707/811 [00:15<00:02, 46.00it/s][A
 88%|████████▊ | 712/811 [00:15<00:02, 45.89it/s][A
 88%|████████▊ | 717/811 [00:15<00:02, 45.87it/s][A
 89%|████████▉ | 722/811 [00:15<00:01, 46.02it/s][A
 90%|████████▉ | 727/811 [00:15<00:01, 46.16it/s][A
 90%|█████████ | 732/811 [00:16<00:01, 46.16it/s][A
 91%|█████████ | 737/811 [00:16<00:01, 46.28it/s][A
 91%|█████████▏| 742/811 [00:16<00:01, 46.24it/s][A
 92%|█████████▏| 747/811 [00:16<00:01, 44.06it/s][A
 93%|█████████▎| 752/811 [00:16<00:01, 44.73it/s][A
 93%|█████████▎| 757/811 [00:16<00:01, 45.05it/s][A
 94%|█████████▍| 762/811 [00:16<00:01, 45.37it/s][A
 95%|█████████▍| 767/811 [00:16<00:00, 45.65it/s][A
 95%|█████████▌| 772/811 [00:16<00:00, 45.86it/s][A
 96%|█████████▌| 777/811 [00:17<00:00, 46.03it/s][A
 96%|█████████▋| 782/811 [00:17<00:00, 46.12it/s][A
 97%|█████████▋| 787/811 [00:17<00:00, 45.98it/s][A
 98%|█████████▊| 792/811 [00:17<00:00, 45.94it/s][A
 98%|█████████▊| 797/811 [00:17<00:00, 45.98it/s][A
 99%|█████████▉| 802/811 [00:17<00:00, 45.94it/s][A
100%|█████████▉| 807/811 [00:17<00:00, 45.96it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [05:00<00:46,  3.39it/s]
100%|██████████| 811/811 [00:17<00:00, 45.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:14:39,286 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 13:14:39,459 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:14:42,783 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:14:42,942 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:14:43,018 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [05:14<25:37,  9.92s/it] 80%|████████  | 626/780 [05:14<18:05,  7.05s/it] 80%|████████  | 627/780 [05:15<12:47,  5.02s/it] 81%|████████  | 628/780 [05:15<09:07,  3.60s/it] 81%|████████  | 629/780 [05:15<06:33,  2.61s/it] 81%|████████  | 630/780 [05:16<04:46,  1.91s/it] 81%|████████  | 631/780 [05:16<03:32,  1.43s/it] 81%|████████  | 632/780 [05:16<02:40,  1.09s/it] 81%|████████  | 633/780 [05:17<02:04,  1.18it/s] 81%|████████▏ | 634/780 [05:17<01:39,  1.47it/s] 81%|████████▏ | 635/780 [05:17<01:21,  1.78it/s] 82%|████████▏ | 636/780 [05:17<01:09,  2.08it/s] 82%|████████▏ | 637/780 [05:18<01:02,  2.30it/s] 82%|████████▏ | 638/780 [05:18<00:55,  2.55it/s] 82%|████████▏ | 639/780 [05:18<00:50,  2.77it/s] 82%|████████▏ | 640/780 [05:19<00:47,  2.94it/s] 82%|████████▏ | 641/780 [05:19<00:45,  3.07it/s] 82%|████████▏ | 642/780 [05:19<00:43,  3.17it/s] 82%|████████▏ | 643/780 [05:19<00:42,  3.25it/s] 83%|████████▎ | 644/780 [05:20<00:41,  3.30it/s] 83%|████████▎ | 645/780 [05:20<00:43,  3.12it/s] 83%|████████▎ | 646/780 [05:20<00:41,  3.21it/s] 83%|████████▎ | 647/780 [05:21<00:40,  3.27it/s] 83%|████████▎ | 648/780 [05:21<00:39,  3.32it/s] 83%|████████▎ | 649/780 [05:21<00:39,  3.35it/s] 83%|████████▎ | 650/780 [05:22<00:38,  3.37it/s] 83%|████████▎ | 651/780 [05:22<00:38,  3.39it/s] 84%|████████▎ | 652/780 [05:22<00:37,  3.40it/s] 84%|████████▎ | 653/780 [05:22<00:37,  3.41it/s] 84%|████████▍ | 654/780 [05:23<00:36,  3.42it/s] 84%|████████▍ | 655/780 [05:23<00:37,  3.31it/s] 84%|████████▍ | 656/780 [05:23<00:37,  3.34it/s] 84%|████████▍ | 657/780 [05:24<00:36,  3.37it/s] 84%|████████▍ | 658/780 [05:24<00:36,  3.39it/s] 84%|████████▍ | 659/780 [05:24<00:35,  3.40it/s] 85%|████████▍ | 660/780 [05:25<00:35,  3.41it/s] 85%|████████▍ | 661/780 [05:25<00:34,  3.41it/s] 85%|████████▍ | 662/780 [05:25<00:34,  3.42it/s] 85%|████████▌ | 663/780 [05:25<00:34,  3.42it/s] 85%|████████▌ | 664/780 [05:26<00:33,  3.43it/s] 85%|████████▌ | 665/780 [05:26<00:33,  3.43it/s] 85%|████████▌ | 666/780 [05:26<00:34,  3.29it/s] 86%|████████▌ | 667/780 [05:27<00:33,  3.33it/s] 86%|████████▌ | 668/780 [05:27<00:33,  3.36it/s] 86%|████████▌ | 669/780 [05:27<00:32,  3.38it/s] 86%|████████▌ | 670/780 [05:27<00:32,  3.40it/s] 86%|████████▌ | 671/780 [05:28<00:31,  3.41it/s] 86%|████████▌ | 672/780 [05:28<00:31,  3.42it/s] 86%|████████▋ | 673/780 [05:28<00:31,  3.42it/s] 86%|████████▋ | 674/780 [05:29<00:30,  3.42it/s] 87%|████████▋ | 675/780 [05:29<00:30,  3.43it/s] 87%|████████▋ | 676/780 [05:29<00:30,  3.43it/s] 87%|████████▋ | 677/780 [05:30<00:30,  3.36it/s] 87%|████████▋ | 678/780 [05:30<00:30,  3.38it/s] 87%|████████▋ | 679/780 [05:30<00:29,  3.40it/s] 87%|████████▋ | 680/780 [05:30<00:29,  3.41it/s] 87%|████████▋ | 681/780 [05:31<00:28,  3.42it/s] 87%|████████▋ | 682/780 [05:31<00:28,  3.42it/s] 88%|████████▊ | 683/780 [05:31<00:28,  3.42it/s] 88%|████████▊ | 684/780 [05:32<00:28,  3.42it/s] 88%|████████▊ | 685/780 [05:32<00:27,  3.43it/s] 88%|████████▊ | 686/780 [05:32<00:27,  3.43it/s] 88%|████████▊ | 687/780 [05:32<00:27,  3.42it/s] 88%|████████▊ | 688/780 [05:33<00:27,  3.33it/s] 88%|████████▊ | 689/780 [05:33<00:27,  3.36it/s] 88%|████████▊ | 690/780 [05:33<00:26,  3.38it/s] 89%|████████▊ | 691/780 [05:34<00:26,  3.40it/s] 89%|████████▊ | 692/780 [05:34<00:25,  3.41it/s] 89%|████████▉ | 693/780 [05:34<00:25,  3.42it/s] 89%|████████▉ | 694/780 [05:35<00:25,  3.42it/s] 89%|████████▉ | 695/780 [05:35<00:24,  3.42it/s] 89%|████████▉ | 696/780 [05:35<00:24,  3.42it/s] 89%|████████▉ | 697/780 [05:35<00:24,  3.43it/s] 89%|████████▉ | 698/780 [05:36<00:23,  3.42it/s] 90%|████████▉ | 699/780 [05:36<00:24,  3.26it/s] 90%|████████▉ | 700/780 [05:36<00:24,  3.31it/s] 90%|████████▉ | 701/780 [05:37<00:23,  3.34it/s] 90%|█████████ | 702/780 [05:37<00:23,  3.37it/s] 90%|█████████ | 703/780 [05:37<00:22,  3.39it/s] 90%|█████████ | 704/780 [05:37<00:22,  3.40it/s] 90%|█████████ | 705/780 [05:38<00:22,  3.41it/s] 91%|█████████ | 706/780 [05:38<00:21,  3.42it/s] 91%|█████████ | 707/780 [05:38<00:21,  3.42it/s] 91%|█████████ | 708/780 [05:39<00:21,  3.43it/s] 91%|█████████ | 709/780 [05:39<00:20,  3.43it/s] 91%|█████████ | 710/780 [05:39<00:21,  3.31it/s] 91%|█████████ | 711/780 [05:40<00:20,  3.34it/s] 91%|█████████▏| 712/780 [05:40<00:20,  3.37it/s] 91%|█████████▏| 713/780 [05:40<00:19,  3.39it/s] 92%|█████████▏| 714/780 [05:40<00:19,  3.40it/s] 92%|█████████▏| 715/780 [05:41<00:19,  3.41it/s] 92%|█████████▏| 716/780 [05:41<00:18,  3.41it/s] 92%|█████████▏| 717/780 [05:41<00:18,  3.42it/s] 92%|█████████▏| 718/780 [05:42<00:18,  3.42it/s] 92%|█████████▏| 719/780 [05:42<00:17,  3.43it/s] 92%|█████████▏| 720/780 [05:42<00:17,  3.43it/s] 92%|█████████▏| 721/780 [05:43<00:17,  3.31it/s] 93%|█████████▎| 722/780 [05:43<00:17,  3.25it/s] 93%|█████████▎| 723/780 [05:43<00:17,  3.29it/s] 93%|█████████▎| 724/780 [05:43<00:16,  3.33it/s] 93%|█████████▎| 725/780 [05:44<00:16,  3.36it/s] 93%|█████████▎| 726/780 [05:44<00:15,  3.38it/s] 93%|█████████▎| 727/780 [05:44<00:15,  3.40it/s] 93%|█████████▎| 728/780 [05:45<00:15,  3.40it/s] 93%|█████████▎| 729/780 [05:45<00:14,  3.41it/s] 94%|█████████▎| 730/780 [05:45<00:14,  3.42it/s] 94%|█████████▎| 731/780 [05:45<00:14,  3.42it/s] 94%|█████████▍| 732/780 [05:46<00:14,  3.36it/s] 94%|█████████▍| 733/780 [05:46<00:13,  3.38it/s] 94%|█████████▍| 734/780 [05:46<00:13,  3.39it/s] 94%|█████████▍| 735/780 [05:47<00:13,  3.40it/s] 94%|█████████▍| 736/780 [05:47<00:12,  3.41it/s] 94%|█████████▍| 737/780 [05:47<00:12,  3.42it/s] 95%|█████████▍| 738/780 [05:48<00:12,  3.31it/s] 95%|█████████▍| 739/780 [05:48<00:12,  3.34it/s] 95%|█████████▍| 740/780 [05:48<00:11,  3.36it/s] 95%|█████████▌| 741/780 [05:48<00:11,  3.38it/s] 95%|█████████▌| 742/780 [05:49<00:11,  3.40it/s] 95%|█████████▌| 743/780 [05:49<00:11,  3.29it/s] 95%|█████████▌| 744/780 [05:49<00:10,  3.33it/s] 96%|█████████▌| 745/780 [05:50<00:10,  3.36it/s] 96%|█████████▌| 746/780 [05:50<00:10,  3.38it/s] 96%|█████████▌| 747/780 [05:50<00:09,  3.39it/s] 96%|█████████▌| 748/780 [05:51<00:09,  3.40it/s] 96%|█████████▌| 749/780 [05:51<00:09,  3.41it/s] 96%|█████████▌| 750/780 [05:51<00:08,  3.41it/s] 96%|█████████▋| 751/780 [05:51<00:08,  3.42it/s] 96%|█████████▋| 752/780 [05:52<00:08,  3.42it/s] 97%|█████████▋| 753/780 [05:52<00:07,  3.42it/s] 97%|█████████▋| 754/780 [05:52<00:07,  3.42it/s] 97%|█████████▋| 755/780 [05:53<00:07,  3.43it/s] 97%|█████████▋| 756/780 [05:53<00:07,  3.42it/s] 97%|█████████▋| 757/780 [05:53<00:06,  3.43it/s] 97%|█████████▋| 758/780 [05:53<00:06,  3.43it/s] 97%|█████████▋| 759/780 [05:54<00:06,  3.43it/s] 97%|█████████▋| 760/780 [05:54<00:06,  3.28it/s] 98%|█████████▊| 761/780 [05:54<00:05,  3.32it/s] 98%|█████████▊| 762/780 [05:55<00:05,  3.35it/s] 98%|█████████▊| 763/780 [05:55<00:05,  3.38it/s] 98%|█████████▊| 764/780 [05:55<00:04,  3.39it/s] 98%|█████████▊| 765/780 [05:56<00:04,  3.40it/s] 98%|█████████▊| 766/780 [05:56<00:04,  3.41it/s] 98%|█████████▊| 767/780 [05:56<00:03,  3.42it/s] 98%|█████████▊| 768/780 [05:56<00:03,  3.42it/s] 99%|█████████▊| 769/780 [05:57<00:03,  3.42it/s] 99%|█████████▊| 770/780 [05:57<00:02,  3.42it/s] 99%|█████████▉| 771/780 [05:57<00:02,  3.37it/s] 99%|█████████▉| 772/780 [05:58<00:02,  3.39it/s] 99%|█████████▉| 773/780 [05:58<00:02,  3.40it/s] 99%|█████████▉| 774/780 [05:58<00:01,  3.41it/s] 99%|█████████▉| 775/780 [05:58<00:01,  3.41it/s] 99%|█████████▉| 776/780 [05:59<00:01,  3.42it/s]100%|█████████▉| 777/780 [05:59<00:00,  3.42it/s]100%|█████████▉| 778/780 [05:59<00:00,  3.42it/s]100%|█████████▉| 779/780 [06:00<00:00,  3.43it/s]100%|██████████| 780/780 [06:00<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 13:15:39,307 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:15:39,307 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 13:15:39,307 >>   Batch size = 8
{'eval_loss': 0.9633327126502991, 'eval_runtime': 17.8779, 'eval_samples_per_second': 362.85, 'eval_steps_per_second': 45.363, 'epoch': 4.0}

  0%|          | 0/811 [00:00<?, ?it/s][A
  1%|          | 6/811 [00:00<00:14, 56.84it/s][A
  1%|▏         | 12/811 [00:00<00:16, 47.10it/s][A
  2%|▏         | 17/811 [00:00<00:16, 46.74it/s][A
  3%|▎         | 22/811 [00:00<00:16, 46.64it/s][A
  3%|▎         | 27/811 [00:00<00:16, 46.42it/s][A
  4%|▍         | 32/811 [00:00<00:16, 46.50it/s][A
  5%|▍         | 37/811 [00:00<00:16, 46.49it/s][A
  5%|▌         | 42/811 [00:00<00:16, 46.35it/s][A
  6%|▌         | 47/811 [00:01<00:16, 46.35it/s][A
  6%|▋         | 52/811 [00:01<00:16, 46.19it/s][A
  7%|▋         | 57/811 [00:01<00:16, 46.19it/s][A
  8%|▊         | 62/811 [00:01<00:16, 46.29it/s][A
  8%|▊         | 67/811 [00:01<00:16, 46.15it/s][A
  9%|▉         | 72/811 [00:01<00:16, 46.13it/s][A
  9%|▉         | 77/811 [00:01<00:15, 46.18it/s][A
 10%|█         | 82/811 [00:01<00:15, 46.24it/s][A
 11%|█         | 87/811 [00:01<00:15, 46.24it/s][A
 11%|█▏        | 92/811 [00:01<00:15, 46.06it/s][A
 12%|█▏        | 97/811 [00:02<00:15, 46.15it/s][A
 13%|█▎        | 102/811 [00:02<00:15, 46.13it/s][A
 13%|█▎        | 107/811 [00:02<00:15, 46.14it/s][A
 14%|█▍        | 112/811 [00:02<00:15, 45.73it/s][A
 14%|█▍        | 117/811 [00:02<00:14, 46.40it/s][A
 15%|█▌        | 122/811 [00:02<00:14, 46.25it/s][A
 16%|█▌        | 127/811 [00:02<00:14, 46.27it/s][A
 16%|█▋        | 132/811 [00:02<00:14, 46.21it/s][A
 17%|█▋        | 137/811 [00:02<00:14, 46.26it/s][A
 18%|█▊        | 142/811 [00:03<00:14, 46.29it/s][A
 18%|█▊        | 147/811 [00:03<00:14, 46.25it/s][A
 19%|█▊        | 152/811 [00:03<00:14, 44.25it/s][A
 19%|█▉        | 157/811 [00:03<00:14, 44.87it/s][A
 20%|█▉        | 162/811 [00:03<00:14, 45.31it/s][A
 21%|██        | 167/811 [00:03<00:14, 45.55it/s][A
 21%|██        | 172/811 [00:03<00:13, 45.70it/s][A
 22%|██▏       | 177/811 [00:03<00:13, 45.87it/s][A
 22%|██▏       | 182/811 [00:03<00:13, 45.99it/s][A
 23%|██▎       | 187/811 [00:04<00:13, 46.06it/s][A
 24%|██▎       | 192/811 [00:04<00:13, 46.02it/s][A
 24%|██▍       | 197/811 [00:04<00:13, 45.06it/s][A
 25%|██▍       | 202/811 [00:04<00:13, 45.38it/s][A
 26%|██▌       | 207/811 [00:04<00:13, 45.61it/s][A
 26%|██▌       | 212/811 [00:04<00:13, 45.85it/s][A
 27%|██▋       | 217/811 [00:04<00:12, 45.77it/s][A
 27%|██▋       | 222/811 [00:04<00:12, 45.97it/s][A
 28%|██▊       | 227/811 [00:04<00:12, 46.05it/s][A
 29%|██▊       | 232/811 [00:05<00:12, 46.09it/s][A
 29%|██▉       | 237/811 [00:05<00:12, 46.11it/s][A
 30%|██▉       | 242/811 [00:05<00:12, 46.16it/s][A
 30%|███       | 247/811 [00:05<00:12, 46.16it/s][A
 31%|███       | 252/811 [00:05<00:12, 46.13it/s][A
 32%|███▏      | 257/811 [00:05<00:12, 46.17it/s][A
 32%|███▏      | 262/811 [00:05<00:11, 46.16it/s][A
 33%|███▎      | 267/811 [00:05<00:11, 46.11it/s][A
 34%|███▎      | 272/811 [00:05<00:11, 46.10it/s][A
 34%|███▍      | 277/811 [00:06<00:11, 46.19it/s][A
 35%|███▍      | 282/811 [00:06<00:11, 46.18it/s][A
 35%|███▌      | 287/811 [00:06<00:11, 46.24it/s][A
 36%|███▌      | 292/811 [00:06<00:11, 44.68it/s][A
 37%|███▋      | 297/811 [00:06<00:11, 45.21it/s][A
 37%|███▋      | 302/811 [00:06<00:11, 45.40it/s][A
 38%|███▊      | 307/811 [00:06<00:11, 45.58it/s][A
 38%|███▊      | 312/811 [00:06<00:10, 45.75it/s][A
 39%|███▉      | 317/811 [00:06<00:10, 45.93it/s][A
 40%|███▉      | 322/811 [00:06<00:10, 46.01it/s][A
 40%|████      | 327/811 [00:07<00:10, 46.10it/s][A
 41%|████      | 332/811 [00:07<00:10, 46.03it/s][A
 42%|████▏     | 337/811 [00:07<00:10, 46.09it/s][A
 42%|████▏     | 342/811 [00:07<00:10, 46.20it/s][A
 43%|████▎     | 347/811 [00:07<00:10, 46.19it/s][A
 43%|████▎     | 352/811 [00:07<00:09, 46.19it/s][A
 44%|████▍     | 357/811 [00:07<00:09, 46.25it/s][A
 45%|████▍     | 362/811 [00:07<00:09, 46.18it/s][A
 45%|████▌     | 367/811 [00:07<00:09, 46.25it/s][A
 46%|████▌     | 372/811 [00:08<00:09, 46.12it/s][A
 46%|████▋     | 377/811 [00:08<00:09, 46.11it/s][A
 47%|████▋     | 382/811 [00:08<00:09, 46.15it/s][A
 48%|████▊     | 387/811 [00:08<00:09, 46.16it/s][A
 48%|████▊     | 392/811 [00:08<00:09, 46.21it/s][A
 49%|████▉     | 397/811 [00:08<00:08, 46.26it/s][A
 50%|████▉     | 402/811 [00:08<00:08, 46.26it/s][A
 50%|█████     | 407/811 [00:08<00:08, 46.22it/s][A
 51%|█████     | 412/811 [00:08<00:08, 46.21it/s][A
 51%|█████▏    | 417/811 [00:09<00:08, 46.19it/s][A
 52%|█████▏    | 422/811 [00:09<00:08, 46.16it/s][A
 53%|█████▎    | 427/811 [00:09<00:08, 46.20it/s][A
 53%|█████▎    | 432/811 [00:09<00:08, 44.25it/s][A
 54%|█████▍    | 437/811 [00:09<00:08, 44.83it/s][A
 55%|█████▍    | 442/811 [00:09<00:08, 45.29it/s][A
 55%|█████▌    | 447/811 [00:09<00:07, 45.52it/s][A
 56%|█████▌    | 452/811 [00:09<00:07, 45.75it/s][A
 56%|█████▋    | 457/811 [00:09<00:07, 45.89it/s][A
 57%|█████▋    | 462/811 [00:10<00:07, 46.07it/s][A
 58%|█████▊    | 467/811 [00:10<00:07, 46.04it/s][A
 58%|█████▊    | 472/811 [00:10<00:07, 46.07it/s][A
 59%|█████▉    | 477/811 [00:10<00:07, 46.05it/s][A
 59%|█████▉    | 482/811 [00:10<00:07, 46.08it/s][A
 60%|██████    | 487/811 [00:10<00:07, 46.07it/s][A
 61%|██████    | 492/811 [00:10<00:06, 46.12it/s][A
 61%|██████▏   | 497/811 [00:10<00:06, 46.11it/s][A
 62%|██████▏   | 502/811 [00:10<00:06, 46.10it/s][A
 63%|██████▎   | 507/811 [00:11<00:06, 46.21it/s][A
 63%|██████▎   | 512/811 [00:11<00:06, 46.26it/s][A
 64%|██████▎   | 517/811 [00:11<00:06, 46.19it/s][A
 64%|██████▍   | 522/811 [00:11<00:06, 46.10it/s][A
 65%|██████▍   | 527/811 [00:11<00:06, 46.12it/s][A
 66%|██████▌   | 532/811 [00:11<00:06, 46.24it/s][A
 66%|██████▌   | 537/811 [00:11<00:05, 46.27it/s][A
 67%|██████▋   | 542/811 [00:11<00:05, 46.16it/s][A
 67%|██████▋   | 547/811 [00:11<00:05, 46.09it/s][A
 68%|██████▊   | 552/811 [00:11<00:05, 46.14it/s][A
 69%|██████▊   | 557/811 [00:12<00:05, 46.16it/s][A
 69%|██████▉   | 562/811 [00:12<00:05, 46.25it/s][A
 70%|██████▉   | 567/811 [00:12<00:05, 46.26it/s][A
 71%|███████   | 572/811 [00:12<00:05, 42.39it/s][A
 71%|███████   | 577/811 [00:12<00:05, 39.61it/s][A
 72%|███████▏  | 582/811 [00:12<00:05, 41.40it/s][A
 72%|███████▏  | 587/811 [00:12<00:05, 42.77it/s][A
 73%|███████▎  | 592/811 [00:12<00:04, 43.80it/s][A
 74%|███████▎  | 597/811 [00:13<00:04, 44.37it/s][A
 74%|███████▍  | 602/811 [00:13<00:04, 44.97it/s][A
 75%|███████▍  | 607/811 [00:13<00:04, 45.37it/s][A
 75%|███████▌  | 612/811 [00:13<00:10, 19.57it/s][A
 76%|███████▌  | 617/811 [00:13<00:08, 23.65it/s][A
 77%|███████▋  | 622/811 [00:14<00:06, 27.69it/s][A
 77%|███████▋  | 627/811 [00:14<00:05, 31.49it/s][A
 78%|███████▊  | 632/811 [00:14<00:05, 34.77it/s][A
 79%|███████▊  | 637/811 [00:14<00:04, 37.60it/s][A
 79%|███████▉  | 642/811 [00:14<00:04, 39.84it/s][A
 80%|███████▉  | 647/811 [00:14<00:03, 41.58it/s][A
 80%|████████  | 652/811 [00:14<00:03, 42.70it/s][A
 81%|████████  | 657/811 [00:14<00:03, 43.70it/s][A
 82%|████████▏ | 662/811 [00:14<00:03, 44.48it/s][A
 82%|████████▏ | 667/811 [00:15<00:03, 45.05it/s][A
 83%|████████▎ | 672/811 [00:15<00:03, 45.24it/s][A
 83%|████████▎ | 677/811 [00:15<00:02, 45.52it/s][A
 84%|████████▍ | 682/811 [00:15<00:02, 45.68it/s][A
 85%|████████▍ | 687/811 [00:15<00:03, 41.19it/s][A
 85%|████████▌ | 692/811 [00:15<00:02, 42.64it/s][A
 86%|████████▌ | 697/811 [00:15<00:02, 43.61it/s][A
 87%|████████▋ | 702/811 [00:15<00:02, 44.41it/s][A
 87%|████████▋ | 707/811 [00:15<00:02, 45.00it/s][A
 88%|████████▊ | 712/811 [00:16<00:02, 45.32it/s][A
 88%|████████▊ | 717/811 [00:16<00:02, 45.50it/s][A
 89%|████████▉ | 722/811 [00:16<00:01, 45.75it/s][A
 90%|████████▉ | 727/811 [00:16<00:01, 45.73it/s][A
 90%|█████████ | 732/811 [00:16<00:01, 45.91it/s][A
 91%|█████████ | 737/811 [00:16<00:01, 46.04it/s][A
 91%|█████████▏| 742/811 [00:16<00:01, 46.07it/s][A
 92%|█████████▏| 747/811 [00:16<00:01, 46.13it/s][A
 93%|█████████▎| 752/811 [00:16<00:01, 46.24it/s][A
 93%|█████████▎| 757/811 [00:17<00:01, 46.14it/s][A
 94%|█████████▍| 762/811 [00:17<00:01, 46.11it/s][A
 95%|█████████▍| 767/811 [00:17<00:00, 46.17it/s][A
 95%|█████████▌| 772/811 [00:17<00:00, 45.93it/s][A
 96%|█████████▌| 777/811 [00:17<00:00, 46.05it/s][A
 96%|█████████▋| 782/811 [00:17<00:00, 46.02it/s][A
 97%|█████████▋| 787/811 [00:17<00:00, 46.12it/s][A
 98%|█████████▊| 792/811 [00:17<00:00, 46.22it/s][A
 98%|█████████▊| 797/811 [00:17<00:00, 46.19it/s][A
 99%|█████████▉| 802/811 [00:18<00:00, 46.23it/s][A
100%|█████████▉| 807/811 [00:18<00:00, 46.11it/s][A                                                 
                                                 [A100%|██████████| 780/780 [06:18<00:00,  3.43it/s]
100%|██████████| 811/811 [00:18<00:00, 46.11it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 13:15:57,833 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 13:15:58,165 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:16:02,411 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:16:02,506 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:16:02,554 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 13:16:10,720 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 13:16:10,746 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156 (score: 0.9258166551589966).
                                                 100%|██████████| 780/780 [06:41<00:00,  3.43it/s]100%|██████████| 780/780 [06:41<00:00,  1.94it/s]
[INFO|trainer.py:1894] 2023-08-29 13:16:19,975 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 13:16:20,154 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 13:16:24,139 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 13:16:24,334 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 13:16:24,445 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 13:16:24,961 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:24,961 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:24,961 >>   train_loss               =      0.519
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:24,962 >>   train_runtime            = 0:06:41.02
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:24,962 >>   train_samples            =      10003
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:24,962 >>   train_samples_per_second =    124.717
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:24,962 >>   train_steps_per_second   =      1.945
{'eval_loss': 0.9654278755187988, 'eval_runtime': 18.2204, 'eval_samples_per_second': 356.03, 'eval_steps_per_second': 44.511, 'epoch': 5.0}
{'train_runtime': 401.0292, 'train_samples_per_second': 124.717, 'train_steps_per_second': 1.945, 'train_loss': 0.5189669095552885, 'epoch': 5.0}
08/29/2023 13:16:25 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 13:16:25,162 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 13:16:25,163 >>   Num examples = 6487
[INFO|trainer.py:2145] 2023-08-29 13:16:25,163 >>   Batch size = 8
  0%|          | 0/811 [00:00<?, ?it/s]  1%|          | 6/811 [00:00<00:13, 58.18it/s]  1%|▏         | 12/811 [00:00<00:15, 50.75it/s]  2%|▏         | 18/811 [00:00<00:16, 48.76it/s]  3%|▎         | 23/811 [00:00<00:16, 48.14it/s]  3%|▎         | 28/811 [00:00<00:16, 47.70it/s]  4%|▍         | 33/811 [00:00<00:16, 47.48it/s]  5%|▍         | 38/811 [00:00<00:16, 47.19it/s]  5%|▌         | 43/811 [00:00<00:16, 46.85it/s]  6%|▌         | 48/811 [00:01<00:16, 46.68it/s]  7%|▋         | 53/811 [00:01<00:16, 46.67it/s]  7%|▋         | 58/811 [00:01<00:16, 46.71it/s]  8%|▊         | 63/811 [00:01<00:16, 46.56it/s]  8%|▊         | 68/811 [00:01<00:15, 46.60it/s]  9%|▉         | 73/811 [00:01<00:15, 46.70it/s] 10%|▉         | 78/811 [00:01<00:15, 46.65it/s] 10%|█         | 83/811 [00:01<00:16, 43.59it/s] 11%|█         | 88/811 [00:01<00:16, 45.02it/s] 11%|█▏        | 93/811 [00:01<00:15, 45.52it/s] 12%|█▏        | 98/811 [00:02<00:16, 42.61it/s] 13%|█▎        | 103/811 [00:02<00:16, 43.82it/s] 13%|█▎        | 108/811 [00:02<00:15, 44.56it/s] 14%|█▍        | 113/811 [00:02<00:15, 45.21it/s] 15%|█▍        | 118/811 [00:02<00:15, 45.66it/s] 15%|█▌        | 123/811 [00:02<00:14, 45.93it/s] 16%|█▌        | 128/811 [00:02<00:14, 46.10it/s] 16%|█▋        | 133/811 [00:02<00:14, 46.31it/s] 17%|█▋        | 138/811 [00:02<00:14, 46.10it/s] 18%|█▊        | 143/811 [00:03<00:14, 46.22it/s] 18%|█▊        | 148/811 [00:03<00:14, 46.35it/s] 19%|█▉        | 153/811 [00:03<00:14, 46.46it/s] 19%|█▉        | 158/811 [00:03<00:14, 46.39it/s] 20%|██        | 163/811 [00:03<00:13, 46.48it/s] 21%|██        | 168/811 [00:03<00:13, 46.56it/s] 21%|██▏       | 173/811 [00:03<00:13, 46.56it/s] 22%|██▏       | 178/811 [00:03<00:13, 46.53it/s] 23%|██▎       | 183/811 [00:03<00:13, 46.49it/s] 23%|██▎       | 188/811 [00:04<00:13, 46.42it/s] 24%|██▍       | 193/811 [00:04<00:13, 46.34it/s] 24%|██▍       | 198/811 [00:04<00:13, 46.25it/s] 25%|██▌       | 203/811 [00:04<00:13, 46.36it/s] 26%|██▌       | 208/811 [00:04<00:13, 46.35it/s] 26%|██▋       | 213/811 [00:04<00:12, 46.47it/s] 27%|██▋       | 218/811 [00:04<00:12, 46.59it/s] 27%|██▋       | 223/811 [00:04<00:12, 46.55it/s] 28%|██▊       | 228/811 [00:04<00:12, 46.50it/s] 29%|██▊       | 233/811 [00:05<00:12, 46.33it/s] 29%|██▉       | 238/811 [00:05<00:13, 41.70it/s] 30%|██▉       | 243/811 [00:05<00:13, 43.08it/s] 31%|███       | 248/811 [00:05<00:12, 44.01it/s] 31%|███       | 253/811 [00:05<00:12, 44.76it/s] 32%|███▏      | 258/811 [00:05<00:12, 45.25it/s] 32%|███▏      | 263/811 [00:05<00:12, 45.65it/s] 33%|███▎      | 268/811 [00:05<00:11, 45.96it/s] 34%|███▎      | 273/811 [00:05<00:11, 46.18it/s] 34%|███▍      | 278/811 [00:06<00:11, 46.09it/s] 35%|███▍      | 283/811 [00:06<00:11, 46.12it/s] 36%|███▌      | 288/811 [00:06<00:12, 41.53it/s] 36%|███▌      | 293/811 [00:06<00:12, 42.91it/s] 37%|███▋      | 298/811 [00:06<00:11, 43.92it/s] 37%|███▋      | 303/811 [00:06<00:11, 44.59it/s] 38%|███▊      | 308/811 [00:06<00:11, 45.10it/s] 39%|███▊      | 313/811 [00:06<00:10, 45.54it/s] 39%|███▉      | 318/811 [00:06<00:10, 45.84it/s] 40%|███▉      | 323/811 [00:07<00:10, 46.07it/s] 40%|████      | 328/811 [00:07<00:10, 46.09it/s] 41%|████      | 333/811 [00:07<00:10, 46.22it/s] 42%|████▏     | 338/811 [00:07<00:10, 46.17it/s] 42%|████▏     | 343/811 [00:07<00:10, 46.36it/s] 43%|████▎     | 348/811 [00:07<00:09, 46.38it/s] 44%|████▎     | 353/811 [00:07<00:09, 46.40it/s] 44%|████▍     | 358/811 [00:07<00:09, 46.40it/s] 45%|████▍     | 363/811 [00:07<00:09, 46.42it/s] 45%|████▌     | 368/811 [00:08<00:09, 46.39it/s] 46%|████▌     | 373/811 [00:08<00:09, 46.37it/s] 47%|████▋     | 378/811 [00:08<00:09, 46.38it/s] 47%|████▋     | 383/811 [00:08<00:09, 46.45it/s] 48%|████▊     | 388/811 [00:08<00:09, 46.39it/s] 48%|████▊     | 393/811 [00:08<00:09, 46.40it/s] 49%|████▉     | 398/811 [00:08<00:08, 46.40it/s] 50%|████▉     | 403/811 [00:08<00:08, 46.30it/s] 50%|█████     | 408/811 [00:08<00:08, 46.24it/s] 51%|█████     | 413/811 [00:08<00:08, 46.36it/s] 52%|█████▏    | 418/811 [00:09<00:08, 46.40it/s] 52%|█████▏    | 423/811 [00:09<00:09, 42.86it/s] 53%|█████▎    | 428/811 [00:09<00:08, 43.70it/s] 53%|█████▎    | 433/811 [00:09<00:08, 44.76it/s] 54%|█████▍    | 438/811 [00:09<00:08, 45.21it/s] 55%|█████▍    | 443/811 [00:09<00:08, 45.67it/s] 55%|█████▌    | 448/811 [00:09<00:07, 45.82it/s] 56%|█████▌    | 453/811 [00:09<00:07, 46.05it/s] 56%|█████▋    | 458/811 [00:09<00:07, 46.26it/s] 57%|█████▋    | 463/811 [00:10<00:07, 46.07it/s] 58%|█████▊    | 468/811 [00:10<00:07, 46.25it/s] 58%|█████▊    | 473/811 [00:10<00:07, 46.35it/s] 59%|█████▉    | 478/811 [00:10<00:07, 46.32it/s] 60%|█████▉    | 483/811 [00:10<00:07, 46.38it/s] 60%|██████    | 488/811 [00:10<00:06, 46.51it/s] 61%|██████    | 493/811 [00:10<00:06, 46.32it/s] 61%|██████▏   | 498/811 [00:10<00:06, 46.47it/s] 62%|██████▏   | 503/811 [00:10<00:06, 46.36it/s] 63%|██████▎   | 508/811 [00:11<00:06, 46.32it/s] 63%|██████▎   | 513/811 [00:11<00:06, 46.31it/s] 64%|██████▍   | 518/811 [00:11<00:06, 46.37it/s] 64%|██████▍   | 523/811 [00:11<00:06, 46.32it/s] 65%|██████▌   | 528/811 [00:11<00:06, 46.40it/s] 66%|██████▌   | 533/811 [00:11<00:05, 46.45it/s] 66%|██████▋   | 538/811 [00:11<00:05, 46.45it/s] 67%|██████▋   | 543/811 [00:11<00:05, 46.42it/s] 68%|██████▊   | 548/811 [00:11<00:05, 46.39it/s] 68%|██████▊   | 553/811 [00:12<00:05, 46.38it/s] 69%|██████▉   | 558/811 [00:12<00:05, 46.39it/s] 69%|██████▉   | 563/811 [00:12<00:05, 45.22it/s] 70%|███████   | 568/811 [00:12<00:05, 45.61it/s] 71%|███████   | 573/811 [00:12<00:05, 45.87it/s] 71%|███████▏  | 578/811 [00:12<00:05, 46.08it/s] 72%|███████▏  | 583/811 [00:12<00:04, 46.15it/s] 73%|███████▎  | 588/811 [00:12<00:04, 46.28it/s] 73%|███████▎  | 593/811 [00:12<00:04, 46.31it/s] 74%|███████▎  | 598/811 [00:13<00:04, 46.36it/s] 74%|███████▍  | 603/811 [00:13<00:04, 46.20it/s] 75%|███████▍  | 608/811 [00:13<00:04, 46.19it/s] 76%|███████▌  | 613/811 [00:13<00:04, 46.25it/s] 76%|███████▌  | 618/811 [00:13<00:04, 46.37it/s] 77%|███████▋  | 623/811 [00:13<00:04, 46.46it/s] 77%|███████▋  | 628/811 [00:13<00:03, 46.39it/s] 78%|███████▊  | 633/811 [00:13<00:03, 46.47it/s] 79%|███████▊  | 638/811 [00:13<00:03, 46.53it/s] 79%|███████▉  | 643/811 [00:13<00:03, 46.55it/s] 80%|███████▉  | 648/811 [00:14<00:03, 46.49it/s] 81%|████████  | 653/811 [00:14<00:03, 46.43it/s] 81%|████████  | 658/811 [00:14<00:03, 46.52it/s] 82%|████████▏ | 663/811 [00:14<00:03, 46.50it/s] 82%|████████▏ | 668/811 [00:14<00:03, 46.48it/s] 83%|████████▎ | 673/811 [00:14<00:02, 46.59it/s] 84%|████████▎ | 678/811 [00:14<00:02, 46.44it/s] 84%|████████▍ | 683/811 [00:14<00:02, 46.56it/s] 85%|████████▍ | 688/811 [00:14<00:02, 46.57it/s] 85%|████████▌ | 693/811 [00:15<00:02, 46.53it/s] 86%|████████▌ | 698/811 [00:15<00:02, 46.51it/s] 87%|████████▋ | 703/811 [00:15<00:02, 43.43it/s] 87%|████████▋ | 708/811 [00:15<00:02, 44.26it/s] 88%|████████▊ | 713/811 [00:15<00:02, 45.00it/s] 89%|████████▊ | 718/811 [00:15<00:02, 45.47it/s] 89%|████████▉ | 723/811 [00:15<00:01, 45.72it/s] 90%|████████▉ | 728/811 [00:15<00:01, 45.82it/s] 90%|█████████ | 733/811 [00:15<00:01, 46.12it/s] 91%|█████████ | 738/811 [00:16<00:01, 46.17it/s] 92%|█████████▏| 743/811 [00:16<00:01, 46.24it/s] 92%|█████████▏| 748/811 [00:16<00:01, 46.32it/s] 93%|█████████▎| 753/811 [00:16<00:01, 46.37it/s] 93%|█████████▎| 758/811 [00:16<00:01, 46.45it/s] 94%|█████████▍| 763/811 [00:16<00:01, 46.43it/s] 95%|█████████▍| 768/811 [00:16<00:00, 46.50it/s] 95%|█████████▌| 773/811 [00:16<00:00, 46.52it/s] 96%|█████████▌| 778/811 [00:16<00:00, 45.99it/s] 97%|█████████▋| 783/811 [00:17<00:00, 46.58it/s] 97%|█████████▋| 788/811 [00:17<00:00, 46.57it/s] 98%|█████████▊| 793/811 [00:17<00:00, 46.49it/s] 98%|█████████▊| 798/811 [00:17<00:00, 46.56it/s] 99%|█████████▉| 803/811 [00:17<00:00, 46.49it/s]100%|█████████▉| 808/811 [00:17<00:00, 46.42it/s]100%|██████████| 811/811 [00:17<00:00, 46.01it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 13:16:42,813 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:42,813 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:42,813 >>   eval_loss               =     0.9258
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:42,813 >>   eval_runtime            = 0:00:17.65
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:42,813 >>   eval_samples            =       6487
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:42,813 >>   eval_samples_per_second =    367.533
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:42,813 >>   eval_steps_per_second   =     45.949
[INFO|trainer_pt_utils.py:913] 2023-08-29 13:16:42,813 >>   perplexity              =     2.5239
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:16:55,209 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:16:55,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:16:55,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:16:55,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:16:55,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:16:56,245 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:16:56,247 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:16:57,027 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:16:58,206 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:16:58,206 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:17:01,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:17:01,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:17:01,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:17:01,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:17:01,501 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:17:02,430 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:17:02,431 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:17:03,039 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:17:03,277 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:17:03,277 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/dev.jsonl', 'labels': ['member of political party', 'military branch', 'occupation', 'part of the series', 'place of death'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 17184
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17284, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.55it/s]Extractor Predicting: 14it [00:09,  1.59it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.54it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.44it/s]Extractor Predicting: 31it [00:20,  1.48it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:25,  1.53it/s]Extractor Predicting: 40it [00:26,  1.52it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:27,  1.52it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:30,  1.60it/s]Extractor Predicting: 48it [00:31,  1.61it/s]Extractor Predicting: 49it [00:31,  1.65it/s]Extractor Predicting: 50it [00:32,  1.62it/s]Extractor Predicting: 51it [00:33,  1.61it/s]Extractor Predicting: 52it [00:33,  1.62it/s]Extractor Predicting: 53it [00:34,  1.61it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.55it/s]Extractor Predicting: 57it [00:36,  1.58it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:38,  1.53it/s]Extractor Predicting: 60it [00:38,  1.56it/s]Extractor Predicting: 61it [00:39,  1.58it/s]Extractor Predicting: 62it [00:40,  1.58it/s]Extractor Predicting: 63it [00:40,  1.60it/s]Extractor Predicting: 64it [00:41,  1.61it/s]Extractor Predicting: 65it [00:41,  1.62it/s]Extractor Predicting: 66it [00:42,  1.66it/s]Extractor Predicting: 67it [00:43,  1.65it/s]Extractor Predicting: 68it [00:43,  1.63it/s]Extractor Predicting: 69it [00:44,  1.62it/s]Extractor Predicting: 70it [00:45,  1.60it/s]Extractor Predicting: 71it [00:45,  1.61it/s]Extractor Predicting: 72it [00:46,  1.61it/s]Extractor Predicting: 73it [00:46,  1.59it/s]Extractor Predicting: 74it [00:47,  1.60it/s]Extractor Predicting: 75it [00:48,  1.58it/s]Extractor Predicting: 76it [00:48,  1.60it/s]Extractor Predicting: 77it [00:49,  1.60it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:50,  1.57it/s]Extractor Predicting: 80it [00:51,  1.52it/s]Extractor Predicting: 81it [00:52,  1.57it/s]Extractor Predicting: 82it [00:52,  1.61it/s]Extractor Predicting: 83it [00:53,  1.66it/s]Extractor Predicting: 84it [00:53,  1.61it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:55,  1.49it/s]Extractor Predicting: 88it [00:56,  1.46it/s]Extractor Predicting: 89it [00:57,  1.48it/s]Extractor Predicting: 90it [00:58,  1.48it/s]Extractor Predicting: 91it [00:58,  1.48it/s]Extractor Predicting: 92it [00:59,  1.49it/s]Extractor Predicting: 93it [01:00,  1.43it/s]Extractor Predicting: 94it [01:00,  1.41it/s]Extractor Predicting: 95it [01:01,  1.47it/s]Extractor Predicting: 96it [01:02,  1.48it/s]Extractor Predicting: 97it [01:02,  1.51it/s]Extractor Predicting: 98it [01:03,  1.48it/s]Extractor Predicting: 99it [01:04,  1.46it/s]Extractor Predicting: 100it [01:04,  1.49it/s]Extractor Predicting: 101it [01:05,  1.51it/s]Extractor Predicting: 102it [01:06,  1.53it/s]Extractor Predicting: 103it [01:06,  1.54it/s]Extractor Predicting: 104it [01:07,  1.50it/s]Extractor Predicting: 105it [01:08,  1.54it/s]Extractor Predicting: 106it [01:08,  1.57it/s]Extractor Predicting: 107it [01:09,  1.54it/s]Extractor Predicting: 108it [01:09,  1.57it/s]Extractor Predicting: 109it [01:10,  1.57it/s]Extractor Predicting: 110it [01:11,  1.58it/s]Extractor Predicting: 111it [01:11,  1.55it/s]Extractor Predicting: 112it [01:12,  1.57it/s]Extractor Predicting: 113it [01:13,  1.53it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:14,  1.57it/s]Extractor Predicting: 116it [01:15,  1.55it/s]Extractor Predicting: 117it [01:15,  1.54it/s]Extractor Predicting: 118it [01:16,  1.52it/s]Extractor Predicting: 119it [01:17,  1.52it/s]Extractor Predicting: 120it [01:17,  1.55it/s]Extractor Predicting: 121it [01:18,  1.36it/s]Extractor Predicting: 122it [01:19,  1.42it/s]Extractor Predicting: 123it [01:19,  1.45it/s]Extractor Predicting: 124it [01:20,  1.48it/s]Extractor Predicting: 125it [01:21,  1.49it/s]Extractor Predicting: 126it [01:21,  1.46it/s]Extractor Predicting: 127it [01:22,  1.48it/s]Extractor Predicting: 128it [01:23,  1.48it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.52it/s]Extractor Predicting: 131it [01:25,  1.53it/s]Extractor Predicting: 132it [01:25,  1.51it/s]Extractor Predicting: 133it [01:26,  1.48it/s]Extractor Predicting: 134it [01:27,  1.47it/s]Extractor Predicting: 135it [01:28,  1.44it/s]Extractor Predicting: 136it [01:28,  1.42it/s]Extractor Predicting: 137it [01:29,  1.40it/s]Extractor Predicting: 138it [01:30,  1.41it/s]Extractor Predicting: 139it [01:30,  1.39it/s]Extractor Predicting: 140it [01:31,  1.41it/s]Extractor Predicting: 141it [01:32,  1.40it/s]Extractor Predicting: 142it [01:33,  1.42it/s]Extractor Predicting: 143it [01:33,  1.43it/s]Extractor Predicting: 144it [01:34,  1.40it/s]Extractor Predicting: 145it [01:35,  1.40it/s]Extractor Predicting: 146it [01:35,  1.42it/s]Extractor Predicting: 147it [01:36,  1.40it/s]Extractor Predicting: 148it [01:37,  1.40it/s]Extractor Predicting: 149it [01:38,  1.41it/s]Extractor Predicting: 150it [01:38,  1.43it/s]Extractor Predicting: 151it [01:39,  1.45it/s]Extractor Predicting: 152it [01:40,  1.44it/s]Extractor Predicting: 153it [01:40,  1.43it/s]Extractor Predicting: 154it [01:41,  1.42it/s]Extractor Predicting: 155it [01:42,  1.39it/s]Extractor Predicting: 156it [01:42,  1.41it/s]Extractor Predicting: 157it [01:43,  1.42it/s]Extractor Predicting: 158it [01:44,  1.41it/s]Extractor Predicting: 159it [01:45,  1.40it/s]Extractor Predicting: 160it [01:45,  1.40it/s]Extractor Predicting: 161it [01:46,  1.42it/s]Extractor Predicting: 162it [01:47,  1.41it/s]Extractor Predicting: 163it [01:47,  1.46it/s]Extractor Predicting: 164it [01:48,  1.44it/s]Extractor Predicting: 165it [01:49,  1.48it/s]Extractor Predicting: 166it [01:49,  1.47it/s]Extractor Predicting: 167it [01:50,  1.47it/s]Extractor Predicting: 168it [01:51,  1.42it/s]Extractor Predicting: 169it [01:52,  1.39it/s]Extractor Predicting: 170it [01:52,  1.41it/s]Extractor Predicting: 171it [01:53,  1.41it/s]Extractor Predicting: 172it [01:54,  1.41it/s]Extractor Predicting: 173it [01:54,  1.41it/s]Extractor Predicting: 174it [01:55,  1.42it/s]Extractor Predicting: 175it [01:56,  1.38it/s]Extractor Predicting: 176it [01:57,  1.36it/s]Extractor Predicting: 177it [01:57,  1.42it/s]Extractor Predicting: 178it [01:58,  1.43it/s]Extractor Predicting: 179it [01:59,  1.46it/s]Extractor Predicting: 180it [01:59,  1.46it/s]Extractor Predicting: 181it [02:00,  1.49it/s]Extractor Predicting: 182it [02:01,  1.50it/s]Extractor Predicting: 183it [02:01,  1.46it/s]Extractor Predicting: 184it [02:02,  1.46it/s]Extractor Predicting: 185it [02:03,  1.48it/s]Extractor Predicting: 186it [02:03,  1.49it/s]Extractor Predicting: 187it [02:04,  1.54it/s]Extractor Predicting: 188it [02:05,  1.49it/s]Extractor Predicting: 189it [02:05,  1.50it/s]Extractor Predicting: 190it [02:06,  1.50it/s]Extractor Predicting: 191it [02:07,  1.51it/s]Extractor Predicting: 192it [02:07,  1.52it/s]Extractor Predicting: 193it [02:08,  1.48it/s]Extractor Predicting: 194it [02:09,  1.51it/s]Extractor Predicting: 195it [02:09,  1.51it/s]Extractor Predicting: 196it [02:10,  1.52it/s]Extractor Predicting: 197it [02:11,  1.52it/s]Extractor Predicting: 198it [02:11,  1.52it/s]Extractor Predicting: 199it [02:12,  1.50it/s]Extractor Predicting: 200it [02:12,  1.53it/s]Extractor Predicting: 201it [02:13,  1.55it/s]Extractor Predicting: 202it [02:14,  1.53it/s]Extractor Predicting: 203it [02:14,  1.51it/s]Extractor Predicting: 204it [02:15,  1.52it/s]Extractor Predicting: 205it [02:16,  1.52it/s]Extractor Predicting: 206it [02:16,  1.52it/s]Extractor Predicting: 207it [02:17,  1.50it/s]Extractor Predicting: 208it [02:18,  1.51it/s]Extractor Predicting: 209it [02:18,  1.53it/s]Extractor Predicting: 210it [02:19,  1.53it/s]Extractor Predicting: 211it [02:20,  1.49it/s]Extractor Predicting: 212it [02:20,  1.52it/s]Extractor Predicting: 213it [02:21,  1.36it/s]Extractor Predicting: 214it [02:22,  1.42it/s]Extractor Predicting: 215it [02:23,  1.44it/s]Extractor Predicting: 216it [02:23,  1.47it/s]Extractor Predicting: 217it [02:24,  1.48it/s]Extractor Predicting: 218it [02:25,  1.45it/s]Extractor Predicting: 219it [02:25,  1.50it/s]Extractor Predicting: 220it [02:26,  1.48it/s]Extractor Predicting: 221it [02:27,  1.45it/s]Extractor Predicting: 222it [02:27,  1.47it/s]Extractor Predicting: 223it [02:28,  1.47it/s]Extractor Predicting: 224it [02:29,  1.46it/s]Extractor Predicting: 225it [02:29,  1.45it/s]Extractor Predicting: 226it [02:30,  1.46it/s]Extractor Predicting: 227it [02:31,  1.48it/s]Extractor Predicting: 228it [02:31,  1.48it/s]Extractor Predicting: 229it [02:32,  1.44it/s]Extractor Predicting: 230it [02:33,  1.47it/s]Extractor Predicting: 231it [02:34,  1.44it/s]Extractor Predicting: 232it [02:34,  1.46it/s]Extractor Predicting: 233it [02:35,  1.39it/s]Extractor Predicting: 234it [02:36,  1.37it/s]Extractor Predicting: 235it [02:36,  1.38it/s]Extractor Predicting: 236it [02:37,  1.40it/s]Extractor Predicting: 237it [02:38,  1.40it/s]Extractor Predicting: 238it [02:39,  1.38it/s]Extractor Predicting: 239it [02:39,  1.42it/s]Extractor Predicting: 240it [02:40,  1.43it/s]Extractor Predicting: 241it [02:41,  1.41it/s]Extractor Predicting: 242it [02:41,  1.41it/s]Extractor Predicting: 243it [02:42,  1.40it/s]Extractor Predicting: 244it [02:43,  1.38it/s]Extractor Predicting: 245it [02:44,  1.44it/s]Extractor Predicting: 246it [02:44,  1.47it/s]Extractor Predicting: 247it [02:45,  1.65it/s]Extractor Predicting: 247it [02:45,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:05,745 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:05,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:05,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:05,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:05,798 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:20:06,791 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:20:06,792 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:20:07,396 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:20:08,492 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:20:08,492 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:11,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:11,469 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:11,469 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:11,469 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:20:11,469 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:20:12,224 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:20:12,225 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:20:12,808 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:20:12,988 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:20:12,988 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4394724446537918,
  "recall": 0.14382611376599352,
  "score": 0.21672473867595818,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'manufacturer', 'mother', 'occupant', 'organization directed by the office or position', 'owned by', 'part of', 'screenwriter', 'sport', 'sports discipline competed in', 'use', 'voice type'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26970
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27070, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.24it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.37it/s]Extractor Predicting: 7it [00:05,  1.38it/s]Extractor Predicting: 8it [00:05,  1.38it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.38it/s]Extractor Predicting: 11it [00:07,  1.37it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.45it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.52it/s]Extractor Predicting: 19it [00:13,  1.53it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:14,  1.51it/s]Extractor Predicting: 22it [00:15,  1.51it/s]Extractor Predicting: 23it [00:15,  1.52it/s]Extractor Predicting: 24it [00:16,  1.53it/s]Extractor Predicting: 25it [00:17,  1.51it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:18,  1.55it/s]Extractor Predicting: 28it [00:19,  1.55it/s]Extractor Predicting: 29it [00:19,  1.56it/s]Extractor Predicting: 30it [00:20,  1.53it/s]Extractor Predicting: 31it [00:21,  1.53it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:22,  1.52it/s]Extractor Predicting: 34it [00:23,  1.49it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:24,  1.53it/s]Extractor Predicting: 37it [00:25,  1.52it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.50it/s]Extractor Predicting: 40it [00:27,  1.52it/s]Extractor Predicting: 41it [00:27,  1.53it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:30,  1.48it/s]Extractor Predicting: 46it [00:31,  1.51it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:32,  1.50it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.50it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.47it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:36,  1.45it/s]Extractor Predicting: 55it [00:37,  1.47it/s]Extractor Predicting: 56it [00:37,  1.48it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.47it/s]Extractor Predicting: 59it [00:39,  1.49it/s]Extractor Predicting: 60it [00:40,  1.52it/s]Extractor Predicting: 61it [00:41,  1.50it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:42,  1.48it/s]Extractor Predicting: 64it [00:43,  1.45it/s]Extractor Predicting: 65it [00:43,  1.47it/s]Extractor Predicting: 66it [00:44,  1.49it/s]Extractor Predicting: 67it [00:45,  1.50it/s]Extractor Predicting: 68it [00:45,  1.50it/s]Extractor Predicting: 69it [00:46,  1.42it/s]Extractor Predicting: 70it [00:47,  1.45it/s]Extractor Predicting: 71it [00:47,  1.46it/s]Extractor Predicting: 72it [00:48,  1.47it/s]Extractor Predicting: 73it [00:49,  1.49it/s]Extractor Predicting: 74it [00:49,  1.52it/s]Extractor Predicting: 75it [00:50,  1.52it/s]Extractor Predicting: 76it [00:51,  1.51it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:52,  1.52it/s]Extractor Predicting: 79it [00:53,  1.54it/s]Extractor Predicting: 80it [00:53,  1.52it/s]Extractor Predicting: 81it [00:54,  1.52it/s]Extractor Predicting: 82it [00:55,  1.56it/s]Extractor Predicting: 83it [00:55,  1.56it/s]Extractor Predicting: 84it [00:56,  1.56it/s]Extractor Predicting: 85it [00:56,  1.58it/s]Extractor Predicting: 86it [00:57,  1.60it/s]Extractor Predicting: 87it [00:58,  1.57it/s]Extractor Predicting: 88it [00:58,  1.59it/s]Extractor Predicting: 89it [00:59,  1.57it/s]Extractor Predicting: 90it [01:00,  1.56it/s]Extractor Predicting: 91it [01:00,  1.54it/s]Extractor Predicting: 92it [01:01,  1.56it/s]Extractor Predicting: 93it [01:02,  1.55it/s]Extractor Predicting: 94it [01:02,  1.57it/s]Extractor Predicting: 95it [01:03,  1.57it/s]Extractor Predicting: 96it [01:04,  1.56it/s]Extractor Predicting: 97it [01:04,  1.55it/s]Extractor Predicting: 98it [01:05,  1.52it/s]Extractor Predicting: 99it [01:06,  1.54it/s]Extractor Predicting: 100it [01:06,  1.57it/s]Extractor Predicting: 101it [01:07,  1.56it/s]Extractor Predicting: 102it [01:07,  1.54it/s]Extractor Predicting: 103it [01:08,  1.52it/s]Extractor Predicting: 104it [01:09,  1.56it/s]Extractor Predicting: 105it [01:09,  1.55it/s]Extractor Predicting: 106it [01:10,  1.34it/s]Extractor Predicting: 107it [01:11,  1.40it/s]Extractor Predicting: 108it [01:12,  1.45it/s]Extractor Predicting: 109it [01:12,  1.49it/s]Extractor Predicting: 110it [01:13,  1.50it/s]Extractor Predicting: 111it [01:14,  1.53it/s]Extractor Predicting: 112it [01:14,  1.50it/s]Extractor Predicting: 113it [01:15,  1.52it/s]Extractor Predicting: 114it [01:16,  1.47it/s]Extractor Predicting: 115it [01:16,  1.44it/s]Extractor Predicting: 116it [01:17,  1.46it/s]Extractor Predicting: 117it [01:18,  1.45it/s]Extractor Predicting: 118it [01:18,  1.48it/s]Extractor Predicting: 119it [01:19,  1.46it/s]Extractor Predicting: 120it [01:20,  1.47it/s]Extractor Predicting: 121it [01:20,  1.48it/s]Extractor Predicting: 122it [01:21,  1.47it/s]Extractor Predicting: 123it [01:22,  1.45it/s]Extractor Predicting: 124it [01:22,  1.43it/s]Extractor Predicting: 125it [01:23,  1.43it/s]Extractor Predicting: 126it [01:24,  1.44it/s]Extractor Predicting: 127it [01:25,  1.43it/s]Extractor Predicting: 128it [01:25,  1.41it/s]Extractor Predicting: 129it [01:26,  1.42it/s]Extractor Predicting: 130it [01:27,  1.46it/s]Extractor Predicting: 131it [01:27,  1.47it/s]Extractor Predicting: 132it [01:28,  1.47it/s]Extractor Predicting: 133it [01:29,  1.51it/s]Extractor Predicting: 134it [01:29,  1.53it/s]Extractor Predicting: 135it [01:30,  1.54it/s]Extractor Predicting: 136it [01:31,  1.53it/s]Extractor Predicting: 137it [01:31,  1.56it/s]Extractor Predicting: 138it [01:32,  1.55it/s]Extractor Predicting: 139it [01:32,  1.57it/s]Extractor Predicting: 140it [01:33,  1.57it/s]Extractor Predicting: 141it [01:34,  1.54it/s]Extractor Predicting: 142it [01:34,  1.52it/s]Extractor Predicting: 143it [01:35,  1.54it/s]Extractor Predicting: 144it [01:36,  1.55it/s]Extractor Predicting: 145it [01:36,  1.56it/s]Extractor Predicting: 146it [01:37,  1.54it/s]Extractor Predicting: 147it [01:38,  1.52it/s]Extractor Predicting: 148it [01:38,  1.52it/s]Extractor Predicting: 149it [01:39,  1.52it/s]Extractor Predicting: 150it [01:40,  1.51it/s]Extractor Predicting: 151it [01:40,  1.45it/s]Extractor Predicting: 152it [01:41,  1.42it/s]Extractor Predicting: 153it [01:42,  1.44it/s]Extractor Predicting: 154it [01:42,  1.46it/s]Extractor Predicting: 155it [01:43,  1.41it/s]Extractor Predicting: 156it [01:44,  1.40it/s]Extractor Predicting: 157it [01:45,  1.36it/s]Extractor Predicting: 158it [01:45,  1.39it/s]Extractor Predicting: 159it [01:46,  1.40it/s]Extractor Predicting: 160it [01:47,  1.42it/s]Extractor Predicting: 161it [01:48,  1.42it/s]Extractor Predicting: 162it [01:48,  1.40it/s]Extractor Predicting: 163it [01:49,  1.46it/s]Extractor Predicting: 164it [01:49,  1.56it/s]Extractor Predicting: 165it [01:50,  1.54it/s]Extractor Predicting: 166it [01:51,  1.52it/s]Extractor Predicting: 167it [01:51,  1.48it/s]Extractor Predicting: 168it [01:52,  1.46it/s]Extractor Predicting: 169it [01:53,  1.45it/s]Extractor Predicting: 170it [01:53,  1.50it/s]Extractor Predicting: 171it [01:54,  1.49it/s]Extractor Predicting: 172it [01:55,  1.47it/s]Extractor Predicting: 173it [01:56,  1.49it/s]Extractor Predicting: 174it [01:56,  1.48it/s]Extractor Predicting: 175it [01:57,  1.55it/s]Extractor Predicting: 176it [01:57,  1.59it/s]Extractor Predicting: 177it [01:58,  1.58it/s]Extractor Predicting: 178it [01:59,  1.58it/s]Extractor Predicting: 179it [01:59,  1.56it/s]Extractor Predicting: 180it [02:00,  1.51it/s]Extractor Predicting: 181it [02:01,  1.51it/s]Extractor Predicting: 182it [02:01,  1.53it/s]Extractor Predicting: 183it [02:02,  1.53it/s]Extractor Predicting: 184it [02:03,  1.55it/s]Extractor Predicting: 185it [02:03,  1.58it/s]Extractor Predicting: 186it [02:04,  1.62it/s]Extractor Predicting: 187it [02:04,  1.57it/s]Extractor Predicting: 188it [02:05,  1.56it/s]Extractor Predicting: 189it [02:06,  1.56it/s]Extractor Predicting: 190it [02:06,  1.57it/s]Extractor Predicting: 191it [02:07,  1.55it/s]Extractor Predicting: 192it [02:08,  1.53it/s]Extractor Predicting: 193it [02:08,  1.51it/s]Extractor Predicting: 194it [02:09,  1.50it/s]Extractor Predicting: 195it [02:10,  1.54it/s]Extractor Predicting: 196it [02:10,  1.54it/s]Extractor Predicting: 197it [02:11,  1.55it/s]Extractor Predicting: 198it [02:12,  1.57it/s]Extractor Predicting: 199it [02:12,  1.57it/s]Extractor Predicting: 200it [02:13,  1.55it/s]Extractor Predicting: 201it [02:14,  1.57it/s]Extractor Predicting: 202it [02:14,  1.54it/s]Extractor Predicting: 203it [02:15,  1.53it/s]Extractor Predicting: 204it [02:15,  1.56it/s]Extractor Predicting: 205it [02:16,  1.57it/s]Extractor Predicting: 206it [02:17,  1.56it/s]Extractor Predicting: 207it [02:17,  1.62it/s]Extractor Predicting: 208it [02:18,  1.57it/s]Extractor Predicting: 209it [02:19,  1.58it/s]Extractor Predicting: 210it [02:19,  1.53it/s]Extractor Predicting: 211it [02:20,  1.55it/s]Extractor Predicting: 212it [02:21,  1.49it/s]Extractor Predicting: 213it [02:21,  1.45it/s]Extractor Predicting: 214it [02:22,  1.49it/s]Extractor Predicting: 215it [02:23,  1.51it/s]Extractor Predicting: 216it [02:23,  1.51it/s]Extractor Predicting: 217it [02:24,  1.50it/s]Extractor Predicting: 218it [02:25,  1.52it/s]Extractor Predicting: 219it [02:25,  1.52it/s]Extractor Predicting: 220it [02:26,  1.53it/s]Extractor Predicting: 221it [02:27,  1.57it/s]Extractor Predicting: 222it [02:27,  1.52it/s]Extractor Predicting: 223it [02:28,  1.51it/s]Extractor Predicting: 224it [02:29,  1.51it/s]Extractor Predicting: 225it [02:29,  1.51it/s]Extractor Predicting: 226it [02:30,  1.53it/s]Extractor Predicting: 227it [02:31,  1.55it/s]Extractor Predicting: 228it [02:31,  1.54it/s]Extractor Predicting: 229it [02:32,  1.54it/s]Extractor Predicting: 230it [02:33,  1.51it/s]Extractor Predicting: 231it [02:33,  1.54it/s]Extractor Predicting: 232it [02:34,  1.36it/s]Extractor Predicting: 233it [02:35,  1.42it/s]Extractor Predicting: 234it [02:35,  1.45it/s]Extractor Predicting: 235it [02:36,  1.45it/s]Extractor Predicting: 236it [02:37,  1.47it/s]Extractor Predicting: 237it [02:37,  1.46it/s]Extractor Predicting: 238it [02:38,  1.50it/s]Extractor Predicting: 239it [02:39,  1.50it/s]Extractor Predicting: 240it [02:39,  1.50it/s]Extractor Predicting: 241it [02:40,  1.52it/s]Extractor Predicting: 242it [02:41,  1.51it/s]Extractor Predicting: 243it [02:41,  1.54it/s]Extractor Predicting: 244it [02:42,  1.51it/s]Extractor Predicting: 245it [02:43,  1.50it/s]Extractor Predicting: 246it [02:43,  1.50it/s]Extractor Predicting: 247it [02:44,  1.54it/s]Extractor Predicting: 248it [02:45,  1.53it/s]Extractor Predicting: 249it [02:45,  1.55it/s]Extractor Predicting: 250it [02:46,  1.54it/s]Extractor Predicting: 251it [02:47,  1.50it/s]Extractor Predicting: 252it [02:47,  1.52it/s]Extractor Predicting: 253it [02:48,  1.56it/s]Extractor Predicting: 254it [02:48,  1.57it/s]Extractor Predicting: 255it [02:49,  1.51it/s]Extractor Predicting: 256it [02:50,  1.49it/s]Extractor Predicting: 257it [02:51,  1.45it/s]Extractor Predicting: 258it [02:51,  1.41it/s]Extractor Predicting: 259it [02:52,  1.41it/s]Extractor Predicting: 260it [02:53,  1.41it/s]Extractor Predicting: 261it [02:53,  1.43it/s]Extractor Predicting: 262it [02:54,  1.45it/s]Extractor Predicting: 263it [02:55,  1.48it/s]Extractor Predicting: 264it [02:55,  1.48it/s]Extractor Predicting: 265it [02:56,  1.51it/s]Extractor Predicting: 266it [02:57,  1.50it/s]Extractor Predicting: 267it [02:57,  1.52it/s]Extractor Predicting: 268it [02:58,  1.52it/s]Extractor Predicting: 269it [02:59,  1.53it/s]Extractor Predicting: 270it [02:59,  1.55it/s]Extractor Predicting: 271it [03:00,  1.54it/s]Extractor Predicting: 272it [03:01,  1.52it/s]Extractor Predicting: 273it [03:01,  1.55it/s]Extractor Predicting: 274it [03:02,  1.53it/s]Extractor Predicting: 275it [03:03,  1.55it/s]Extractor Predicting: 276it [03:03,  1.53it/s]Extractor Predicting: 277it [03:04,  1.54it/s]Extractor Predicting: 278it [03:05,  1.52it/s]Extractor Predicting: 279it [03:05,  1.51it/s]Extractor Predicting: 280it [03:06,  1.53it/s]Extractor Predicting: 281it [03:07,  1.52it/s]Extractor Predicting: 282it [03:07,  1.52it/s]Extractor Predicting: 283it [03:08,  1.54it/s]Extractor Predicting: 284it [03:08,  1.51it/s]Extractor Predicting: 285it [03:09,  1.50it/s]Extractor Predicting: 286it [03:10,  1.49it/s]Extractor Predicting: 287it [03:10,  1.52it/s]Extractor Predicting: 288it [03:11,  1.52it/s]Extractor Predicting: 289it [03:12,  1.53it/s]Extractor Predicting: 290it [03:12,  1.52it/s]Extractor Predicting: 291it [03:13,  1.53it/s]Extractor Predicting: 292it [03:14,  1.55it/s]Extractor Predicting: 293it [03:14,  1.55it/s]Extractor Predicting: 294it [03:15,  1.55it/s]Extractor Predicting: 295it [03:16,  1.55it/s]Extractor Predicting: 296it [03:16,  1.50it/s]Extractor Predicting: 297it [03:17,  1.50it/s]Extractor Predicting: 298it [03:18,  1.49it/s]Extractor Predicting: 299it [03:18,  1.48it/s]Extractor Predicting: 300it [03:19,  1.47it/s]Extractor Predicting: 301it [03:20,  1.46it/s]Extractor Predicting: 302it [03:20,  1.48it/s]Extractor Predicting: 303it [03:21,  1.52it/s]Extractor Predicting: 304it [03:22,  1.57it/s]Extractor Predicting: 305it [03:22,  1.57it/s]Extractor Predicting: 306it [03:23,  1.59it/s]Extractor Predicting: 307it [03:24,  1.60it/s]Extractor Predicting: 308it [03:24,  1.62it/s]Extractor Predicting: 309it [03:25,  1.64it/s]Extractor Predicting: 310it [03:25,  1.64it/s]Extractor Predicting: 311it [03:26,  1.64it/s]Extractor Predicting: 312it [03:27,  1.59it/s]Extractor Predicting: 313it [03:27,  1.59it/s]Extractor Predicting: 314it [03:28,  1.53it/s]Extractor Predicting: 315it [03:29,  1.52it/s]Extractor Predicting: 316it [03:29,  1.54it/s]Extractor Predicting: 317it [03:30,  1.58it/s]Extractor Predicting: 318it [03:31,  1.54it/s]Extractor Predicting: 319it [03:31,  1.57it/s]Extractor Predicting: 320it [03:32,  1.57it/s]Extractor Predicting: 321it [03:32,  1.57it/s]Extractor Predicting: 322it [03:33,  1.60it/s]Extractor Predicting: 323it [03:34,  1.58it/s]Extractor Predicting: 324it [03:34,  1.58it/s]Extractor Predicting: 325it [03:35,  1.55it/s]Extractor Predicting: 326it [03:36,  1.57it/s]Extractor Predicting: 327it [03:36,  1.60it/s]Extractor Predicting: 328it [03:37,  1.60it/s]Extractor Predicting: 329it [03:37,  1.64it/s]Extractor Predicting: 330it [03:38,  1.62it/s]Extractor Predicting: 331it [03:39,  1.63it/s]Extractor Predicting: 332it [03:39,  1.61it/s]Extractor Predicting: 333it [03:40,  1.59it/s]Extractor Predicting: 334it [03:40,  1.61it/s]Extractor Predicting: 335it [03:41,  1.58it/s]Extractor Predicting: 336it [03:42,  1.54it/s]Extractor Predicting: 337it [03:43,  1.50it/s]Extractor Predicting: 338it [03:43,  1.49it/s]Extractor Predicting: 339it [03:44,  1.54it/s]Extractor Predicting: 340it [03:44,  1.55it/s]Extractor Predicting: 341it [03:45,  1.54it/s]Extractor Predicting: 342it [03:46,  1.50it/s]Extractor Predicting: 343it [03:47,  1.44it/s]Extractor Predicting: 344it [03:47,  1.47it/s]Extractor Predicting: 345it [03:48,  1.48it/s]Extractor Predicting: 346it [03:49,  1.52it/s]Extractor Predicting: 347it [03:49,  1.54it/s]Extractor Predicting: 348it [03:50,  1.53it/s]Extractor Predicting: 349it [03:50,  1.55it/s]Extractor Predicting: 350it [03:51,  1.53it/s]Extractor Predicting: 351it [03:52,  1.57it/s]Extractor Predicting: 352it [03:53,  1.35it/s]Extractor Predicting: 353it [03:53,  1.41it/s]Extractor Predicting: 354it [03:54,  1.46it/s]Extractor Predicting: 355it [03:55,  1.41it/s]Extractor Predicting: 356it [03:55,  1.45it/s]Extractor Predicting: 357it [03:56,  1.48it/s]Extractor Predicting: 358it [03:57,  1.48it/s]Extractor Predicting: 359it [03:57,  1.50it/s]Extractor Predicting: 360it [03:58,  1.50it/s]Extractor Predicting: 361it [03:59,  1.47it/s]Extractor Predicting: 362it [03:59,  1.49it/s]Extractor Predicting: 363it [04:00,  1.49it/s]Extractor Predicting: 364it [04:01,  1.50it/s]Extractor Predicting: 365it [04:01,  1.46it/s]Extractor Predicting: 366it [04:02,  1.45it/s]Extractor Predicting: 367it [04:03,  1.48it/s]Extractor Predicting: 368it [04:03,  1.54it/s]Extractor Predicting: 369it [04:04,  1.55it/s]Extractor Predicting: 370it [04:05,  1.54it/s]Extractor Predicting: 371it [04:05,  1.55it/s]Extractor Predicting: 372it [04:06,  1.55it/s]Extractor Predicting: 373it [04:07,  1.54it/s]Extractor Predicting: 374it [04:07,  1.55it/s]Extractor Predicting: 375it [04:08,  1.58it/s]Extractor Predicting: 376it [04:08,  1.57it/s]Extractor Predicting: 377it [04:09,  1.50it/s]Extractor Predicting: 378it [04:10,  1.44it/s]Extractor Predicting: 379it [04:11,  1.49it/s]Extractor Predicting: 380it [04:11,  1.47it/s]Extractor Predicting: 381it [04:12,  1.50it/s]Extractor Predicting: 382it [04:13,  1.51it/s]Extractor Predicting: 383it [04:13,  1.51it/s]Extractor Predicting: 384it [04:14,  1.53it/s]Extractor Predicting: 385it [04:15,  1.53it/s]Extractor Predicting: 386it [04:15,  1.53it/s]Extractor Predicting: 387it [04:16,  1.55it/s]Extractor Predicting: 388it [04:16,  1.58it/s]Extractor Predicting: 389it [04:17,  1.57it/s]Extractor Predicting: 390it [04:18,  1.52it/s]Extractor Predicting: 391it [04:18,  1.53it/s]Extractor Predicting: 392it [04:19,  1.56it/s]Extractor Predicting: 393it [04:20,  1.55it/s]Extractor Predicting: 394it [04:20,  1.57it/s]Extractor Predicting: 395it [04:21,  1.54it/s]Extractor Predicting: 396it [04:22,  1.54it/s]Extractor Predicting: 397it [04:22,  1.57it/s]Extractor Predicting: 398it [04:23,  1.61it/s]Extractor Predicting: 399it [04:23,  1.58it/s]Extractor Predicting: 400it [04:24,  1.59it/s]Extractor Predicting: 401it [04:25,  1.59it/s]Extractor Predicting: 402it [04:25,  1.59it/s]Extractor Predicting: 403it [04:26,  1.58it/s]Extractor Predicting: 404it [04:27,  1.55it/s]Extractor Predicting: 405it [04:27,  1.55it/s]Extractor Predicting: 406it [04:28,  1.54it/s]Extractor Predicting: 407it [04:29,  1.51it/s]Extractor Predicting: 408it [04:29,  1.52it/s]Extractor Predicting: 409it [04:30,  1.49it/s]Extractor Predicting: 410it [04:31,  1.46it/s]Extractor Predicting: 411it [04:31,  1.50it/s]Extractor Predicting: 412it [04:32,  1.49it/s]Extractor Predicting: 413it [04:33,  1.46it/s]Extractor Predicting: 414it [04:33,  1.47it/s]Extractor Predicting: 415it [04:34,  1.51it/s]Extractor Predicting: 416it [04:35,  1.49it/s]Extractor Predicting: 417it [04:35,  1.47it/s]Extractor Predicting: 418it [04:36,  1.44it/s]Extractor Predicting: 419it [04:37,  1.47it/s]Extractor Predicting: 420it [04:38,  1.41it/s]Extractor Predicting: 421it [04:38,  1.42it/s]Extractor Predicting: 422it [04:39,  1.43it/s]Extractor Predicting: 423it [04:40,  1.48it/s]Extractor Predicting: 424it [04:40,  1.50it/s]Extractor Predicting: 425it [04:41,  1.52it/s]Extractor Predicting: 426it [04:42,  1.50it/s]Extractor Predicting: 427it [04:42,  1.45it/s]Extractor Predicting: 428it [04:43,  1.46it/s]Extractor Predicting: 429it [04:44,  1.50it/s]Extractor Predicting: 430it [04:44,  1.45it/s]Extractor Predicting: 431it [04:45,  1.46it/s]Extractor Predicting: 432it [04:46,  1.42it/s]Extractor Predicting: 433it [04:46,  1.46it/s]Extractor Predicting: 434it [04:47,  1.49it/s]Extractor Predicting: 435it [04:48,  1.56it/s]Extractor Predicting: 436it [04:48,  1.63it/s]Extractor Predicting: 437it [04:49,  1.65it/s]Extractor Predicting: 438it [04:49,  1.62it/s]Extractor Predicting: 439it [04:50,  1.66it/s]Extractor Predicting: 440it [04:50,  1.76it/s]Extractor Predicting: 441it [04:51,  1.64it/s]Extractor Predicting: 442it [04:52,  1.54it/s]Extractor Predicting: 443it [04:53,  1.45it/s]Extractor Predicting: 444it [04:53,  1.43it/s]Extractor Predicting: 445it [04:54,  1.43it/s]Extractor Predicting: 446it [04:55,  1.41it/s]Extractor Predicting: 447it [04:56,  1.40it/s]Extractor Predicting: 448it [04:56,  1.39it/s]Extractor Predicting: 449it [04:57,  1.39it/s]Extractor Predicting: 450it [04:58,  1.41it/s]Extractor Predicting: 451it [04:58,  1.41it/s]Extractor Predicting: 452it [04:59,  1.38it/s]Extractor Predicting: 453it [05:00,  1.41it/s]Extractor Predicting: 454it [05:01,  1.41it/s]Extractor Predicting: 455it [05:01,  1.41it/s]Extractor Predicting: 456it [05:02,  1.40it/s]Extractor Predicting: 457it [05:03,  1.36it/s]Extractor Predicting: 458it [05:03,  1.38it/s]Extractor Predicting: 459it [05:04,  1.41it/s]Extractor Predicting: 460it [05:05,  1.43it/s]Extractor Predicting: 461it [05:06,  1.44it/s]Extractor Predicting: 462it [05:06,  1.45it/s]Extractor Predicting: 463it [05:07,  1.45it/s]Extractor Predicting: 464it [05:08,  1.44it/s]Extractor Predicting: 465it [05:08,  1.47it/s]Extractor Predicting: 466it [05:09,  1.68it/s]Extractor Predicting: 466it [05:09,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:36,144 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:36,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:36,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:36,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:36,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:25:36,648 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:25:36,649 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:25:36,952 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:25:38,050 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:25:38,050 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:40,880 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:40,910 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:40,910 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:40,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:25:40,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:25:41,697 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:25:41,699 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:25:42,293 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:25:42,500 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:25:42,500 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.30429641308632244,
  "recall": 0.06911988539708121,
  "score": 0.11265139355026997,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_2/test.jsonl', 'labels': ['director', 'drafted by', 'lyrics by', 'main subject', 'manufacturer', 'mother', 'occupant', 'organization directed by the office or position', 'owned by', 'part of', 'screenwriter', 'sport', 'sports discipline competed in', 'use', 'voice type'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 8266
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8366, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.37it/s]Extractor Predicting: 5it [00:03,  1.38it/s]Extractor Predicting: 6it [00:04,  1.37it/s]Extractor Predicting: 7it [00:05,  1.40it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:07,  1.45it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.38it/s]Extractor Predicting: 14it [00:10,  1.38it/s]Extractor Predicting: 15it [00:10,  1.40it/s]Extractor Predicting: 16it [00:11,  1.39it/s]Extractor Predicting: 17it [00:12,  1.33it/s]Extractor Predicting: 18it [00:12,  1.39it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:14,  1.43it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:15,  1.44it/s]Extractor Predicting: 23it [00:16,  1.44it/s]Extractor Predicting: 24it [00:17,  1.44it/s]Extractor Predicting: 25it [00:17,  1.44it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:19,  1.39it/s]Extractor Predicting: 28it [00:19,  1.38it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:21,  1.44it/s]Extractor Predicting: 31it [00:22,  1.39it/s]Extractor Predicting: 32it [00:22,  1.40it/s]Extractor Predicting: 33it [00:23,  1.41it/s]Extractor Predicting: 34it [00:24,  1.42it/s]Extractor Predicting: 35it [00:24,  1.44it/s]Extractor Predicting: 36it [00:25,  1.42it/s]Extractor Predicting: 37it [00:26,  1.43it/s]Extractor Predicting: 38it [00:26,  1.45it/s]Extractor Predicting: 39it [00:27,  1.42it/s]Extractor Predicting: 40it [00:28,  1.39it/s]Extractor Predicting: 41it [00:29,  1.35it/s]Extractor Predicting: 42it [00:29,  1.35it/s]Extractor Predicting: 43it [00:30,  1.35it/s]Extractor Predicting: 44it [00:31,  1.35it/s]Extractor Predicting: 45it [00:32,  1.36it/s]Extractor Predicting: 46it [00:32,  1.36it/s]Extractor Predicting: 47it [00:33,  1.25it/s]Extractor Predicting: 48it [00:34,  1.26it/s]Extractor Predicting: 49it [00:35,  1.28it/s]Extractor Predicting: 50it [00:36,  1.30it/s]Extractor Predicting: 51it [00:36,  1.31it/s]Extractor Predicting: 52it [00:37,  1.30it/s]Extractor Predicting: 53it [00:38,  1.36it/s]Extractor Predicting: 54it [00:38,  1.38it/s]Extractor Predicting: 55it [00:39,  1.42it/s]Extractor Predicting: 56it [00:40,  1.40it/s]Extractor Predicting: 57it [00:41,  1.38it/s]Extractor Predicting: 58it [00:41,  1.37it/s]Extractor Predicting: 59it [00:42,  1.36it/s]Extractor Predicting: 60it [00:43,  1.36it/s]Extractor Predicting: 61it [00:44,  1.37it/s]Extractor Predicting: 62it [00:44,  1.39it/s]Extractor Predicting: 63it [00:45,  1.40it/s]Extractor Predicting: 64it [00:46,  1.41it/s]Extractor Predicting: 65it [00:46,  1.40it/s]Extractor Predicting: 66it [00:47,  1.43it/s]Extractor Predicting: 67it [00:48,  1.43it/s]Extractor Predicting: 68it [00:48,  1.45it/s]Extractor Predicting: 69it [00:49,  1.47it/s]Extractor Predicting: 70it [00:50,  1.49it/s]Extractor Predicting: 71it [00:50,  1.46it/s]Extractor Predicting: 72it [00:51,  1.45it/s]Extractor Predicting: 73it [00:52,  1.43it/s]Extractor Predicting: 74it [00:53,  1.41it/s]Extractor Predicting: 75it [00:53,  1.44it/s]Extractor Predicting: 76it [00:54,  1.42it/s]Extractor Predicting: 77it [00:55,  1.39it/s]Extractor Predicting: 78it [00:55,  1.41it/s]Extractor Predicting: 79it [00:56,  1.42it/s]Extractor Predicting: 80it [00:57,  1.42it/s]Extractor Predicting: 81it [00:58,  1.41it/s]Extractor Predicting: 82it [00:58,  1.41it/s]Extractor Predicting: 83it [00:59,  1.42it/s]Extractor Predicting: 84it [01:00,  1.43it/s]Extractor Predicting: 85it [01:00,  1.43it/s]Extractor Predicting: 86it [01:01,  1.43it/s]Extractor Predicting: 87it [01:02,  1.44it/s]Extractor Predicting: 88it [01:02,  1.41it/s]Extractor Predicting: 89it [01:03,  1.44it/s]Extractor Predicting: 90it [01:04,  1.48it/s]Extractor Predicting: 91it [01:04,  1.49it/s]Extractor Predicting: 92it [01:05,  1.59it/s]Extractor Predicting: 92it [01:05,  1.41it/s]
[INFO|configuration_utils.py:515] 2023-08-29 13:26:51,219 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 13:26:51,220 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 13:26:51,285 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 13:26:51,286 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 13:26:51,314 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 13:27:00,853 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 13:27:00,865 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 13:27:00,979 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 13:27:00,980 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 13:27:01,035 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:27:01,070 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:27:01,070 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:27:01,070 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:27:01,070 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:27:01,070 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 13:27:01,070 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.36486486486486486,
  "recall": 0.05828100470957614,
  "score": 0.10050761421319797,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 13:27:01,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:02,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:02,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:03,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:04,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:04,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:05,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:05,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:06,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:07,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:07,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:08,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:09,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:10,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:10,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:11,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:12,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:12,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:13,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:14,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:14,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:28, 14.15s/it][WARNING|generation_utils.py:914] 2023-08-29 13:27:15,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:16,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:17,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:18,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:19,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:19,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:20,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:21,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:22,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:22,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:23,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:24,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:24,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:25,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:26,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:27,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:27,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:28,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:29,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:30,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:30,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:30<04:34, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-29 13:27:31,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:32,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:33,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:33,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:34,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:35,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:36,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:36,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:37,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:38,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:38,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:39,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:40,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:41,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:41,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:42,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:43,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:44,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:44,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:45,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:46,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:20, 15.30s/it][WARNING|generation_utils.py:914] 2023-08-29 13:27:46,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:47,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:48,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:49,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:50,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:50,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:51,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:52,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:53,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:54,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:54,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:55,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:56,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:57,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:57,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:58,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:27:59,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:00,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:01,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:02,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:02,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:02<04:13, 15.84s/it][WARNING|generation_utils.py:914] 2023-08-29 13:28:03,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:04,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:05,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:06,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:06,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:07,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:08,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:08,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:09,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:10,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:11,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:12,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:12,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:13,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:14,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:15,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:16,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:17,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:17,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:18,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:18<03:57, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-29 13:28:19,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:20,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:21,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:21,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:22,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:23,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:23,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:24,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:25,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:26,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:26,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:27,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:28,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:28,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:29,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:30,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:31,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:31,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:32,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:33,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:32<03:35, 15.41s/it][WARNING|generation_utils.py:914] 2023-08-29 13:28:34,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:34,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:35,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:36,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:37,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:37,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:38,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:39,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:40,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:40,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:41,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:42,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:43,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:44,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:45,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:45,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:46,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:47,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:47,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:49,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:48<03:22, 15.60s/it][WARNING|generation_utils.py:914] 2023-08-29 13:28:50,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:50,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:51,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:52,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:52,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:53,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:54,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:55,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:55,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:56,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:57,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:58,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:58,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:28:59,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:00,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:01,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:01,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:02,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:03,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:03,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:03<03:03, 15.32s/it][WARNING|generation_utils.py:914] 2023-08-29 13:29:04,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:05,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:06,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:06,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:07,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:08,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:08,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:09,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:10,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:11,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:12,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:12,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:13,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:14,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:15,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:16,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:16,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:17,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:18,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:19,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:20,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:20,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:20<02:53, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-29 13:29:21,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:22,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:22,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:23,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:24,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:24,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:25,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:25,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:26,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:27,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:27,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:28,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:29,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:30,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:30,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:31,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:32,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:32,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:33,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:34,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:33<02:30, 15.07s/it][WARNING|generation_utils.py:914] 2023-08-29 13:29:34,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:35,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:36,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:37,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:37,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:38,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:39,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:40,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:41,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:41,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:42,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:43,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:44,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:45,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:46,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:46,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:47,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:48,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:49,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:49,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:50,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:51,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:50<02:21, 15.73s/it][WARNING|generation_utils.py:914] 2023-08-29 13:29:52,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:52,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:53,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:54,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:55,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:55,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:56,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:57,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:58,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:58,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:29:59,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:00,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:01,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:01,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:02,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:03,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:04,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:04,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:05,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:06,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:07,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:06<02:05, 15.68s/it][WARNING|generation_utils.py:914] 2023-08-29 13:30:07,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:08,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:09,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:09,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:10,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:11,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:12,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:12,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:13,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:14,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:15,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:15,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:16,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:17,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:17,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:18,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:19,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:20,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:21,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:21,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:22,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:21<01:49, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-29 13:30:23,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:23,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:24,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:25,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:26,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:27,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:27,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:28,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:29,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:30,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:31,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:31,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:32,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:33,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:34,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:34,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:35,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:36,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:37,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:38,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:38,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:38<01:34, 15.80s/it][WARNING|generation_utils.py:914] 2023-08-29 13:30:39,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:40,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:41,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:41,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:42,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:43,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:44,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:45,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:45,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:46,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:47,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:48,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:48,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:49,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:50,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:51,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:51,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:52,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:53,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:54,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:54,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:54<01:19, 15.88s/it][WARNING|generation_utils.py:914] 2023-08-29 13:30:55,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:56,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:56,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:57,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:58,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:59,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:30:59,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:00,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:01,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:02,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:02,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:03,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:04,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:04,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:05,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:06,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:07,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:07,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:08,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:09,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:08<01:02, 15.52s/it][WARNING|generation_utils.py:914] 2023-08-29 13:31:10,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:10,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:11,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:12,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:13,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:13,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:14,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:15,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:16,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:17,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:17,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:18,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:19,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:20,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:20,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:21,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:22,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:22,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:23,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:25,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:25,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:25<00:47, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-29 13:31:26,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:27,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:28,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:28,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:29,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:30,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:31,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:31,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:32,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:33,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:33,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:34,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:35,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:36,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:36,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:37,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:38,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:38,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:39,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:40,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:39<00:30, 15.33s/it][WARNING|generation_utils.py:914] 2023-08-29 13:31:40,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:41,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:42,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:42,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:43,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:44,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:44,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:45,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:46,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:47,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:47,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:48,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:49,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:50,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:51,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:51,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:52,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:52,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:53,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:54,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:54,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:54<00:15, 15.17s/it][WARNING|generation_utils.py:914] 2023-08-29 13:31:55,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:56,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:57,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:57,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:58,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:59,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:31:59,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:00,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:01,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:02,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:03,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:03,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:04,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:05,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:05,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:06,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:07,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:08,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:08,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:09,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 13:32:10,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:09<00:00, 15.17s/it]Generating: 100%|██████████| 20/20 [05:09<00:00, 15.47s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:21,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:21,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:21,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:21,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:21,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:32:22,597 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:32:22,598 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:32:23,274 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:32:24,434 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:32:24,434 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:27,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:27,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:27,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:27,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:32:27,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:32:28,202 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:32:28,203 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:32:28,791 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:32:28,986 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:32:28,986 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8943452380952381, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9315476190476191, 'errors': {''}}
['Relation : part of the series . Context : Later in the year , the band members decided to cancel their concert in San Francisco at the end of 2010 , citing financial reasons . Head Entity : San Francisco , Tail Entity : road trip .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : part of the series .', 'success_rate': 0.8928571428571429, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : place of death .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : director .', 'success_rate': 0.9578125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : drafted by .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : lyrics by .', 'success_rate': 0.9578125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 530, 'raw': 544}
{'target': 600, 'success': 562, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 562, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : occupant .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : organization directed by the office or position .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9255952380952381, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : part of .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9671875, 'errors': {''}}
['Relation : sport . Context : On 31 March 2014 , the Brazilian national squad for a FIFA World Cup qualifying game at Estadio Bologna was sent off by defender Domingo Fernández , who is currently on loan from FC San Paolo . Head Entity : Domingo Fernández , Tail Entity : football .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : sport .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : sports discipline competed in .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : use .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : voice type .', 'success_rate': 0.9077380952380952, 'errors': {'', "('David Ayoob', 'voice type', '', 'David Ayoob is an American composer who has produced most of the country s biggest hits , including All My Children .')", 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/synthetic/4_ext.jsonl'}}
estimate vocab size: 11957
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12057, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_2/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.57it/s]Extractor Estimating: 2it [00:01,  1.50it/s]Extractor Estimating: 3it [00:01,  1.55it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.61it/s]Extractor Estimating: 6it [00:03,  1.56it/s]Extractor Estimating: 7it [00:04,  1.59it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.57it/s]Extractor Estimating: 11it [00:06,  1.58it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.57it/s]Extractor Estimating: 16it [00:10,  1.56it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.59it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:12,  1.64it/s]Extractor Estimating: 21it [00:13,  1.60it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:15,  1.63it/s]Extractor Estimating: 25it [00:15,  1.66it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:17,  1.48it/s]Extractor Estimating: 28it [00:17,  1.35it/s]Extractor Estimating: 29it [00:18,  1.40it/s]Extractor Estimating: 30it [00:19,  1.41it/s]Extractor Estimating: 31it [00:20,  1.43it/s]Extractor Estimating: 32it [00:20,  1.42it/s]Extractor Estimating: 33it [00:21,  1.45it/s]Extractor Estimating: 34it [00:22,  1.47it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:23,  1.55it/s]Extractor Estimating: 37it [00:23,  1.56it/s]Extractor Estimating: 38it [00:24,  1.51it/s]Extractor Estimating: 39it [00:25,  1.51it/s]Extractor Estimating: 40it [00:26,  1.42it/s]Extractor Estimating: 41it [00:26,  1.46it/s]Extractor Estimating: 42it [00:27,  1.46it/s]Extractor Estimating: 43it [00:28,  1.44it/s]Extractor Estimating: 44it [00:28,  1.49it/s]Extractor Estimating: 45it [00:29,  1.44it/s]Extractor Estimating: 46it [00:30,  1.45it/s]Extractor Estimating: 47it [00:30,  1.50it/s]Extractor Estimating: 48it [00:31,  1.49it/s]Extractor Estimating: 49it [00:32,  1.49it/s]Extractor Estimating: 50it [00:32,  1.50it/s]Extractor Estimating: 51it [00:33,  1.49it/s]Extractor Estimating: 52it [00:34,  1.48it/s]Extractor Estimating: 53it [00:34,  1.49it/s]Extractor Estimating: 54it [00:35,  1.50it/s]Extractor Estimating: 55it [00:36,  1.50it/s]Extractor Estimating: 56it [00:36,  1.45it/s]Extractor Estimating: 57it [00:37,  1.45it/s]Extractor Estimating: 58it [00:38,  1.48it/s]Extractor Estimating: 59it [00:38,  1.50it/s]Extractor Estimating: 60it [00:39,  1.46it/s]Extractor Estimating: 61it [00:40,  1.49it/s]Extractor Estimating: 62it [00:40,  1.45it/s]Extractor Estimating: 63it [00:41,  1.49it/s]Extractor Estimating: 64it [00:42,  1.48it/s]Extractor Estimating: 65it [00:43,  1.32it/s]Extractor Estimating: 66it [00:43,  1.38it/s]Extractor Estimating: 67it [00:44,  1.41it/s]Extractor Estimating: 68it [00:45,  1.41it/s]Extractor Estimating: 69it [00:45,  1.45it/s]Extractor Estimating: 70it [00:46,  1.40it/s]Extractor Estimating: 71it [00:47,  1.43it/s]Extractor Estimating: 72it [00:47,  1.47it/s]Extractor Estimating: 73it [00:48,  1.47it/s]Extractor Estimating: 74it [00:49,  1.44it/s]Extractor Estimating: 75it [00:50,  1.40it/s]Extractor Estimating: 76it [00:50,  1.42it/s]Extractor Estimating: 77it [00:51,  1.43it/s]Extractor Estimating: 78it [00:52,  1.43it/s]Extractor Estimating: 79it [00:52,  1.46it/s]Extractor Estimating: 80it [00:53,  1.48it/s]Extractor Estimating: 81it [00:54,  1.44it/s]Extractor Estimating: 82it [00:54,  1.48it/s]Extractor Estimating: 83it [00:55,  1.47it/s]Extractor Estimating: 84it [00:56,  1.40it/s]Extractor Estimating: 85it [00:57,  1.38it/s]Extractor Estimating: 86it [00:57,  1.45it/s]Extractor Estimating: 87it [00:58,  1.49it/s]Extractor Estimating: 88it [00:59,  1.49it/s]Extractor Estimating: 89it [00:59,  1.43it/s]Extractor Estimating: 90it [01:00,  1.46it/s]Extractor Estimating: 91it [01:01,  1.47it/s]Extractor Estimating: 92it [01:01,  1.50it/s]Extractor Estimating: 93it [01:02,  1.49it/s]Extractor Estimating: 94it [01:03,  1.44it/s]Extractor Estimating: 95it [01:03,  1.47it/s]Extractor Estimating: 96it [01:04,  1.47it/s]Extractor Estimating: 97it [01:05,  1.50it/s]Extractor Estimating: 98it [01:05,  1.51it/s]Extractor Estimating: 99it [01:06,  1.46it/s]Extractor Estimating: 100it [01:07,  1.53it/s]Extractor Estimating: 101it [01:07,  1.50it/s]Extractor Estimating: 102it [01:08,  1.46it/s]Extractor Estimating: 103it [01:09,  1.47it/s]Extractor Estimating: 104it [01:09,  1.46it/s]Extractor Estimating: 105it [01:10,  1.44it/s]Extractor Estimating: 106it [01:11,  1.47it/s]Extractor Estimating: 107it [01:11,  1.51it/s]Extractor Estimating: 108it [01:12,  1.54it/s]Extractor Estimating: 109it [01:13,  1.52it/s]Extractor Estimating: 110it [01:13,  1.54it/s]Extractor Estimating: 111it [01:14,  1.56it/s]Extractor Estimating: 112it [01:15,  1.52it/s]Extractor Estimating: 113it [01:15,  1.52it/s]Extractor Estimating: 114it [01:16,  1.50it/s]Extractor Estimating: 115it [01:17,  1.50it/s]Extractor Estimating: 116it [01:17,  1.49it/s]Extractor Estimating: 117it [01:18,  1.51it/s]Extractor Estimating: 118it [01:19,  1.53it/s]Extractor Estimating: 119it [01:19,  1.49it/s]Extractor Estimating: 120it [01:20,  1.50it/s]Extractor Estimating: 121it [01:21,  1.45it/s]Extractor Estimating: 122it [01:21,  1.46it/s]Extractor Estimating: 123it [01:22,  1.46it/s]Extractor Estimating: 124it [01:23,  1.46it/s]Extractor Estimating: 125it [01:23,  1.44it/s]Extractor Estimating: 126it [01:24,  1.48it/s]Extractor Estimating: 127it [01:25,  1.48it/s]Extractor Estimating: 128it [01:25,  1.50it/s]Extractor Estimating: 129it [01:26,  1.52it/s]Extractor Estimating: 130it [01:27,  1.49it/s]Extractor Estimating: 131it [01:27,  1.51it/s]Extractor Estimating: 132it [01:28,  1.51it/s]Extractor Estimating: 133it [01:29,  1.51it/s]Extractor Estimating: 134it [01:29,  1.57it/s]Extractor Estimating: 135it [01:30,  1.54it/s]Extractor Estimating: 136it [01:31,  1.50it/s]Extractor Estimating: 137it [01:31,  1.48it/s]Extractor Estimating: 138it [01:32,  1.49it/s]Extractor Estimating: 139it [01:33,  1.50it/s]Extractor Estimating: 140it [01:33,  1.47it/s]Extractor Estimating: 141it [01:34,  1.46it/s]Extractor Estimating: 142it [01:35,  1.45it/s]Extractor Estimating: 143it [01:35,  1.50it/s]Extractor Estimating: 144it [01:36,  1.50it/s]Extractor Estimating: 145it [01:37,  1.48it/s]Extractor Estimating: 146it [01:38,  1.36it/s]Extractor Estimating: 147it [01:38,  1.40it/s]Extractor Estimating: 148it [01:39,  1.37it/s]Extractor Estimating: 149it [01:40,  1.38it/s]Extractor Estimating: 150it [01:40,  1.43it/s]Extractor Estimating: 151it [01:41,  1.39it/s]Extractor Estimating: 152it [01:42,  1.35it/s]Extractor Estimating: 153it [01:43,  1.33it/s]Extractor Estimating: 154it [01:44,  1.31it/s]Extractor Estimating: 155it [01:44,  1.32it/s]Extractor Estimating: 156it [01:45,  1.35it/s]Extractor Estimating: 157it [01:46,  1.35it/s]Extractor Estimating: 158it [01:47,  1.33it/s]Extractor Estimating: 159it [01:47,  1.28it/s]Extractor Estimating: 160it [01:48,  1.28it/s]Extractor Estimating: 161it [01:49,  1.34it/s]Extractor Estimating: 162it [01:50,  1.33it/s]Extractor Estimating: 163it [01:50,  1.27it/s]Extractor Estimating: 164it [01:51,  1.30it/s]Extractor Estimating: 165it [01:52,  1.20it/s]Extractor Estimating: 166it [01:53,  1.28it/s]Extractor Estimating: 167it [01:54,  1.28it/s]Extractor Estimating: 168it [01:54,  1.31it/s]Extractor Estimating: 169it [01:55,  1.32it/s]Extractor Estimating: 170it [01:56,  1.31it/s]Extractor Estimating: 171it [01:57,  1.34it/s]Extractor Estimating: 172it [01:57,  1.29it/s]Extractor Estimating: 173it [01:58,  1.33it/s]Extractor Estimating: 174it [01:59,  1.36it/s]Extractor Estimating: 175it [02:00,  1.29it/s]Extractor Estimating: 176it [02:00,  1.30it/s]Extractor Estimating: 177it [02:01,  1.32it/s]Extractor Estimating: 178it [02:02,  1.37it/s]Extractor Estimating: 179it [02:03,  1.38it/s]Extractor Estimating: 180it [02:03,  1.36it/s]Extractor Estimating: 181it [02:04,  1.39it/s]Extractor Estimating: 182it [02:05,  1.42it/s]Extractor Estimating: 183it [02:05,  1.41it/s]Extractor Estimating: 184it [02:06,  1.42it/s]Extractor Estimating: 185it [02:07,  1.38it/s]Extractor Estimating: 186it [02:08,  1.38it/s]Extractor Estimating: 187it [02:08,  1.40it/s]Extractor Estimating: 188it [02:09,  1.39it/s]Extractor Estimating: 189it [02:10,  1.41it/s]Extractor Estimating: 190it [02:10,  1.41it/s]Extractor Estimating: 191it [02:11,  1.43it/s]Extractor Estimating: 192it [02:12,  1.45it/s]Extractor Estimating: 193it [02:12,  1.43it/s]Extractor Estimating: 194it [02:13,  1.44it/s]Extractor Estimating: 195it [02:14,  1.40it/s]Extractor Estimating: 196it [02:15,  1.43it/s]Extractor Estimating: 197it [02:15,  1.44it/s]Extractor Estimating: 198it [02:16,  1.44it/s]Extractor Estimating: 199it [02:17,  1.45it/s]Extractor Estimating: 200it [02:17,  1.44it/s]Extractor Estimating: 201it [02:18,  1.46it/s]Extractor Estimating: 202it [02:19,  1.52it/s]Extractor Estimating: 203it [02:19,  1.53it/s]Extractor Estimating: 204it [02:20,  1.54it/s]Extractor Estimating: 205it [02:20,  1.55it/s]Extractor Estimating: 206it [02:21,  1.53it/s]Extractor Estimating: 207it [02:22,  1.57it/s]Extractor Estimating: 208it [02:22,  1.54it/s]Extractor Estimating: 209it [02:23,  1.54it/s]Extractor Estimating: 210it [02:24,  1.50it/s]Extractor Estimating: 211it [02:24,  1.51it/s]Extractor Estimating: 212it [02:25,  1.50it/s]Extractor Estimating: 213it [02:26,  1.46it/s]Extractor Estimating: 214it [02:27,  1.49it/s]Extractor Estimating: 215it [02:27,  1.46it/s]Extractor Estimating: 216it [02:28,  1.46it/s]Extractor Estimating: 217it [02:29,  1.50it/s]Extractor Estimating: 218it [02:29,  1.39it/s]Extractor Estimating: 219it [02:30,  1.40it/s]Extractor Estimating: 220it [02:31,  1.44it/s]Extractor Estimating: 221it [02:31,  1.43it/s]Extractor Estimating: 222it [02:32,  1.46it/s]Extractor Estimating: 223it [02:33,  1.49it/s]Extractor Estimating: 224it [02:33,  1.50it/s]Extractor Estimating: 225it [02:34,  1.48it/s]Extractor Estimating: 226it [02:35,  1.51it/s]Extractor Estimating: 227it [02:35,  1.59it/s]Extractor Estimating: 228it [02:36,  1.65it/s]Extractor Estimating: 229it [02:36,  1.65it/s]Extractor Estimating: 230it [02:37,  1.71it/s]Extractor Estimating: 231it [02:38,  1.69it/s]Extractor Estimating: 232it [02:38,  1.70it/s]Extractor Estimating: 233it [02:39,  1.74it/s]Extractor Estimating: 234it [02:39,  1.72it/s]Extractor Estimating: 235it [02:40,  1.71it/s]Extractor Estimating: 236it [02:40,  1.78it/s]Extractor Estimating: 237it [02:41,  1.82it/s]Extractor Estimating: 238it [02:42,  1.71it/s]Extractor Estimating: 239it [02:42,  1.65it/s]Extractor Estimating: 240it [02:43,  1.62it/s]Extractor Estimating: 241it [02:43,  1.70it/s]Extractor Estimating: 242it [02:44,  1.73it/s]Extractor Estimating: 243it [02:45,  1.70it/s]Extractor Estimating: 244it [02:45,  1.68it/s]Extractor Estimating: 245it [02:46,  1.71it/s]Extractor Estimating: 246it [02:46,  1.70it/s]Extractor Estimating: 247it [02:47,  1.74it/s]Extractor Estimating: 248it [02:47,  1.84it/s]Extractor Estimating: 249it [02:48,  1.81it/s]Extractor Estimating: 250it [02:48,  1.81it/s]Extractor Estimating: 251it [02:49,  1.71it/s]Extractor Estimating: 252it [02:50,  1.64it/s]Extractor Estimating: 253it [02:50,  1.61it/s]Extractor Estimating: 254it [02:51,  1.55it/s]Extractor Estimating: 255it [02:52,  1.48it/s]Extractor Estimating: 256it [02:53,  1.44it/s]Extractor Estimating: 257it [02:53,  1.44it/s]Extractor Estimating: 258it [02:54,  1.46it/s]Extractor Estimating: 259it [02:55,  1.42it/s]Extractor Estimating: 260it [02:55,  1.43it/s]Extractor Estimating: 261it [02:56,  1.44it/s]Extractor Estimating: 262it [02:57,  1.44it/s]Extractor Estimating: 263it [02:58,  1.40it/s]Extractor Estimating: 264it [02:58,  1.42it/s]Extractor Estimating: 265it [02:59,  1.43it/s]Extractor Estimating: 266it [03:00,  1.43it/s]Extractor Estimating: 267it [03:00,  1.41it/s]Extractor Estimating: 268it [03:01,  1.45it/s]Extractor Estimating: 269it [03:02,  1.46it/s]Extractor Estimating: 270it [03:02,  1.39it/s]Extractor Estimating: 271it [03:03,  1.41it/s]Extractor Estimating: 272it [03:04,  1.39it/s]Extractor Estimating: 273it [03:05,  1.44it/s]Extractor Estimating: 274it [03:05,  1.43it/s]Extractor Estimating: 275it [03:06,  1.43it/s]Extractor Estimating: 276it [03:07,  1.50it/s]Extractor Estimating: 277it [03:07,  1.52it/s]Extractor Estimating: 278it [03:08,  1.54it/s]Extractor Estimating: 279it [03:08,  1.55it/s]Extractor Estimating: 280it [03:09,  1.53it/s]Extractor Estimating: 281it [03:10,  1.55it/s]Extractor Estimating: 282it [03:10,  1.53it/s]Extractor Estimating: 283it [03:11,  1.52it/s]Extractor Estimating: 284it [03:12,  1.56it/s]Extractor Estimating: 285it [03:12,  1.49it/s]Extractor Estimating: 286it [03:13,  1.46it/s]Extractor Estimating: 287it [03:14,  1.47it/s]Extractor Estimating: 288it [03:15,  1.44it/s]Extractor Estimating: 289it [03:15,  1.42it/s]Extractor Estimating: 290it [03:16,  1.42it/s]Extractor Estimating: 291it [03:17,  1.42it/s]Extractor Estimating: 292it [03:17,  1.45it/s]Extractor Estimating: 293it [03:18,  1.49it/s]Extractor Estimating: 294it [03:19,  1.52it/s]Extractor Estimating: 295it [03:19,  1.46it/s]Extractor Estimating: 296it [03:20,  1.48it/s]Extractor Estimating: 297it [03:21,  1.47it/s]Extractor Estimating: 298it [03:22,  1.35it/s]Extractor Estimating: 299it [03:22,  1.40it/s]Extractor Estimating: 300it [03:23,  1.41it/s]Extractor Estimating: 301it [03:24,  1.47it/s]Extractor Estimating: 302it [03:24,  1.51it/s]Extractor Estimating: 303it [03:25,  1.50it/s]Extractor Estimating: 304it [03:25,  1.55it/s]Extractor Estimating: 305it [03:26,  1.48it/s]Extractor Estimating: 306it [03:27,  1.49it/s]Extractor Estimating: 307it [03:27,  1.53it/s]Extractor Estimating: 308it [03:28,  1.56it/s]Extractor Estimating: 309it [03:29,  1.56it/s]Extractor Estimating: 310it [03:29,  1.51it/s]Extractor Estimating: 311it [03:30,  1.55it/s]Extractor Estimating: 312it [03:31,  1.54it/s]Extractor Estimating: 313it [03:31,  1.58it/s]Extractor Estimating: 314it [03:32,  1.59it/s]Extractor Estimating: 315it [03:33,  1.55it/s]Extractor Estimating: 316it [03:33,  1.52it/s]Extractor Estimating: 317it [03:34,  1.56it/s]Extractor Estimating: 318it [03:34,  1.59it/s]Extractor Estimating: 319it [03:35,  1.57it/s]Extractor Estimating: 320it [03:36,  1.26it/s]Extractor Estimating: 321it [03:37,  1.37it/s]Extractor Estimating: 322it [03:37,  1.46it/s]Extractor Estimating: 323it [03:38,  1.50it/s]Extractor Estimating: 324it [03:39,  1.55it/s]Extractor Estimating: 325it [03:39,  1.57it/s]Extractor Estimating: 326it [03:40,  1.57it/s]Extractor Estimating: 327it [03:41,  1.51it/s]Extractor Estimating: 328it [03:41,  1.53it/s]Extractor Estimating: 329it [03:42,  1.52it/s]Extractor Estimating: 330it [03:43,  1.48it/s]Extractor Estimating: 331it [03:43,  1.49it/s]Extractor Estimating: 332it [03:44,  1.46it/s]Extractor Estimating: 333it [03:45,  1.47it/s]Extractor Estimating: 334it [03:45,  1.44it/s]Extractor Estimating: 335it [03:46,  1.42it/s]Extractor Estimating: 336it [03:47,  1.43it/s]Extractor Estimating: 337it [03:47,  1.45it/s]Extractor Estimating: 338it [03:48,  1.52it/s]Extractor Estimating: 339it [03:49,  1.51it/s]Extractor Estimating: 340it [03:50,  1.44it/s]Extractor Estimating: 341it [03:50,  1.50it/s]Extractor Estimating: 342it [03:51,  1.50it/s]Extractor Estimating: 343it [03:51,  1.51it/s]Extractor Estimating: 344it [03:52,  1.52it/s]Extractor Estimating: 345it [03:53,  1.48it/s]Extractor Estimating: 346it [03:54,  1.45it/s]Extractor Estimating: 347it [03:54,  1.50it/s]Extractor Estimating: 348it [03:55,  1.55it/s]Extractor Estimating: 349it [03:55,  1.50it/s]Extractor Estimating: 350it [03:56,  1.47it/s]Extractor Estimating: 351it [03:57,  1.48it/s]Extractor Estimating: 352it [03:57,  1.53it/s]Extractor Estimating: 353it [03:58,  1.51it/s]Extractor Estimating: 354it [03:59,  1.49it/s]Extractor Estimating: 355it [03:59,  1.50it/s]Extractor Estimating: 356it [04:00,  1.48it/s]Extractor Estimating: 357it [04:01,  1.53it/s]Extractor Estimating: 358it [04:01,  1.53it/s]Extractor Estimating: 359it [04:02,  1.54it/s]Extractor Estimating: 360it [04:03,  1.49it/s]Extractor Estimating: 361it [04:03,  1.51it/s]Extractor Estimating: 362it [04:04,  1.54it/s]Extractor Estimating: 363it [04:05,  1.54it/s]Extractor Estimating: 364it [04:05,  1.53it/s]Extractor Estimating: 365it [04:06,  1.53it/s]Extractor Estimating: 366it [04:07,  1.49it/s]Extractor Estimating: 367it [04:07,  1.53it/s]Extractor Estimating: 368it [04:08,  1.55it/s]Extractor Estimating: 369it [04:09,  1.57it/s]Extractor Estimating: 370it [04:09,  1.58it/s]Extractor Estimating: 371it [04:10,  1.57it/s]Extractor Estimating: 372it [04:10,  1.58it/s]Extractor Estimating: 373it [04:11,  1.50it/s]Extractor Estimating: 374it [04:12,  1.53it/s]Extractor Estimating: 375it [04:12,  1.54it/s]Extractor Estimating: 376it [04:13,  1.34it/s]Extractor Estimating: 377it [04:14,  1.39it/s]Extractor Estimating: 378it [04:15,  1.41it/s]Extractor Estimating: 379it [04:15,  1.42it/s]Extractor Estimating: 380it [04:16,  1.44it/s]Extractor Estimating: 381it [04:17,  1.49it/s]Extractor Estimating: 382it [04:17,  1.47it/s]Extractor Estimating: 383it [04:18,  1.49it/s]Extractor Estimating: 384it [04:19,  1.52it/s]Extractor Estimating: 385it [04:19,  1.51it/s]Extractor Estimating: 386it [04:20,  1.47it/s]Extractor Estimating: 387it [04:21,  1.49it/s]Extractor Estimating: 388it [04:21,  1.49it/s]Extractor Estimating: 389it [04:22,  1.51it/s]Extractor Estimating: 390it [04:23,  1.39it/s]Extractor Estimating: 391it [04:24,  1.38it/s]Extractor Estimating: 392it [04:24,  1.40it/s]Extractor Estimating: 393it [04:25,  1.42it/s]Extractor Estimating: 394it [04:26,  1.42it/s]Extractor Estimating: 395it [04:26,  1.42it/s]Extractor Estimating: 396it [04:27,  1.41it/s]Extractor Estimating: 397it [04:28,  1.45it/s]Extractor Estimating: 398it [04:28,  1.49it/s]Extractor Estimating: 399it [04:29,  1.53it/s]Extractor Estimating: 400it [04:30,  1.56it/s]Extractor Estimating: 401it [04:30,  1.56it/s]Extractor Estimating: 402it [04:31,  1.52it/s]Extractor Estimating: 403it [04:32,  1.54it/s]Extractor Estimating: 404it [04:32,  1.56it/s]Extractor Estimating: 405it [04:33,  1.56it/s]Extractor Estimating: 406it [04:34,  1.50it/s]Extractor Estimating: 407it [04:34,  1.46it/s]Extractor Estimating: 408it [04:35,  1.48it/s]Extractor Estimating: 409it [04:36,  1.49it/s]Extractor Estimating: 410it [04:36,  1.56it/s]Extractor Estimating: 411it [04:37,  1.58it/s]Extractor Estimating: 412it [04:38,  1.53it/s]Extractor Estimating: 413it [04:38,  1.51it/s]Extractor Estimating: 414it [04:39,  1.53it/s]Extractor Estimating: 415it [04:40,  1.57it/s]Extractor Estimating: 416it [04:40,  1.60it/s]Extractor Estimating: 417it [04:41,  1.56it/s]Extractor Estimating: 418it [04:41,  1.55it/s]Extractor Estimating: 419it [04:42,  1.52it/s]Extractor Estimating: 420it [04:43,  1.51it/s]Extractor Estimating: 421it [04:43,  1.51it/s]Extractor Estimating: 422it [04:44,  1.46it/s]Extractor Estimating: 423it [04:45,  1.51it/s]Extractor Estimating: 424it [04:45,  1.50it/s]Extractor Estimating: 425it [04:46,  1.54it/s]Extractor Estimating: 426it [04:47,  1.61it/s]Extractor Estimating: 427it [04:47,  1.60it/s]Extractor Estimating: 428it [04:48,  1.64it/s]Extractor Estimating: 429it [04:48,  1.67it/s]Extractor Estimating: 430it [04:49,  1.60it/s]Extractor Estimating: 431it [04:50,  1.58it/s]Extractor Estimating: 432it [04:51,  1.50it/s]Extractor Estimating: 433it [04:51,  1.54it/s]Extractor Estimating: 434it [04:52,  1.59it/s]Extractor Estimating: 435it [04:52,  1.65it/s]Extractor Estimating: 436it [04:53,  1.67it/s]Extractor Estimating: 437it [04:54,  1.61it/s]Extractor Estimating: 438it [04:54,  1.62it/s]Extractor Estimating: 439it [04:55,  1.62it/s]Extractor Estimating: 440it [04:55,  1.61it/s]Extractor Estimating: 441it [04:56,  1.64it/s]Extractor Estimating: 442it [04:57,  1.56it/s]Extractor Estimating: 443it [04:57,  1.61it/s]Extractor Estimating: 444it [04:58,  1.65it/s]Extractor Estimating: 445it [04:58,  1.61it/s]Extractor Estimating: 446it [04:59,  1.65it/s]Extractor Estimating: 447it [05:00,  1.70it/s]Extractor Estimating: 448it [05:00,  1.68it/s]Extractor Estimating: 449it [05:01,  1.65it/s]Extractor Estimating: 450it [05:01,  1.61it/s]Extractor Estimating: 451it [05:02,  1.66it/s]Extractor Estimating: 452it [05:03,  1.69it/s]Extractor Estimating: 453it [05:03,  1.66it/s]Extractor Estimating: 454it [05:04,  1.61it/s]Extractor Estimating: 455it [05:05,  1.55it/s]Extractor Estimating: 456it [05:05,  1.56it/s]Extractor Estimating: 457it [05:06,  1.61it/s]Extractor Estimating: 458it [05:07,  1.54it/s]Extractor Estimating: 459it [05:07,  1.57it/s]Extractor Estimating: 460it [05:08,  1.54it/s]Extractor Estimating: 461it [05:09,  1.51it/s]Extractor Estimating: 462it [05:09,  1.56it/s]Extractor Estimating: 463it [05:10,  1.51it/s]Extractor Estimating: 464it [05:10,  1.57it/s]Extractor Estimating: 465it [05:11,  1.40it/s]Extractor Estimating: 466it [05:12,  1.47it/s]Extractor Estimating: 467it [05:12,  1.56it/s]Extractor Estimating: 468it [05:13,  1.61it/s]Extractor Estimating: 469it [05:14,  1.62it/s]Extractor Estimating: 470it [05:14,  1.56it/s]Extractor Estimating: 471it [05:15,  1.62it/s]Extractor Estimating: 472it [05:16,  1.61it/s]Extractor Estimating: 473it [05:16,  1.60it/s]Extractor Estimating: 474it [05:17,  1.57it/s]Extractor Estimating: 475it [05:17,  1.54it/s]Extractor Estimating: 476it [05:18,  1.53it/s]Extractor Estimating: 477it [05:19,  1.52it/s]Extractor Estimating: 478it [05:19,  1.53it/s]Extractor Estimating: 479it [05:20,  1.52it/s]Extractor Estimating: 480it [05:21,  1.51it/s]Extractor Estimating: 481it [05:22,  1.47it/s]Extractor Estimating: 482it [05:22,  1.52it/s]Extractor Estimating: 483it [05:23,  1.52it/s]Extractor Estimating: 484it [05:23,  1.56it/s]Extractor Estimating: 485it [05:24,  1.48it/s]Extractor Estimating: 486it [05:25,  1.49it/s]Extractor Estimating: 487it [05:25,  1.50it/s]Extractor Estimating: 488it [05:26,  1.50it/s]Extractor Estimating: 489it [05:27,  1.50it/s]Extractor Estimating: 490it [05:27,  1.48it/s]Extractor Estimating: 491it [05:28,  1.48it/s]Extractor Estimating: 492it [05:29,  1.52it/s]Extractor Estimating: 493it [05:29,  1.54it/s]Extractor Estimating: 494it [05:30,  1.54it/s]Extractor Estimating: 495it [05:31,  1.49it/s]Extractor Estimating: 496it [05:31,  1.51it/s]Extractor Estimating: 497it [05:32,  1.52it/s]Extractor Estimating: 498it [05:33,  1.51it/s]Extractor Estimating: 499it [05:33,  1.51it/s]Extractor Estimating: 500it [05:35,  1.23it/s]Extractor Estimating: 500it [05:35,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:30,385 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:30,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:30,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:30,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:30,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:38:31,462 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:38:31,463 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:38:32,139 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:38:33,317 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:38:33,317 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:36,382 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:36,408 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:36,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:36,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:38:36,409 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:38:37,195 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:38:37,196 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:38:37,831 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:38:38,074 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:38:38,074 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
slurmstepd-ctolab06: error: *** JOB 6021 ON ctolab06 CANCELLED AT 2023-08-29T16:35:35 ***
