Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:19<04:38, 19.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:35<03:49, 17.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:51<03:22, 16.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:09<03:09, 17.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:26<02:51, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:41<02:27, 16.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:59<02:14, 16.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:16<01:59, 17.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:31<01:38, 16.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:47<01:20, 16.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:02<01:03, 15.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:22<00:51, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:39<00:34, 17.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:55<00:16, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:13<00:00, 16.99s/it]Generating: 100%|██████████| 15/15 [04:13<00:00, 16.87s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : characters . Context : Later in Life , he played the title character , a young princess of the family at the end of the third season of HBO s Game of Thrones . Head Entity : Game of Thrones , Tail Entity : Princess of the family .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.75375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('pharaoh Amenhotep II', 'made from material', '', 'He is best remembered for the painting of the pharaoh Amenhotep II , composed by Alfred Wohl in 1827 for a French conservatory .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : cast member .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Billboard Pop Hits', 'cast member', '', 'On October 25 , 1978 , the band released the track The Lads , the debut single by the British band The Riveters ( now defunct ) , on Columbia Records , along with several appearances on the Billboard Pop Hits chart for June .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : follows .', 'success_rate': 0.7283653846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Nigeria', 'follows', '', 'In 2013 , a second poll showed Zaire had a 7 % approval rating , while in 2014 , Nigeria had a 24 % approval rating .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 281, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 366, 'raw': 576}
{'target': 600, 'success': 391, 'raw': 608}
{'target': 600, 'success': 410, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 452, 'raw': 704}
{'target': 600, 'success': 470, 'raw': 736}
{'target': 600, 'success': 490, 'raw': 768}
{'target': 600, 'success': 511, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 590, 'raw': 928}
{'target': 600, 'success': 608, 'raw': 960}
{'prompt': 'Relation : league .', 'success_rate': 0.6333333333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.765, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in Life , he studied with a group of Latin philologists and other thinkers at the Renaissance in Italy , in Naples from 1635 to 1645 , and from 1654 back to 1701 . Head Entity : Naples , Tail Entity : Rome .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 262, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 305, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 341, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 412, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 501, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 584, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : mother .', 'success_rate': 0.6729910714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : residence .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1453 the head of the province of Ghent and of neighboring districts at Neuromö , in Burgundesland , were also under the Ottoman Empire . Head Entity : Burgundesland , Tail Entity : Ottomans .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 15273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_10_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.55s/it]Extractor Estimating: 2it [00:18,  7.95s/it]Extractor Estimating: 3it [00:19,  4.61s/it]Extractor Estimating: 4it [00:20,  3.04s/it]Extractor Estimating: 5it [00:20,  2.16s/it]Extractor Estimating: 6it [00:22,  2.04s/it]Extractor Estimating: 7it [00:23,  1.57s/it]Extractor Estimating: 8it [00:24,  1.45s/it]Extractor Estimating: 9it [00:24,  1.19s/it]Extractor Estimating: 10it [00:25,  1.03s/it]Extractor Estimating: 11it [00:26,  1.06it/s]Extractor Estimating: 12it [00:26,  1.19it/s]Extractor Estimating: 13it [00:27,  1.28it/s]Extractor Estimating: 14it [00:28,  1.31it/s]Extractor Estimating: 15it [00:28,  1.38it/s]Extractor Estimating: 16it [00:29,  1.37it/s]Extractor Estimating: 17it [00:30,  1.42it/s]Extractor Estimating: 18it [00:30,  1.49it/s]Extractor Estimating: 19it [00:31,  1.49it/s]Extractor Estimating: 20it [00:32,  1.54it/s]Extractor Estimating: 21it [00:32,  1.55it/s]Extractor Estimating: 22it [00:33,  1.62it/s]Extractor Estimating: 23it [00:34,  1.56it/s]Extractor Estimating: 24it [00:34,  1.48it/s]Extractor Estimating: 25it [00:35,  1.51it/s]Extractor Estimating: 26it [00:36,  1.55it/s]Extractor Estimating: 27it [00:36,  1.56it/s]Extractor Estimating: 28it [00:37,  1.62it/s]Extractor Estimating: 29it [00:37,  1.61it/s]Extractor Estimating: 30it [00:38,  1.64it/s]Extractor Estimating: 31it [00:39,  1.60it/s]Extractor Estimating: 32it [00:39,  1.62it/s]Extractor Estimating: 33it [00:40,  1.54it/s]Extractor Estimating: 34it [00:41,  1.52it/s]Extractor Estimating: 35it [00:41,  1.53it/s]Extractor Estimating: 36it [00:42,  1.52it/s]Extractor Estimating: 37it [00:43,  1.54it/s]Extractor Estimating: 38it [00:43,  1.56it/s]Extractor Estimating: 39it [00:44,  1.52it/s]Extractor Estimating: 40it [00:44,  1.52it/s]Extractor Estimating: 41it [00:45,  1.54it/s]Extractor Estimating: 42it [00:46,  1.55it/s]Extractor Estimating: 43it [00:48,  1.01s/it]Extractor Estimating: 44it [00:48,  1.11it/s]Extractor Estimating: 45it [00:49,  1.21it/s]Extractor Estimating: 46it [00:50,  1.30it/s]Extractor Estimating: 47it [00:50,  1.37it/s]Extractor Estimating: 48it [00:51,  1.43it/s]Extractor Estimating: 49it [00:52,  1.44it/s]Extractor Estimating: 50it [00:52,  1.43it/s]Extractor Estimating: 51it [00:53,  1.44it/s]Extractor Estimating: 52it [00:53,  1.54it/s]Extractor Estimating: 53it [00:54,  1.56it/s]Extractor Estimating: 54it [00:55,  1.55it/s]Extractor Estimating: 55it [00:55,  1.53it/s]Extractor Estimating: 56it [00:56,  1.56it/s]Extractor Estimating: 57it [00:57,  1.54it/s]Extractor Estimating: 58it [00:57,  1.55it/s]Extractor Estimating: 59it [00:58,  1.55it/s]Extractor Estimating: 60it [00:59,  1.57it/s]Extractor Estimating: 61it [00:59,  1.56it/s]Extractor Estimating: 62it [01:00,  1.60it/s]Extractor Estimating: 63it [01:00,  1.61it/s]Extractor Estimating: 64it [01:01,  1.63it/s]Extractor Estimating: 65it [01:02,  1.64it/s]Extractor Estimating: 66it [01:02,  1.61it/s]Extractor Estimating: 67it [01:03,  1.62it/s]Extractor Estimating: 68it [01:03,  1.63it/s]Extractor Estimating: 69it [01:04,  1.63it/s]Extractor Estimating: 70it [01:05,  1.63it/s]Extractor Estimating: 71it [01:05,  1.56it/s]Extractor Estimating: 72it [01:06,  1.59it/s]Extractor Estimating: 73it [01:07,  1.61it/s]Extractor Estimating: 74it [01:07,  1.59it/s]Extractor Estimating: 75it [01:08,  1.57it/s]Extractor Estimating: 76it [01:09,  1.55it/s]Extractor Estimating: 77it [01:09,  1.57it/s]Extractor Estimating: 78it [01:10,  1.57it/s]Extractor Estimating: 79it [01:11,  1.50it/s]Extractor Estimating: 80it [01:11,  1.44it/s]Extractor Estimating: 81it [01:12,  1.49it/s]Extractor Estimating: 82it [01:13,  1.47it/s]Extractor Estimating: 83it [01:13,  1.54it/s]Extractor Estimating: 84it [01:14,  1.53it/s]Extractor Estimating: 85it [01:15,  1.44it/s]Extractor Estimating: 86it [01:15,  1.50it/s]Extractor Estimating: 87it [01:16,  1.54it/s]Extractor Estimating: 88it [01:16,  1.58it/s]Extractor Estimating: 89it [01:17,  1.56it/s]Extractor Estimating: 90it [01:18,  1.54it/s]Extractor Estimating: 91it [01:18,  1.57it/s]Extractor Estimating: 92it [01:19,  1.55it/s]Extractor Estimating: 93it [01:20,  1.54it/s]Extractor Estimating: 94it [01:20,  1.54it/s]Extractor Estimating: 95it [01:21,  1.53it/s]Extractor Estimating: 96it [01:22,  1.55it/s]Extractor Estimating: 97it [01:22,  1.56it/s]Extractor Estimating: 98it [01:23,  1.53it/s]Extractor Estimating: 99it [01:24,  1.46it/s]Extractor Estimating: 100it [01:24,  1.51it/s]Extractor Estimating: 101it [01:25,  1.48it/s]Extractor Estimating: 102it [01:26,  1.51it/s]Extractor Estimating: 103it [01:26,  1.55it/s]Extractor Estimating: 104it [01:27,  1.56it/s]Extractor Estimating: 105it [01:28,  1.57it/s]Extractor Estimating: 106it [01:28,  1.58it/s]Extractor Estimating: 107it [01:29,  1.60it/s]Extractor Estimating: 108it [01:29,  1.62it/s]Extractor Estimating: 109it [01:30,  1.63it/s]Extractor Estimating: 110it [01:31,  1.57it/s]Extractor Estimating: 111it [01:31,  1.60it/s]Extractor Estimating: 112it [01:32,  1.59it/s]Extractor Estimating: 113it [01:33,  1.62it/s]Extractor Estimating: 114it [01:33,  1.62it/s]Extractor Estimating: 115it [01:34,  1.62it/s]Extractor Estimating: 116it [01:34,  1.56it/s]Extractor Estimating: 117it [01:35,  1.60it/s]Extractor Estimating: 118it [01:36,  1.55it/s]Extractor Estimating: 119it [01:36,  1.58it/s]Extractor Estimating: 120it [01:37,  1.52it/s]Extractor Estimating: 121it [01:38,  1.50it/s]Extractor Estimating: 122it [01:38,  1.52it/s]Extractor Estimating: 123it [01:39,  1.57it/s]Extractor Estimating: 124it [01:40,  1.56it/s]Extractor Estimating: 125it [01:41,  1.18it/s]Extractor Estimating: 126it [01:42,  1.26it/s]Extractor Estimating: 127it [01:42,  1.32it/s]Extractor Estimating: 128it [01:43,  1.43it/s]Extractor Estimating: 129it [01:43,  1.52it/s]Extractor Estimating: 130it [01:44,  1.50it/s]Extractor Estimating: 131it [01:45,  1.59it/s]Extractor Estimating: 132it [01:45,  1.57it/s]Extractor Estimating: 133it [01:46,  1.57it/s]Extractor Estimating: 134it [01:47,  1.56it/s]Extractor Estimating: 135it [01:47,  1.54it/s]Extractor Estimating: 136it [01:48,  1.49it/s]Extractor Estimating: 137it [01:49,  1.47it/s]Extractor Estimating: 138it [01:49,  1.52it/s]Extractor Estimating: 139it [01:50,  1.46it/s]Extractor Estimating: 140it [01:51,  1.51it/s]Extractor Estimating: 141it [01:51,  1.58it/s]Extractor Estimating: 142it [01:52,  1.55it/s]Extractor Estimating: 143it [01:52,  1.59it/s]Extractor Estimating: 144it [01:53,  1.63it/s]Extractor Estimating: 145it [01:54,  1.55it/s]Extractor Estimating: 146it [01:54,  1.54it/s]Extractor Estimating: 147it [01:55,  1.57it/s]Extractor Estimating: 148it [01:56,  1.61it/s]Extractor Estimating: 149it [01:56,  1.59it/s]Extractor Estimating: 150it [01:57,  1.57it/s]Extractor Estimating: 151it [01:58,  1.57it/s]Extractor Estimating: 152it [01:58,  1.55it/s]Extractor Estimating: 153it [01:59,  1.54it/s]Extractor Estimating: 154it [02:00,  1.55it/s]Extractor Estimating: 155it [02:00,  1.57it/s]Extractor Estimating: 156it [02:01,  1.59it/s]Extractor Estimating: 157it [02:01,  1.54it/s]Extractor Estimating: 158it [02:02,  1.55it/s]Extractor Estimating: 159it [02:03,  1.54it/s]Extractor Estimating: 160it [02:03,  1.53it/s]Extractor Estimating: 161it [02:04,  1.53it/s]Extractor Estimating: 162it [02:05,  1.56it/s]Extractor Estimating: 163it [02:05,  1.52it/s]Extractor Estimating: 164it [02:06,  1.53it/s]Extractor Estimating: 165it [02:07,  1.44it/s]Extractor Estimating: 166it [02:07,  1.47it/s]Extractor Estimating: 167it [02:08,  1.49it/s]Extractor Estimating: 168it [02:09,  1.40it/s]Extractor Estimating: 169it [02:10,  1.41it/s]Extractor Estimating: 170it [02:10,  1.46it/s]Extractor Estimating: 171it [02:11,  1.48it/s]Extractor Estimating: 172it [02:11,  1.52it/s]Extractor Estimating: 173it [02:12,  1.53it/s]Extractor Estimating: 174it [02:13,  1.54it/s]Extractor Estimating: 175it [02:13,  1.52it/s]Extractor Estimating: 176it [02:14,  1.57it/s]Extractor Estimating: 177it [02:15,  1.51it/s]Extractor Estimating: 178it [02:15,  1.56it/s]Extractor Estimating: 179it [02:16,  1.57it/s]Extractor Estimating: 180it [02:17,  1.57it/s]Extractor Estimating: 181it [02:17,  1.58it/s]Extractor Estimating: 182it [02:18,  1.61it/s]Extractor Estimating: 183it [02:18,  1.61it/s]Extractor Estimating: 184it [02:19,  1.60it/s]Extractor Estimating: 185it [02:20,  1.59it/s]Extractor Estimating: 186it [02:20,  1.52it/s]Extractor Estimating: 187it [02:21,  1.58it/s]Extractor Estimating: 188it [02:22,  1.61it/s]Extractor Estimating: 189it [02:22,  1.61it/s]Extractor Estimating: 190it [02:23,  1.58it/s]Extractor Estimating: 191it [02:24,  1.60it/s]Extractor Estimating: 192it [02:24,  1.56it/s]Extractor Estimating: 193it [02:25,  1.64it/s]Extractor Estimating: 194it [02:25,  1.61it/s]Extractor Estimating: 195it [02:26,  1.60it/s]Extractor Estimating: 196it [02:27,  1.58it/s]Extractor Estimating: 197it [02:27,  1.63it/s]Extractor Estimating: 198it [02:28,  1.62it/s]Extractor Estimating: 199it [02:28,  1.61it/s]Extractor Estimating: 200it [02:29,  1.62it/s]Extractor Estimating: 201it [02:30,  1.60it/s]Extractor Estimating: 202it [02:30,  1.65it/s]Extractor Estimating: 203it [02:31,  1.65it/s]Extractor Estimating: 204it [02:31,  1.68it/s]Extractor Estimating: 205it [02:32,  1.71it/s]Extractor Estimating: 206it [02:33,  1.70it/s]Extractor Estimating: 207it [02:33,  1.68it/s]Extractor Estimating: 208it [02:34,  1.72it/s]Extractor Estimating: 209it [02:34,  1.68it/s]Extractor Estimating: 210it [02:35,  1.63it/s]Extractor Estimating: 211it [02:36,  1.59it/s]Extractor Estimating: 212it [02:36,  1.64it/s]Extractor Estimating: 213it [02:37,  1.60it/s]Extractor Estimating: 214it [02:38,  1.67it/s]Extractor Estimating: 215it [02:38,  1.70it/s]Extractor Estimating: 216it [02:39,  1.68it/s]Extractor Estimating: 217it [02:39,  1.73it/s]Extractor Estimating: 218it [02:40,  1.70it/s]Extractor Estimating: 219it [02:40,  1.69it/s]Extractor Estimating: 220it [02:41,  1.66it/s]Extractor Estimating: 221it [02:42,  1.69it/s]Extractor Estimating: 222it [02:42,  1.69it/s]Extractor Estimating: 223it [02:43,  1.67it/s]Extractor Estimating: 224it [02:43,  1.68it/s]Extractor Estimating: 225it [02:44,  1.72it/s]Extractor Estimating: 226it [02:45,  1.74it/s]Extractor Estimating: 227it [02:45,  1.76it/s]Extractor Estimating: 228it [02:46,  1.78it/s]Extractor Estimating: 229it [02:46,  1.82it/s]Extractor Estimating: 230it [02:47,  1.84it/s]Extractor Estimating: 231it [02:47,  1.81it/s]Extractor Estimating: 232it [02:48,  1.74it/s]Extractor Estimating: 233it [02:48,  1.73it/s]Extractor Estimating: 234it [02:49,  1.79it/s]Extractor Estimating: 235it [02:50,  1.75it/s]Extractor Estimating: 236it [02:50,  1.73it/s]Extractor Estimating: 237it [02:51,  1.73it/s]Extractor Estimating: 238it [02:51,  1.72it/s]Extractor Estimating: 239it [02:52,  1.54it/s]Extractor Estimating: 240it [02:53,  1.60it/s]Extractor Estimating: 241it [02:53,  1.68it/s]Extractor Estimating: 242it [02:54,  1.70it/s]Extractor Estimating: 243it [02:54,  1.71it/s]Extractor Estimating: 244it [02:55,  1.68it/s]Extractor Estimating: 245it [02:56,  1.69it/s]Extractor Estimating: 246it [02:56,  1.70it/s]Extractor Estimating: 247it [02:57,  1.66it/s]Extractor Estimating: 248it [02:57,  1.67it/s]Extractor Estimating: 249it [02:58,  1.71it/s]Extractor Estimating: 250it [02:59,  1.72it/s]Extractor Estimating: 251it [02:59,  1.69it/s]Extractor Estimating: 252it [03:00,  1.66it/s]Extractor Estimating: 253it [03:00,  1.63it/s]Extractor Estimating: 254it [03:01,  1.63it/s]Extractor Estimating: 255it [03:02,  1.60it/s]Extractor Estimating: 256it [03:02,  1.60it/s]Extractor Estimating: 257it [03:03,  1.56it/s]Extractor Estimating: 258it [03:04,  1.58it/s]Extractor Estimating: 259it [03:04,  1.60it/s]Extractor Estimating: 260it [03:05,  1.60it/s]Extractor Estimating: 261it [03:05,  1.59it/s]Extractor Estimating: 262it [03:06,  1.58it/s]Extractor Estimating: 263it [03:07,  1.59it/s]Extractor Estimating: 264it [03:07,  1.60it/s]Extractor Estimating: 265it [03:08,  1.62it/s]Extractor Estimating: 266it [03:09,  1.62it/s]Extractor Estimating: 267it [03:09,  1.61it/s]Extractor Estimating: 268it [03:10,  1.58it/s]Extractor Estimating: 269it [03:10,  1.61it/s]Extractor Estimating: 270it [03:11,  1.64it/s]Extractor Estimating: 271it [03:12,  1.66it/s]Extractor Estimating: 272it [03:12,  1.66it/s]Extractor Estimating: 273it [03:13,  1.60it/s]Extractor Estimating: 274it [03:14,  1.60it/s]Extractor Estimating: 275it [03:14,  1.61it/s]Extractor Estimating: 276it [03:15,  1.57it/s]Extractor Estimating: 277it [03:16,  1.51it/s]Extractor Estimating: 278it [03:16,  1.49it/s]Extractor Estimating: 279it [03:17,  1.48it/s]Extractor Estimating: 280it [03:17,  1.54it/s]Extractor Estimating: 281it [03:18,  1.51it/s]Extractor Estimating: 282it [03:19,  1.50it/s]Extractor Estimating: 283it [03:20,  1.50it/s]Extractor Estimating: 284it [03:20,  1.46it/s]Extractor Estimating: 285it [03:21,  1.52it/s]Extractor Estimating: 286it [03:21,  1.54it/s]Extractor Estimating: 287it [03:22,  1.49it/s]Extractor Estimating: 288it [03:23,  1.52it/s]Extractor Estimating: 289it [03:24,  1.49it/s]Extractor Estimating: 290it [03:24,  1.50it/s]Extractor Estimating: 291it [03:25,  1.52it/s]Extractor Estimating: 292it [03:26,  1.50it/s]Extractor Estimating: 293it [03:26,  1.56it/s]Extractor Estimating: 294it [03:27,  1.55it/s]Extractor Estimating: 295it [03:27,  1.52it/s]Extractor Estimating: 296it [03:28,  1.53it/s]Extractor Estimating: 297it [03:29,  1.52it/s]Extractor Estimating: 298it [03:29,  1.51it/s]Extractor Estimating: 299it [03:30,  1.50it/s]Extractor Estimating: 300it [03:31,  1.53it/s]Extractor Estimating: 301it [03:31,  1.51it/s]Extractor Estimating: 302it [03:32,  1.55it/s]Extractor Estimating: 303it [03:33,  1.49it/s]Extractor Estimating: 304it [03:33,  1.51it/s]Extractor Estimating: 305it [03:34,  1.58it/s]Extractor Estimating: 306it [03:35,  1.61it/s]Extractor Estimating: 307it [03:35,  1.59it/s]Extractor Estimating: 308it [03:36,  1.58it/s]Extractor Estimating: 309it [03:36,  1.59it/s]Extractor Estimating: 310it [03:37,  1.59it/s]Extractor Estimating: 311it [03:38,  1.52it/s]Extractor Estimating: 312it [03:38,  1.51it/s]Extractor Estimating: 313it [03:39,  1.52it/s]Extractor Estimating: 314it [03:40,  1.57it/s]Extractor Estimating: 315it [03:40,  1.59it/s]Extractor Estimating: 316it [03:41,  1.50it/s]Extractor Estimating: 317it [03:42,  1.50it/s]Extractor Estimating: 318it [03:42,  1.52it/s]Extractor Estimating: 319it [03:43,  1.38it/s]Extractor Estimating: 320it [03:44,  1.44it/s]Extractor Estimating: 321it [03:45,  1.44it/s]Extractor Estimating: 322it [03:45,  1.40it/s]Extractor Estimating: 323it [03:46,  1.47it/s]Extractor Estimating: 324it [03:47,  1.51it/s]Extractor Estimating: 325it [03:47,  1.57it/s]Extractor Estimating: 326it [03:48,  1.60it/s]Extractor Estimating: 327it [03:48,  1.63it/s]Extractor Estimating: 328it [03:49,  1.68it/s]Extractor Estimating: 329it [03:49,  1.78it/s]Extractor Estimating: 330it [03:50,  1.75it/s]Extractor Estimating: 331it [03:51,  1.73it/s]Extractor Estimating: 332it [03:51,  1.77it/s]Extractor Estimating: 333it [03:52,  1.72it/s]Extractor Estimating: 334it [03:52,  1.74it/s]Extractor Estimating: 335it [03:53,  1.72it/s]Extractor Estimating: 336it [03:53,  1.74it/s]Extractor Estimating: 337it [03:54,  1.68it/s]Extractor Estimating: 338it [03:55,  1.64it/s]Extractor Estimating: 339it [03:55,  1.60it/s]Extractor Estimating: 340it [03:56,  1.56it/s]Extractor Estimating: 341it [03:57,  1.58it/s]Extractor Estimating: 342it [03:57,  1.62it/s]Extractor Estimating: 343it [03:58,  1.60it/s]Extractor Estimating: 344it [03:58,  1.62it/s]Extractor Estimating: 345it [03:59,  1.72it/s]Extractor Estimating: 346it [04:00,  1.69it/s]Extractor Estimating: 347it [04:00,  1.72it/s]Extractor Estimating: 348it [04:01,  1.71it/s]Extractor Estimating: 349it [04:01,  1.68it/s]Extractor Estimating: 350it [04:02,  1.69it/s]Extractor Estimating: 351it [04:03,  1.70it/s]Extractor Estimating: 352it [04:03,  1.68it/s]Extractor Estimating: 353it [04:04,  1.69it/s]Extractor Estimating: 354it [04:04,  1.66it/s]Extractor Estimating: 355it [04:05,  1.63it/s]Extractor Estimating: 356it [04:06,  1.58it/s]Extractor Estimating: 357it [04:06,  1.53it/s]Extractor Estimating: 358it [04:07,  1.54it/s]Extractor Estimating: 359it [04:08,  1.60it/s]Extractor Estimating: 360it [04:08,  1.56it/s]Extractor Estimating: 361it [04:09,  1.62it/s]Extractor Estimating: 362it [04:09,  1.61it/s]Extractor Estimating: 363it [04:10,  1.63it/s]Extractor Estimating: 364it [04:11,  1.67it/s]Extractor Estimating: 365it [04:11,  1.69it/s]Extractor Estimating: 366it [04:12,  1.65it/s]Extractor Estimating: 367it [04:12,  1.61it/s]Extractor Estimating: 368it [04:13,  1.58it/s]Extractor Estimating: 369it [04:14,  1.60it/s]Extractor Estimating: 370it [04:14,  1.64it/s]Extractor Estimating: 371it [04:15,  1.63it/s]Extractor Estimating: 372it [04:16,  1.59it/s]Extractor Estimating: 373it [04:16,  1.53it/s]Extractor Estimating: 374it [04:17,  1.51it/s]Extractor Estimating: 375it [04:18,  1.49it/s]Extractor Estimating: 375it [04:18,  1.45it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7402 mean pseudo reward: 0.9183873265478993
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 27460
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27560, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_10_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27560, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.270, loss:2620.2583
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.021, loss:1926.4766
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.975, loss:1828.1424
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 91, avg_time 0.974, loss:1614.6519
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 191, avg_time 0.975, loss:1542.4800
>> valid entity prec:0.5310, rec:0.4053, f1:0.4597
>> valid relation prec:0.0584, rec:0.0021, f1:0.0040
>> valid relation with NER prec:0.0584, rec:0.0021, f1:0.0040
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 291, avg_time 2.393, loss:1502.9494
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 82, avg_time 0.971, loss:1461.6699
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 182, avg_time 0.981, loss:1350.8518
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 282, avg_time 0.980, loss:1314.4121
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 73, avg_time 0.973, loss:1222.9586
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4858, rec:0.4042, f1:0.4413
>> valid relation prec:0.1134, rec:0.0051, f1:0.0097
>> valid relation with NER prec:0.1134, rec:0.0051, f1:0.0097
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 173, avg_time 2.378, loss:1219.9491
g_step 1200, step 273, avg_time 0.982, loss:1177.8405
g_step 1300, step 64, avg_time 0.967, loss:1154.2482
g_step 1400, step 164, avg_time 0.979, loss:1150.5818
g_step 1500, step 264, avg_time 0.982, loss:1098.3553
>> valid entity prec:0.4816, rec:0.5689, f1:0.5216
>> valid relation prec:0.0332, rec:0.0046, f1:0.0081
>> valid relation with NER prec:0.0332, rec:0.0046, f1:0.0081
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 55, avg_time 2.398, loss:1054.1538
g_step 1700, step 155, avg_time 0.982, loss:1065.9588
g_step 1800, step 255, avg_time 0.968, loss:1050.2417
g_step 1900, step 46, avg_time 0.957, loss:1025.1088
g_step 2000, step 146, avg_time 0.982, loss:990.1745
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5041, rec:0.2860, f1:0.3649
>> valid relation prec:0.0667, rec:0.0023, f1:0.0045
>> valid relation with NER prec:0.0667, rec:0.0023, f1:0.0045
g_step 2100, step 246, avg_time 2.366, loss:1001.7473
g_step 2200, step 37, avg_time 0.983, loss:999.1738
g_step 2300, step 137, avg_time 0.974, loss:929.0583
g_step 2400, step 237, avg_time 0.970, loss:960.0138
g_step 2500, step 28, avg_time 0.965, loss:941.3575
>> valid entity prec:0.5038, rec:0.3328, f1:0.4008
>> valid relation prec:0.0623, rec:0.0046, f1:0.0086
>> valid relation with NER prec:0.0623, rec:0.0046, f1:0.0086
g_step 2600, step 128, avg_time 2.377, loss:892.8462
g_step 2700, step 228, avg_time 0.970, loss:920.4454
g_step 2800, step 19, avg_time 0.961, loss:911.3780
g_step 2900, step 119, avg_time 0.980, loss:846.1179
g_step 3000, step 219, avg_time 0.975, loss:883.0933
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4406, rec:0.4339, f1:0.4372
>> valid relation prec:0.0329, rec:0.0039, f1:0.0070
>> valid relation with NER prec:0.0329, rec:0.0039, f1:0.0070
g_step 3100, step 10, avg_time 2.375, loss:915.6556
g_step 3200, step 110, avg_time 0.977, loss:853.7107
g_step 3300, step 210, avg_time 0.966, loss:859.3535
g_step 3400, step 1, avg_time 0.983, loss:842.4773
g_step 3500, step 101, avg_time 0.978, loss:793.8089
>> valid entity prec:0.5162, rec:0.4622, f1:0.4877
>> valid relation prec:0.0566, rec:0.0048, f1:0.0089
>> valid relation with NER prec:0.0566, rec:0.0048, f1:0.0089
g_step 3600, step 201, avg_time 2.388, loss:816.3099
g_step 3700, step 301, avg_time 0.979, loss:829.1198
g_step 3800, step 92, avg_time 0.970, loss:791.6019
g_step 3900, step 192, avg_time 0.984, loss:801.5554
g_step 4000, step 292, avg_time 0.967, loss:818.7846
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4961, rec:0.4260, f1:0.4584
>> valid relation prec:0.0407, rec:0.0053, f1:0.0094
>> valid relation with NER prec:0.0407, rec:0.0053, f1:0.0094
g_step 4100, step 83, avg_time 2.396, loss:748.9432
g_step 4200, step 183, avg_time 0.970, loss:742.9726
g_step 4300, step 283, avg_time 0.973, loss:780.9312
g_step 4400, step 74, avg_time 0.971, loss:750.2092
g_step 4500, step 174, avg_time 0.978, loss:734.8795
>> valid entity prec:0.4762, rec:0.4167, f1:0.4445
>> valid relation prec:0.0343, rec:0.0048, f1:0.0085
>> valid relation with NER prec:0.0343, rec:0.0048, f1:0.0085
g_step 4600, step 274, avg_time 2.383, loss:740.4093
g_step 4700, step 65, avg_time 0.969, loss:719.4453
g_step 4800, step 165, avg_time 0.985, loss:715.1084
g_step 4900, step 265, avg_time 0.963, loss:720.8623
g_step 5000, step 56, avg_time 0.978, loss:679.0180
learning rate was adjusted to 0.0008
>> valid entity prec:0.5113, rec:0.3036, f1:0.3810
>> valid relation prec:0.0488, rec:0.0055, f1:0.0099
>> valid relation with NER prec:0.0488, rec:0.0055, f1:0.0099
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 156, avg_time 2.358, loss:718.3946
g_step 5200, step 256, avg_time 0.975, loss:662.5981
g_step 5300, step 47, avg_time 0.984, loss:672.7599
g_step 5400, step 147, avg_time 0.979, loss:677.3245
g_step 5500, step 247, avg_time 0.983, loss:665.9467
>> valid entity prec:0.4960, rec:0.2493, f1:0.3318
>> valid relation prec:0.0418, rec:0.0037, f1:0.0068
>> valid relation with NER prec:0.0418, rec:0.0037, f1:0.0068
g_step 5600, step 38, avg_time 2.363, loss:644.4197
g_step 5700, step 138, avg_time 0.968, loss:630.0787
g_step 5800, step 238, avg_time 0.966, loss:665.6509
g_step 5900, step 29, avg_time 0.985, loss:630.7541
g_step 6000, step 129, avg_time 0.976, loss:620.2883
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4514, rec:0.4633, f1:0.4573
>> valid relation prec:0.0355, rec:0.0062, f1:0.0106
>> valid relation with NER prec:0.0355, rec:0.0062, f1:0.0106
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 229, avg_time 2.385, loss:647.4281
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 23:15:30 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 23:15:30 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_23-15-30_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 23:15:31 - WARNING - datasets.builder -   Using custom data configuration default-6fcc0237fbc20e52
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6fcc0237fbc20e52/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 23:15:31,594 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:15:31,596 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 23:15:31,596 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:15:31,597 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 23:15:31,604 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:15:31,607 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:15:31,607 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:15:31,607 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:15:31,607 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:15:31,607 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:15:31,607 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 23:15:31,794 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 23:15:34,851 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 23:15:34,854 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6fcc0237fbc20e52/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 23:15:34 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1474f102b950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.16ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.17ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.70ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.02ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.19ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.32ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.41ba/s]100%|██████████| 8/8 [00:01<00:00,  5.21ba/s]100%|██████████| 8/8 [00:01<00:00,  4.25ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.18ba/s] 40%|████      | 2/5 [00:00<00:00,  4.35ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.41ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.43ba/s]100%|██████████| 5/5 [00:00<00:00,  5.05ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.79ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.93ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.11ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.28ba/s]100%|██████████| 8/8 [00:00<00:00, 10.79ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.96ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.99ba/s]100%|██████████| 5/5 [00:00<00:00, 12.26ba/s]100%|██████████| 5/5 [00:00<00:00, 11.58ba/s]
[INFO|trainer.py:414] 2023-08-27 23:15:39,273 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 23:15:39,285 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 23:15:39,285 >>   Num examples = 7520
[INFO|trainer.py:1149] 2023-08-27 23:15:39,285 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 23:15:39,285 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 23:15:39,285 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 23:15:39,285 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 23:15:39,285 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:51,  3.39it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:49,  3.43it/s]  1%|          | 6/585 [00:01<02:48,  3.43it/s]  1%|          | 7/585 [00:02<02:49,  3.42it/s]  1%|▏         | 8/585 [00:02<02:48,  3.43it/s]  2%|▏         | 9/585 [00:02<02:48,  3.43it/s]  2%|▏         | 10/585 [00:02<02:47,  3.43it/s]  2%|▏         | 11/585 [00:03<02:47,  3.43it/s]  2%|▏         | 12/585 [00:03<02:46,  3.44it/s]  2%|▏         | 13/585 [00:03<02:46,  3.44it/s]  2%|▏         | 14/585 [00:04<02:46,  3.44it/s]  3%|▎         | 15/585 [00:04<02:45,  3.44it/s]  3%|▎         | 16/585 [00:04<02:45,  3.44it/s]  3%|▎         | 17/585 [00:04<02:45,  3.44it/s]  3%|▎         | 18/585 [00:05<02:44,  3.44it/s]  3%|▎         | 19/585 [00:05<02:44,  3.44it/s]  3%|▎         | 20/585 [00:05<02:44,  3.44it/s]  4%|▎         | 21/585 [00:06<02:43,  3.44it/s]  4%|▍         | 22/585 [00:06<02:43,  3.44it/s]  4%|▍         | 23/585 [00:06<02:43,  3.44it/s]  4%|▍         | 24/585 [00:06<02:43,  3.43it/s]  4%|▍         | 25/585 [00:07<02:43,  3.44it/s]  4%|▍         | 26/585 [00:07<02:42,  3.44it/s]  5%|▍         | 27/585 [00:07<02:42,  3.43it/s]  5%|▍         | 28/585 [00:08<02:42,  3.44it/s]  5%|▍         | 29/585 [00:08<02:41,  3.44it/s]  5%|▌         | 30/585 [00:08<02:41,  3.44it/s]  5%|▌         | 31/585 [00:09<02:41,  3.44it/s]  5%|▌         | 32/585 [00:09<02:40,  3.43it/s]  6%|▌         | 33/585 [00:09<02:40,  3.43it/s]  6%|▌         | 34/585 [00:09<02:40,  3.43it/s]  6%|▌         | 35/585 [00:10<02:40,  3.43it/s]  6%|▌         | 36/585 [00:10<02:39,  3.43it/s]  6%|▋         | 37/585 [00:10<02:39,  3.43it/s]  6%|▋         | 38/585 [00:11<02:39,  3.43it/s]  7%|▋         | 39/585 [00:11<02:38,  3.43it/s]  7%|▋         | 40/585 [00:11<02:38,  3.43it/s]  7%|▋         | 41/585 [00:11<02:38,  3.44it/s]  7%|▋         | 42/585 [00:12<02:38,  3.43it/s]  7%|▋         | 43/585 [00:12<02:37,  3.43it/s]  8%|▊         | 44/585 [00:12<02:37,  3.43it/s]  8%|▊         | 45/585 [00:13<02:37,  3.43it/s]  8%|▊         | 46/585 [00:13<02:37,  3.43it/s]  8%|▊         | 47/585 [00:13<02:36,  3.43it/s]  8%|▊         | 48/585 [00:13<02:36,  3.43it/s]  8%|▊         | 49/585 [00:14<02:41,  3.33it/s]  9%|▊         | 50/585 [00:14<02:39,  3.36it/s]  9%|▊         | 51/585 [00:14<02:37,  3.38it/s]  9%|▉         | 52/585 [00:15<02:36,  3.40it/s]  9%|▉         | 53/585 [00:15<02:36,  3.41it/s]  9%|▉         | 54/585 [00:15<02:35,  3.42it/s]  9%|▉         | 55/585 [00:16<02:34,  3.42it/s] 10%|▉         | 56/585 [00:16<02:34,  3.42it/s] 10%|▉         | 57/585 [00:16<02:34,  3.43it/s] 10%|▉         | 58/585 [00:16<02:33,  3.43it/s] 10%|█         | 59/585 [00:17<02:33,  3.43it/s] 10%|█         | 60/585 [00:17<02:33,  3.43it/s] 10%|█         | 61/585 [00:17<02:32,  3.43it/s] 11%|█         | 62/585 [00:18<02:32,  3.43it/s] 11%|█         | 63/585 [00:18<02:32,  3.43it/s] 11%|█         | 64/585 [00:18<02:31,  3.43it/s] 11%|█         | 65/585 [00:18<02:31,  3.43it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.43it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.43it/s] 12%|█▏        | 73/585 [00:21<02:29,  3.43it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.43it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 76/585 [00:22<02:28,  3.43it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.43it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.43it/s] 14%|█▎        | 79/585 [00:23<02:27,  3.43it/s] 14%|█▎        | 80/585 [00:23<02:27,  3.43it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.43it/s] 14%|█▍        | 82/585 [00:23<02:26,  3.43it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:25<02:25,  3.43it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.43it/s] 15%|█▌        | 89/585 [00:25<02:24,  3.43it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.43it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.43it/s] 16%|█▋        | 96/585 [00:27<02:22,  3.43it/s] 17%|█▋        | 97/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 98/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.43it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 106/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.43it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.43it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 113/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.43it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.43it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.43it/s] 20%|██        | 117/585 [00:34<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-27 23:16:13,542 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:16:13,542 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-27 23:16:13,542 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.59it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.60it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.91it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.10it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.70it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.45it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.28it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.15it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.21it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.28it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.19it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 43.95it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 43.95it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.99it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.98it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.95it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.04it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.14it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.11it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.06it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 43.87it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.83it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.95it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.94it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.03it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.07it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.12it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 43.96it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.02it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.92it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.91it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.93it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.06it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.03it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.04it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.09it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.09it/s][A
 35%|███▌      | 192/543 [00:04<00:08, 43.86it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.93it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.99it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.00it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.07it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.05it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.05it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.04it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 43.99it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 43.78it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.88it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.91it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.84it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.93it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.13it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.00it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.09it/s][A
 51%|█████     | 277/543 [00:06<00:06, 43.94it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 43.88it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.84it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.98it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.97it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.12it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.10it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.98it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.96it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.94it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.90it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.87it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.04it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.90it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.10it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.02it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.06it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.00it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.97it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.86it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.04it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.09it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 43.98it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.15it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.13it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 43.94it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.90it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.97it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.90it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.97it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.01it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.07it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.08it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.01it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.86it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.98it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.02it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.93it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.01it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.12it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.05it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.93it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.86it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.95it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 26.27it/s][A
 96%|█████████▌| 522/543 [00:12<00:00, 29.98it/s][A
 97%|█████████▋| 527/543 [00:12<00:00, 33.25it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 35.96it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 38.18it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 39.91it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:46<02:16,  3.43it/s]
100%|██████████| 543/543 [00:12<00:00, 39.91it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:16:26,157 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-27 23:16:26,183 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:16:28,799 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:16:28,812 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:16:28,820 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:53<45:44,  5.88s/it] 20%|██        | 119/585 [00:53<32:38,  4.20s/it] 21%|██        | 120/585 [00:53<23:29,  3.03s/it] 21%|██        | 121/585 [00:53<17:05,  2.21s/it] 21%|██        | 122/585 [00:54<12:37,  1.64s/it] 21%|██        | 123/585 [00:54<09:30,  1.23s/it] 21%|██        | 124/585 [00:54<07:19,  1.05it/s] 21%|██▏       | 125/585 [00:55<05:47,  1.32it/s] 22%|██▏       | 126/585 [00:55<04:43,  1.62it/s] 22%|██▏       | 127/585 [00:55<03:57,  1.93it/s] 22%|██▏       | 128/585 [00:55<03:25,  2.22it/s] 22%|██▏       | 129/585 [00:56<03:03,  2.48it/s] 22%|██▏       | 130/585 [00:56<02:48,  2.70it/s] 22%|██▏       | 131/585 [00:56<02:37,  2.89it/s] 23%|██▎       | 132/585 [00:57<02:29,  3.03it/s] 23%|██▎       | 133/585 [00:57<02:24,  3.14it/s] 23%|██▎       | 134/585 [00:57<02:20,  3.22it/s] 23%|██▎       | 135/585 [00:58<02:17,  3.27it/s] 23%|██▎       | 136/585 [00:58<02:15,  3.30it/s] 23%|██▎       | 137/585 [00:58<02:14,  3.33it/s] 24%|██▎       | 138/585 [00:58<02:13,  3.34it/s] 24%|██▍       | 139/585 [00:59<02:12,  3.36it/s] 24%|██▍       | 140/585 [00:59<02:12,  3.37it/s] 24%|██▍       | 141/585 [00:59<02:11,  3.36it/s] 24%|██▍       | 142/585 [01:00<02:12,  3.35it/s] 24%|██▍       | 143/585 [01:00<02:11,  3.36it/s] 25%|██▍       | 144/585 [01:00<02:10,  3.37it/s] 25%|██▍       | 145/585 [01:00<02:10,  3.37it/s] 25%|██▍       | 146/585 [01:01<02:10,  3.38it/s] 25%|██▌       | 147/585 [01:01<02:09,  3.38it/s] 25%|██▌       | 148/585 [01:01<02:09,  3.38it/s] 25%|██▌       | 149/585 [01:02<02:08,  3.38it/s] 26%|██▌       | 150/585 [01:02<02:08,  3.38it/s] 26%|██▌       | 151/585 [01:02<02:08,  3.38it/s] 26%|██▌       | 152/585 [01:03<02:10,  3.33it/s] 26%|██▌       | 153/585 [01:03<02:09,  3.35it/s] 26%|██▋       | 154/585 [01:03<02:08,  3.36it/s] 26%|██▋       | 155/585 [01:03<02:07,  3.36it/s] 27%|██▋       | 156/585 [01:04<02:07,  3.37it/s] 27%|██▋       | 157/585 [01:04<02:06,  3.37it/s] 27%|██▋       | 158/585 [01:04<02:06,  3.37it/s] 27%|██▋       | 159/585 [01:05<02:06,  3.38it/s] 27%|██▋       | 160/585 [01:05<02:05,  3.38it/s] 28%|██▊       | 161/585 [01:05<02:05,  3.38it/s] 28%|██▊       | 162/585 [01:06<02:05,  3.38it/s] 28%|██▊       | 163/585 [01:06<02:04,  3.38it/s] 28%|██▊       | 164/585 [01:06<02:04,  3.38it/s] 28%|██▊       | 165/585 [01:06<02:04,  3.38it/s] 28%|██▊       | 166/585 [01:07<02:03,  3.38it/s] 29%|██▊       | 167/585 [01:07<02:03,  3.38it/s] 29%|██▊       | 168/585 [01:07<02:03,  3.38it/s] 29%|██▉       | 169/585 [01:08<02:03,  3.38it/s] 29%|██▉       | 170/585 [01:08<02:02,  3.38it/s] 29%|██▉       | 171/585 [01:08<02:02,  3.38it/s] 29%|██▉       | 172/585 [01:08<02:02,  3.38it/s] 30%|██▉       | 173/585 [01:09<02:01,  3.38it/s] 30%|██▉       | 174/585 [01:09<02:01,  3.38it/s] 30%|██▉       | 175/585 [01:09<02:01,  3.38it/s] 30%|███       | 176/585 [01:10<02:00,  3.38it/s] 30%|███       | 177/585 [01:10<02:00,  3.38it/s] 30%|███       | 178/585 [01:10<02:00,  3.38it/s] 31%|███       | 179/585 [01:11<02:00,  3.38it/s] 31%|███       | 180/585 [01:11<01:59,  3.38it/s] 31%|███       | 181/585 [01:11<01:59,  3.38it/s] 31%|███       | 182/585 [01:11<01:59,  3.38it/s] 31%|███▏      | 183/585 [01:12<01:58,  3.38it/s] 31%|███▏      | 184/585 [01:12<01:58,  3.38it/s] 32%|███▏      | 185/585 [01:12<01:58,  3.37it/s] 32%|███▏      | 186/585 [01:13<01:58,  3.37it/s] 32%|███▏      | 187/585 [01:13<01:57,  3.38it/s] 32%|███▏      | 188/585 [01:13<01:57,  3.38it/s] 32%|███▏      | 189/585 [01:14<01:57,  3.38it/s] 32%|███▏      | 190/585 [01:14<01:56,  3.38it/s] 33%|███▎      | 191/585 [01:14<01:56,  3.38it/s] 33%|███▎      | 192/585 [01:14<01:56,  3.38it/s] 33%|███▎      | 193/585 [01:15<01:55,  3.38it/s] 33%|███▎      | 194/585 [01:15<01:55,  3.38it/s] 33%|███▎      | 195/585 [01:15<01:55,  3.38it/s] 34%|███▎      | 196/585 [01:16<01:55,  3.37it/s] 34%|███▎      | 197/585 [01:16<01:55,  3.37it/s] 34%|███▍      | 198/585 [01:16<01:54,  3.38it/s] 34%|███▍      | 199/585 [01:16<01:54,  3.38it/s] 34%|███▍      | 200/585 [01:17<01:54,  3.38it/s] 34%|███▍      | 201/585 [01:17<01:53,  3.38it/s] 35%|███▍      | 202/585 [01:17<01:53,  3.38it/s] 35%|███▍      | 203/585 [01:18<01:53,  3.38it/s] 35%|███▍      | 204/585 [01:18<01:52,  3.38it/s] 35%|███▌      | 205/585 [01:18<01:52,  3.38it/s] 35%|███▌      | 206/585 [01:19<01:52,  3.38it/s] 35%|███▌      | 207/585 [01:19<01:51,  3.38it/s] 36%|███▌      | 208/585 [01:19<01:51,  3.38it/s] 36%|███▌      | 209/585 [01:19<01:51,  3.38it/s] 36%|███▌      | 210/585 [01:20<01:50,  3.38it/s] 36%|███▌      | 211/585 [01:20<01:50,  3.38it/s] 36%|███▌      | 212/585 [01:20<01:50,  3.38it/s] 36%|███▋      | 213/585 [01:21<01:49,  3.38it/s] 37%|███▋      | 214/585 [01:21<01:49,  3.37it/s] 37%|███▋      | 215/585 [01:21<01:49,  3.38it/s] 37%|███▋      | 216/585 [01:22<01:49,  3.38it/s] 37%|███▋      | 217/585 [01:22<01:48,  3.38it/s] 37%|███▋      | 218/585 [01:22<01:48,  3.38it/s] 37%|███▋      | 219/585 [01:22<01:48,  3.38it/s] 38%|███▊      | 220/585 [01:23<01:48,  3.37it/s] 38%|███▊      | 221/585 [01:23<01:47,  3.37it/s] 38%|███▊      | 222/585 [01:23<01:47,  3.38it/s] 38%|███▊      | 223/585 [01:24<01:47,  3.38it/s] 38%|███▊      | 224/585 [01:24<01:46,  3.38it/s] 38%|███▊      | 225/585 [01:24<01:46,  3.38it/s] 39%|███▊      | 226/585 [01:24<01:45,  3.39it/s] 39%|███▉      | 227/585 [01:25<01:45,  3.40it/s] 39%|███▉      | 228/585 [01:25<01:44,  3.41it/s] 39%|███▉      | 229/585 [01:25<01:44,  3.41it/s] 39%|███▉      | 230/585 [01:26<01:43,  3.42it/s] 39%|███▉      | 231/585 [01:26<01:43,  3.42it/s] 40%|███▉      | 232/585 [01:26<01:43,  3.42it/s] 40%|███▉      | 233/585 [01:27<01:42,  3.42it/s] 40%|████      | 234/585 [01:27<01:42,  3.43it/s][INFO|trainer.py:2140] 2023-08-27 23:17:06,725 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:17:06,727 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-27 23:17:06,727 >>   Batch size = 8
{'eval_loss': 0.9760096669197083, 'eval_runtime': 12.5881, 'eval_samples_per_second': 344.929, 'eval_steps_per_second': 43.136, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.30it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.52it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.81it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.11it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.83it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.56it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.28it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.18it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.11it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.18it/s][A
 10%|█         | 57/543 [00:01<00:11, 44.06it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 43.97it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 43.83it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.93it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.96it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.90it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.98it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.04it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.98it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.01it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.10it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.92it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.03it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.04it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.94it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.86it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.97it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 43.96it/s][A
 27%|██▋       | 147/543 [00:03<00:09, 43.98it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.91it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.01it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.93it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.94it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.91it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 43.92it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.01it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 43.92it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 43.99it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.03it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.01it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.91it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.95it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.93it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.93it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.96it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.06it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.05it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.01it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.95it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.92it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.93it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.89it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.96it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.97it/s][A
 51%|█████     | 277/543 [00:06<00:06, 43.34it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 43.51it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.58it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.70it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.75it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.78it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.83it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.87it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.96it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.93it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.99it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.83it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.97it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.84it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.95it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.94it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.90it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.92it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.04it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.08it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.93it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.91it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.89it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.82it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 43.80it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.90it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 43.93it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.15it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.05it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.98it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.05it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.04it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.81it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.90it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.86it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.90it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.85it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.06it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.00it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.04it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.99it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.82it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.91it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.83it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 43.95it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.02it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.01it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.03it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.98it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.97it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.85it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.91it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.80it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.00it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:39<01:42,  3.43it/s]
100%|██████████| 543/543 [00:12<00:00, 44.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:17:19,110 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-27 23:17:19,127 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:17:20,676 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:17:20,687 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:17:20,699 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:46<34:17,  5.88s/it] 40%|████      | 236/585 [01:46<24:27,  4.21s/it] 41%|████      | 237/585 [01:46<17:35,  3.03s/it] 41%|████      | 238/585 [01:47<12:47,  2.21s/it] 41%|████      | 239/585 [01:47<09:26,  1.64s/it] 41%|████      | 240/585 [01:47<07:05,  1.23s/it] 41%|████      | 241/585 [01:47<05:27,  1.05it/s] 41%|████▏     | 242/585 [01:48<04:18,  1.33it/s] 42%|████▏     | 243/585 [01:48<03:30,  1.63it/s] 42%|████▏     | 244/585 [01:48<02:56,  1.93it/s] 42%|████▏     | 245/585 [01:49<02:32,  2.22it/s] 42%|████▏     | 246/585 [01:49<02:16,  2.49it/s] 42%|████▏     | 247/585 [01:49<02:04,  2.71it/s] 42%|████▏     | 248/585 [01:50<01:56,  2.89it/s] 43%|████▎     | 249/585 [01:50<01:50,  3.04it/s] 43%|████▎     | 250/585 [01:50<01:46,  3.14it/s] 43%|████▎     | 251/585 [01:50<01:43,  3.23it/s] 43%|████▎     | 252/585 [01:51<01:41,  3.29it/s] 43%|████▎     | 253/585 [01:51<01:39,  3.33it/s] 43%|████▎     | 254/585 [01:51<01:38,  3.36it/s] 44%|████▎     | 255/585 [01:52<01:37,  3.38it/s] 44%|████▍     | 256/585 [01:52<01:37,  3.39it/s] 44%|████▍     | 257/585 [01:52<01:36,  3.39it/s] 44%|████▍     | 258/585 [01:52<01:36,  3.40it/s] 44%|████▍     | 259/585 [01:53<01:35,  3.41it/s] 44%|████▍     | 260/585 [01:53<01:35,  3.42it/s] 45%|████▍     | 261/585 [01:53<01:34,  3.42it/s] 45%|████▍     | 262/585 [01:54<01:34,  3.42it/s] 45%|████▍     | 263/585 [01:54<01:34,  3.42it/s] 45%|████▌     | 264/585 [01:54<01:33,  3.42it/s] 45%|████▌     | 265/585 [01:54<01:33,  3.42it/s] 45%|████▌     | 266/585 [01:55<01:33,  3.43it/s] 46%|████▌     | 267/585 [01:55<01:32,  3.42it/s] 46%|████▌     | 268/585 [01:55<01:32,  3.42it/s] 46%|████▌     | 269/585 [01:56<01:32,  3.42it/s] 46%|████▌     | 270/585 [01:56<01:31,  3.42it/s] 46%|████▋     | 271/585 [01:56<01:31,  3.43it/s] 46%|████▋     | 272/585 [01:57<01:31,  3.43it/s] 47%|████▋     | 273/585 [01:57<01:31,  3.43it/s] 47%|████▋     | 274/585 [01:57<01:30,  3.43it/s] 47%|████▋     | 275/585 [01:57<01:30,  3.43it/s] 47%|████▋     | 276/585 [01:58<01:30,  3.43it/s] 47%|████▋     | 277/585 [01:58<01:29,  3.43it/s] 48%|████▊     | 278/585 [01:58<01:29,  3.42it/s] 48%|████▊     | 279/585 [01:59<01:29,  3.42it/s] 48%|████▊     | 280/585 [01:59<01:29,  3.42it/s] 48%|████▊     | 281/585 [01:59<01:28,  3.43it/s] 48%|████▊     | 282/585 [01:59<01:28,  3.43it/s] 48%|████▊     | 283/585 [02:00<01:28,  3.43it/s] 49%|████▊     | 284/585 [02:00<01:27,  3.43it/s] 49%|████▊     | 285/585 [02:00<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:01<01:27,  3.43it/s] 49%|████▉     | 287/585 [02:01<01:26,  3.43it/s] 49%|████▉     | 288/585 [02:01<01:26,  3.43it/s] 49%|████▉     | 289/585 [02:01<01:26,  3.42it/s] 50%|████▉     | 290/585 [02:02<01:26,  3.42it/s] 50%|████▉     | 291/585 [02:02<01:25,  3.43it/s] 50%|████▉     | 292/585 [02:02<01:25,  3.42it/s] 50%|█████     | 293/585 [02:03<01:25,  3.42it/s] 50%|█████     | 294/585 [02:03<01:25,  3.42it/s] 50%|█████     | 295/585 [02:03<01:24,  3.43it/s] 51%|█████     | 296/585 [02:04<01:24,  3.43it/s] 51%|█████     | 297/585 [02:04<01:24,  3.43it/s] 51%|█████     | 298/585 [02:04<01:23,  3.43it/s] 51%|█████     | 299/585 [02:04<01:23,  3.43it/s] 51%|█████▏    | 300/585 [02:05<01:23,  3.42it/s] 51%|█████▏    | 301/585 [02:05<01:23,  3.42it/s] 52%|█████▏    | 302/585 [02:05<01:22,  3.42it/s] 52%|█████▏    | 303/585 [02:06<01:22,  3.42it/s] 52%|█████▏    | 304/585 [02:06<01:22,  3.42it/s] 52%|█████▏    | 305/585 [02:06<01:21,  3.42it/s] 52%|█████▏    | 306/585 [02:06<01:21,  3.42it/s] 52%|█████▏    | 307/585 [02:07<01:21,  3.42it/s] 53%|█████▎    | 308/585 [02:07<01:20,  3.42it/s] 53%|█████▎    | 309/585 [02:07<01:20,  3.43it/s] 53%|█████▎    | 310/585 [02:08<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:08<01:20,  3.42it/s] 53%|█████▎    | 312/585 [02:08<01:19,  3.42it/s] 54%|█████▎    | 313/585 [02:09<01:19,  3.42it/s] 54%|█████▎    | 314/585 [02:09<01:19,  3.42it/s] 54%|█████▍    | 315/585 [02:09<01:18,  3.42it/s] 54%|█████▍    | 316/585 [02:09<01:18,  3.42it/s] 54%|█████▍    | 317/585 [02:10<01:18,  3.42it/s] 54%|█████▍    | 318/585 [02:10<01:18,  3.42it/s] 55%|█████▍    | 319/585 [02:10<01:17,  3.42it/s] 55%|█████▍    | 320/585 [02:11<01:17,  3.42it/s] 55%|█████▍    | 321/585 [02:11<01:17,  3.43it/s] 55%|█████▌    | 322/585 [02:11<01:17,  3.41it/s] 55%|█████▌    | 323/585 [02:11<01:16,  3.41it/s] 55%|█████▌    | 324/585 [02:12<01:16,  3.42it/s] 56%|█████▌    | 325/585 [02:12<01:16,  3.41it/s] 56%|█████▌    | 326/585 [02:12<01:16,  3.40it/s] 56%|█████▌    | 327/585 [02:13<01:16,  3.39it/s] 56%|█████▌    | 328/585 [02:13<01:15,  3.39it/s] 56%|█████▌    | 329/585 [02:13<01:15,  3.39it/s] 56%|█████▋    | 330/585 [02:14<01:15,  3.39it/s] 57%|█████▋    | 331/585 [02:14<01:15,  3.38it/s] 57%|█████▋    | 332/585 [02:14<01:14,  3.38it/s] 57%|█████▋    | 333/585 [02:14<01:15,  3.35it/s] 57%|█████▋    | 334/585 [02:15<01:14,  3.36it/s] 57%|█████▋    | 335/585 [02:15<01:14,  3.37it/s] 57%|█████▋    | 336/585 [02:15<01:13,  3.37it/s] 58%|█████▊    | 337/585 [02:16<01:13,  3.37it/s] 58%|█████▊    | 338/585 [02:16<01:13,  3.37it/s] 58%|█████▊    | 339/585 [02:16<01:12,  3.38it/s] 58%|█████▊    | 340/585 [02:16<01:12,  3.38it/s] 58%|█████▊    | 341/585 [02:17<01:12,  3.37it/s] 58%|█████▊    | 342/585 [02:17<01:11,  3.38it/s] 59%|█████▊    | 343/585 [02:17<01:11,  3.38it/s] 59%|█████▉    | 344/585 [02:18<01:11,  3.36it/s] 59%|█████▉    | 345/585 [02:18<01:11,  3.37it/s] 59%|█████▉    | 346/585 [02:18<01:10,  3.37it/s] 59%|█████▉    | 347/585 [02:19<01:10,  3.37it/s] 59%|█████▉    | 348/585 [02:19<01:10,  3.38it/s] 60%|█████▉    | 349/585 [02:19<01:09,  3.38it/s] 60%|█████▉    | 350/585 [02:19<01:09,  3.38it/s] 60%|██████    | 351/585 [02:20<01:09,  3.38it/s][INFO|trainer.py:2140] 2023-08-27 23:17:59,655 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:17:59,656 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-27 23:17:59,656 >>   Batch size = 8
{'eval_loss': 0.9734402894973755, 'eval_runtime': 12.3599, 'eval_samples_per_second': 351.297, 'eval_steps_per_second': 43.932, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.56it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.26it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.84it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.01it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.58it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.50it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.22it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.22it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.29it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.23it/s][A
 10%|█         | 57/543 [00:01<00:11, 44.02it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 43.94it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 43.99it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.89it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.94it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.99it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.06it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.18it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.09it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 43.99it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 43.93it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.91it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.85it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.85it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.95it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.99it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.07it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.07it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.03it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.99it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.91it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.95it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.97it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.00it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 44.07it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.06it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.08it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 43.97it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.86it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.85it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.83it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.87it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.01it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.10it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.12it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.07it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 43.90it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.90it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.92it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.89it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.99it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.07it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.04it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.11it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.06it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 43.91it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.93it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.93it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.97it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.03it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.01it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.91it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.92it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.75it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.66it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.70it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.74it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.87it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.03it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.99it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.89it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.94it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.91it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.98it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.84it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.90it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.00it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.02it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 44.02it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.02it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 43.87it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 43.82it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.74it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.71it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.76it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.77it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.88it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.85it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.95it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.82it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.82it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.89it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.83it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.80it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.93it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.99it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.00it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.02it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 43.87it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.85it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.62it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.96it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.00it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.96it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.99it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.98it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.01it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.85it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:32<01:09,  3.38it/s]
100%|██████████| 543/543 [00:12<00:00, 43.85it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:18:12,026 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-27 23:18:12,063 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:18:13,588 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:18:13,605 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:18:13,614 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:38<22:08,  5.70s/it] 60%|██████    | 353/585 [02:38<15:46,  4.08s/it] 61%|██████    | 354/585 [02:39<11:20,  2.94s/it] 61%|██████    | 355/585 [02:39<08:14,  2.15s/it] 61%|██████    | 356/585 [02:39<06:04,  1.59s/it] 61%|██████    | 357/585 [02:40<04:34,  1.20s/it] 61%|██████    | 358/585 [02:40<03:31,  1.07it/s] 61%|██████▏   | 359/585 [02:40<02:47,  1.35it/s] 62%|██████▏   | 360/585 [02:40<02:16,  1.65it/s] 62%|██████▏   | 361/585 [02:41<01:54,  1.96it/s] 62%|██████▏   | 362/585 [02:41<01:39,  2.25it/s] 62%|██████▏   | 363/585 [02:41<01:28,  2.51it/s] 62%|██████▏   | 364/585 [02:42<01:21,  2.72it/s] 62%|██████▏   | 365/585 [02:42<01:15,  2.90it/s] 63%|██████▎   | 366/585 [02:42<01:11,  3.04it/s] 63%|██████▎   | 367/585 [02:42<01:09,  3.15it/s] 63%|██████▎   | 368/585 [02:43<01:07,  3.23it/s] 63%|██████▎   | 369/585 [02:43<01:05,  3.29it/s] 63%|██████▎   | 370/585 [02:43<01:04,  3.33it/s] 63%|██████▎   | 371/585 [02:44<01:03,  3.36it/s] 64%|██████▎   | 372/585 [02:44<01:02,  3.38it/s] 64%|██████▍   | 373/585 [02:44<01:02,  3.40it/s] 64%|██████▍   | 374/585 [02:44<01:01,  3.41it/s] 64%|██████▍   | 375/585 [02:45<01:01,  3.39it/s] 64%|██████▍   | 376/585 [02:45<01:01,  3.40it/s] 64%|██████▍   | 377/585 [02:45<01:01,  3.41it/s] 65%|██████▍   | 378/585 [02:46<01:00,  3.41it/s] 65%|██████▍   | 379/585 [02:46<01:00,  3.42it/s] 65%|██████▍   | 380/585 [02:46<00:59,  3.42it/s] 65%|██████▌   | 381/585 [02:47<00:59,  3.42it/s] 65%|██████▌   | 382/585 [02:47<00:59,  3.43it/s] 65%|██████▌   | 383/585 [02:47<00:58,  3.43it/s] 66%|██████▌   | 384/585 [02:47<00:58,  3.43it/s] 66%|██████▌   | 385/585 [02:48<00:58,  3.43it/s] 66%|██████▌   | 386/585 [02:48<00:58,  3.42it/s] 66%|██████▌   | 387/585 [02:48<00:57,  3.43it/s] 66%|██████▋   | 388/585 [02:49<00:57,  3.42it/s] 66%|██████▋   | 389/585 [02:49<00:57,  3.42it/s] 67%|██████▋   | 390/585 [02:49<00:56,  3.43it/s] 67%|██████▋   | 391/585 [02:49<00:56,  3.43it/s] 67%|██████▋   | 392/585 [02:50<00:56,  3.43it/s] 67%|██████▋   | 393/585 [02:50<00:55,  3.43it/s] 67%|██████▋   | 394/585 [02:50<00:55,  3.43it/s] 68%|██████▊   | 395/585 [02:51<00:55,  3.43it/s] 68%|██████▊   | 396/585 [02:51<00:55,  3.43it/s] 68%|██████▊   | 397/585 [02:51<00:54,  3.43it/s] 68%|██████▊   | 398/585 [02:51<00:54,  3.43it/s] 68%|██████▊   | 399/585 [02:52<00:54,  3.43it/s] 68%|██████▊   | 400/585 [02:52<00:53,  3.43it/s] 69%|██████▊   | 401/585 [02:52<00:53,  3.43it/s] 69%|██████▊   | 402/585 [02:53<00:53,  3.43it/s] 69%|██████▉   | 403/585 [02:53<00:53,  3.43it/s] 69%|██████▉   | 404/585 [02:53<00:52,  3.42it/s] 69%|██████▉   | 405/585 [02:54<00:52,  3.42it/s] 69%|██████▉   | 406/585 [02:54<00:52,  3.43it/s] 70%|██████▉   | 407/585 [02:54<00:51,  3.43it/s] 70%|██████▉   | 408/585 [02:54<00:51,  3.43it/s] 70%|██████▉   | 409/585 [02:55<00:51,  3.43it/s] 70%|███████   | 410/585 [02:55<00:51,  3.43it/s] 70%|███████   | 411/585 [02:55<00:50,  3.43it/s] 70%|███████   | 412/585 [02:56<00:50,  3.43it/s] 71%|███████   | 413/585 [02:56<00:50,  3.43it/s] 71%|███████   | 414/585 [02:56<00:49,  3.43it/s] 71%|███████   | 415/585 [02:56<00:49,  3.42it/s] 71%|███████   | 416/585 [02:57<00:49,  3.43it/s] 71%|███████▏  | 417/585 [02:57<00:49,  3.43it/s] 71%|███████▏  | 418/585 [02:57<00:48,  3.43it/s] 72%|███████▏  | 419/585 [02:58<00:48,  3.43it/s] 72%|███████▏  | 420/585 [02:58<00:48,  3.43it/s] 72%|███████▏  | 421/585 [02:58<00:47,  3.43it/s] 72%|███████▏  | 422/585 [02:58<00:47,  3.43it/s] 72%|███████▏  | 423/585 [02:59<00:47,  3.43it/s] 72%|███████▏  | 424/585 [02:59<00:46,  3.43it/s] 73%|███████▎  | 425/585 [02:59<00:46,  3.43it/s] 73%|███████▎  | 426/585 [03:00<00:46,  3.40it/s] 73%|███████▎  | 427/585 [03:00<00:46,  3.41it/s] 73%|███████▎  | 428/585 [03:00<00:45,  3.42it/s] 73%|███████▎  | 429/585 [03:01<00:45,  3.41it/s] 74%|███████▎  | 430/585 [03:01<00:45,  3.40it/s] 74%|███████▎  | 431/585 [03:01<00:45,  3.39it/s] 74%|███████▍  | 432/585 [03:01<00:45,  3.39it/s] 74%|███████▍  | 433/585 [03:02<00:44,  3.39it/s] 74%|███████▍  | 434/585 [03:02<00:44,  3.38it/s] 74%|███████▍  | 435/585 [03:02<00:44,  3.38it/s] 75%|███████▍  | 436/585 [03:03<00:44,  3.38it/s] 75%|███████▍  | 437/585 [03:03<00:44,  3.36it/s] 75%|███████▍  | 438/585 [03:03<00:43,  3.37it/s] 75%|███████▌  | 439/585 [03:04<00:43,  3.37it/s] 75%|███████▌  | 440/585 [03:04<00:43,  3.37it/s] 75%|███████▌  | 441/585 [03:04<00:42,  3.37it/s] 76%|███████▌  | 442/585 [03:04<00:42,  3.38it/s] 76%|███████▌  | 443/585 [03:05<00:42,  3.38it/s] 76%|███████▌  | 444/585 [03:05<00:41,  3.38it/s] 76%|███████▌  | 445/585 [03:05<00:41,  3.38it/s] 76%|███████▌  | 446/585 [03:06<00:41,  3.38it/s] 76%|███████▋  | 447/585 [03:06<00:40,  3.38it/s] 77%|███████▋  | 448/585 [03:06<00:40,  3.37it/s] 77%|███████▋  | 449/585 [03:06<00:40,  3.37it/s] 77%|███████▋  | 450/585 [03:07<00:39,  3.38it/s] 77%|███████▋  | 451/585 [03:07<00:39,  3.37it/s] 77%|███████▋  | 452/585 [03:07<00:39,  3.37it/s] 77%|███████▋  | 453/585 [03:08<00:39,  3.37it/s] 78%|███████▊  | 454/585 [03:08<00:38,  3.38it/s] 78%|███████▊  | 455/585 [03:08<00:38,  3.38it/s] 78%|███████▊  | 456/585 [03:09<00:38,  3.38it/s] 78%|███████▊  | 457/585 [03:09<00:37,  3.38it/s] 78%|███████▊  | 458/585 [03:09<00:37,  3.38it/s] 78%|███████▊  | 459/585 [03:09<00:37,  3.36it/s] 79%|███████▊  | 460/585 [03:10<00:37,  3.37it/s] 79%|███████▉  | 461/585 [03:10<00:36,  3.37it/s] 79%|███████▉  | 462/585 [03:10<00:36,  3.37it/s] 79%|███████▉  | 463/585 [03:11<00:36,  3.37it/s] 79%|███████▉  | 464/585 [03:11<00:35,  3.37it/s] 79%|███████▉  | 465/585 [03:11<00:35,  3.38it/s] 80%|███████▉  | 466/585 [03:12<00:35,  3.38it/s] 80%|███████▉  | 467/585 [03:12<00:34,  3.38it/s] 80%|████████  | 468/585 [03:12<00:34,  3.38it/s][INFO|trainer.py:2140] 2023-08-27 23:18:52,024 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:18:52,026 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-27 23:18:52,026 >>   Batch size = 8
{'eval_loss': 0.975967288017273, 'eval_runtime': 12.3575, 'eval_samples_per_second': 351.367, 'eval_steps_per_second': 43.941, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.63it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.62it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.92it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.13it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.59it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.45it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.24it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.19it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.30it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.40it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.18it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.18it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.08it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.81it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.93it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.96it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.07it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.12it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.18it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.08it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.14it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.89it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.88it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.99it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.01it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.15it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.07it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.18it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.14it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.02it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.00it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.94it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.94it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.06it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.09it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.98it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.18it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.09it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.97it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.88it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.95it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.91it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.98it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.13it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.13it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.10it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.03it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.01it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.02it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.98it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.05it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.95it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.07it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.05it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.03it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.07it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.99it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.97it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.02it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.08it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.02it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.03it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.90it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.99it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.96it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.94it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.96it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.04it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.13it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.94it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.88it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.81it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.78it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.85it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.77it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.84it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.03it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.05it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.95it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.08it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.06it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.96it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.01it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.07it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.03it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.08it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.69it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.17it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.09it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.07it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.00it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.88it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.00it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.03it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.01it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.20it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 502/543 [00:11<00:01, 34.93it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 37.27it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 39.13it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 40.63it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 41.78it/s][A
 97%|█████████▋| 527/543 [00:12<00:00, 42.53it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.19it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.31it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.07it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:25<00:34,  3.38it/s]
100%|██████████| 543/543 [00:12<00:00, 43.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:19:04,472 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-27 23:19:04,496 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:19:06,387 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:19:06,471 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:19:06,572 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:31<11:05,  5.74s/it] 80%|████████  | 470/585 [03:31<07:52,  4.11s/it] 81%|████████  | 471/585 [03:31<05:37,  2.96s/it] 81%|████████  | 472/585 [03:31<04:04,  2.16s/it] 81%|████████  | 473/585 [03:32<02:59,  1.60s/it] 81%|████████  | 474/585 [03:32<02:14,  1.21s/it] 81%|████████  | 475/585 [03:32<01:42,  1.07it/s] 81%|████████▏ | 476/585 [03:33<01:21,  1.34it/s] 82%|████████▏ | 477/585 [03:33<01:05,  1.64it/s] 82%|████████▏ | 478/585 [03:33<00:55,  1.94it/s] 82%|████████▏ | 479/585 [03:33<00:47,  2.23it/s] 82%|████████▏ | 480/585 [03:34<00:42,  2.48it/s] 82%|████████▏ | 481/585 [03:34<00:38,  2.69it/s] 82%|████████▏ | 482/585 [03:34<00:35,  2.87it/s] 83%|████████▎ | 483/585 [03:35<00:33,  3.01it/s] 83%|████████▎ | 484/585 [03:35<00:32,  3.11it/s] 83%|████████▎ | 485/585 [03:35<00:31,  3.19it/s] 83%|████████▎ | 486/585 [03:36<00:30,  3.24it/s] 83%|████████▎ | 487/585 [03:36<00:29,  3.28it/s] 83%|████████▎ | 488/585 [03:36<00:29,  3.31it/s] 84%|████████▎ | 489/585 [03:36<00:28,  3.33it/s] 84%|████████▍ | 490/585 [03:37<00:28,  3.35it/s] 84%|████████▍ | 491/585 [03:37<00:28,  3.35it/s] 84%|████████▍ | 492/585 [03:37<00:27,  3.35it/s] 84%|████████▍ | 493/585 [03:38<00:27,  3.36it/s] 84%|████████▍ | 494/585 [03:38<00:27,  3.37it/s] 85%|████████▍ | 495/585 [03:38<00:26,  3.37it/s] 85%|████████▍ | 496/585 [03:39<00:26,  3.37it/s] 85%|████████▍ | 497/585 [03:39<00:26,  3.38it/s] 85%|████████▌ | 498/585 [03:39<00:25,  3.38it/s] 85%|████████▌ | 499/585 [03:39<00:25,  3.38it/s] 85%|████████▌ | 500/585 [03:40<00:25,  3.38it/s]                                                  85%|████████▌ | 500/585 [03:40<00:25,  3.38it/s] 86%|████████▌ | 501/585 [03:40<00:24,  3.38it/s] 86%|████████▌ | 502/585 [03:40<00:24,  3.38it/s] 86%|████████▌ | 503/585 [03:41<00:24,  3.37it/s] 86%|████████▌ | 504/585 [03:41<00:24,  3.37it/s] 86%|████████▋ | 505/585 [03:41<00:23,  3.38it/s] 86%|████████▋ | 506/585 [03:41<00:23,  3.38it/s] 87%|████████▋ | 507/585 [03:42<00:23,  3.38it/s] 87%|████████▋ | 508/585 [03:42<00:22,  3.38it/s] 87%|████████▋ | 509/585 [03:42<00:22,  3.38it/s] 87%|████████▋ | 510/585 [03:43<00:22,  3.38it/s] 87%|████████▋ | 511/585 [03:43<00:21,  3.38it/s] 88%|████████▊ | 512/585 [03:43<00:21,  3.38it/s] 88%|████████▊ | 513/585 [03:44<00:21,  3.38it/s] 88%|████████▊ | 514/585 [03:44<00:21,  3.38it/s] 88%|████████▊ | 515/585 [03:44<00:20,  3.38it/s] 88%|████████▊ | 516/585 [03:44<00:20,  3.38it/s] 88%|████████▊ | 517/585 [03:45<00:20,  3.38it/s] 89%|████████▊ | 518/585 [03:45<00:19,  3.38it/s] 89%|████████▊ | 519/585 [03:45<00:19,  3.38it/s] 89%|████████▉ | 520/585 [03:46<00:19,  3.38it/s] 89%|████████▉ | 521/585 [03:46<00:18,  3.38it/s] 89%|████████▉ | 522/585 [03:46<00:18,  3.38it/s] 89%|████████▉ | 523/585 [03:47<00:18,  3.39it/s] 90%|████████▉ | 524/585 [03:47<00:18,  3.38it/s] 90%|████████▉ | 525/585 [03:47<00:17,  3.37it/s] 90%|████████▉ | 526/585 [03:47<00:17,  3.37it/s] 90%|█████████ | 527/585 [03:48<00:17,  3.38it/s] 90%|█████████ | 528/585 [03:48<00:16,  3.38it/s] 90%|█████████ | 529/585 [03:48<00:16,  3.38it/s] 91%|█████████ | 530/585 [03:49<00:16,  3.38it/s] 91%|█████████ | 531/585 [03:49<00:15,  3.38it/s] 91%|█████████ | 532/585 [03:49<00:15,  3.38it/s] 91%|█████████ | 533/585 [03:49<00:15,  3.38it/s] 91%|█████████▏| 534/585 [03:50<00:15,  3.38it/s] 91%|█████████▏| 535/585 [03:50<00:14,  3.38it/s] 92%|█████████▏| 536/585 [03:50<00:14,  3.35it/s] 92%|█████████▏| 537/585 [03:51<00:14,  3.36it/s] 92%|█████████▏| 538/585 [03:51<00:13,  3.37it/s] 92%|█████████▏| 539/585 [03:51<00:13,  3.37it/s] 92%|█████████▏| 540/585 [03:52<00:13,  3.38it/s] 92%|█████████▏| 541/585 [03:52<00:13,  3.38it/s] 93%|█████████▎| 542/585 [03:52<00:12,  3.38it/s] 93%|█████████▎| 543/585 [03:52<00:12,  3.31it/s] 93%|█████████▎| 544/585 [03:53<00:12,  3.33it/s] 93%|█████████▎| 545/585 [03:53<00:11,  3.34it/s] 93%|█████████▎| 546/585 [03:53<00:11,  3.35it/s] 94%|█████████▎| 547/585 [03:54<00:11,  3.36it/s] 94%|█████████▎| 548/585 [03:54<00:10,  3.37it/s] 94%|█████████▍| 549/585 [03:54<00:10,  3.37it/s] 94%|█████████▍| 550/585 [03:55<00:10,  3.36it/s] 94%|█████████▍| 551/585 [03:55<00:10,  3.37it/s] 94%|█████████▍| 552/585 [03:55<00:09,  3.37it/s] 95%|█████████▍| 553/585 [03:55<00:09,  3.37it/s] 95%|█████████▍| 554/585 [03:56<00:09,  3.38it/s] 95%|█████████▍| 555/585 [03:56<00:08,  3.38it/s] 95%|█████████▌| 556/585 [03:56<00:08,  3.38it/s] 95%|█████████▌| 557/585 [03:57<00:08,  3.38it/s] 95%|█████████▌| 558/585 [03:57<00:07,  3.38it/s] 96%|█████████▌| 559/585 [03:57<00:07,  3.38it/s] 96%|█████████▌| 560/585 [03:57<00:07,  3.38it/s] 96%|█████████▌| 561/585 [03:58<00:07,  3.38it/s] 96%|█████████▌| 562/585 [03:58<00:06,  3.40it/s] 96%|█████████▌| 563/585 [03:58<00:06,  3.41it/s] 96%|█████████▋| 564/585 [03:59<00:06,  3.41it/s] 97%|█████████▋| 565/585 [03:59<00:05,  3.42it/s] 97%|█████████▋| 566/585 [03:59<00:05,  3.42it/s] 97%|█████████▋| 567/585 [04:00<00:05,  3.43it/s] 97%|█████████▋| 568/585 [04:00<00:04,  3.41it/s] 97%|█████████▋| 569/585 [04:00<00:04,  3.42it/s] 97%|█████████▋| 570/585 [04:00<00:04,  3.42it/s] 98%|█████████▊| 571/585 [04:01<00:04,  3.42it/s] 98%|█████████▊| 572/585 [04:01<00:03,  3.42it/s] 98%|█████████▊| 573/585 [04:01<00:03,  3.42it/s] 98%|█████████▊| 574/585 [04:02<00:03,  3.42it/s] 98%|█████████▊| 575/585 [04:02<00:02,  3.42it/s] 98%|█████████▊| 576/585 [04:02<00:02,  3.42it/s] 99%|█████████▊| 577/585 [04:02<00:02,  3.42it/s] 99%|█████████▉| 578/585 [04:03<00:02,  3.43it/s] 99%|█████████▉| 579/585 [04:03<00:01,  3.43it/s] 99%|█████████▉| 580/585 [04:03<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:04<00:01,  3.43it/s] 99%|█████████▉| 582/585 [04:04<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:04<00:00,  3.42it/s]100%|█████████▉| 584/585 [04:05<00:00,  3.43it/s]100%|██████████| 585/585 [04:05<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-27 23:19:44,585 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:19:44,585 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-27 23:19:44,586 >>   Batch size = 8
{'eval_loss': 0.975273072719574, 'eval_runtime': 12.4353, 'eval_samples_per_second': 349.168, 'eval_steps_per_second': 43.666, 'epoch': 4.0}
{'loss': 0.8206, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.30it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.48it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.82it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.20it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.78it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.45it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.29it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.22it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.22it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.35it/s][A
 10%|█         | 57/543 [00:01<00:11, 44.09it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.13it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.08it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.99it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.01it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.98it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.04it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.05it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.12it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.11it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.00it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.07it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.88it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.97it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.00it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.05it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.05it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.13it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.17it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.01it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.00it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.87it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.04it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.95it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.92it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.07it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.12it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.06it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.06it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.00it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.07it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.01it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.93it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.98it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.01it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.06it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.08it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.01it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.01it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.10it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.02it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.07it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.92it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.13it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.12it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.09it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.05it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.05it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.06it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.94it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.98it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.00it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.15it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.03it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.06it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.97it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.04it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.08it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.07it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.11it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.10it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.05it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.97it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.07it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.16it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.04it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.03it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.05it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.07it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.03it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.07it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.98it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.99it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.02it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.06it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.03it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.02it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.10it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.12it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.05it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.99it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.04it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.08it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.99it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.01it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.08it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.02it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.03it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.96it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.88it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.93it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.05it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.04it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.04it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.08it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:17<00:00,  3.43it/s]
100%|██████████| 543/543 [00:12<00:00, 44.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:19:56,932 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-27 23:19:56,946 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:19:59,703 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:19:59,719 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:19:59,734 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 23:20:05,152 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 23:20:05,154 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234 (score: 0.9734402894973755).
                                                 100%|██████████| 585/585 [04:27<00:00,  3.43it/s]100%|██████████| 585/585 [04:27<00:00,  2.19it/s]
[INFO|trainer.py:1894] 2023-08-27 23:20:07,005 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 23:20:07,019 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:20:08,623 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:20:08,642 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:20:08,649 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 23:20:08,844 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:08,845 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:08,845 >>   train_loss               =      0.815
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:08,845 >>   train_runtime            = 0:04:27.70
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:08,845 >>   train_samples            =       7520
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:08,845 >>   train_samples_per_second =    140.451
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:08,845 >>   train_steps_per_second   =      2.185
{'eval_loss': 0.9761122465133667, 'eval_runtime': 12.3271, 'eval_samples_per_second': 352.233, 'eval_steps_per_second': 44.049, 'epoch': 5.0}
{'train_runtime': 267.7088, 'train_samples_per_second': 140.451, 'train_steps_per_second': 2.185, 'train_loss': 0.8150455572666266, 'epoch': 5.0}
08/27/2023 23:20:08 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 23:20:08,889 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:20:08,889 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-27 23:20:08,889 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 56.09it/s]  2%|▏         | 12/543 [00:00<00:10, 48.85it/s]  3%|▎         | 17/543 [00:00<00:11, 47.25it/s]  4%|▍         | 22/543 [00:00<00:11, 46.37it/s]  5%|▍         | 27/543 [00:00<00:11, 45.93it/s]  6%|▌         | 32/543 [00:00<00:11, 45.57it/s]  7%|▋         | 37/543 [00:00<00:11, 45.30it/s]  8%|▊         | 42/543 [00:00<00:11, 44.69it/s]  9%|▊         | 47/543 [00:01<00:11, 44.11it/s] 10%|▉         | 52/543 [00:01<00:11, 43.79it/s] 10%|█         | 57/543 [00:01<00:11, 43.86it/s] 11%|█▏        | 62/543 [00:01<00:10, 44.15it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.43it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.58it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.67it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.52it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.17it/s] 17%|█▋        | 92/543 [00:02<00:10, 43.88it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.73it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.80it/s] 20%|█▉        | 107/543 [00:02<00:09, 43.99it/s] 21%|██        | 112/543 [00:02<00:09, 44.25it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.34it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.56it/s] 23%|██▎       | 127/543 [00:02<00:09, 44.29it/s] 24%|██▍       | 132/543 [00:02<00:09, 43.58it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.83it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.74it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.89it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.06it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.03it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.29it/s] 31%|███       | 167/543 [00:03<00:08, 44.26it/s] 32%|███▏      | 172/543 [00:03<00:08, 44.19it/s] 33%|███▎      | 177/543 [00:03<00:08, 44.03it/s] 34%|███▎      | 182/543 [00:04<00:08, 43.60it/s] 34%|███▍      | 187/543 [00:04<00:08, 43.77it/s] 35%|███▌      | 192/543 [00:04<00:08, 43.77it/s] 36%|███▋      | 197/543 [00:04<00:07, 43.97it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.21it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.27it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.42it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.21it/s] 41%|████      | 222/543 [00:04<00:07, 44.01it/s] 42%|████▏     | 227/543 [00:05<00:07, 43.87it/s] 43%|████▎     | 232/543 [00:05<00:07, 43.92it/s] 44%|████▎     | 237/543 [00:05<00:06, 43.96it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.09it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.22it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.32it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.20it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.11it/s] 49%|████▉     | 267/543 [00:06<00:06, 43.94it/s] 50%|█████     | 272/543 [00:06<00:06, 43.94it/s] 51%|█████     | 277/543 [00:06<00:06, 43.87it/s] 52%|█████▏    | 282/543 [00:06<00:05, 43.94it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.06it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.16it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.25it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.26it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.05it/s] 57%|█████▋    | 312/543 [00:07<00:05, 43.95it/s] 58%|█████▊    | 317/543 [00:07<00:05, 43.98it/s] 59%|█████▉    | 322/543 [00:07<00:05, 43.86it/s] 60%|██████    | 327/543 [00:07<00:04, 43.96it/s] 61%|██████    | 332/543 [00:07<00:04, 44.04it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.03it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.28it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.19it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.02it/s] 66%|██████▌   | 357/543 [00:08<00:04, 43.94it/s] 67%|██████▋   | 362/543 [00:08<00:04, 43.87it/s] 68%|██████▊   | 367/543 [00:08<00:04, 43.95it/s] 69%|██████▊   | 372/543 [00:08<00:03, 43.96it/s] 69%|██████▉   | 377/543 [00:08<00:03, 43.97it/s] 70%|███████   | 382/543 [00:08<00:03, 44.10it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.22it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.03it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.07it/s] 74%|███████▍  | 402/543 [00:09<00:03, 43.98it/s] 75%|███████▍  | 407/543 [00:09<00:03, 43.86it/s] 76%|███████▌  | 412/543 [00:09<00:02, 44.00it/s] 77%|███████▋  | 417/543 [00:09<00:02, 43.92it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.02it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.15it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.10it/s] 80%|████████  | 437/543 [00:09<00:02, 44.04it/s] 81%|████████▏ | 442/543 [00:09<00:02, 44.01it/s] 82%|████████▏ | 447/543 [00:10<00:02, 43.89it/s] 83%|████████▎ | 452/543 [00:10<00:02, 43.96it/s] 84%|████████▍ | 457/543 [00:10<00:01, 43.96it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.03it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.01it/s] 87%|████████▋ | 472/543 [00:10<00:01, 43.95it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.16it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.17it/s] 90%|████████▉ | 487/543 [00:11<00:01, 43.93it/s] 91%|█████████ | 492/543 [00:11<00:01, 44.00it/s] 92%|█████████▏| 497/543 [00:11<00:01, 43.93it/s] 92%|█████████▏| 502/543 [00:11<00:00, 44.04it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.01it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.04it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.05it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.12it/s] 97%|█████████▋| 527/543 [00:11<00:00, 44.07it/s] 98%|█████████▊| 532/543 [00:12<00:00, 43.87it/s] 99%|█████████▉| 537/543 [00:12<00:00, 44.00it/s]100%|█████████▉| 542/543 [00:12<00:00, 43.96it/s]100%|██████████| 543/543 [00:12<00:00, 44.15it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 23:20:21,206 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:21,207 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:21,207 >>   eval_loss               =     0.9734
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:21,207 >>   eval_runtime            = 0:00:12.31
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:21,207 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:21,207 >>   eval_samples_per_second =    352.509
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:21,207 >>   eval_steps_per_second   =     44.084
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:20:21,207 >>   perplexity              =      2.647
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_0', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:57, 16.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:34, 16.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:49<03:16, 16.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:08<03:10, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:24<02:51, 17.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:40<02:30, 16.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:58<02:16, 17.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:16<02:01, 17.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:31<01:40, 16.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:46<01:20, 16.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:02<01:03, 15.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:21<00:50, 16.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:38<00:34, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:54<00:16, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:12<00:00, 17.04s/it]Generating: 100%|██████████| 15/15 [04:12<00:00, 16.83s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : characters . Context : Later in Life , he played the title character , a young princess of the family at the end of the third season of HBO s Game of Thrones . Head Entity : Game of Thrones , Tail Entity : Princess of the family .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.75375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.7525, 'errors': {'', "('pharaoh Amenhotep II', 'made from material', '', 'He is best remembered for the painting of the pharaoh Amenhotep II , composed by Alfred Wohl in 1827 for a French conservatory .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : cast member .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Billboard Pop Hits', 'cast member', '', 'On October 25 , 1978 , the band released the track The Lads , the debut single by the British band The Riveters ( now defunct ) , on Columbia Records , along with several appearances on the Billboard Pop Hits chart for June .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : follows .', 'success_rate': 0.7283653846153846, 'errors': {'', "('Nigeria', 'follows', '', 'In 2013 , a second poll showed Zaire had a 7 % approval rating , while in 2014 , Nigeria had a 24 % approval rating .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 281, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 366, 'raw': 576}
{'target': 600, 'success': 391, 'raw': 608}
{'target': 600, 'success': 410, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 452, 'raw': 704}
{'target': 600, 'success': 470, 'raw': 736}
{'target': 600, 'success': 490, 'raw': 768}
{'target': 600, 'success': 511, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 590, 'raw': 928}
{'target': 600, 'success': 608, 'raw': 960}
{'prompt': 'Relation : league .', 'success_rate': 0.6333333333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.765, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in Life , he studied with a group of Latin philologists and other thinkers at the Renaissance in Italy , in Naples from 1635 to 1645 , and from 1654 back to 1701 . Head Entity : Naples , Tail Entity : Rome .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 262, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 305, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 341, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 412, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 501, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 584, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : mother .', 'success_rate': 0.6729910714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : residence .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1453 the head of the province of Ghent and of neighboring districts at Neuromö , in Burgundesland , were also under the Ottoman Empire . Head Entity : Burgundesland , Tail Entity : Ottomans .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 15273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_10_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.65s/it]Extractor Estimating: 2it [00:18,  8.00s/it]Extractor Estimating: 3it [00:19,  4.63s/it]Extractor Estimating: 4it [00:20,  3.05s/it]Extractor Estimating: 5it [00:20,  2.17s/it]Extractor Estimating: 6it [00:22,  2.04s/it]Extractor Estimating: 7it [00:23,  1.57s/it]Extractor Estimating: 8it [00:24,  1.46s/it]Extractor Estimating: 9it [00:24,  1.19s/it]Extractor Estimating: 10it [00:25,  1.03s/it]Extractor Estimating: 11it [00:26,  1.06it/s]Extractor Estimating: 12it [00:26,  1.19it/s]Extractor Estimating: 13it [00:27,  1.28it/s]Extractor Estimating: 14it [00:28,  1.26it/s]Extractor Estimating: 15it [00:29,  1.34it/s]Extractor Estimating: 16it [00:29,  1.39it/s]Extractor Estimating: 17it [00:30,  1.44it/s]Extractor Estimating: 18it [00:30,  1.50it/s]Extractor Estimating: 19it [00:31,  1.49it/s]Extractor Estimating: 20it [00:32,  1.55it/s]Extractor Estimating: 21it [00:32,  1.56it/s]Extractor Estimating: 22it [00:33,  1.64it/s]Extractor Estimating: 23it [00:34,  1.57it/s]Extractor Estimating: 24it [00:34,  1.49it/s]Extractor Estimating: 25it [00:35,  1.52it/s]Extractor Estimating: 26it [00:36,  1.56it/s]Extractor Estimating: 27it [00:36,  1.57it/s]Extractor Estimating: 28it [00:37,  1.62it/s]Extractor Estimating: 29it [00:37,  1.61it/s]Extractor Estimating: 30it [00:38,  1.64it/s]Extractor Estimating: 31it [00:39,  1.60it/s]Extractor Estimating: 32it [00:39,  1.62it/s]Extractor Estimating: 33it [00:40,  1.54it/s]Extractor Estimating: 34it [00:41,  1.52it/s]Extractor Estimating: 35it [00:41,  1.53it/s]Extractor Estimating: 36it [00:42,  1.52it/s]Extractor Estimating: 37it [00:43,  1.54it/s]Extractor Estimating: 38it [00:43,  1.55it/s]Extractor Estimating: 39it [00:44,  1.51it/s]Extractor Estimating: 40it [00:45,  1.51it/s]Extractor Estimating: 41it [00:45,  1.54it/s]Extractor Estimating: 42it [00:46,  1.55it/s]Extractor Estimating: 43it [00:48,  1.00s/it]Extractor Estimating: 44it [00:48,  1.11it/s]Extractor Estimating: 45it [00:49,  1.21it/s]Extractor Estimating: 46it [00:50,  1.31it/s]Extractor Estimating: 47it [00:50,  1.37it/s]Extractor Estimating: 48it [00:51,  1.43it/s]Extractor Estimating: 49it [00:52,  1.45it/s]Extractor Estimating: 50it [00:52,  1.43it/s]Extractor Estimating: 51it [00:53,  1.45it/s]Extractor Estimating: 52it [00:54,  1.54it/s]Extractor Estimating: 53it [00:54,  1.57it/s]Extractor Estimating: 54it [00:55,  1.56it/s]Extractor Estimating: 55it [00:55,  1.55it/s]Extractor Estimating: 56it [00:56,  1.58it/s]Extractor Estimating: 57it [00:57,  1.55it/s]Extractor Estimating: 58it [00:57,  1.57it/s]Extractor Estimating: 59it [00:58,  1.57it/s]Extractor Estimating: 60it [00:59,  1.59it/s]Extractor Estimating: 61it [00:59,  1.58it/s]Extractor Estimating: 62it [01:00,  1.61it/s]Extractor Estimating: 63it [01:00,  1.63it/s]Extractor Estimating: 64it [01:01,  1.64it/s]Extractor Estimating: 65it [01:02,  1.65it/s]Extractor Estimating: 66it [01:02,  1.62it/s]Extractor Estimating: 67it [01:03,  1.63it/s]Extractor Estimating: 68it [01:03,  1.65it/s]Extractor Estimating: 69it [01:04,  1.64it/s]Extractor Estimating: 70it [01:05,  1.63it/s]Extractor Estimating: 71it [01:05,  1.56it/s]Extractor Estimating: 72it [01:06,  1.59it/s]Extractor Estimating: 73it [01:07,  1.62it/s]Extractor Estimating: 74it [01:07,  1.60it/s]Extractor Estimating: 75it [01:08,  1.58it/s]Extractor Estimating: 76it [01:09,  1.55it/s]Extractor Estimating: 77it [01:09,  1.57it/s]Extractor Estimating: 78it [01:10,  1.58it/s]Extractor Estimating: 79it [01:11,  1.51it/s]Extractor Estimating: 80it [01:11,  1.45it/s]Extractor Estimating: 81it [01:12,  1.50it/s]Extractor Estimating: 82it [01:13,  1.46it/s]Extractor Estimating: 83it [01:13,  1.54it/s]Extractor Estimating: 84it [01:14,  1.53it/s]Extractor Estimating: 85it [01:14,  1.57it/s]Extractor Estimating: 86it [01:15,  1.47it/s]Extractor Estimating: 87it [01:16,  1.52it/s]Extractor Estimating: 88it [01:16,  1.57it/s]Extractor Estimating: 89it [01:17,  1.56it/s]Extractor Estimating: 90it [01:18,  1.54it/s]Extractor Estimating: 91it [01:18,  1.57it/s]Extractor Estimating: 92it [01:19,  1.55it/s]Extractor Estimating: 93it [01:20,  1.55it/s]Extractor Estimating: 94it [01:20,  1.55it/s]Extractor Estimating: 95it [01:21,  1.53it/s]Extractor Estimating: 96it [01:22,  1.55it/s]Extractor Estimating: 97it [01:22,  1.56it/s]Extractor Estimating: 98it [01:23,  1.55it/s]Extractor Estimating: 99it [01:24,  1.46it/s]Extractor Estimating: 100it [01:24,  1.51it/s]Extractor Estimating: 101it [01:25,  1.48it/s]Extractor Estimating: 102it [01:26,  1.52it/s]Extractor Estimating: 103it [01:26,  1.56it/s]Extractor Estimating: 104it [01:27,  1.57it/s]Extractor Estimating: 105it [01:27,  1.56it/s]Extractor Estimating: 106it [01:28,  1.58it/s]Extractor Estimating: 107it [01:29,  1.60it/s]Extractor Estimating: 108it [01:29,  1.64it/s]Extractor Estimating: 109it [01:30,  1.65it/s]Extractor Estimating: 110it [01:31,  1.59it/s]Extractor Estimating: 111it [01:31,  1.62it/s]Extractor Estimating: 112it [01:32,  1.60it/s]Extractor Estimating: 113it [01:32,  1.63it/s]Extractor Estimating: 114it [01:33,  1.63it/s]Extractor Estimating: 115it [01:34,  1.63it/s]Extractor Estimating: 116it [01:34,  1.57it/s]Extractor Estimating: 117it [01:35,  1.61it/s]Extractor Estimating: 118it [01:36,  1.56it/s]Extractor Estimating: 119it [01:36,  1.59it/s]Extractor Estimating: 120it [01:37,  1.53it/s]Extractor Estimating: 121it [01:38,  1.50it/s]Extractor Estimating: 122it [01:38,  1.50it/s]Extractor Estimating: 123it [01:39,  1.55it/s]Extractor Estimating: 124it [01:40,  1.54it/s]Extractor Estimating: 125it [01:41,  1.18it/s]Extractor Estimating: 126it [01:41,  1.26it/s]Extractor Estimating: 127it [01:42,  1.32it/s]Extractor Estimating: 128it [01:43,  1.43it/s]Extractor Estimating: 129it [01:43,  1.53it/s]Extractor Estimating: 130it [01:44,  1.51it/s]Extractor Estimating: 131it [01:45,  1.59it/s]Extractor Estimating: 132it [01:45,  1.56it/s]Extractor Estimating: 133it [01:46,  1.58it/s]Extractor Estimating: 134it [01:46,  1.56it/s]Extractor Estimating: 135it [01:47,  1.54it/s]Extractor Estimating: 136it [01:48,  1.49it/s]Extractor Estimating: 137it [01:49,  1.47it/s]Extractor Estimating: 138it [01:49,  1.52it/s]Extractor Estimating: 139it [01:50,  1.45it/s]Extractor Estimating: 140it [01:51,  1.51it/s]Extractor Estimating: 141it [01:51,  1.58it/s]Extractor Estimating: 142it [01:52,  1.56it/s]Extractor Estimating: 143it [01:52,  1.60it/s]Extractor Estimating: 144it [01:53,  1.63it/s]Extractor Estimating: 145it [01:54,  1.55it/s]Extractor Estimating: 146it [01:54,  1.53it/s]Extractor Estimating: 147it [01:55,  1.57it/s]Extractor Estimating: 148it [01:55,  1.61it/s]Extractor Estimating: 149it [01:56,  1.59it/s]Extractor Estimating: 150it [01:57,  1.57it/s]Extractor Estimating: 151it [01:57,  1.56it/s]Extractor Estimating: 152it [01:58,  1.54it/s]Extractor Estimating: 153it [01:59,  1.53it/s]Extractor Estimating: 154it [02:00,  1.42it/s]Extractor Estimating: 155it [02:00,  1.47it/s]Extractor Estimating: 156it [02:01,  1.52it/s]Extractor Estimating: 157it [02:02,  1.49it/s]Extractor Estimating: 158it [02:02,  1.51it/s]Extractor Estimating: 159it [02:03,  1.49it/s]Extractor Estimating: 160it [02:04,  1.49it/s]Extractor Estimating: 161it [02:04,  1.51it/s]Extractor Estimating: 162it [02:05,  1.55it/s]Extractor Estimating: 163it [02:05,  1.53it/s]Extractor Estimating: 164it [02:06,  1.54it/s]Extractor Estimating: 165it [02:07,  1.57it/s]Extractor Estimating: 166it [02:07,  1.56it/s]Extractor Estimating: 167it [02:08,  1.56it/s]Extractor Estimating: 168it [02:09,  1.43it/s]Extractor Estimating: 169it [02:10,  1.44it/s]Extractor Estimating: 170it [02:10,  1.48it/s]Extractor Estimating: 171it [02:11,  1.50it/s]Extractor Estimating: 172it [02:11,  1.54it/s]Extractor Estimating: 173it [02:12,  1.55it/s]Extractor Estimating: 174it [02:13,  1.55it/s]Extractor Estimating: 175it [02:13,  1.53it/s]Extractor Estimating: 176it [02:14,  1.58it/s]Extractor Estimating: 177it [02:15,  1.53it/s]Extractor Estimating: 178it [02:15,  1.57it/s]Extractor Estimating: 179it [02:16,  1.57it/s]Extractor Estimating: 180it [02:17,  1.57it/s]Extractor Estimating: 181it [02:17,  1.58it/s]Extractor Estimating: 182it [02:18,  1.61it/s]Extractor Estimating: 183it [02:18,  1.62it/s]Extractor Estimating: 184it [02:19,  1.60it/s]Extractor Estimating: 185it [02:20,  1.59it/s]Extractor Estimating: 186it [02:20,  1.52it/s]Extractor Estimating: 187it [02:21,  1.58it/s]Extractor Estimating: 188it [02:22,  1.61it/s]Extractor Estimating: 189it [02:22,  1.57it/s]Extractor Estimating: 190it [02:23,  1.56it/s]Extractor Estimating: 191it [02:23,  1.58it/s]Extractor Estimating: 192it [02:24,  1.55it/s]Extractor Estimating: 193it [02:25,  1.62it/s]Extractor Estimating: 194it [02:25,  1.60it/s]Extractor Estimating: 195it [02:26,  1.59it/s]Extractor Estimating: 196it [02:27,  1.59it/s]Extractor Estimating: 197it [02:27,  1.63it/s]Extractor Estimating: 198it [02:28,  1.62it/s]Extractor Estimating: 199it [02:28,  1.62it/s]Extractor Estimating: 200it [02:29,  1.63it/s]Extractor Estimating: 201it [02:30,  1.61it/s]Extractor Estimating: 202it [02:30,  1.66it/s]Extractor Estimating: 203it [02:31,  1.65it/s]Extractor Estimating: 204it [02:31,  1.68it/s]Extractor Estimating: 205it [02:32,  1.70it/s]Extractor Estimating: 206it [02:33,  1.69it/s]Extractor Estimating: 207it [02:33,  1.67it/s]Extractor Estimating: 208it [02:34,  1.72it/s]Extractor Estimating: 209it [02:34,  1.68it/s]Extractor Estimating: 210it [02:35,  1.63it/s]Extractor Estimating: 211it [02:36,  1.59it/s]Extractor Estimating: 212it [02:36,  1.64it/s]Extractor Estimating: 213it [02:37,  1.60it/s]Extractor Estimating: 214it [02:37,  1.67it/s]Extractor Estimating: 215it [02:38,  1.70it/s]Extractor Estimating: 216it [02:39,  1.68it/s]Extractor Estimating: 217it [02:39,  1.73it/s]Extractor Estimating: 218it [02:40,  1.69it/s]Extractor Estimating: 219it [02:40,  1.69it/s]Extractor Estimating: 220it [02:41,  1.65it/s]Extractor Estimating: 221it [02:42,  1.69it/s]Extractor Estimating: 222it [02:42,  1.70it/s]Extractor Estimating: 223it [02:43,  1.68it/s]Extractor Estimating: 224it [02:43,  1.68it/s]Extractor Estimating: 225it [02:44,  1.73it/s]Extractor Estimating: 226it [02:44,  1.75it/s]Extractor Estimating: 227it [02:45,  1.76it/s]Extractor Estimating: 228it [02:46,  1.78it/s]Extractor Estimating: 229it [02:46,  1.82it/s]Extractor Estimating: 230it [02:47,  1.83it/s]Extractor Estimating: 231it [02:47,  1.81it/s]Extractor Estimating: 232it [02:48,  1.74it/s]Extractor Estimating: 233it [02:49,  1.59it/s]Extractor Estimating: 234it [02:49,  1.68it/s]Extractor Estimating: 235it [02:50,  1.68it/s]Extractor Estimating: 236it [02:50,  1.69it/s]Extractor Estimating: 237it [02:51,  1.71it/s]Extractor Estimating: 238it [02:51,  1.71it/s]Extractor Estimating: 239it [02:52,  1.68it/s]Extractor Estimating: 240it [02:53,  1.71it/s]Extractor Estimating: 241it [02:53,  1.76it/s]Extractor Estimating: 242it [02:54,  1.74it/s]Extractor Estimating: 243it [02:54,  1.74it/s]Extractor Estimating: 244it [02:55,  1.70it/s]Extractor Estimating: 245it [02:55,  1.72it/s]Extractor Estimating: 246it [02:56,  1.73it/s]Extractor Estimating: 247it [02:57,  1.67it/s]Extractor Estimating: 248it [02:57,  1.68it/s]Extractor Estimating: 249it [02:58,  1.72it/s]Extractor Estimating: 250it [02:58,  1.73it/s]Extractor Estimating: 251it [02:59,  1.69it/s]Extractor Estimating: 252it [03:00,  1.67it/s]Extractor Estimating: 253it [03:00,  1.64it/s]Extractor Estimating: 254it [03:01,  1.64it/s]Extractor Estimating: 255it [03:02,  1.60it/s]Extractor Estimating: 256it [03:02,  1.60it/s]Extractor Estimating: 257it [03:03,  1.57it/s]Extractor Estimating: 258it [03:03,  1.58it/s]Extractor Estimating: 259it [03:04,  1.61it/s]Extractor Estimating: 260it [03:05,  1.60it/s]Extractor Estimating: 261it [03:05,  1.59it/s]Extractor Estimating: 262it [03:06,  1.60it/s]Extractor Estimating: 263it [03:07,  1.60it/s]Extractor Estimating: 264it [03:07,  1.62it/s]Extractor Estimating: 265it [03:08,  1.64it/s]Extractor Estimating: 266it [03:08,  1.64it/s]Extractor Estimating: 267it [03:09,  1.63it/s]Extractor Estimating: 268it [03:10,  1.59it/s]Extractor Estimating: 269it [03:10,  1.62it/s]Extractor Estimating: 270it [03:11,  1.66it/s]Extractor Estimating: 271it [03:11,  1.67it/s]Extractor Estimating: 272it [03:12,  1.67it/s]Extractor Estimating: 273it [03:13,  1.62it/s]Extractor Estimating: 274it [03:13,  1.61it/s]Extractor Estimating: 275it [03:14,  1.62it/s]Extractor Estimating: 276it [03:15,  1.58it/s]Extractor Estimating: 277it [03:15,  1.51it/s]Extractor Estimating: 278it [03:16,  1.50it/s]Extractor Estimating: 279it [03:17,  1.49it/s]Extractor Estimating: 280it [03:17,  1.54it/s]Extractor Estimating: 281it [03:18,  1.51it/s]Extractor Estimating: 282it [03:19,  1.52it/s]Extractor Estimating: 283it [03:19,  1.51it/s]Extractor Estimating: 284it [03:20,  1.46it/s]Extractor Estimating: 285it [03:21,  1.52it/s]Extractor Estimating: 286it [03:21,  1.55it/s]Extractor Estimating: 287it [03:22,  1.50it/s]Extractor Estimating: 288it [03:23,  1.52it/s]Extractor Estimating: 289it [03:23,  1.49it/s]Extractor Estimating: 290it [03:24,  1.50it/s]Extractor Estimating: 291it [03:25,  1.52it/s]Extractor Estimating: 292it [03:25,  1.51it/s]Extractor Estimating: 293it [03:26,  1.56it/s]Extractor Estimating: 294it [03:26,  1.53it/s]Extractor Estimating: 295it [03:27,  1.51it/s]Extractor Estimating: 296it [03:28,  1.53it/s]Extractor Estimating: 297it [03:28,  1.52it/s]Extractor Estimating: 298it [03:29,  1.51it/s]Extractor Estimating: 299it [03:30,  1.50it/s]Extractor Estimating: 300it [03:30,  1.52it/s]Extractor Estimating: 301it [03:31,  1.51it/s]Extractor Estimating: 302it [03:32,  1.48it/s]Extractor Estimating: 303it [03:33,  1.45it/s]Extractor Estimating: 304it [03:33,  1.48it/s]Extractor Estimating: 305it [03:34,  1.55it/s]Extractor Estimating: 306it [03:34,  1.59it/s]Extractor Estimating: 307it [03:35,  1.58it/s]Extractor Estimating: 308it [03:36,  1.57it/s]Extractor Estimating: 309it [03:36,  1.59it/s]Extractor Estimating: 310it [03:37,  1.60it/s]Extractor Estimating: 311it [03:38,  1.52it/s]Extractor Estimating: 312it [03:38,  1.39it/s]Extractor Estimating: 313it [03:39,  1.44it/s]Extractor Estimating: 314it [03:40,  1.50it/s]Extractor Estimating: 315it [03:40,  1.55it/s]Extractor Estimating: 316it [03:41,  1.48it/s]Extractor Estimating: 317it [03:42,  1.48it/s]Extractor Estimating: 318it [03:42,  1.51it/s]Extractor Estimating: 319it [03:43,  1.49it/s]Extractor Estimating: 320it [03:44,  1.52it/s]Extractor Estimating: 321it [03:44,  1.50it/s]Extractor Estimating: 322it [03:45,  1.44it/s]Extractor Estimating: 323it [03:46,  1.50it/s]Extractor Estimating: 324it [03:46,  1.53it/s]Extractor Estimating: 325it [03:47,  1.58it/s]Extractor Estimating: 326it [03:48,  1.61it/s]Extractor Estimating: 327it [03:48,  1.64it/s]Extractor Estimating: 328it [03:49,  1.69it/s]Extractor Estimating: 329it [03:49,  1.79it/s]Extractor Estimating: 330it [03:50,  1.77it/s]Extractor Estimating: 331it [03:50,  1.75it/s]Extractor Estimating: 332it [03:51,  1.78it/s]Extractor Estimating: 333it [03:51,  1.73it/s]Extractor Estimating: 334it [03:52,  1.75it/s]Extractor Estimating: 335it [03:53,  1.73it/s]Extractor Estimating: 336it [03:53,  1.74it/s]Extractor Estimating: 337it [03:54,  1.68it/s]Extractor Estimating: 338it [03:54,  1.64it/s]Extractor Estimating: 339it [03:55,  1.59it/s]Extractor Estimating: 340it [03:56,  1.56it/s]Extractor Estimating: 341it [03:56,  1.58it/s]Extractor Estimating: 342it [03:57,  1.62it/s]Extractor Estimating: 343it [03:58,  1.61it/s]Extractor Estimating: 344it [03:58,  1.63it/s]Extractor Estimating: 345it [03:59,  1.72it/s]Extractor Estimating: 346it [03:59,  1.70it/s]Extractor Estimating: 347it [04:00,  1.73it/s]Extractor Estimating: 348it [04:01,  1.71it/s]Extractor Estimating: 349it [04:01,  1.68it/s]Extractor Estimating: 350it [04:02,  1.56it/s]Extractor Estimating: 351it [04:02,  1.60it/s]Extractor Estimating: 352it [04:03,  1.62it/s]Extractor Estimating: 353it [04:04,  1.64it/s]Extractor Estimating: 354it [04:04,  1.63it/s]Extractor Estimating: 355it [04:05,  1.60it/s]Extractor Estimating: 356it [04:06,  1.57it/s]Extractor Estimating: 357it [04:06,  1.52it/s]Extractor Estimating: 358it [04:07,  1.53it/s]Extractor Estimating: 359it [04:08,  1.59it/s]Extractor Estimating: 360it [04:08,  1.55it/s]Extractor Estimating: 361it [04:09,  1.61it/s]Extractor Estimating: 362it [04:09,  1.61it/s]Extractor Estimating: 363it [04:10,  1.62it/s]Extractor Estimating: 364it [04:11,  1.67it/s]Extractor Estimating: 365it [04:11,  1.68it/s]Extractor Estimating: 366it [04:12,  1.66it/s]Extractor Estimating: 367it [04:12,  1.61it/s]Extractor Estimating: 368it [04:13,  1.58it/s]Extractor Estimating: 369it [04:14,  1.61it/s]Extractor Estimating: 370it [04:14,  1.64it/s]Extractor Estimating: 371it [04:15,  1.64it/s]Extractor Estimating: 372it [04:16,  1.58it/s]Extractor Estimating: 373it [04:16,  1.53it/s]Extractor Estimating: 374it [04:17,  1.52it/s]Extractor Estimating: 375it [04:18,  1.51it/s]Extractor Estimating: 375it [04:18,  1.45it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7616 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 27843
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27943, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_train_large/unseen_10_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27943, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.341, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.006, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.010, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 82, avg_time 1.016, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 182, avg_time 1.029, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 282, avg_time 2.163, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 64, avg_time 0.987, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 164, avg_time 1.008, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 264, avg_time 1.010, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 46, avg_time 1.024, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 146, avg_time 2.169, loss:nan
g_step 1200, step 246, avg_time 1.005, loss:nan
g_step 1300, step 28, avg_time 0.999, loss:nan
g_step 1400, step 128, avg_time 1.004, loss:nan
g_step 1500, step 228, avg_time 1.002, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 10, avg_time 2.176, loss:nan
g_step 1700, step 110, avg_time 0.999, loss:nan
g_step 1800, step 210, avg_time 1.019, loss:nan
g_step 1900, step 310, avg_time 1.007, loss:nan
g_step 2000, step 92, avg_time 0.990, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 192, avg_time 2.165, loss:nan
g_step 2200, step 292, avg_time 1.020, loss:nan
g_step 2300, step 74, avg_time 1.007, loss:nan
g_step 2400, step 174, avg_time 1.000, loss:nan
g_step 2500, step 274, avg_time 1.018, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 56, avg_time 2.163, loss:nan
g_step 2700, step 156, avg_time 0.999, loss:nan
g_step 2800, step 256, avg_time 1.006, loss:nan
g_step 2900, step 38, avg_time 1.012, loss:nan
g_step 3000, step 138, avg_time 1.006, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 238, avg_time 2.179, loss:nan
g_step 3200, step 20, avg_time 1.000, loss:nan
g_step 3300, step 120, avg_time 1.003, loss:nan
g_step 3400, step 220, avg_time 1.026, loss:nan
g_step 3500, step 2, avg_time 0.993, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 102, avg_time 2.174, loss:nan
g_step 3700, step 202, avg_time 1.003, loss:nan
g_step 3800, step 302, avg_time 1.008, loss:nan
g_step 3900, step 84, avg_time 0.998, loss:nan
g_step 4000, step 184, avg_time 1.037, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 284, avg_time 2.154, loss:nan
g_step 4200, step 66, avg_time 1.017, loss:nan
g_step 4300, step 166, avg_time 1.010, loss:nan
g_step 4400, step 266, avg_time 0.986, loss:nan
g_step 4500, step 48, avg_time 0.991, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 148, avg_time 2.172, loss:nan
g_step 4700, step 248, avg_time 1.009, loss:nan
g_step 4800, step 30, avg_time 0.998, loss:nan
g_step 4900, step 130, avg_time 1.003, loss:nan
g_step 5000, step 230, avg_time 1.018, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 12, avg_time 2.162, loss:nan
g_step 5200, step 112, avg_time 1.010, loss:nan
g_step 5300, step 212, avg_time 1.018, loss:nan
g_step 5400, step 312, avg_time 0.996, loss:nan
g_step 5500, step 94, avg_time 0.990, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 194, avg_time 2.161, loss:nan
g_step 5700, step 294, avg_time 1.011, loss:nan
g_step 5800, step 76, avg_time 1.015, loss:nan
g_step 5900, step 176, avg_time 0.993, loss:nan
g_step 6000, step 276, avg_time 1.008, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 58, avg_time 2.172, loss:nan
g_step 6200, step 158, avg_time 1.019, loss:nan
g_step 6300, step 258, avg_time 1.000, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 01:40:41 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 01:40:41 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_01-40-41_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 01:40:42 - WARNING - datasets.builder -   Using custom data configuration default-0d380d568b0d2a24
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-0d380d568b0d2a24/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 01:40:42,615 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:40:42,617 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:40:42,617 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:40:42,618 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:40:42,624 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:40:42,629 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:40:42,629 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:40:42,629 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:40:42,629 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:40:42,629 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:40:42,629 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 01:40:42,742 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:40:45,810 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 01:40:45,812 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-0d380d568b0d2a24/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 01:40:45 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1530c0500290> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:05,  1.36ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.34ba/s] 38%|███▊      | 3/8 [00:01<00:01,  3.01ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.48ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.82ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.09ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  4.26ba/s]100%|██████████| 8/8 [00:02<00:00,  4.71ba/s]100%|██████████| 8/8 [00:02<00:00,  3.65ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.15ba/s] 40%|████      | 2/5 [00:00<00:00,  3.30ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.76ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.98ba/s]100%|██████████| 5/5 [00:01<00:00,  4.47ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.07ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.46ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.78ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.89ba/s]100%|██████████| 8/8 [00:00<00:00, 11.13ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.45ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.47ba/s]100%|██████████| 5/5 [00:00<00:00, 12.94ba/s]100%|██████████| 5/5 [00:00<00:00, 12.20ba/s]
[INFO|trainer.py:414] 2023-08-28 01:40:50,714 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 01:40:50,727 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 01:40:50,727 >>   Num examples = 7740
[INFO|trainer.py:1149] 2023-08-28 01:40:50,727 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 01:40:50,727 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 01:40:50,727 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 01:40:50,727 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 01:40:50,727 >>   Total optimization steps = 605
  0%|          | 0/605 [00:00<?, ?it/s]  0%|          | 1/605 [00:00<03:34,  2.82it/s]  0%|          | 2/605 [00:00<03:16,  3.07it/s]  0%|          | 3/605 [00:00<03:04,  3.26it/s]  1%|          | 4/605 [00:01<03:02,  3.28it/s]  1%|          | 5/605 [00:01<02:57,  3.39it/s]  1%|          | 6/605 [00:01<02:52,  3.47it/s]  1%|          | 7/605 [00:02<02:50,  3.50it/s]  1%|▏         | 8/605 [00:02<02:49,  3.52it/s]  1%|▏         | 9/605 [00:02<02:48,  3.54it/s]  2%|▏         | 10/605 [00:02<02:46,  3.57it/s]  2%|▏         | 11/605 [00:03<02:46,  3.57it/s]  2%|▏         | 12/605 [00:03<02:45,  3.59it/s]  2%|▏         | 13/605 [00:03<02:44,  3.61it/s]  2%|▏         | 14/605 [00:04<02:43,  3.61it/s]  2%|▏         | 15/605 [00:04<02:43,  3.62it/s]  3%|▎         | 16/605 [00:04<02:43,  3.61it/s]  3%|▎         | 17/605 [00:04<02:42,  3.61it/s]  3%|▎         | 18/605 [00:05<02:42,  3.61it/s]  3%|▎         | 19/605 [00:05<02:41,  3.62it/s]  3%|▎         | 20/605 [00:05<02:41,  3.62it/s]  3%|▎         | 21/605 [00:05<02:41,  3.62it/s]  4%|▎         | 22/605 [00:06<02:41,  3.62it/s]  4%|▍         | 23/605 [00:06<02:41,  3.60it/s]  4%|▍         | 24/605 [00:06<02:41,  3.61it/s]  4%|▍         | 25/605 [00:07<02:40,  3.61it/s]  4%|▍         | 26/605 [00:07<02:39,  3.62it/s]  4%|▍         | 27/605 [00:07<02:44,  3.50it/s]  5%|▍         | 28/605 [00:07<02:43,  3.53it/s]  5%|▍         | 29/605 [00:08<02:41,  3.56it/s]  5%|▍         | 30/605 [00:08<02:40,  3.58it/s]  5%|▌         | 31/605 [00:08<02:40,  3.59it/s]  5%|▌         | 32/605 [00:09<02:39,  3.58it/s]  5%|▌         | 33/605 [00:09<02:39,  3.59it/s]  6%|▌         | 34/605 [00:09<02:38,  3.61it/s]  6%|▌         | 35/605 [00:09<02:37,  3.61it/s]  6%|▌         | 36/605 [00:10<02:37,  3.62it/s]  6%|▌         | 37/605 [00:10<02:37,  3.61it/s]  6%|▋         | 38/605 [00:10<02:37,  3.60it/s]  6%|▋         | 39/605 [00:10<02:37,  3.60it/s]  7%|▋         | 40/605 [00:11<02:36,  3.61it/s]  7%|▋         | 41/605 [00:11<02:35,  3.62it/s]  7%|▋         | 42/605 [00:11<02:35,  3.62it/s]  7%|▋         | 43/605 [00:12<02:35,  3.62it/s]  7%|▋         | 44/605 [00:12<02:35,  3.61it/s]  7%|▋         | 45/605 [00:12<02:35,  3.61it/s]  8%|▊         | 46/605 [00:12<02:34,  3.61it/s]  8%|▊         | 47/605 [00:13<02:34,  3.62it/s]  8%|▊         | 48/605 [00:13<02:33,  3.63it/s]  8%|▊         | 49/605 [00:13<02:34,  3.59it/s]  8%|▊         | 50/605 [00:14<02:34,  3.60it/s]  8%|▊         | 51/605 [00:14<02:33,  3.60it/s]  9%|▊         | 52/605 [00:14<02:33,  3.60it/s]  9%|▉         | 53/605 [00:14<02:33,  3.60it/s]  9%|▉         | 54/605 [00:15<02:32,  3.60it/s]  9%|▉         | 55/605 [00:15<02:32,  3.61it/s]  9%|▉         | 56/605 [00:15<02:31,  3.61it/s]  9%|▉         | 57/605 [00:15<02:31,  3.62it/s] 10%|▉         | 58/605 [00:16<02:31,  3.61it/s] 10%|▉         | 59/605 [00:16<02:31,  3.61it/s] 10%|▉         | 60/605 [00:16<02:33,  3.56it/s] 10%|█         | 61/605 [00:17<02:31,  3.58it/s] 10%|█         | 62/605 [00:17<02:31,  3.59it/s] 10%|█         | 63/605 [00:17<02:30,  3.59it/s] 11%|█         | 64/605 [00:17<02:30,  3.60it/s] 11%|█         | 65/605 [00:18<02:29,  3.61it/s] 11%|█         | 66/605 [00:18<02:28,  3.62it/s] 11%|█         | 67/605 [00:18<02:28,  3.62it/s] 11%|█         | 68/605 [00:19<02:28,  3.62it/s] 11%|█▏        | 69/605 [00:19<02:28,  3.62it/s] 12%|█▏        | 70/605 [00:19<02:27,  3.63it/s] 12%|█▏        | 71/605 [00:19<02:27,  3.61it/s] 12%|█▏        | 72/605 [00:20<02:27,  3.61it/s] 12%|█▏        | 73/605 [00:20<02:27,  3.61it/s] 12%|█▏        | 74/605 [00:20<02:27,  3.61it/s] 12%|█▏        | 75/605 [00:20<02:27,  3.60it/s] 13%|█▎        | 76/605 [00:21<02:26,  3.60it/s] 13%|█▎        | 77/605 [00:21<02:26,  3.61it/s] 13%|█▎        | 78/605 [00:21<02:25,  3.62it/s] 13%|█▎        | 79/605 [00:22<02:25,  3.62it/s] 13%|█▎        | 80/605 [00:22<02:25,  3.62it/s] 13%|█▎        | 81/605 [00:22<02:24,  3.62it/s] 14%|█▎        | 82/605 [00:22<02:25,  3.60it/s] 14%|█▎        | 83/605 [00:23<02:24,  3.61it/s] 14%|█▍        | 84/605 [00:23<02:24,  3.61it/s] 14%|█▍        | 85/605 [00:23<02:24,  3.61it/s] 14%|█▍        | 86/605 [00:23<02:23,  3.61it/s] 14%|█▍        | 87/605 [00:24<02:23,  3.61it/s] 15%|█▍        | 88/605 [00:24<02:23,  3.61it/s] 15%|█▍        | 89/605 [00:24<02:23,  3.60it/s] 15%|█▍        | 90/605 [00:25<02:22,  3.61it/s] 15%|█▌        | 91/605 [00:25<02:22,  3.61it/s] 15%|█▌        | 92/605 [00:25<02:21,  3.62it/s] 15%|█▌        | 93/605 [00:25<02:21,  3.62it/s] 16%|█▌        | 94/605 [00:26<02:21,  3.61it/s] 16%|█▌        | 95/605 [00:26<02:21,  3.60it/s] 16%|█▌        | 96/605 [00:26<02:21,  3.60it/s] 16%|█▌        | 97/605 [00:27<02:20,  3.61it/s] 16%|█▌        | 98/605 [00:27<02:20,  3.61it/s] 16%|█▋        | 99/605 [00:27<02:19,  3.62it/s] 17%|█▋        | 100/605 [00:27<02:19,  3.62it/s] 17%|█▋        | 101/605 [00:28<02:19,  3.61it/s] 17%|█▋        | 102/605 [00:28<02:19,  3.60it/s] 17%|█▋        | 103/605 [00:28<02:20,  3.58it/s] 17%|█▋        | 104/605 [00:28<02:19,  3.59it/s] 17%|█▋        | 105/605 [00:29<02:18,  3.60it/s] 18%|█▊        | 106/605 [00:29<02:18,  3.61it/s] 18%|█▊        | 107/605 [00:29<02:17,  3.61it/s] 18%|█▊        | 108/605 [00:30<02:17,  3.61it/s] 18%|█▊        | 109/605 [00:30<02:17,  3.62it/s] 18%|█▊        | 110/605 [00:30<02:17,  3.61it/s] 18%|█▊        | 111/605 [00:30<02:16,  3.61it/s] 19%|█▊        | 112/605 [00:31<02:16,  3.61it/s] 19%|█▊        | 113/605 [00:31<02:15,  3.62it/s] 19%|█▉        | 114/605 [00:31<02:16,  3.61it/s] 19%|█▉        | 115/605 [00:32<02:15,  3.61it/s] 19%|█▉        | 116/605 [00:32<02:15,  3.61it/s] 19%|█▉        | 117/605 [00:32<02:15,  3.61it/s] 20%|█▉        | 118/605 [00:32<02:14,  3.62it/s] 20%|█▉        | 119/605 [00:33<02:14,  3.62it/s] 20%|█▉        | 120/605 [00:33<02:13,  3.62it/s] 20%|██        | 121/605 [00:33<02:11,  3.67it/s][INFO|trainer.py:2140] 2023-08-28 01:41:24,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:41:24,403 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 01:41:24,403 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.35it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.58it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.64it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.54it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.96it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.52it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.30it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.15it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.27it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.41it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.56it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.47it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.35it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.21it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.02it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.97it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.04it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.16it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.33it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.45it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.40it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.32it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.16it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.98it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.93it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.08it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.20it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.29it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.44it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.41it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.30it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.13it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.02it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.98it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.02it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.24it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.32it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.33it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.31it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.24it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.13it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.07it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.99it/s][A
 41%|████      | 222/543 [00:04<00:07, 44.15it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.25it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.32it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.27it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.28it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.24it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.10it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.12it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.09it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.24it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.27it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.25it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.31it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.23it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.18it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.16it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.14it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.08it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.21it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.25it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.19it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.17it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.24it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.18it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.19it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.15it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.14it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.19it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.24it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.23it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.23it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.20it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.23it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.15it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.17it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.23it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.19it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.19it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.20it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.22it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.19it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.19it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.16it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.18it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.10it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.16it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.16it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.16it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.15it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.21it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.15it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.17it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.04it/s][A
 90%|████████▉ | 487/543 [00:10<00:01, 44.14it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.20it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.16it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.27it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.23it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.10it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.95it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.97it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.91it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.99it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.99it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.18it/s][A                                                 
                                                 [A 20%|██        | 121/605 [00:45<02:11,  3.67it/s]
100%|██████████| 543/543 [00:12<00:00, 44.18it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:41:36,860 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121
[INFO|configuration_utils.py:351] 2023-08-28 01:41:37,025 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:41:38,946 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:41:38,971 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:41:38,980 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121/special_tokens_map.json
 20%|██        | 122/605 [00:48<38:02,  4.73s/it] 20%|██        | 123/605 [00:49<27:14,  3.39s/it] 20%|██        | 124/605 [00:49<19:42,  2.46s/it] 21%|██        | 125/605 [00:49<14:26,  1.80s/it] 21%|██        | 126/605 [00:49<10:45,  1.35s/it] 21%|██        | 127/605 [00:50<08:10,  1.03s/it] 21%|██        | 128/605 [00:50<06:22,  1.25it/s] 21%|██▏       | 129/605 [00:50<05:07,  1.55it/s] 21%|██▏       | 130/605 [00:51<04:14,  1.86it/s] 22%|██▏       | 131/605 [00:51<03:37,  2.18it/s] 22%|██▏       | 132/605 [00:51<03:12,  2.46it/s] 22%|██▏       | 133/605 [00:51<02:53,  2.71it/s] 22%|██▏       | 134/605 [00:52<02:40,  2.93it/s] 22%|██▏       | 135/605 [00:52<02:31,  3.09it/s] 22%|██▏       | 136/605 [00:52<02:25,  3.22it/s] 23%|██▎       | 137/605 [00:52<02:20,  3.32it/s] 23%|██▎       | 138/605 [00:53<02:17,  3.39it/s] 23%|██▎       | 139/605 [00:53<02:15,  3.45it/s] 23%|██▎       | 140/605 [00:53<02:13,  3.48it/s] 23%|██▎       | 141/605 [00:54<02:12,  3.51it/s] 23%|██▎       | 142/605 [00:54<02:10,  3.54it/s] 24%|██▎       | 143/605 [00:54<02:10,  3.55it/s] 24%|██▍       | 144/605 [00:54<02:09,  3.57it/s] 24%|██▍       | 145/605 [00:55<02:08,  3.58it/s] 24%|██▍       | 146/605 [00:55<02:07,  3.59it/s] 24%|██▍       | 147/605 [00:55<02:07,  3.60it/s] 24%|██▍       | 148/605 [00:56<02:06,  3.61it/s] 25%|██▍       | 149/605 [00:56<02:06,  3.61it/s] 25%|██▍       | 150/605 [00:56<02:05,  3.61it/s] 25%|██▍       | 151/605 [00:56<02:05,  3.61it/s] 25%|██▌       | 152/605 [00:57<02:05,  3.62it/s] 25%|██▌       | 153/605 [00:57<02:04,  3.62it/s] 25%|██▌       | 154/605 [00:57<02:04,  3.62it/s] 26%|██▌       | 155/605 [00:57<02:04,  3.62it/s] 26%|██▌       | 156/605 [00:58<02:04,  3.62it/s] 26%|██▌       | 157/605 [00:58<02:03,  3.62it/s] 26%|██▌       | 158/605 [00:58<02:03,  3.62it/s] 26%|██▋       | 159/605 [00:59<02:03,  3.62it/s] 26%|██▋       | 160/605 [00:59<02:05,  3.55it/s] 27%|██▋       | 161/605 [00:59<02:04,  3.57it/s] 27%|██▋       | 162/605 [00:59<02:03,  3.58it/s] 27%|██▋       | 163/605 [01:00<02:02,  3.60it/s] 27%|██▋       | 164/605 [01:00<02:02,  3.61it/s] 27%|██▋       | 165/605 [01:00<02:01,  3.61it/s] 27%|██▋       | 166/605 [01:01<02:01,  3.61it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|██▊       | 167/605 [01:01<02:03,  3.56it/s] 28%|██▊       | 168/605 [01:01<02:02,  3.57it/s] 28%|██▊       | 169/605 [01:01<02:01,  3.59it/s] 28%|██▊       | 170/605 [01:02<02:01,  3.59it/s] 28%|██▊       | 171/605 [01:02<02:01,  3.56it/s] 28%|██▊       | 172/605 [01:02<02:00,  3.58it/s] 29%|██▊       | 173/605 [01:02<02:00,  3.59it/s] 29%|██▉       | 174/605 [01:03<01:59,  3.60it/s] 29%|██▉       | 175/605 [01:03<01:59,  3.61it/s] 29%|██▉       | 176/605 [01:03<01:58,  3.61it/s] 29%|██▉       | 177/605 [01:04<01:58,  3.61it/s] 29%|██▉       | 178/605 [01:04<01:58,  3.61it/s] 30%|██▉       | 179/605 [01:04<01:57,  3.61it/s] 30%|██▉       | 180/605 [01:04<01:57,  3.61it/s] 30%|██▉       | 181/605 [01:05<01:57,  3.61it/s] 30%|███       | 182/605 [01:05<01:57,  3.60it/s] 30%|███       | 183/605 [01:05<01:57,  3.60it/s] 30%|███       | 184/605 [01:06<01:56,  3.61it/s] 31%|███       | 185/605 [01:06<01:56,  3.61it/s] 31%|███       | 186/605 [01:06<01:56,  3.60it/s] 31%|███       | 187/605 [01:06<01:56,  3.60it/s] 31%|███       | 188/605 [01:07<01:55,  3.61it/s] 31%|███       | 189/605 [01:07<01:55,  3.61it/s] 31%|███▏      | 190/605 [01:07<01:57,  3.53it/s] 32%|███▏      | 191/605 [01:08<01:56,  3.54it/s] 32%|███▏      | 192/605 [01:08<01:55,  3.57it/s] 32%|███▏      | 193/605 [01:08<01:55,  3.57it/s] 32%|███▏      | 194/605 [01:08<01:54,  3.58it/s] 32%|███▏      | 195/605 [01:09<01:54,  3.59it/s] 32%|███▏      | 196/605 [01:09<01:53,  3.60it/s] 33%|███▎      | 197/605 [01:09<01:53,  3.61it/s] 33%|███▎      | 198/605 [01:09<01:52,  3.61it/s] 33%|███▎      | 199/605 [01:10<01:52,  3.61it/s] 33%|███▎      | 200/605 [01:10<01:52,  3.61it/s] 33%|███▎      | 201/605 [01:10<01:51,  3.61it/s] 33%|███▎      | 202/605 [01:11<01:51,  3.61it/s] 34%|███▎      | 203/605 [01:11<01:51,  3.62it/s] 34%|███▎      | 204/605 [01:11<01:51,  3.60it/s] 34%|███▍      | 205/605 [01:11<01:50,  3.61it/s] 34%|███▍      | 206/605 [01:12<01:50,  3.61it/s] 34%|███▍      | 207/605 [01:12<01:50,  3.61it/s] 34%|███▍      | 208/605 [01:12<01:50,  3.61it/s] 35%|███▍      | 209/605 [01:12<01:49,  3.60it/s] 35%|███▍      | 210/605 [01:13<01:49,  3.60it/s] 35%|███▍      | 211/605 [01:13<01:49,  3.60it/s] 35%|███▌      | 212/605 [01:13<01:49,  3.60it/s] 35%|███▌      | 213/605 [01:14<01:48,  3.61it/s] 35%|███▌      | 214/605 [01:14<01:48,  3.61it/s] 36%|███▌      | 215/605 [01:14<01:48,  3.60it/s] 36%|███▌      | 216/605 [01:14<01:48,  3.60it/s] 36%|███▌      | 217/605 [01:15<01:47,  3.60it/s] 36%|███▌      | 218/605 [01:15<01:47,  3.60it/s] 36%|███▌      | 219/605 [01:15<01:47,  3.59it/s] 36%|███▋      | 220/605 [01:16<01:47,  3.59it/s] 37%|███▋      | 221/605 [01:16<01:46,  3.59it/s] 37%|███▋      | 222/605 [01:16<01:46,  3.60it/s] 37%|███▋      | 223/605 [01:16<01:45,  3.61it/s] 37%|███▋      | 224/605 [01:17<01:45,  3.61it/s] 37%|███▋      | 225/605 [01:17<01:45,  3.61it/s] 37%|███▋      | 226/605 [01:17<01:45,  3.60it/s] 38%|███▊      | 227/605 [01:17<01:45,  3.60it/s] 38%|███▊      | 228/605 [01:18<01:44,  3.59it/s] 38%|███▊      | 229/605 [01:18<01:44,  3.59it/s] 38%|███▊      | 230/605 [01:18<01:44,  3.59it/s] 38%|███▊      | 231/605 [01:19<01:43,  3.60it/s] 38%|███▊      | 232/605 [01:19<01:43,  3.61it/s] 39%|███▊      | 233/605 [01:19<01:42,  3.61it/s] 39%|███▊      | 234/605 [01:19<01:42,  3.61it/s] 39%|███▉      | 235/605 [01:20<01:42,  3.61it/s] 39%|███▉      | 236/605 [01:20<01:42,  3.61it/s] 39%|███▉      | 237/605 [01:20<01:42,  3.60it/s] 39%|███▉      | 238/605 [01:21<01:41,  3.60it/s] 40%|███▉      | 239/605 [01:21<01:41,  3.60it/s] 40%|███▉      | 240/605 [01:21<01:41,  3.61it/s] 40%|███▉      | 241/605 [01:21<01:40,  3.61it/s] 40%|████      | 242/605 [01:22<01:39,  3.67it/s][INFO|trainer.py:2140] 2023-08-28 01:42:12,867 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:42:12,868 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 01:42:12,868 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.2851, 'eval_samples_per_second': 353.437, 'eval_steps_per_second': 44.2, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.87it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.15it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.31it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.29it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.81it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.41it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.27it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.25it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.32it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.40it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.31it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.24it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.17it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.06it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.98it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.95it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.01it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.11it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.27it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.30it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.17it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.11it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.06it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.98it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.83it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.01it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.12it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.29it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.25it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.16it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.07it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.07it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.88it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.93it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.04it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.16it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.25it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.22it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.20it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.14it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.03it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.92it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.92it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.02it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.16it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.19it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.19it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.15it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.18it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.05it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.90it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.92it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.04it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.16it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.18it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.14it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.14it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.19it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.06it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.97it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.87it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.19it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.19it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.17it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.19it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.06it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.06it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.00it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.87it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.09it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.16it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.15it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.13it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.18it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.02it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.98it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.91it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.94it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.09it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.12it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.18it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.17it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.07it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.07it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.06it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.05it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.94it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.09it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.17it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.12it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.18it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.14it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.10it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.10it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.06it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.00it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.13it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.15it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.17it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.13it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.08it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.04it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.10it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.07it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.01it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.16it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.20it/s][A                                                 
                                                 [A 40%|████      | 242/605 [01:34<01:39,  3.67it/s]
100%|██████████| 543/543 [00:12<00:00, 44.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:42:25,202 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242
[INFO|configuration_utils.py:351] 2023-08-28 01:42:25,227 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:42:26,766 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:42:26,783 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:42:26,791 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242/special_tokens_map.json
 40%|████      | 243/605 [01:37<28:19,  4.70s/it] 40%|████      | 244/605 [01:37<20:16,  3.37s/it] 40%|████      | 245/605 [01:37<14:39,  2.44s/it] 41%|████      | 246/605 [01:37<10:44,  1.79s/it] 41%|████      | 247/605 [01:38<07:59,  1.34s/it] 41%|████      | 248/605 [01:38<06:04,  1.02s/it] 41%|████      | 249/605 [01:38<04:44,  1.25it/s] 41%|████▏     | 250/605 [01:39<03:48,  1.55it/s] 41%|████▏     | 251/605 [01:39<03:09,  1.87it/s] 42%|████▏     | 252/605 [01:39<02:42,  2.18it/s] 42%|████▏     | 253/605 [01:39<02:22,  2.47it/s] 42%|████▏     | 254/605 [01:40<02:09,  2.72it/s] 42%|████▏     | 255/605 [01:40<01:59,  2.93it/s] 42%|████▏     | 256/605 [01:40<01:52,  3.09it/s] 42%|████▏     | 257/605 [01:41<01:47,  3.22it/s] 43%|████▎     | 258/605 [01:41<01:44,  3.32it/s] 43%|████▎     | 259/605 [01:41<01:42,  3.39it/s] 43%|████▎     | 260/605 [01:41<01:40,  3.44it/s] 43%|████▎     | 261/605 [01:42<01:38,  3.48it/s] 43%|████▎     | 262/605 [01:42<01:38,  3.49it/s] 43%|████▎     | 263/605 [01:42<01:37,  3.51it/s] 44%|████▎     | 264/605 [01:43<01:36,  3.53it/s] 44%|████▍     | 265/605 [01:43<01:36,  3.54it/s] 44%|████▍     | 266/605 [01:43<01:35,  3.54it/s] 44%|████▍     | 267/605 [01:43<01:35,  3.55it/s] 44%|████▍     | 268/605 [01:44<01:34,  3.55it/s] 44%|████▍     | 269/605 [01:44<01:34,  3.55it/s] 45%|████▍     | 270/605 [01:44<01:34,  3.56it/s] 45%|████▍     | 271/605 [01:45<01:33,  3.56it/s] 45%|████▍     | 272/605 [01:45<01:33,  3.56it/s] 45%|████▌     | 273/605 [01:45<01:33,  3.55it/s] 45%|████▌     | 274/605 [01:45<01:32,  3.56it/s] 45%|████▌     | 275/605 [01:46<01:32,  3.56it/s] 46%|████▌     | 276/605 [01:46<01:31,  3.58it/s] 46%|████▌     | 277/605 [01:46<01:31,  3.59it/s] 46%|████▌     | 278/605 [01:46<01:30,  3.60it/s] 46%|████▌     | 279/605 [01:47<01:30,  3.60it/s] 46%|████▋     | 280/605 [01:47<01:30,  3.60it/s] 46%|████▋     | 281/605 [01:47<01:29,  3.61it/s] 47%|████▋     | 282/605 [01:48<01:29,  3.61it/s] 47%|████▋     | 283/605 [01:48<01:29,  3.61it/s] 47%|████▋     | 284/605 [01:48<01:29,  3.60it/s] 47%|████▋     | 285/605 [01:48<01:28,  3.61it/s] 47%|████▋     | 286/605 [01:49<01:28,  3.61it/s] 47%|████▋     | 287/605 [01:49<01:28,  3.61it/s] 48%|████▊     | 288/605 [01:49<01:27,  3.61it/s] 48%|████▊     | 289/605 [01:50<01:27,  3.62it/s] 48%|████▊     | 290/605 [01:50<01:27,  3.61it/s] 48%|████▊     | 291/605 [01:50<01:26,  3.62it/s] 48%|████▊     | 292/605 [01:50<01:26,  3.62it/s] 48%|████▊     | 293/605 [01:51<01:26,  3.62it/s] 49%|████▊     | 294/605 [01:51<01:25,  3.62it/s] 49%|████▉     | 295/605 [01:51<01:25,  3.61it/s] 49%|████▉     | 296/605 [01:51<01:25,  3.61it/s] 49%|████▉     | 297/605 [01:52<01:25,  3.61it/s] 49%|████▉     | 298/605 [01:52<01:24,  3.61it/s] 49%|████▉     | 299/605 [01:52<01:24,  3.61it/s] 50%|████▉     | 300/605 [01:53<01:24,  3.62it/s] 50%|████▉     | 301/605 [01:53<01:24,  3.62it/s] 50%|████▉     | 302/605 [01:53<01:23,  3.61it/s] 50%|█████     | 303/605 [01:53<01:23,  3.61it/s] 50%|█████     | 304/605 [01:54<01:23,  3.61it/s] 50%|█████     | 305/605 [01:54<01:23,  3.61it/s] 51%|█████     | 306/605 [01:54<01:23,  3.60it/s] 51%|█████     | 307/605 [01:54<01:22,  3.60it/s] 51%|█████     | 308/605 [01:55<01:22,  3.61it/s] 51%|█████     | 309/605 [01:55<01:21,  3.61it/s] 51%|█████     | 310/605 [01:55<01:21,  3.61it/s] 51%|█████▏    | 311/605 [01:56<01:21,  3.61it/s] 52%|█████▏    | 312/605 [01:56<01:21,  3.62it/s] 52%|█████▏    | 313/605 [01:56<01:20,  3.62it/s] 52%|█████▏    | 314/605 [01:56<01:20,  3.62it/s] 52%|█████▏    | 315/605 [01:57<01:20,  3.62it/s] 52%|█████▏    | 316/605 [01:57<01:19,  3.62it/s] 52%|█████▏    | 317/605 [01:57<01:20,  3.59it/s] 53%|█████▎    | 318/605 [01:58<01:19,  3.60it/s] 53%|█████▎    | 319/605 [01:58<01:19,  3.60it/s] 53%|█████▎    | 320/605 [01:58<01:19,  3.61it/s] 53%|█████▎    | 321/605 [01:58<01:18,  3.61it/s] 53%|█████▎    | 322/605 [01:59<01:18,  3.61it/s] 53%|█████▎    | 323/605 [01:59<01:18,  3.61it/s] 54%|█████▎    | 324/605 [01:59<01:17,  3.61it/s] 54%|█████▎    | 325/605 [01:59<01:17,  3.61it/s] 54%|█████▍    | 326/605 [02:00<01:17,  3.62it/s] 54%|█████▍    | 327/605 [02:00<01:16,  3.62it/s] 54%|█████▍    | 328/605 [02:00<01:16,  3.61it/s] 54%|█████▍    | 329/605 [02:01<01:16,  3.61it/s] 55%|█████▍    | 330/605 [02:01<01:16,  3.60it/s] 55%|█████▍    | 331/605 [02:01<01:15,  3.61it/s] 55%|█████▍    | 332/605 [02:01<01:15,  3.61it/s] 55%|█████▌    | 333/605 [02:02<01:15,  3.61it/s] 55%|█████▌    | 334/605 [02:02<01:15,  3.61it/s] 55%|█████▌    | 335/605 [02:02<01:14,  3.61it/s] 56%|█████▌    | 336/605 [02:03<01:14,  3.61it/s] 56%|█████▌    | 337/605 [02:03<01:14,  3.62it/s] 56%|█████▌    | 338/605 [02:03<01:13,  3.61it/s] 56%|█████▌    | 339/605 [02:03<01:13,  3.60it/s] 56%|█████▌    | 340/605 [02:04<01:13,  3.60it/s] 56%|█████▋    | 341/605 [02:04<01:13,  3.60it/s] 57%|█████▋    | 342/605 [02:04<01:12,  3.61it/s] 57%|█████▋    | 343/605 [02:04<01:12,  3.61it/s] 57%|█████▋    | 344/605 [02:05<01:12,  3.61it/s] 57%|█████▋    | 345/605 [02:05<01:11,  3.61it/s] 57%|█████▋    | 346/605 [02:05<01:11,  3.61it/s] 57%|█████▋    | 347/605 [02:06<01:11,  3.61it/s] 58%|█████▊    | 348/605 [02:06<01:11,  3.61it/s] 58%|█████▊    | 349/605 [02:06<01:11,  3.60it/s] 58%|█████▊    | 350/605 [02:06<01:10,  3.59it/s] 58%|█████▊    | 351/605 [02:07<01:10,  3.60it/s] 58%|█████▊    | 352/605 [02:07<01:10,  3.61it/s] 58%|█████▊    | 353/605 [02:07<01:11,  3.53it/s] 59%|█████▊    | 354/605 [02:08<01:10,  3.54it/s] 59%|█████▊    | 355/605 [02:08<01:10,  3.56it/s] 59%|█████▉    | 356/605 [02:08<01:09,  3.58it/s] 59%|█████▉    | 357/605 [02:08<01:09,  3.58it/s] 59%|█████▉    | 358/605 [02:09<01:08,  3.59it/s] 59%|█████▉    | 359/605 [02:09<01:08,  3.60it/s] 60%|█████▉    | 360/605 [02:09<01:07,  3.61it/s] 60%|█████▉    | 361/605 [02:09<01:07,  3.60it/s] 60%|█████▉    | 362/605 [02:10<01:07,  3.60it/s] 60%|██████    | 363/605 [02:10<01:06,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 01:43:01,250 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:43:01,250 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 01:43:01,250 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3101, 'eval_samples_per_second': 352.719, 'eval_steps_per_second': 44.11, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.18it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.42it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.46it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.36it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.55it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.28it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.05it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.95it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.03it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.18it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.21it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.32it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.13it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.05it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.93it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.84it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.86it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.95it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.01it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.12it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.22it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.19it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.08it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.01it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.91it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.96it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.02it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.06it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.14it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.15it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.14it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.11it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.02it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.98it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.01it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.97it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.05it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.20it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.19it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.15it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.11it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.07it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.04it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.06it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.98it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.00it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.03it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.12it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.05it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.11it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.08it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.10it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.05it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.03it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.06it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.18it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.16it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.06it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.10it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.15it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.11it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.01it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.03it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.06it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.09it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.05it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.00it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.06it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.07it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.11it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.06it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.05it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.09it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.11it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.07it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.05it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.06it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.08it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.08it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.05it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.00it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.10it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.09it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.09it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.08it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.08it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.99it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.95it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.06it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.07it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.15it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.15it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.10it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.06it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.06it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.05it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.03it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.93it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.15it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.14it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.09it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.04it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.94it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.05it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.04it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.00it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.14it/s][A                                                 
                                                 [A 60%|██████    | 363/605 [02:22<01:06,  3.65it/s]
100%|██████████| 543/543 [00:12<00:00, 44.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:43:13,588 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363
[INFO|configuration_utils.py:351] 2023-08-28 01:43:13,610 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:43:16,095 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:43:16,108 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:43:16,123 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363/special_tokens_map.json
 60%|██████    | 364/605 [02:25<19:21,  4.82s/it] 60%|██████    | 365/605 [02:26<13:50,  3.46s/it] 60%|██████    | 366/605 [02:26<10:00,  2.51s/it] 61%|██████    | 367/605 [02:26<07:18,  1.84s/it] 61%|██████    | 368/605 [02:27<05:25,  1.37s/it] 61%|██████    | 369/605 [02:27<04:06,  1.05s/it] 61%|██████    | 370/605 [02:27<03:11,  1.23it/s] 61%|██████▏   | 371/605 [02:27<02:33,  1.53it/s] 61%|██████▏   | 372/605 [02:28<02:06,  1.84it/s] 62%|██████▏   | 373/605 [02:28<01:47,  2.15it/s] 62%|██████▏   | 374/605 [02:28<01:34,  2.44it/s] 62%|██████▏   | 375/605 [02:29<01:25,  2.70it/s] 62%|██████▏   | 376/605 [02:29<01:18,  2.91it/s] 62%|██████▏   | 377/605 [02:29<01:14,  3.07it/s] 62%|██████▏   | 378/605 [02:29<01:10,  3.20it/s] 63%|██████▎   | 379/605 [02:30<01:08,  3.30it/s] 63%|██████▎   | 380/605 [02:30<01:06,  3.37it/s] 63%|██████▎   | 381/605 [02:30<01:05,  3.43it/s] 63%|██████▎   | 382/605 [02:31<01:04,  3.46it/s] 63%|██████▎   | 383/605 [02:31<01:03,  3.49it/s] 63%|██████▎   | 384/605 [02:31<01:02,  3.52it/s] 64%|██████▎   | 385/605 [02:31<01:02,  3.53it/s] 64%|██████▍   | 386/605 [02:32<01:01,  3.54it/s] 64%|██████▍   | 387/605 [02:32<01:01,  3.55it/s] 64%|██████▍   | 388/605 [02:32<01:01,  3.54it/s] 64%|██████▍   | 389/605 [02:32<01:00,  3.55it/s] 64%|██████▍   | 390/605 [02:33<01:00,  3.56it/s] 65%|██████▍   | 391/605 [02:33<01:00,  3.56it/s] 65%|██████▍   | 392/605 [02:33<00:59,  3.56it/s] 65%|██████▍   | 393/605 [02:34<00:59,  3.56it/s] 65%|██████▌   | 394/605 [02:34<00:59,  3.57it/s] 65%|██████▌   | 395/605 [02:34<00:58,  3.57it/s] 65%|██████▌   | 396/605 [02:34<00:58,  3.57it/s] 66%|██████▌   | 397/605 [02:35<00:58,  3.57it/s] 66%|██████▌   | 398/605 [02:35<00:57,  3.57it/s] 66%|██████▌   | 399/605 [02:35<00:57,  3.56it/s] 66%|██████▌   | 400/605 [02:36<00:57,  3.57it/s] 66%|██████▋   | 401/605 [02:36<00:57,  3.57it/s] 66%|██████▋   | 402/605 [02:36<00:56,  3.57it/s] 67%|██████▋   | 403/605 [02:36<00:56,  3.57it/s] 67%|██████▋   | 404/605 [02:37<00:56,  3.57it/s] 67%|██████▋   | 405/605 [02:37<00:56,  3.57it/s] 67%|██████▋   | 406/605 [02:37<00:55,  3.57it/s] 67%|██████▋   | 407/605 [02:38<00:55,  3.57it/s] 67%|██████▋   | 408/605 [02:38<00:55,  3.57it/s] 68%|██████▊   | 409/605 [02:38<00:54,  3.57it/s] 68%|██████▊   | 410/605 [02:38<00:54,  3.55it/s] 68%|██████▊   | 411/605 [02:39<00:54,  3.57it/s] 68%|██████▊   | 412/605 [02:39<00:53,  3.58it/s] 68%|██████▊   | 413/605 [02:39<00:53,  3.59it/s] 68%|██████▊   | 414/605 [02:39<00:52,  3.61it/s] 69%|██████▊   | 415/605 [02:40<00:52,  3.61it/s] 69%|██████▉   | 416/605 [02:40<00:52,  3.60it/s] 69%|██████▉   | 417/605 [02:40<00:52,  3.60it/s] 69%|██████▉   | 418/605 [02:41<00:51,  3.61it/s] 69%|██████▉   | 419/605 [02:41<00:51,  3.61it/s] 69%|██████▉   | 420/605 [02:41<00:51,  3.61it/s] 70%|██████▉   | 421/605 [02:41<00:51,  3.60it/s] 70%|██████▉   | 422/605 [02:42<00:50,  3.61it/s] 70%|██████▉   | 423/605 [02:42<00:50,  3.61it/s] 70%|███████   | 424/605 [02:42<00:50,  3.60it/s] 70%|███████   | 425/605 [02:43<00:49,  3.61it/s] 70%|███████   | 426/605 [02:43<00:49,  3.62it/s] 71%|███████   | 427/605 [02:43<00:49,  3.61it/s] 71%|███████   | 428/605 [02:43<00:48,  3.62it/s] 71%|███████   | 429/605 [02:44<00:48,  3.61it/s] 71%|███████   | 430/605 [02:44<00:48,  3.62it/s] 71%|███████   | 431/605 [02:44<00:48,  3.62it/s] 71%|███████▏  | 432/605 [02:44<00:47,  3.61it/s] 72%|███████▏  | 433/605 [02:45<00:47,  3.61it/s] 72%|███████▏  | 434/605 [02:45<00:47,  3.61it/s] 72%|███████▏  | 435/605 [02:45<00:47,  3.61it/s] 72%|███████▏  | 436/605 [02:46<00:46,  3.61it/s] 72%|███████▏  | 437/605 [02:46<00:46,  3.61it/s] 72%|███████▏  | 438/605 [02:46<00:46,  3.61it/s] 73%|███████▎  | 439/605 [02:46<00:45,  3.61it/s] 73%|███████▎  | 440/605 [02:47<00:45,  3.61it/s] 73%|███████▎  | 441/605 [02:47<00:45,  3.61it/s] 73%|███████▎  | 442/605 [02:47<00:45,  3.61it/s] 73%|███████▎  | 443/605 [02:48<00:45,  3.59it/s] 73%|███████▎  | 444/605 [02:48<00:44,  3.60it/s] 74%|███████▎  | 445/605 [02:48<00:44,  3.60it/s] 74%|███████▎  | 446/605 [02:48<00:44,  3.61it/s] 74%|███████▍  | 447/605 [02:49<00:43,  3.61it/s] 74%|███████▍  | 448/605 [02:49<00:43,  3.62it/s] 74%|███████▍  | 449/605 [02:49<00:43,  3.62it/s] 74%|███████▍  | 450/605 [02:49<00:42,  3.62it/s] 75%|███████▍  | 451/605 [02:50<00:42,  3.62it/s] 75%|███████▍  | 452/605 [02:50<00:42,  3.62it/s] 75%|███████▍  | 453/605 [02:50<00:41,  3.62it/s] 75%|███████▌  | 454/605 [02:51<00:41,  3.61it/s] 75%|███████▌  | 455/605 [02:51<00:41,  3.61it/s] 75%|███████▌  | 456/605 [02:51<00:41,  3.61it/s] 76%|███████▌  | 457/605 [02:51<00:40,  3.62it/s] 76%|███████▌  | 458/605 [02:52<00:40,  3.62it/s] 76%|███████▌  | 459/605 [02:52<00:40,  3.62it/s] 76%|███████▌  | 460/605 [02:52<00:40,  3.61it/s] 76%|███████▌  | 461/605 [02:53<00:39,  3.61it/s] 76%|███████▋  | 462/605 [02:53<00:39,  3.61it/s] 77%|███████▋  | 463/605 [02:53<00:39,  3.61it/s] 77%|███████▋  | 464/605 [02:53<00:38,  3.62it/s] 77%|███████▋  | 465/605 [02:54<00:38,  3.60it/s] 77%|███████▋  | 466/605 [02:54<00:38,  3.61it/s] 77%|███████▋  | 467/605 [02:54<00:38,  3.61it/s] 77%|███████▋  | 468/605 [02:54<00:37,  3.62it/s] 78%|███████▊  | 469/605 [02:55<00:37,  3.62it/s] 78%|███████▊  | 470/605 [02:55<00:37,  3.62it/s] 78%|███████▊  | 471/605 [02:55<00:36,  3.62it/s] 78%|███████▊  | 472/605 [02:56<00:36,  3.62it/s] 78%|███████▊  | 473/605 [02:56<00:36,  3.62it/s] 78%|███████▊  | 474/605 [02:56<00:36,  3.61it/s] 79%|███████▊  | 475/605 [02:56<00:35,  3.61it/s] 79%|███████▊  | 476/605 [02:57<00:35,  3.60it/s] 79%|███████▉  | 477/605 [02:57<00:35,  3.60it/s] 79%|███████▉  | 478/605 [02:57<00:35,  3.61it/s] 79%|███████▉  | 479/605 [02:57<00:34,  3.61it/s] 79%|███████▉  | 480/605 [02:58<00:34,  3.61it/s] 80%|███████▉  | 481/605 [02:58<00:34,  3.61it/s] 80%|███████▉  | 482/605 [02:58<00:34,  3.61it/s] 80%|███████▉  | 483/605 [02:59<00:33,  3.61it/s] 80%|████████  | 484/605 [02:59<00:33,  3.66it/s][INFO|trainer.py:2140] 2023-08-28 01:43:50,087 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:43:50,087 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 01:43:50,087 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3201, 'eval_samples_per_second': 352.434, 'eval_steps_per_second': 44.074, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.15it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.32it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.21it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.20it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.78it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.39it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.18it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.08it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.18it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.32it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.40it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.28it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 43.95it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.01it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.89it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.87it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.97it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.06it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.25it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.15it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.09it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.93it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.03it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.01it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.91it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.04it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.19it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.17it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.07it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.06it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.89it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.01it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.02it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.08it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.18it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.22it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.11it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.06it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.98it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.02it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.00it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.93it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.03it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.15it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.25it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.15it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.07it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.02it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.03it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.01it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.93it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.07it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.14it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.18it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.10it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.03it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.02it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.02it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.94it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.97it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.05it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.11it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.08it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.07it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.95it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.03it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.00it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.92it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.02it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.02it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.99it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.11it/s][A
 69%|██████▊   | 372/543 [00:08<00:04, 42.29it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.08it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.34it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.57it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.72it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 43.83it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.91it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 43.99it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 43.87it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.92it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.94it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.00it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.03it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.09it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.09it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.92it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.01it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.04it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.05it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.02it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.02it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.06it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.12it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.99it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.96it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.05it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.04it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.09it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.07it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.09it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.04it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.15it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.08it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.94it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.93it/s][A                                                 
                                                 [A 80%|████████  | 484/605 [03:11<00:33,  3.66it/s]
100%|██████████| 543/543 [00:12<00:00, 43.93it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:44:02,443 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484
[INFO|configuration_utils.py:351] 2023-08-28 01:44:02,461 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:44:04,575 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:44:04,594 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:44:04,609 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484/special_tokens_map.json
 80%|████████  | 485/605 [03:14<09:25,  4.71s/it] 80%|████████  | 486/605 [03:14<06:42,  3.38s/it] 80%|████████  | 487/605 [03:14<04:49,  2.45s/it] 81%|████████  | 488/605 [03:15<03:30,  1.80s/it] 81%|████████  | 489/605 [03:15<02:35,  1.34s/it] 81%|████████  | 490/605 [03:15<01:57,  1.02s/it] 81%|████████  | 491/605 [03:16<01:31,  1.25it/s] 81%|████████▏ | 492/605 [03:16<01:12,  1.56it/s] 81%|████████▏ | 493/605 [03:16<00:59,  1.88it/s] 82%|████████▏ | 494/605 [03:16<00:50,  2.20it/s] 82%|████████▏ | 495/605 [03:17<00:44,  2.48it/s] 82%|████████▏ | 496/605 [03:17<00:39,  2.74it/s] 82%|████████▏ | 497/605 [03:17<00:36,  2.95it/s] 82%|████████▏ | 498/605 [03:18<00:34,  3.12it/s] 82%|████████▏ | 499/605 [03:18<00:32,  3.26it/s] 83%|████████▎ | 500/605 [03:18<00:31,  3.36it/s]                                                  83%|████████▎ | 500/605 [03:18<00:31,  3.36it/s] 83%|████████▎ | 501/605 [03:18<00:30,  3.43it/s] 83%|████████▎ | 502/605 [03:19<00:29,  3.48it/s] 83%|████████▎ | 503/605 [03:19<00:28,  3.52it/s] 83%|████████▎ | 504/605 [03:19<00:28,  3.55it/s] 83%|████████▎ | 505/605 [03:19<00:28,  3.57it/s] 84%|████████▎ | 506/605 [03:20<00:27,  3.56it/s] 84%|████████▍ | 507/605 [03:20<00:27,  3.58it/s] 84%|████████▍ | 508/605 [03:20<00:27,  3.59it/s] 84%|████████▍ | 509/605 [03:21<00:26,  3.60it/s] 84%|████████▍ | 510/605 [03:21<00:26,  3.60it/s] 84%|████████▍ | 511/605 [03:21<00:26,  3.61it/s] 85%|████████▍ | 512/605 [03:21<00:25,  3.60it/s] 85%|████████▍ | 513/605 [03:22<00:25,  3.61it/s] 85%|████████▍ | 514/605 [03:22<00:25,  3.61it/s] 85%|████████▌ | 515/605 [03:22<00:24,  3.61it/s] 85%|████████▌ | 516/605 [03:23<00:24,  3.62it/s] 85%|████████▌ | 517/605 [03:23<00:24,  3.58it/s] 86%|████████▌ | 518/605 [03:23<00:24,  3.59it/s] 86%|████████▌ | 519/605 [03:23<00:23,  3.60it/s] 86%|████████▌ | 520/605 [03:24<00:23,  3.60it/s] 86%|████████▌ | 521/605 [03:24<00:23,  3.60it/s] 86%|████████▋ | 522/605 [03:24<00:22,  3.61it/s] 86%|████████▋ | 523/605 [03:24<00:22,  3.61it/s] 87%|████████▋ | 524/605 [03:25<00:22,  3.61it/s] 87%|████████▋ | 525/605 [03:25<00:22,  3.62it/s] 87%|████████▋ | 526/605 [03:25<00:21,  3.61it/s] 87%|████████▋ | 527/605 [03:26<00:21,  3.61it/s] 87%|████████▋ | 528/605 [03:26<00:21,  3.61it/s] 87%|████████▋ | 529/605 [03:26<00:21,  3.61it/s] 88%|████████▊ | 530/605 [03:26<00:20,  3.61it/s] 88%|████████▊ | 531/605 [03:27<00:20,  3.61it/s] 88%|████████▊ | 532/605 [03:27<00:20,  3.61it/s] 88%|████████▊ | 533/605 [03:27<00:19,  3.61it/s] 88%|████████▊ | 534/605 [03:28<00:19,  3.60it/s] 88%|████████▊ | 535/605 [03:28<00:19,  3.60it/s] 89%|████████▊ | 536/605 [03:28<00:19,  3.61it/s] 89%|████████▉ | 537/605 [03:28<00:18,  3.61it/s] 89%|████████▉ | 538/605 [03:29<00:18,  3.61it/s] 89%|████████▉ | 539/605 [03:29<00:18,  3.61it/s] 89%|████████▉ | 540/605 [03:29<00:17,  3.62it/s] 89%|████████▉ | 541/605 [03:29<00:17,  3.61it/s] 90%|████████▉ | 542/605 [03:30<00:17,  3.62it/s] 90%|████████▉ | 543/605 [03:30<00:17,  3.62it/s] 90%|████████▉ | 544/605 [03:30<00:16,  3.62it/s] 90%|█████████ | 545/605 [03:31<00:16,  3.60it/s] 90%|█████████ | 546/605 [03:31<00:16,  3.60it/s] 90%|█████████ | 547/605 [03:31<00:16,  3.60it/s] 91%|█████████ | 548/605 [03:31<00:15,  3.61it/s] 91%|█████████ | 549/605 [03:32<00:15,  3.61it/s] 91%|█████████ | 550/605 [03:32<00:15,  3.61it/s] 91%|█████████ | 551/605 [03:32<00:14,  3.61it/s] 91%|█████████ | 552/605 [03:32<00:14,  3.60it/s] 91%|█████████▏| 553/605 [03:33<00:14,  3.59it/s] 92%|█████████▏| 554/605 [03:33<00:14,  3.58it/s] 92%|█████████▏| 555/605 [03:33<00:13,  3.58it/s] 92%|█████████▏| 556/605 [03:34<00:13,  3.56it/s] 92%|█████████▏| 557/605 [03:34<00:13,  3.56it/s] 92%|█████████▏| 558/605 [03:34<00:13,  3.56it/s] 92%|█████████▏| 559/605 [03:34<00:12,  3.57it/s] 93%|█████████▎| 560/605 [03:35<00:12,  3.57it/s] 93%|█████████▎| 561/605 [03:35<00:12,  3.57it/s] 93%|█████████▎| 562/605 [03:35<00:12,  3.57it/s] 93%|█████████▎| 563/605 [03:36<00:11,  3.57it/s] 93%|█████████▎| 564/605 [03:36<00:11,  3.57it/s] 93%|█████████▎| 565/605 [03:36<00:11,  3.57it/s] 94%|█████████▎| 566/605 [03:36<00:10,  3.57it/s] 94%|█████████▎| 567/605 [03:37<00:10,  3.53it/s] 94%|█████████▍| 568/605 [03:37<00:10,  3.54it/s] 94%|█████████▍| 569/605 [03:37<00:10,  3.55it/s] 94%|█████████▍| 570/605 [03:38<00:09,  3.55it/s] 94%|█████████▍| 571/605 [03:38<00:09,  3.56it/s] 95%|█████████▍| 572/605 [03:38<00:09,  3.56it/s] 95%|█████████▍| 573/605 [03:38<00:08,  3.56it/s] 95%|█████████▍| 574/605 [03:39<00:08,  3.56it/s] 95%|█████████▌| 575/605 [03:39<00:08,  3.56it/s] 95%|█████████▌| 576/605 [03:39<00:08,  3.56it/s] 95%|█████████▌| 577/605 [03:40<00:07,  3.57it/s] 96%|█████████▌| 578/605 [03:40<00:07,  3.55it/s] 96%|█████████▌| 579/605 [03:40<00:07,  3.56it/s] 96%|█████████▌| 580/605 [03:40<00:07,  3.56it/s] 96%|█████████▌| 581/605 [03:41<00:06,  3.56it/s] 96%|█████████▌| 582/605 [03:41<00:06,  3.56it/s] 96%|█████████▋| 583/605 [03:41<00:06,  3.56it/s] 97%|█████████▋| 584/605 [03:41<00:05,  3.56it/s] 97%|█████████▋| 585/605 [03:42<00:05,  3.56it/s] 97%|█████████▋| 586/605 [03:42<00:05,  3.56it/s] 97%|█████████▋| 587/605 [03:42<00:05,  3.56it/s] 97%|█████████▋| 588/605 [03:43<00:04,  3.56it/s] 97%|█████████▋| 589/605 [03:43<00:04,  3.54it/s] 98%|█████████▊| 590/605 [03:43<00:04,  3.55it/s] 98%|█████████▊| 591/605 [03:43<00:03,  3.56it/s] 98%|█████████▊| 592/605 [03:44<00:03,  3.56it/s] 98%|█████████▊| 593/605 [03:44<00:03,  3.57it/s] 98%|█████████▊| 594/605 [03:44<00:03,  3.57it/s] 98%|█████████▊| 595/605 [03:45<00:02,  3.57it/s] 99%|█████████▊| 596/605 [03:45<00:02,  3.57it/s] 99%|█████████▊| 597/605 [03:45<00:02,  3.57it/s] 99%|█████████▉| 598/605 [03:45<00:01,  3.57it/s] 99%|█████████▉| 599/605 [03:46<00:01,  3.57it/s] 99%|█████████▉| 600/605 [03:46<00:01,  3.56it/s] 99%|█████████▉| 601/605 [03:46<00:01,  3.56it/s]100%|█████████▉| 602/605 [03:47<00:00,  3.56it/s]100%|█████████▉| 603/605 [03:47<00:00,  3.57it/s]100%|█████████▉| 604/605 [03:47<00:00,  3.56it/s]100%|██████████| 605/605 [03:47<00:00,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 01:44:38,599 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:44:38,599 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 01:44:38,599 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3369, 'eval_samples_per_second': 351.952, 'eval_steps_per_second': 44.014, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6797520661157023e-05, 'epoch': 4.13}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.85it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.10it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.39it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.39it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.94it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.34it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.14it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.95it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.10it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.29it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.36it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.25it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.17it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.00it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.90it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.84it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.84it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.02it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.15it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.21it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.25it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.07it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.00it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.85it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.87it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.03it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.11it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.09it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.21it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.14it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.16it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.91it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.91it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.88it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.01it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.14it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.10it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.19it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.20it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.14it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.95it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.82it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.90it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.08it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.11it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.24it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.22it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.08it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.02it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.92it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.93it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.91it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.04it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.11it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.25it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.28it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.09it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.98it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.85it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.96it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.05it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.14it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.12it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 44.20it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.18it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.10it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.89it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.87it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.92it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.99it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.12it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.21it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.18it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.16it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.01it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.90it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.87it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.97it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.98it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.14it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.26it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.14it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.12it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.04it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.95it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.85it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.91it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.95it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.00it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.21it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.20it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.09it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.09it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.04it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.00it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.01it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.94it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.20it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.18it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.99it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.06it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.94it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.97it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.03it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.95it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.06it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.12it/s][A                                                 
                                                 [A100%|██████████| 605/605 [04:00<00:00,  3.61it/s]
100%|██████████| 543/543 [00:12<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 01:44:50,934 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605
[INFO|configuration_utils.py:351] 2023-08-28 01:44:50,960 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:44:52,728 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:44:52,747 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:44:52,762 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 01:44:53,077 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 01:44:53,078 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121 (score: 1.0794728994369507).
                                                 100%|██████████| 605/605 [04:04<00:00,  3.61it/s]100%|██████████| 605/605 [04:04<00:00,  2.48it/s]
[INFO|trainer.py:1894] 2023-08-28 01:44:55,132 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 01:44:55,154 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 01:44:57,301 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 01:44:57,314 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 01:44:57,323 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 01:44:57,511 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:44:57,511 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:44:57,511 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:44:57,512 >>   train_runtime            = 0:04:04.39
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:44:57,512 >>   train_samples            =       7740
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:44:57,512 >>   train_samples_per_second =    158.348
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:44:57,512 >>   train_steps_per_second   =      2.475
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3232, 'eval_samples_per_second': 352.343, 'eval_steps_per_second': 44.063, 'epoch': 5.0}
{'train_runtime': 244.398, 'train_samples_per_second': 158.348, 'train_steps_per_second': 2.475, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 01:44:57 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 01:44:57,555 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 01:44:57,555 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 01:44:57,555 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 56.66it/s]  2%|▏         | 12/543 [00:00<00:10, 48.93it/s]  3%|▎         | 17/543 [00:00<00:11, 47.20it/s]  4%|▍         | 22/543 [00:00<00:11, 46.44it/s]  5%|▍         | 27/543 [00:00<00:11, 45.94it/s]  6%|▌         | 32/543 [00:00<00:11, 45.37it/s]  7%|▋         | 37/543 [00:00<00:11, 45.35it/s]  8%|▊         | 42/543 [00:00<00:12, 40.91it/s]  9%|▊         | 47/543 [00:01<00:11, 43.10it/s] 10%|▉         | 52/543 [00:01<00:11, 43.28it/s] 10%|█         | 57/543 [00:01<00:11, 43.56it/s] 11%|█▏        | 62/543 [00:01<00:10, 43.89it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.14it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.42it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.55it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.42it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.09it/s] 17%|█▋        | 92/543 [00:02<00:10, 43.86it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.79it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.94it/s] 20%|█▉        | 107/543 [00:02<00:09, 44.09it/s] 21%|██        | 112/543 [00:02<00:09, 44.30it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.42it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.54it/s] 23%|██▎       | 127/543 [00:02<00:09, 44.49it/s] 24%|██▍       | 132/543 [00:02<00:09, 44.19it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.96it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.94it/s] 27%|██▋       | 147/543 [00:03<00:08, 44.09it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.22it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.39it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.44it/s] 31%|███       | 167/543 [00:03<00:08, 44.47it/s] 32%|███▏      | 172/543 [00:03<00:08, 44.35it/s] 33%|███▎      | 177/543 [00:03<00:08, 44.18it/s] 34%|███▎      | 182/543 [00:04<00:08, 43.94it/s] 34%|███▍      | 187/543 [00:04<00:08, 44.01it/s] 35%|███▌      | 192/543 [00:04<00:07, 44.10it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.24it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.37it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.43it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.29it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.31it/s] 41%|████      | 222/543 [00:05<00:07, 44.16it/s] 42%|████▏     | 227/543 [00:05<00:07, 44.06it/s] 43%|████▎     | 232/543 [00:05<00:07, 44.04it/s] 44%|████▎     | 237/543 [00:05<00:06, 44.02it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.26it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.33it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.39it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.39it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.15it/s] 49%|████▉     | 267/543 [00:06<00:06, 44.14it/s] 50%|█████     | 272/543 [00:06<00:06, 44.05it/s] 51%|█████     | 277/543 [00:06<00:06, 44.01it/s] 52%|█████▏    | 282/543 [00:06<00:05, 44.13it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.21it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.40it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.41it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.35it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.30it/s] 57%|█████▋    | 312/543 [00:07<00:05, 44.12it/s] 58%|█████▊    | 317/543 [00:07<00:05, 44.02it/s] 59%|█████▉    | 322/543 [00:07<00:05, 43.99it/s] 60%|██████    | 327/543 [00:07<00:04, 44.11it/s] 61%|██████    | 332/543 [00:07<00:04, 44.28it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.48it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.29it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.29it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.23it/s] 66%|██████▌   | 357/543 [00:08<00:04, 44.07it/s] 67%|██████▋   | 362/543 [00:08<00:04, 44.14it/s] 68%|██████▊   | 367/543 [00:08<00:03, 44.06it/s] 69%|██████▊   | 372/543 [00:08<00:03, 44.10it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.29it/s] 70%|███████   | 382/543 [00:08<00:03, 44.41it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.33it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.20it/s] 73%|███████▎  | 397/543 [00:08<00:03, 43.88it/s] 74%|███████▍  | 402/543 [00:09<00:03, 44.00it/s] 75%|███████▍  | 407/543 [00:09<00:03, 44.06it/s] 76%|███████▌  | 412/543 [00:09<00:02, 44.19it/s] 77%|███████▋  | 417/543 [00:09<00:02, 44.09it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.30it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.37it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.30it/s] 80%|████████  | 437/543 [00:09<00:02, 44.22it/s] 81%|████████▏ | 442/543 [00:09<00:02, 44.10it/s] 82%|████████▏ | 447/543 [00:10<00:02, 44.11it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.17it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.13it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.16it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.28it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.27it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.24it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.13it/s] 90%|████████▉ | 487/543 [00:10<00:01, 44.15it/s] 91%|█████████ | 492/543 [00:11<00:01, 44.17it/s] 92%|█████████▏| 497/543 [00:11<00:01, 44.06it/s] 92%|█████████▏| 502/543 [00:11<00:00, 44.16it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.18it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.27it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.10it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.21it/s] 97%|█████████▋| 527/543 [00:11<00:00, 44.19it/s] 98%|█████████▊| 532/543 [00:12<00:00, 44.22it/s] 99%|█████████▉| 537/543 [00:12<00:00, 44.17it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.07it/s]100%|██████████| 543/543 [00:12<00:00, 44.23it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 01:45:09,849 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:45:09,849 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:45:09,849 >>   eval_loss               =     1.0795
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:45:09,849 >>   eval_runtime            = 0:00:12.29
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:45:09,849 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:45:09,849 >>   eval_samples_per_second =    353.174
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:45:09,850 >>   eval_steps_per_second   =     44.167
[INFO|trainer_pt_utils.py:913] 2023-08-28 01:45:09,850 >>   perplexity              =     2.9431
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:16,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:16,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:16,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:16,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:16,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:45:16,977 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:45:16,978 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:45:17,541 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:45:18,574 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:45:18,574 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:21,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:21,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:21,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:21,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:45:21,377 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:45:21,993 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:45:21,995 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:45:22,700 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:45:22,865 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:45:22,865 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.89it/s]Extractor Predicting: 2it [00:01,  1.85it/s]Extractor Predicting: 3it [00:01,  1.87it/s]Extractor Predicting: 4it [00:02,  1.96it/s]Extractor Predicting: 5it [00:02,  1.91it/s]Extractor Predicting: 6it [00:03,  1.87it/s]Extractor Predicting: 7it [00:03,  1.89it/s]Extractor Predicting: 8it [00:04,  1.92it/s]Extractor Predicting: 9it [00:04,  1.98it/s]Extractor Predicting: 10it [00:05,  1.99it/s]Extractor Predicting: 11it [00:05,  2.01it/s]Extractor Predicting: 12it [00:06,  2.02it/s]Extractor Predicting: 13it [00:06,  1.92it/s]Extractor Predicting: 14it [00:07,  1.86it/s]Extractor Predicting: 15it [00:07,  1.80it/s]Extractor Predicting: 16it [00:08,  1.69it/s]Extractor Predicting: 17it [00:09,  1.71it/s]Extractor Predicting: 18it [00:09,  1.71it/s]Extractor Predicting: 19it [00:10,  1.67it/s]Extractor Predicting: 20it [00:10,  1.65it/s]Extractor Predicting: 21it [00:11,  1.69it/s]Extractor Predicting: 22it [00:12,  1.73it/s]Extractor Predicting: 23it [00:12,  1.74it/s]Extractor Predicting: 24it [00:13,  1.74it/s]Extractor Predicting: 25it [00:13,  1.75it/s]Extractor Predicting: 26it [00:14,  1.74it/s]Extractor Predicting: 27it [00:14,  1.74it/s]Extractor Predicting: 28it [00:15,  1.76it/s]Extractor Predicting: 29it [00:16,  1.73it/s]Extractor Predicting: 30it [00:16,  1.74it/s]Extractor Predicting: 31it [00:17,  1.76it/s]Extractor Predicting: 32it [00:17,  1.77it/s]Extractor Predicting: 33it [00:18,  1.78it/s]Extractor Predicting: 34it [00:18,  1.73it/s]Extractor Predicting: 35it [00:19,  1.73it/s]Extractor Predicting: 36it [00:20,  1.70it/s]Extractor Predicting: 37it [00:20,  1.70it/s]Extractor Predicting: 38it [00:21,  1.69it/s]Extractor Predicting: 39it [00:21,  1.73it/s]Extractor Predicting: 40it [00:22,  1.75it/s]Extractor Predicting: 41it [00:23,  1.73it/s]Extractor Predicting: 42it [00:23,  1.71it/s]Extractor Predicting: 43it [00:24,  1.74it/s]Extractor Predicting: 44it [00:24,  1.73it/s]Extractor Predicting: 45it [00:25,  1.73it/s]Extractor Predicting: 46it [00:25,  1.75it/s]Extractor Predicting: 47it [00:26,  1.76it/s]Extractor Predicting: 48it [00:27,  1.73it/s]Extractor Predicting: 49it [00:27,  1.72it/s]Extractor Predicting: 50it [00:28,  1.74it/s]Extractor Predicting: 51it [00:28,  1.73it/s]Extractor Predicting: 52it [00:29,  1.72it/s]Extractor Predicting: 53it [00:29,  1.69it/s]Extractor Predicting: 54it [00:30,  1.73it/s]Extractor Predicting: 55it [00:31,  1.78it/s]Extractor Predicting: 56it [00:31,  1.73it/s]Extractor Predicting: 57it [00:32,  1.71it/s]Extractor Predicting: 58it [00:32,  1.76it/s]Extractor Predicting: 59it [00:33,  1.71it/s]Extractor Predicting: 60it [00:33,  1.72it/s]Extractor Predicting: 61it [00:34,  1.75it/s]Extractor Predicting: 62it [00:35,  1.75it/s]Extractor Predicting: 63it [00:35,  1.75it/s]Extractor Predicting: 64it [00:36,  1.75it/s]Extractor Predicting: 65it [00:36,  1.81it/s]Extractor Predicting: 66it [00:37,  1.80it/s]Extractor Predicting: 67it [00:37,  1.79it/s]Extractor Predicting: 68it [00:38,  1.76it/s]Extractor Predicting: 69it [00:39,  1.77it/s]Extractor Predicting: 70it [00:39,  1.66it/s]Extractor Predicting: 71it [00:40,  1.68it/s]Extractor Predicting: 72it [00:40,  1.70it/s]Extractor Predicting: 73it [00:41,  1.70it/s]Extractor Predicting: 74it [00:42,  1.69it/s]Extractor Predicting: 75it [00:42,  1.64it/s]Extractor Predicting: 76it [00:43,  1.67it/s]Extractor Predicting: 77it [00:43,  1.72it/s]Extractor Predicting: 78it [00:44,  1.71it/s]Extractor Predicting: 79it [00:45,  1.69it/s]Extractor Predicting: 80it [00:45,  1.71it/s]Extractor Predicting: 81it [00:46,  1.70it/s]Extractor Predicting: 82it [00:46,  1.71it/s]Extractor Predicting: 83it [00:47,  1.73it/s]Extractor Predicting: 84it [00:47,  1.75it/s]Extractor Predicting: 85it [00:48,  1.77it/s]Extractor Predicting: 86it [00:49,  1.74it/s]Extractor Predicting: 87it [00:49,  1.74it/s]Extractor Predicting: 88it [00:50,  1.76it/s]Extractor Predicting: 89it [00:50,  1.79it/s]Extractor Predicting: 90it [00:51,  1.76it/s]Extractor Predicting: 91it [00:51,  1.73it/s]Extractor Predicting: 92it [00:52,  1.70it/s]Extractor Predicting: 93it [00:53,  1.76it/s]Extractor Predicting: 94it [00:53,  1.61it/s]Extractor Predicting: 95it [00:54,  1.65it/s]Extractor Predicting: 96it [00:54,  1.70it/s]Extractor Predicting: 97it [00:55,  1.74it/s]Extractor Predicting: 98it [00:56,  1.73it/s]Extractor Predicting: 99it [00:56,  1.72it/s]Extractor Predicting: 100it [00:57,  1.74it/s]Extractor Predicting: 101it [00:57,  1.75it/s]Extractor Predicting: 102it [00:58,  1.74it/s]Extractor Predicting: 103it [00:58,  1.76it/s]Extractor Predicting: 104it [00:59,  1.77it/s]Extractor Predicting: 105it [01:00,  1.74it/s]Extractor Predicting: 106it [01:00,  1.77it/s]Extractor Predicting: 107it [01:01,  1.76it/s]Extractor Predicting: 108it [01:01,  1.75it/s]Extractor Predicting: 109it [01:02,  1.75it/s]Extractor Predicting: 110it [01:02,  1.73it/s]Extractor Predicting: 111it [01:03,  1.75it/s]Extractor Predicting: 112it [01:03,  1.79it/s]Extractor Predicting: 113it [01:04,  1.79it/s]Extractor Predicting: 114it [01:05,  1.78it/s]Extractor Predicting: 115it [01:05,  1.77it/s]Extractor Predicting: 116it [01:06,  1.75it/s]Extractor Predicting: 117it [01:06,  1.77it/s]Extractor Predicting: 118it [01:07,  1.72it/s]Extractor Predicting: 119it [01:08,  1.73it/s]Extractor Predicting: 120it [01:08,  1.71it/s]Extractor Predicting: 121it [01:09,  1.69it/s]Extractor Predicting: 122it [01:09,  1.69it/s]Extractor Predicting: 123it [01:10,  1.74it/s]Extractor Predicting: 124it [01:10,  1.75it/s]Extractor Predicting: 125it [01:11,  1.78it/s]Extractor Predicting: 126it [01:12,  1.71it/s]Extractor Predicting: 127it [01:12,  1.70it/s]Extractor Predicting: 128it [01:13,  1.70it/s]Extractor Predicting: 129it [01:13,  1.72it/s]Extractor Predicting: 130it [01:14,  1.78it/s]Extractor Predicting: 131it [01:14,  1.74it/s]Extractor Predicting: 132it [01:15,  1.74it/s]Extractor Predicting: 133it [01:16,  1.74it/s]Extractor Predicting: 134it [01:16,  1.75it/s]Extractor Predicting: 135it [01:17,  1.77it/s]Extractor Predicting: 136it [01:17,  1.77it/s]Extractor Predicting: 137it [01:18,  1.79it/s]Extractor Predicting: 138it [01:18,  1.76it/s]Extractor Predicting: 139it [01:19,  1.73it/s]Extractor Predicting: 140it [01:20,  1.77it/s]Extractor Predicting: 141it [01:20,  1.78it/s]Extractor Predicting: 142it [01:21,  1.81it/s]Extractor Predicting: 143it [01:21,  1.80it/s]Extractor Predicting: 144it [01:22,  1.79it/s]Extractor Predicting: 145it [01:22,  1.76it/s]Extractor Predicting: 146it [01:23,  1.78it/s]Extractor Predicting: 147it [01:23,  1.78it/s]Extractor Predicting: 148it [01:24,  1.78it/s]Extractor Predicting: 149it [01:25,  1.80it/s]Extractor Predicting: 150it [01:25,  1.74it/s]Extractor Predicting: 151it [01:26,  1.75it/s]Extractor Predicting: 152it [01:26,  1.75it/s]Extractor Predicting: 153it [01:27,  1.73it/s]Extractor Predicting: 154it [01:27,  1.74it/s]Extractor Predicting: 155it [01:28,  1.71it/s]Extractor Predicting: 156it [01:29,  1.70it/s]Extractor Predicting: 157it [01:29,  1.71it/s]Extractor Predicting: 158it [01:30,  1.72it/s]Extractor Predicting: 159it [01:30,  1.76it/s]Extractor Predicting: 160it [01:31,  1.79it/s]Extractor Predicting: 161it [01:31,  1.81it/s]Extractor Predicting: 162it [01:32,  1.81it/s]Extractor Predicting: 163it [01:33,  1.71it/s]Extractor Predicting: 163it [01:33,  1.75it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:06,782 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:06,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:06,784 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:06,785 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:06,785 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:47:07,399 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:47:07,403 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:47:07,967 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:47:08,992 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:47:08,992 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:11,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:11,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:11,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:11,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:47:11,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:47:12,461 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:47:12,462 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:47:13,043 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:47:13,214 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:47:13,215 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.80it/s]Extractor Predicting: 4it [00:02,  1.83it/s]Extractor Predicting: 5it [00:02,  1.81it/s]Extractor Predicting: 6it [00:03,  1.80it/s]Extractor Predicting: 7it [00:03,  1.84it/s]Extractor Predicting: 8it [00:04,  1.83it/s]Extractor Predicting: 9it [00:04,  1.86it/s]Extractor Predicting: 10it [00:05,  1.88it/s]Extractor Predicting: 11it [00:05,  1.89it/s]Extractor Predicting: 12it [00:06,  1.90it/s]Extractor Predicting: 13it [00:07,  1.87it/s]Extractor Predicting: 14it [00:07,  1.79it/s]Extractor Predicting: 15it [00:08,  1.80it/s]Extractor Predicting: 16it [00:08,  1.80it/s]Extractor Predicting: 17it [00:09,  1.80it/s]Extractor Predicting: 18it [00:09,  1.81it/s]Extractor Predicting: 19it [00:10,  1.87it/s]Extractor Predicting: 20it [00:10,  1.83it/s]Extractor Predicting: 21it [00:11,  1.84it/s]Extractor Predicting: 22it [00:12,  1.83it/s]Extractor Predicting: 23it [00:12,  1.81it/s]Extractor Predicting: 24it [00:13,  1.79it/s]Extractor Predicting: 25it [00:13,  1.80it/s]Extractor Predicting: 26it [00:14,  1.81it/s]Extractor Predicting: 27it [00:14,  1.85it/s]Extractor Predicting: 28it [00:15,  1.84it/s]Extractor Predicting: 29it [00:15,  1.84it/s]Extractor Predicting: 30it [00:16,  1.86it/s]Extractor Predicting: 31it [00:16,  1.86it/s]Extractor Predicting: 32it [00:17,  1.86it/s]Extractor Predicting: 33it [00:18,  1.84it/s]Extractor Predicting: 34it [00:18,  1.83it/s]Extractor Predicting: 35it [00:19,  1.87it/s]Extractor Predicting: 36it [00:19,  1.83it/s]Extractor Predicting: 37it [00:20,  1.82it/s]Extractor Predicting: 38it [00:20,  1.80it/s]Extractor Predicting: 39it [00:21,  1.86it/s]Extractor Predicting: 40it [00:21,  1.84it/s]Extractor Predicting: 41it [00:22,  1.75it/s]Extractor Predicting: 42it [00:23,  1.76it/s]Extractor Predicting: 43it [00:23,  1.74it/s]Extractor Predicting: 44it [00:24,  1.74it/s]Extractor Predicting: 45it [00:24,  1.72it/s]Extractor Predicting: 46it [00:25,  1.70it/s]Extractor Predicting: 47it [00:25,  1.67it/s]Extractor Predicting: 48it [00:26,  1.71it/s]Extractor Predicting: 49it [00:27,  1.70it/s]Extractor Predicting: 50it [00:27,  1.71it/s]Extractor Predicting: 51it [00:28,  1.72it/s]Extractor Predicting: 52it [00:28,  1.71it/s]Extractor Predicting: 53it [00:29,  1.69it/s]Extractor Predicting: 54it [00:30,  1.64it/s]Extractor Predicting: 55it [00:30,  1.68it/s]Extractor Predicting: 56it [00:31,  1.74it/s]Extractor Predicting: 57it [00:31,  1.72it/s]Extractor Predicting: 58it [00:32,  1.71it/s]Extractor Predicting: 59it [00:33,  1.68it/s]Extractor Predicting: 60it [00:33,  1.67it/s]Extractor Predicting: 61it [00:34,  1.71it/s]Extractor Predicting: 62it [00:34,  1.71it/s]Extractor Predicting: 63it [00:35,  1.65it/s]Extractor Predicting: 64it [00:36,  1.67it/s]Extractor Predicting: 65it [00:36,  1.67it/s]Extractor Predicting: 66it [00:37,  1.65it/s]Extractor Predicting: 67it [00:37,  1.66it/s]Extractor Predicting: 68it [00:38,  1.65it/s]Extractor Predicting: 69it [00:39,  1.66it/s]Extractor Predicting: 70it [00:39,  1.68it/s]Extractor Predicting: 71it [00:40,  1.70it/s]Extractor Predicting: 72it [00:40,  1.70it/s]Extractor Predicting: 73it [00:41,  1.68it/s]Extractor Predicting: 74it [00:41,  1.72it/s]Extractor Predicting: 75it [00:42,  1.72it/s]Extractor Predicting: 76it [00:43,  1.74it/s]Extractor Predicting: 77it [00:43,  1.76it/s]Extractor Predicting: 78it [00:44,  1.75it/s]Extractor Predicting: 79it [00:44,  1.66it/s]Extractor Predicting: 80it [00:45,  1.71it/s]Extractor Predicting: 81it [00:46,  1.59it/s]Extractor Predicting: 82it [00:46,  1.64it/s]Extractor Predicting: 83it [00:47,  1.50it/s]Extractor Predicting: 84it [00:48,  1.52it/s]Extractor Predicting: 85it [00:48,  1.57it/s]Extractor Predicting: 86it [00:49,  1.62it/s]Extractor Predicting: 87it [00:49,  1.67it/s]Extractor Predicting: 88it [00:50,  1.67it/s]Extractor Predicting: 89it [00:51,  1.72it/s]Extractor Predicting: 90it [00:51,  1.74it/s]Extractor Predicting: 91it [00:52,  1.77it/s]Extractor Predicting: 92it [00:52,  1.77it/s]Extractor Predicting: 93it [00:53,  1.77it/s]Extractor Predicting: 94it [00:53,  1.76it/s]Extractor Predicting: 95it [00:54,  1.78it/s]Extractor Predicting: 96it [00:54,  1.80it/s]Extractor Predicting: 97it [00:55,  1.79it/s]Extractor Predicting: 98it [00:56,  1.72it/s]Extractor Predicting: 99it [00:56,  1.73it/s]Extractor Predicting: 100it [00:57,  1.73it/s]Extractor Predicting: 101it [00:57,  1.72it/s]Extractor Predicting: 102it [00:58,  1.75it/s]Extractor Predicting: 103it [00:59,  1.72it/s]Extractor Predicting: 104it [00:59,  1.75it/s]Extractor Predicting: 105it [01:00,  1.74it/s]Extractor Predicting: 106it [01:00,  1.69it/s]Extractor Predicting: 107it [01:01,  1.67it/s]Extractor Predicting: 108it [01:01,  1.70it/s]Extractor Predicting: 109it [01:02,  1.63it/s]Extractor Predicting: 110it [01:03,  1.66it/s]Extractor Predicting: 111it [01:03,  1.66it/s]Extractor Predicting: 112it [01:04,  1.69it/s]Extractor Predicting: 113it [01:05,  1.68it/s]Extractor Predicting: 114it [01:05,  1.69it/s]Extractor Predicting: 115it [01:06,  1.69it/s]Extractor Predicting: 116it [01:06,  1.67it/s]Extractor Predicting: 117it [01:07,  1.69it/s]Extractor Predicting: 118it [01:07,  1.74it/s]Extractor Predicting: 119it [01:08,  1.74it/s]Extractor Predicting: 120it [01:09,  1.78it/s]Extractor Predicting: 121it [01:09,  1.77it/s]Extractor Predicting: 122it [01:10,  1.80it/s]Extractor Predicting: 123it [01:10,  1.83it/s]Extractor Predicting: 124it [01:11,  1.85it/s]Extractor Predicting: 125it [01:11,  1.83it/s]Extractor Predicting: 126it [01:12,  1.79it/s]Extractor Predicting: 127it [01:12,  1.79it/s]Extractor Predicting: 128it [01:13,  1.77it/s]Extractor Predicting: 129it [01:14,  1.76it/s]Extractor Predicting: 130it [01:14,  1.74it/s]Extractor Predicting: 131it [01:15,  1.81it/s]Extractor Predicting: 132it [01:15,  1.77it/s]Extractor Predicting: 133it [01:16,  1.78it/s]Extractor Predicting: 134it [01:16,  1.80it/s]Extractor Predicting: 135it [01:17,  1.80it/s]Extractor Predicting: 136it [01:17,  1.79it/s]Extractor Predicting: 137it [01:18,  1.85it/s]Extractor Predicting: 138it [01:19,  1.82it/s]Extractor Predicting: 139it [01:19,  1.82it/s]Extractor Predicting: 140it [01:20,  1.80it/s]Extractor Predicting: 141it [01:20,  1.81it/s]Extractor Predicting: 142it [01:21,  1.79it/s]Extractor Predicting: 143it [01:21,  1.75it/s]Extractor Predicting: 144it [01:22,  1.75it/s]Extractor Predicting: 145it [01:23,  1.70it/s]Extractor Predicting: 146it [01:23,  1.64it/s]Extractor Predicting: 147it [01:24,  1.68it/s]Extractor Predicting: 148it [01:24,  1.68it/s]Extractor Predicting: 149it [01:25,  1.71it/s]Extractor Predicting: 150it [01:25,  1.72it/s]Extractor Predicting: 151it [01:26,  1.73it/s]Extractor Predicting: 152it [01:27,  1.73it/s]Extractor Predicting: 153it [01:27,  1.76it/s]Extractor Predicting: 154it [01:28,  1.78it/s]Extractor Predicting: 155it [01:28,  1.82it/s]Extractor Predicting: 156it [01:29,  1.76it/s]Extractor Predicting: 157it [01:29,  1.77it/s]Extractor Predicting: 158it [01:30,  1.77it/s]Extractor Predicting: 159it [01:31,  1.76it/s]Extractor Predicting: 160it [01:31,  1.80it/s]Extractor Predicting: 161it [01:32,  1.69it/s]Extractor Predicting: 162it [01:32,  1.76it/s]Extractor Predicting: 163it [01:33,  1.78it/s]Extractor Predicting: 164it [01:33,  1.76it/s]Extractor Predicting: 165it [01:34,  1.84it/s]Extractor Predicting: 166it [01:34,  1.84it/s]Extractor Predicting: 167it [01:35,  1.82it/s]Extractor Predicting: 168it [01:35,  1.89it/s]Extractor Predicting: 169it [01:36,  1.90it/s]Extractor Predicting: 170it [01:37,  1.84it/s]Extractor Predicting: 171it [01:37,  1.83it/s]Extractor Predicting: 172it [01:38,  1.83it/s]Extractor Predicting: 173it [01:38,  1.78it/s]Extractor Predicting: 174it [01:39,  1.82it/s]Extractor Predicting: 175it [01:39,  1.81it/s]Extractor Predicting: 176it [01:40,  1.80it/s]Extractor Predicting: 177it [01:41,  1.77it/s]Extractor Predicting: 178it [01:41,  1.77it/s]Extractor Predicting: 179it [01:42,  1.86it/s]Extractor Predicting: 180it [01:42,  1.83it/s]Extractor Predicting: 181it [01:43,  1.83it/s]Extractor Predicting: 182it [01:43,  1.80it/s]Extractor Predicting: 183it [01:44,  1.79it/s]Extractor Predicting: 184it [01:44,  1.78it/s]Extractor Predicting: 185it [01:45,  1.82it/s]Extractor Predicting: 186it [01:45,  1.79it/s]Extractor Predicting: 187it [01:46,  1.57it/s]Extractor Predicting: 188it [01:47,  1.61it/s]Extractor Predicting: 189it [01:47,  1.64it/s]Extractor Predicting: 190it [01:48,  1.64it/s]Extractor Predicting: 191it [01:49,  1.65it/s]Extractor Predicting: 192it [01:49,  1.70it/s]Extractor Predicting: 193it [01:50,  1.75it/s]Extractor Predicting: 194it [01:50,  1.77it/s]Extractor Predicting: 195it [01:51,  1.74it/s]Extractor Predicting: 196it [01:51,  1.76it/s]Extractor Predicting: 197it [01:52,  1.76it/s]Extractor Predicting: 198it [01:53,  1.77it/s]Extractor Predicting: 199it [01:53,  1.77it/s]Extractor Predicting: 200it [01:54,  1.76it/s]Extractor Predicting: 201it [01:54,  1.77it/s]Extractor Predicting: 202it [01:55,  1.76it/s]Extractor Predicting: 203it [01:55,  1.76it/s]Extractor Predicting: 204it [01:56,  1.77it/s]Extractor Predicting: 205it [01:57,  1.76it/s]Extractor Predicting: 206it [01:57,  1.75it/s]Extractor Predicting: 207it [01:58,  1.78it/s]Extractor Predicting: 208it [01:58,  1.77it/s]Extractor Predicting: 209it [01:59,  1.77it/s]Extractor Predicting: 210it [01:59,  1.77it/s]Extractor Predicting: 211it [02:00,  1.76it/s]Extractor Predicting: 212it [02:01,  1.78it/s]Extractor Predicting: 213it [02:01,  1.77it/s]Extractor Predicting: 214it [02:02,  1.77it/s]Extractor Predicting: 215it [02:02,  1.71it/s]Extractor Predicting: 216it [02:03,  1.74it/s]Extractor Predicting: 217it [02:03,  1.77it/s]Extractor Predicting: 218it [02:04,  1.78it/s]Extractor Predicting: 219it [02:05,  1.75it/s]Extractor Predicting: 220it [02:05,  1.76it/s]Extractor Predicting: 221it [02:06,  1.74it/s]Extractor Predicting: 222it [02:06,  1.75it/s]Extractor Predicting: 223it [02:07,  1.68it/s]Extractor Predicting: 224it [02:07,  1.70it/s]Extractor Predicting: 225it [02:08,  1.69it/s]Extractor Predicting: 226it [02:09,  1.70it/s]Extractor Predicting: 227it [02:09,  1.66it/s]Extractor Predicting: 228it [02:10,  1.61it/s]Extractor Predicting: 229it [02:11,  1.63it/s]Extractor Predicting: 230it [02:11,  1.65it/s]Extractor Predicting: 231it [02:12,  1.66it/s]Extractor Predicting: 232it [02:12,  1.67it/s]Extractor Predicting: 233it [02:13,  1.69it/s]Extractor Predicting: 234it [02:13,  1.71it/s]Extractor Predicting: 235it [02:14,  1.71it/s]Extractor Predicting: 236it [02:15,  1.72it/s]Extractor Predicting: 237it [02:15,  1.71it/s]Extractor Predicting: 238it [02:16,  1.77it/s]Extractor Predicting: 239it [02:16,  1.80it/s]Extractor Predicting: 240it [02:17,  1.87it/s]Extractor Predicting: 241it [02:17,  1.85it/s]Extractor Predicting: 242it [02:18,  1.84it/s]Extractor Predicting: 243it [02:18,  1.79it/s]Extractor Predicting: 244it [02:19,  1.80it/s]Extractor Predicting: 245it [02:20,  1.84it/s]Extractor Predicting: 246it [02:20,  1.81it/s]Extractor Predicting: 247it [02:21,  1.82it/s]Extractor Predicting: 248it [02:21,  1.83it/s]Extractor Predicting: 249it [02:22,  1.89it/s]Extractor Predicting: 250it [02:22,  1.88it/s]Extractor Predicting: 251it [02:23,  1.95it/s]Extractor Predicting: 252it [02:23,  1.98it/s]Extractor Predicting: 253it [02:24,  1.93it/s]Extractor Predicting: 254it [02:24,  1.96it/s]Extractor Predicting: 255it [02:25,  1.89it/s]Extractor Predicting: 256it [02:25,  1.94it/s]Extractor Predicting: 257it [02:26,  1.89it/s]Extractor Predicting: 258it [02:26,  1.83it/s]Extractor Predicting: 259it [02:27,  1.78it/s]Extractor Predicting: 260it [02:27,  1.85it/s]Extractor Predicting: 261it [02:28,  1.81it/s]Extractor Predicting: 262it [02:29,  1.83it/s]Extractor Predicting: 263it [02:29,  1.88it/s]Extractor Predicting: 264it [02:30,  1.89it/s]Extractor Predicting: 265it [02:30,  1.88it/s]Extractor Predicting: 266it [02:31,  1.91it/s]Extractor Predicting: 267it [02:31,  1.94it/s]Extractor Predicting: 268it [02:32,  1.93it/s]Extractor Predicting: 269it [02:32,  1.88it/s]Extractor Predicting: 270it [02:33,  1.90it/s]Extractor Predicting: 271it [02:33,  1.88it/s]Extractor Predicting: 272it [02:34,  1.85it/s]Extractor Predicting: 273it [02:34,  1.92it/s]Extractor Predicting: 274it [02:35,  1.93it/s]Extractor Predicting: 275it [02:35,  1.90it/s]Extractor Predicting: 276it [02:36,  1.87it/s]Extractor Predicting: 277it [02:36,  1.86it/s]Extractor Predicting: 278it [02:37,  1.90it/s]Extractor Predicting: 279it [02:38,  1.84it/s]Extractor Predicting: 280it [02:38,  1.91it/s]Extractor Predicting: 281it [02:39,  1.93it/s]Extractor Predicting: 282it [02:39,  1.93it/s]Extractor Predicting: 283it [02:40,  1.95it/s]Extractor Predicting: 284it [02:40,  1.94it/s]Extractor Predicting: 285it [02:41,  1.94it/s]Extractor Predicting: 286it [02:41,  1.94it/s]Extractor Predicting: 287it [02:42,  1.93it/s]Extractor Predicting: 288it [02:42,  1.88it/s]Extractor Predicting: 289it [02:43,  1.88it/s]Extractor Predicting: 290it [02:43,  1.79it/s]Extractor Predicting: 291it [02:44,  1.82it/s]Extractor Predicting: 292it [02:44,  1.87it/s]Extractor Predicting: 293it [02:45,  1.88it/s]Extractor Predicting: 294it [02:45,  1.88it/s]Extractor Predicting: 295it [02:46,  1.90it/s]Extractor Predicting: 296it [02:47,  1.79it/s]Extractor Predicting: 297it [02:47,  1.78it/s]Extractor Predicting: 298it [02:48,  1.53it/s]Extractor Predicting: 299it [02:49,  1.59it/s]Extractor Predicting: 300it [02:49,  1.65it/s]Extractor Predicting: 301it [02:50,  1.70it/s]Extractor Predicting: 302it [02:50,  1.73it/s]Extractor Predicting: 303it [02:51,  1.71it/s]Extractor Predicting: 304it [02:51,  1.72it/s]Extractor Predicting: 305it [02:52,  1.75it/s]Extractor Predicting: 306it [02:53,  1.70it/s]Extractor Predicting: 307it [02:53,  1.68it/s]Extractor Predicting: 308it [02:54,  1.68it/s]Extractor Predicting: 309it [02:54,  1.71it/s]Extractor Predicting: 310it [02:55,  1.74it/s]Extractor Predicting: 311it [02:55,  1.76it/s]Extractor Predicting: 312it [02:56,  1.75it/s]Extractor Predicting: 313it [02:57,  1.70it/s]Extractor Predicting: 314it [02:57,  1.63it/s]Extractor Predicting: 315it [02:58,  1.54it/s]Extractor Predicting: 316it [02:59,  1.58it/s]Extractor Predicting: 317it [02:59,  1.61it/s]Extractor Predicting: 318it [03:00,  1.65it/s]Extractor Predicting: 319it [03:00,  1.67it/s]Extractor Predicting: 320it [03:01,  1.69it/s]Extractor Predicting: 321it [03:02,  1.76it/s]Extractor Predicting: 322it [03:02,  1.77it/s]Extractor Predicting: 323it [03:03,  1.75it/s]Extractor Predicting: 324it [03:03,  1.74it/s]Extractor Predicting: 325it [03:04,  1.73it/s]Extractor Predicting: 326it [03:04,  1.72it/s]Extractor Predicting: 327it [03:05,  1.69it/s]Extractor Predicting: 328it [03:06,  1.70it/s]Extractor Predicting: 329it [03:06,  1.74it/s]Extractor Predicting: 330it [03:07,  1.77it/s]Extractor Predicting: 331it [03:07,  1.99it/s]Extractor Predicting: 331it [03:07,  1.76it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:28,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:28,664 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:28,664 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:28,664 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:28,664 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:50:28,963 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:50:28,964 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:50:29,224 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:50:30,298 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:50:30,298 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:31,569 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:31,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:31,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:31,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:50:31,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:50:32,294 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:50:32,299 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:50:32,549 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:50:32,695 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:50:32,695 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.68it/s]Extractor Predicting: 8it [00:04,  1.70it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.67it/s]Extractor Predicting: 16it [00:09,  1.69it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.70it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:11,  1.65it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:13,  1.60it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:16,  1.67it/s]Extractor Predicting: 29it [00:17,  1.67it/s]Extractor Predicting: 30it [00:17,  1.68it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.70it/s]Extractor Predicting: 33it [00:19,  1.71it/s]Extractor Predicting: 34it [00:20,  1.70it/s]Extractor Predicting: 35it [00:20,  1.72it/s]Extractor Predicting: 36it [00:21,  1.72it/s]Extractor Predicting: 37it [00:22,  1.73it/s]Extractor Predicting: 38it [00:22,  1.74it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:23,  1.68it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:24,  1.71it/s]Extractor Predicting: 43it [00:25,  1.70it/s]Extractor Predicting: 44it [00:26,  1.71it/s]Extractor Predicting: 45it [00:26,  1.70it/s]Extractor Predicting: 46it [00:27,  1.73it/s]Extractor Predicting: 47it [00:27,  1.70it/s]Extractor Predicting: 48it [00:28,  1.74it/s]Extractor Predicting: 49it [00:29,  1.76it/s]Extractor Predicting: 50it [00:30,  1.14it/s]Extractor Predicting: 51it [00:31,  1.28it/s]Extractor Predicting: 52it [00:31,  1.39it/s]Extractor Predicting: 53it [00:32,  1.42it/s]Extractor Predicting: 54it [00:32,  1.51it/s]Extractor Predicting: 55it [00:33,  1.58it/s]Extractor Predicting: 56it [00:34,  1.70it/s]Extractor Predicting: 57it [00:34,  1.78it/s]Extractor Predicting: 58it [00:35,  1.86it/s]Extractor Predicting: 59it [00:35,  1.96it/s]Extractor Predicting: 60it [00:35,  2.06it/s]Extractor Predicting: 61it [00:36,  2.13it/s]Extractor Predicting: 62it [00:36,  2.12it/s]Extractor Predicting: 63it [00:37,  2.15it/s]Extractor Predicting: 64it [00:37,  2.12it/s]Extractor Predicting: 65it [00:38,  2.12it/s]Extractor Predicting: 66it [00:38,  2.12it/s]Extractor Predicting: 67it [00:39,  2.11it/s]Extractor Predicting: 68it [00:39,  2.13it/s]Extractor Predicting: 69it [00:40,  2.17it/s]Extractor Predicting: 70it [00:40,  2.12it/s]Extractor Predicting: 71it [00:41,  2.13it/s]Extractor Predicting: 72it [00:41,  2.16it/s]Extractor Predicting: 73it [00:41,  2.21it/s]Extractor Predicting: 74it [00:42,  2.23it/s]Extractor Predicting: 75it [00:42,  2.21it/s]Extractor Predicting: 76it [00:43,  2.15it/s]Extractor Predicting: 77it [00:43,  2.23it/s]Extractor Predicting: 78it [00:44,  2.15it/s]Extractor Predicting: 79it [00:44,  2.15it/s]Extractor Predicting: 80it [00:45,  2.13it/s]Extractor Predicting: 81it [00:45,  2.11it/s]Extractor Predicting: 82it [00:46,  1.93it/s]Extractor Predicting: 83it [00:46,  2.00it/s]Extractor Predicting: 84it [00:47,  2.05it/s]Extractor Predicting: 85it [00:47,  2.09it/s]Extractor Predicting: 86it [00:48,  1.95it/s]Extractor Predicting: 87it [00:48,  1.91it/s]Extractor Predicting: 88it [00:49,  1.88it/s]Extractor Predicting: 89it [00:49,  1.86it/s]Extractor Predicting: 90it [00:50,  1.88it/s]Extractor Predicting: 91it [00:50,  1.84it/s]Extractor Predicting: 92it [00:51,  1.83it/s]Extractor Predicting: 93it [00:52,  1.84it/s]Extractor Predicting: 94it [00:52,  1.83it/s]Extractor Predicting: 95it [00:53,  1.86it/s]Extractor Predicting: 96it [00:53,  1.86it/s]Extractor Predicting: 97it [00:54,  1.87it/s]Extractor Predicting: 98it [00:54,  1.87it/s]Extractor Predicting: 99it [00:55,  1.84it/s]Extractor Predicting: 100it [00:55,  1.78it/s]Extractor Predicting: 101it [00:56,  1.80it/s]Extractor Predicting: 102it [00:56,  1.80it/s]Extractor Predicting: 103it [00:57,  1.74it/s]Extractor Predicting: 104it [00:58,  1.73it/s]Extractor Predicting: 105it [00:58,  1.72it/s]Extractor Predicting: 106it [00:59,  1.70it/s]Extractor Predicting: 107it [00:59,  1.68it/s]Extractor Predicting: 108it [01:00,  1.64it/s]Extractor Predicting: 108it [01:00,  1.78it/s]
[INFO|configuration_utils.py:515] 2023-08-28 01:51:35,041 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:51:35,042 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:51:35,045 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:51:35,046 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 01:51:35,048 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:51:38,176 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 01:51:38,178 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 01:51:38,195 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:51:38,196 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:51:38,207 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:51:38,210 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:51:38,211 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:51:38,211 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:51:38,211 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:51:38,211 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:51:38,211 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 01:51:38,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:39,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:39,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:40,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:41,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:41,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:42,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:43,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:43,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:44,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:45,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:46,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:46,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:47,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:48,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:48,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:49,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:50,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:50,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:51,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:51,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:52,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:53,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:35, 15.40s/it][WARNING|generation_utils.py:914] 2023-08-28 01:51:53,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:54,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:54,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:55,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:56,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:56,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:57,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:57,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:58,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:59,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:51:59,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:00,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:00,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:01,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:02,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:03,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:03,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:04,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:05,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:05,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:06,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:06,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:07,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:08,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:08,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:30<03:21, 15.48s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:09,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:10,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:10,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:11,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:12,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:12,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:13,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:13,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:14,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:15,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:15,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:16,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:16,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:17,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:18,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:18,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:19,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:20,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:20,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:21,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:21,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:22,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:23,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:23,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:24,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:46<03:07, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:25,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:25,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:26,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:27,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:27,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:28,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:29,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:30,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:30,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:31,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:32,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:32,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:33,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:34,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:35,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:35,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:36,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:37,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:37,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:38,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:38,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:39,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:40,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:40,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:41,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:04<02:59, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:42,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:43,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:43,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:44,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:45,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:45,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:46,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:46,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:47,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:47,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:48,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:49,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:50,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:50,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:51,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:51,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:52,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:53,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:53,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:54,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:55,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:56,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:56,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:57,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:58,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:20<02:42, 16.28s/it][WARNING|generation_utils.py:914] 2023-08-28 01:52:58,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:52:59,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:00,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:00,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:01,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:02,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:02,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:03,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:03,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:04,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:05,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:05,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:06,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:06,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:07,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:08,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:08,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:09,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:09,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:10,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:11,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:11,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:12,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:13,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:35<02:22, 15.81s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:13,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:14,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:14,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:15,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:16,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:16,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:17,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:18,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:18,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:19,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:20,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:20,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:21,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:22,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:23,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:23,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:24,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:24,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:25,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:26,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:27,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:27,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:28,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:28,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:29,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:30,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:52<02:10, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:30,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:31,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:32,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:32,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:33,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:33,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:34,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:35,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:35,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:36,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:36,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:37,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:37,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:38,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:38,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:39,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:40,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:40,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:41,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:41,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:42,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:43,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:43,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:44,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:44,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:45,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:45,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:46,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:47,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:47,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:09<01:56, 16.60s/it][WARNING|generation_utils.py:914] 2023-08-28 01:53:48,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:48,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:49,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:50,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:50,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:51,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:51,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:52,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:53,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:53,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:54,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:54,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:55,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:56,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:56,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:57,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:57,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:58,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:59,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:53:59,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:00,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:00,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:01,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:02,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:24<01:35, 15.98s/it][WARNING|generation_utils.py:914] 2023-08-28 01:54:02,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:03,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:03,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:04,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:05,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:05,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:06,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:06,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:07,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:07,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:08,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:09,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:09,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:10,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:10,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:11,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:12,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:12,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:13,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:13,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:14,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:14,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:15,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:16,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:16,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:38<01:17, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-28 01:54:17,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:18,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:18,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:19,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:19,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:20,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:20,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:21,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:22,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:22,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:23,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:24,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:24,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:25,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:26,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:26,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:27,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:27,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:28,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:29,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:29,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:30,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:30,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:31,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:53<01:01, 15.31s/it][WARNING|generation_utils.py:914] 2023-08-28 01:54:32,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:32,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:33,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:34,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:34,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:35,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:36,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:36,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:37,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:38,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:39,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:39,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:40,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:41,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:41,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:42,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:42,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:43,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:44,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:44,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:45,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:46,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:46,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:47,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:48,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:48,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:49,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:50,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:12<00:49, 16.38s/it][WARNING|generation_utils.py:914] 2023-08-28 01:54:51,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:51,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:52,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:52,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:53,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:54,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:54,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:55,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:55,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:56,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:57,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:57,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:58,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:59,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:54:59,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:00,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:01,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:01,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:02,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:03,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:03,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:04,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:05,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:05,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:06,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:07,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:29<00:33, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 01:55:07,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:08,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:09,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:09,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:10,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:10,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:11,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:12,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:12,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:13,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:13,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:14,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:15,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:15,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:16,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:16,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:17,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:18,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:18,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:19,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:20,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:20,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:21,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:21,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:22,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:23,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:45<00:16, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 01:55:23,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:24,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:24,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:25,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:26,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:26,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:27,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:28,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:29,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:29,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:30,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:30,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:31,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:32,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:32,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:33,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:34,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:34,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:35,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:36,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:36,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:37,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:38,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:38,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:39,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:55:40,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:02<00:00, 16.55s/it]Generating: 100%|██████████| 15/15 [04:02<00:00, 16.16s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:47,307 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:47,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:47,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:47,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:47,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:55:47,900 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:55:47,901 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:55:48,464 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:55:49,536 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:55:49,536 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:52,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:52,398 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:52,398 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:52,398 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:55:52,398 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:55:53,048 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:55:53,049 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:55:53,587 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:55:53,760 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:55:53,760 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in Life , he played the title character , a young princess of the family at the end of the third season of HBO s Game of Thrones . Head Entity : Game of Thrones , Tail Entity : Princess of the family .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.75375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.7525, 'errors': {'', "('pharaoh Amenhotep II', 'made from material', '', 'He is best remembered for the painting of the pharaoh Amenhotep II , composed by Alfred Wohl in 1827 for a French conservatory .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : cast member .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Billboard Pop Hits', 'cast member', '', 'On October 25 , 1978 , the band released the track The Lads , the debut single by the British band The Riveters ( now defunct ) , on Columbia Records , along with several appearances on the Billboard Pop Hits chart for June .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : follows .', 'success_rate': 0.7283653846153846, 'errors': {'', "('Nigeria', 'follows', '', 'In 2013 , a second poll showed Zaire had a 7 % approval rating , while in 2014 , Nigeria had a 24 % approval rating .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 281, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 366, 'raw': 576}
{'target': 600, 'success': 391, 'raw': 608}
{'target': 600, 'success': 410, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 452, 'raw': 704}
{'target': 600, 'success': 470, 'raw': 736}
{'target': 600, 'success': 490, 'raw': 768}
{'target': 600, 'success': 511, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 590, 'raw': 928}
{'target': 600, 'success': 608, 'raw': 960}
{'prompt': 'Relation : league .', 'success_rate': 0.6333333333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.765, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in Life , he studied with a group of Latin philologists and other thinkers at the Renaissance in Italy , in Naples from 1635 to 1645 , and from 1654 back to 1701 . Head Entity : Naples , Tail Entity : Rome .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 262, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 305, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 341, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 412, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 501, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 584, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : mother .', 'success_rate': 0.6729910714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : residence .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1453 the head of the province of Ghent and of neighboring districts at Neuromö , in Burgundesland , were also under the Ottoman Empire . Head Entity : Burgundesland , Tail Entity : Ottomans .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 15273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.36it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.50it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:04,  1.49it/s]Extractor Estimating: 7it [00:04,  1.54it/s]Extractor Estimating: 8it [00:05,  1.51it/s]Extractor Estimating: 9it [00:05,  1.55it/s]Extractor Estimating: 10it [00:06,  1.54it/s]Extractor Estimating: 11it [00:07,  1.46it/s]Extractor Estimating: 12it [00:07,  1.52it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:09,  1.48it/s]Extractor Estimating: 15it [00:09,  1.51it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.52it/s]Extractor Estimating: 18it [00:11,  1.56it/s]Extractor Estimating: 19it [00:12,  1.53it/s]Extractor Estimating: 20it [00:13,  1.58it/s]Extractor Estimating: 21it [00:13,  1.58it/s]Extractor Estimating: 22it [00:14,  1.65it/s]Extractor Estimating: 23it [00:15,  1.57it/s]Extractor Estimating: 24it [00:15,  1.48it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:17,  1.56it/s]Extractor Estimating: 27it [00:17,  1.56it/s]Extractor Estimating: 28it [00:18,  1.61it/s]Extractor Estimating: 29it [00:18,  1.61it/s]Extractor Estimating: 30it [00:19,  1.63it/s]Extractor Estimating: 31it [00:20,  1.59it/s]Extractor Estimating: 32it [00:20,  1.61it/s]Extractor Estimating: 33it [00:21,  1.53it/s]Extractor Estimating: 34it [00:22,  1.51it/s]Extractor Estimating: 35it [00:22,  1.53it/s]Extractor Estimating: 36it [00:23,  1.52it/s]Extractor Estimating: 37it [00:24,  1.54it/s]Extractor Estimating: 38it [00:24,  1.55it/s]Extractor Estimating: 39it [00:25,  1.52it/s]Extractor Estimating: 40it [00:26,  1.52it/s]Extractor Estimating: 41it [00:26,  1.54it/s]Extractor Estimating: 42it [00:27,  1.55it/s]Extractor Estimating: 43it [00:28,  1.46it/s]Extractor Estimating: 44it [00:28,  1.47it/s]Extractor Estimating: 45it [00:29,  1.49it/s]Extractor Estimating: 46it [00:30,  1.42it/s]Extractor Estimating: 47it [00:30,  1.45it/s]Extractor Estimating: 48it [00:31,  1.48it/s]Extractor Estimating: 49it [00:32,  1.49it/s]Extractor Estimating: 50it [00:32,  1.46it/s]Extractor Estimating: 51it [00:33,  1.47it/s]Extractor Estimating: 52it [00:34,  1.56it/s]Extractor Estimating: 53it [00:34,  1.58it/s]Extractor Estimating: 54it [00:35,  1.57it/s]Extractor Estimating: 55it [00:35,  1.55it/s]Extractor Estimating: 56it [00:36,  1.58it/s]Extractor Estimating: 57it [00:37,  1.55it/s]Extractor Estimating: 58it [00:37,  1.56it/s]Extractor Estimating: 59it [00:38,  1.57it/s]Extractor Estimating: 60it [00:39,  1.59it/s]Extractor Estimating: 61it [00:39,  1.59it/s]Extractor Estimating: 62it [00:40,  1.61it/s]Extractor Estimating: 63it [00:40,  1.63it/s]Extractor Estimating: 64it [00:41,  1.64it/s]Extractor Estimating: 65it [00:42,  1.65it/s]Extractor Estimating: 66it [00:42,  1.62it/s]Extractor Estimating: 67it [00:43,  1.62it/s]Extractor Estimating: 68it [00:44,  1.64it/s]Extractor Estimating: 69it [00:44,  1.64it/s]Extractor Estimating: 70it [00:45,  1.63it/s]Extractor Estimating: 71it [00:45,  1.56it/s]Extractor Estimating: 72it [00:46,  1.59it/s]Extractor Estimating: 73it [00:47,  1.62it/s]Extractor Estimating: 74it [00:47,  1.60it/s]Extractor Estimating: 75it [00:48,  1.58it/s]Extractor Estimating: 76it [00:49,  1.55it/s]Extractor Estimating: 77it [00:49,  1.58it/s]Extractor Estimating: 78it [00:50,  1.58it/s]Extractor Estimating: 79it [00:51,  1.52it/s]Extractor Estimating: 80it [00:51,  1.45it/s]Extractor Estimating: 81it [00:52,  1.50it/s]Extractor Estimating: 82it [00:53,  1.47it/s]Extractor Estimating: 83it [00:53,  1.54it/s]Extractor Estimating: 84it [00:54,  1.54it/s]Extractor Estimating: 85it [00:54,  1.57it/s]Extractor Estimating: 86it [00:55,  1.60it/s]Extractor Estimating: 87it [00:56,  1.60it/s]Extractor Estimating: 88it [00:56,  1.63it/s]Extractor Estimating: 89it [00:57,  1.60it/s]Extractor Estimating: 90it [00:58,  1.57it/s]Extractor Estimating: 91it [00:58,  1.59it/s]Extractor Estimating: 92it [00:59,  1.56it/s]Extractor Estimating: 93it [01:00,  1.55it/s]Extractor Estimating: 94it [01:00,  1.55it/s]Extractor Estimating: 95it [01:01,  1.54it/s]Extractor Estimating: 96it [01:01,  1.56it/s]Extractor Estimating: 97it [01:02,  1.57it/s]Extractor Estimating: 98it [01:03,  1.54it/s]Extractor Estimating: 99it [01:04,  1.47it/s]Extractor Estimating: 100it [01:04,  1.50it/s]Extractor Estimating: 101it [01:05,  1.47it/s]Extractor Estimating: 102it [01:06,  1.50it/s]Extractor Estimating: 103it [01:06,  1.55it/s]Extractor Estimating: 104it [01:07,  1.56it/s]Extractor Estimating: 105it [01:07,  1.56it/s]Extractor Estimating: 106it [01:08,  1.58it/s]Extractor Estimating: 107it [01:09,  1.60it/s]Extractor Estimating: 108it [01:09,  1.63it/s]Extractor Estimating: 109it [01:10,  1.64it/s]Extractor Estimating: 110it [01:10,  1.59it/s]Extractor Estimating: 111it [01:11,  1.62it/s]Extractor Estimating: 112it [01:12,  1.59it/s]Extractor Estimating: 113it [01:12,  1.62it/s]Extractor Estimating: 114it [01:13,  1.62it/s]Extractor Estimating: 115it [01:14,  1.62it/s]Extractor Estimating: 116it [01:14,  1.56it/s]Extractor Estimating: 117it [01:15,  1.60it/s]Extractor Estimating: 118it [01:16,  1.43it/s]Extractor Estimating: 119it [01:16,  1.49it/s]Extractor Estimating: 120it [01:17,  1.46it/s]Extractor Estimating: 121it [01:18,  1.46it/s]Extractor Estimating: 122it [01:18,  1.49it/s]Extractor Estimating: 123it [01:20,  1.16it/s]Extractor Estimating: 124it [01:20,  1.25it/s]Extractor Estimating: 125it [01:21,  1.26it/s]Extractor Estimating: 126it [01:22,  1.33it/s]Extractor Estimating: 127it [01:22,  1.38it/s]Extractor Estimating: 128it [01:23,  1.47it/s]Extractor Estimating: 129it [01:24,  1.56it/s]Extractor Estimating: 130it [01:24,  1.53it/s]Extractor Estimating: 131it [01:25,  1.60it/s]Extractor Estimating: 132it [01:25,  1.58it/s]Extractor Estimating: 133it [01:26,  1.58it/s]Extractor Estimating: 134it [01:27,  1.56it/s]Extractor Estimating: 135it [01:27,  1.54it/s]Extractor Estimating: 136it [01:28,  1.35it/s]Extractor Estimating: 137it [01:29,  1.37it/s]Extractor Estimating: 138it [01:30,  1.44it/s]Extractor Estimating: 139it [01:30,  1.41it/s]Extractor Estimating: 140it [01:31,  1.47it/s]Extractor Estimating: 141it [01:32,  1.56it/s]Extractor Estimating: 142it [01:32,  1.54it/s]Extractor Estimating: 143it [01:33,  1.59it/s]Extractor Estimating: 144it [01:33,  1.63it/s]Extractor Estimating: 145it [01:34,  1.56it/s]Extractor Estimating: 146it [01:35,  1.54it/s]Extractor Estimating: 147it [01:35,  1.57it/s]Extractor Estimating: 148it [01:36,  1.62it/s]Extractor Estimating: 149it [01:37,  1.60it/s]Extractor Estimating: 150it [01:37,  1.58it/s]Extractor Estimating: 151it [01:38,  1.58it/s]Extractor Estimating: 152it [01:39,  1.55it/s]Extractor Estimating: 153it [01:39,  1.54it/s]Extractor Estimating: 154it [01:40,  1.56it/s]Extractor Estimating: 155it [01:40,  1.57it/s]Extractor Estimating: 156it [01:41,  1.59it/s]Extractor Estimating: 157it [01:42,  1.54it/s]Extractor Estimating: 158it [01:42,  1.55it/s]Extractor Estimating: 159it [01:43,  1.54it/s]Extractor Estimating: 160it [01:44,  1.53it/s]Extractor Estimating: 161it [01:44,  1.53it/s]Extractor Estimating: 162it [01:45,  1.57it/s]Extractor Estimating: 163it [01:46,  1.54it/s]Extractor Estimating: 164it [01:46,  1.55it/s]Extractor Estimating: 165it [01:47,  1.58it/s]Extractor Estimating: 166it [01:48,  1.56it/s]Extractor Estimating: 167it [01:48,  1.56it/s]Extractor Estimating: 168it [01:49,  1.42it/s]Extractor Estimating: 169it [01:50,  1.43it/s]Extractor Estimating: 170it [01:50,  1.48it/s]Extractor Estimating: 171it [01:51,  1.50it/s]Extractor Estimating: 172it [01:52,  1.53it/s]Extractor Estimating: 173it [01:52,  1.54it/s]Extractor Estimating: 174it [01:53,  1.55it/s]Extractor Estimating: 175it [01:54,  1.53it/s]Extractor Estimating: 176it [01:54,  1.57it/s]Extractor Estimating: 177it [01:55,  1.51it/s]Extractor Estimating: 178it [01:55,  1.56it/s]Extractor Estimating: 179it [01:56,  1.56it/s]Extractor Estimating: 180it [01:57,  1.57it/s]Extractor Estimating: 181it [01:57,  1.58it/s]Extractor Estimating: 182it [01:58,  1.61it/s]Extractor Estimating: 183it [01:59,  1.61it/s]Extractor Estimating: 184it [01:59,  1.61it/s]Extractor Estimating: 185it [02:00,  1.60it/s]Extractor Estimating: 186it [02:01,  1.52it/s]Extractor Estimating: 187it [02:01,  1.58it/s]Extractor Estimating: 188it [02:02,  1.61it/s]Extractor Estimating: 189it [02:02,  1.61it/s]Extractor Estimating: 190it [02:03,  1.59it/s]Extractor Estimating: 191it [02:04,  1.62it/s]Extractor Estimating: 192it [02:04,  1.45it/s]Extractor Estimating: 193it [02:05,  1.55it/s]Extractor Estimating: 194it [02:06,  1.56it/s]Extractor Estimating: 195it [02:06,  1.57it/s]Extractor Estimating: 196it [02:07,  1.56it/s]Extractor Estimating: 197it [02:07,  1.62it/s]Extractor Estimating: 198it [02:08,  1.62it/s]Extractor Estimating: 199it [02:09,  1.62it/s]Extractor Estimating: 200it [02:09,  1.64it/s]Extractor Estimating: 201it [02:10,  1.62it/s]Extractor Estimating: 202it [02:10,  1.68it/s]Extractor Estimating: 203it [02:11,  1.66it/s]Extractor Estimating: 204it [02:12,  1.69it/s]Extractor Estimating: 205it [02:12,  1.70it/s]Extractor Estimating: 206it [02:13,  1.68it/s]Extractor Estimating: 207it [02:13,  1.67it/s]Extractor Estimating: 208it [02:14,  1.71it/s]Extractor Estimating: 209it [02:15,  1.67it/s]Extractor Estimating: 210it [02:15,  1.63it/s]Extractor Estimating: 211it [02:16,  1.58it/s]Extractor Estimating: 212it [02:17,  1.63it/s]Extractor Estimating: 213it [02:17,  1.60it/s]Extractor Estimating: 214it [02:18,  1.67it/s]Extractor Estimating: 215it [02:18,  1.70it/s]Extractor Estimating: 216it [02:19,  1.68it/s]Extractor Estimating: 217it [02:19,  1.73it/s]Extractor Estimating: 218it [02:20,  1.70it/s]Extractor Estimating: 219it [02:21,  1.70it/s]Extractor Estimating: 220it [02:21,  1.66it/s]Extractor Estimating: 221it [02:22,  1.69it/s]Extractor Estimating: 222it [02:22,  1.71it/s]Extractor Estimating: 223it [02:23,  1.68it/s]Extractor Estimating: 224it [02:24,  1.69it/s]Extractor Estimating: 225it [02:24,  1.74it/s]Extractor Estimating: 226it [02:25,  1.77it/s]Extractor Estimating: 227it [02:25,  1.80it/s]Extractor Estimating: 228it [02:26,  1.81it/s]Extractor Estimating: 229it [02:26,  1.85it/s]Extractor Estimating: 230it [02:27,  1.86it/s]Extractor Estimating: 231it [02:27,  1.83it/s]Extractor Estimating: 232it [02:28,  1.77it/s]Extractor Estimating: 233it [02:29,  1.77it/s]Extractor Estimating: 234it [02:29,  1.83it/s]Extractor Estimating: 235it [02:30,  1.77it/s]Extractor Estimating: 236it [02:30,  1.75it/s]Extractor Estimating: 237it [02:31,  1.74it/s]Extractor Estimating: 238it [02:31,  1.73it/s]Extractor Estimating: 239it [02:32,  1.70it/s]Extractor Estimating: 240it [02:33,  1.71it/s]Extractor Estimating: 241it [02:33,  1.77it/s]Extractor Estimating: 242it [02:34,  1.76it/s]Extractor Estimating: 243it [02:34,  1.75it/s]Extractor Estimating: 244it [02:35,  1.71it/s]Extractor Estimating: 245it [02:35,  1.72it/s]Extractor Estimating: 246it [02:36,  1.73it/s]Extractor Estimating: 247it [02:37,  1.67it/s]Extractor Estimating: 248it [02:37,  1.68it/s]Extractor Estimating: 249it [02:38,  1.72it/s]Extractor Estimating: 250it [02:38,  1.71it/s]Extractor Estimating: 251it [02:39,  1.69it/s]Extractor Estimating: 252it [02:40,  1.67it/s]Extractor Estimating: 253it [02:40,  1.64it/s]Extractor Estimating: 254it [02:41,  1.64it/s]Extractor Estimating: 255it [02:42,  1.60it/s]Extractor Estimating: 256it [02:42,  1.60it/s]Extractor Estimating: 257it [02:43,  1.56it/s]Extractor Estimating: 258it [02:44,  1.55it/s]Extractor Estimating: 259it [02:44,  1.59it/s]Extractor Estimating: 260it [02:45,  1.59it/s]Extractor Estimating: 261it [02:45,  1.59it/s]Extractor Estimating: 262it [02:46,  1.59it/s]Extractor Estimating: 263it [02:47,  1.59it/s]Extractor Estimating: 264it [02:47,  1.61it/s]Extractor Estimating: 265it [02:48,  1.62it/s]Extractor Estimating: 266it [02:48,  1.63it/s]Extractor Estimating: 267it [02:49,  1.62it/s]Extractor Estimating: 268it [02:50,  1.46it/s]Extractor Estimating: 269it [02:50,  1.52it/s]Extractor Estimating: 270it [02:51,  1.59it/s]Extractor Estimating: 271it [02:52,  1.62it/s]Extractor Estimating: 272it [02:52,  1.63it/s]Extractor Estimating: 273it [02:53,  1.59it/s]Extractor Estimating: 274it [02:54,  1.58it/s]Extractor Estimating: 275it [02:54,  1.60it/s]Extractor Estimating: 276it [02:55,  1.57it/s]Extractor Estimating: 277it [02:56,  1.51it/s]Extractor Estimating: 278it [02:56,  1.49it/s]Extractor Estimating: 279it [02:57,  1.48it/s]Extractor Estimating: 280it [02:58,  1.54it/s]Extractor Estimating: 281it [02:58,  1.51it/s]Extractor Estimating: 282it [02:59,  1.52it/s]Extractor Estimating: 283it [03:00,  1.51it/s]Extractor Estimating: 284it [03:00,  1.46it/s]Extractor Estimating: 285it [03:01,  1.52it/s]Extractor Estimating: 286it [03:01,  1.55it/s]Extractor Estimating: 287it [03:02,  1.50it/s]Extractor Estimating: 288it [03:03,  1.52it/s]Extractor Estimating: 289it [03:04,  1.49it/s]Extractor Estimating: 290it [03:04,  1.51it/s]Extractor Estimating: 291it [03:05,  1.53it/s]Extractor Estimating: 292it [03:05,  1.51it/s]Extractor Estimating: 293it [03:06,  1.58it/s]Extractor Estimating: 294it [03:07,  1.55it/s]Extractor Estimating: 295it [03:07,  1.52it/s]Extractor Estimating: 296it [03:08,  1.54it/s]Extractor Estimating: 297it [03:09,  1.53it/s]Extractor Estimating: 298it [03:09,  1.51it/s]Extractor Estimating: 299it [03:10,  1.51it/s]Extractor Estimating: 300it [03:11,  1.53it/s]Extractor Estimating: 301it [03:11,  1.52it/s]Extractor Estimating: 302it [03:12,  1.56it/s]Extractor Estimating: 303it [03:13,  1.50it/s]Extractor Estimating: 304it [03:13,  1.52it/s]Extractor Estimating: 305it [03:14,  1.58it/s]Extractor Estimating: 306it [03:14,  1.61it/s]Extractor Estimating: 307it [03:15,  1.60it/s]Extractor Estimating: 308it [03:16,  1.59it/s]Extractor Estimating: 309it [03:16,  1.60it/s]Extractor Estimating: 310it [03:17,  1.61it/s]Extractor Estimating: 311it [03:18,  1.53it/s]Extractor Estimating: 312it [03:18,  1.52it/s]Extractor Estimating: 313it [03:19,  1.53it/s]Extractor Estimating: 314it [03:20,  1.57it/s]Extractor Estimating: 315it [03:20,  1.61it/s]Extractor Estimating: 316it [03:21,  1.51it/s]Extractor Estimating: 317it [03:22,  1.51it/s]Extractor Estimating: 318it [03:22,  1.53it/s]Extractor Estimating: 319it [03:23,  1.51it/s]Extractor Estimating: 320it [03:24,  1.53it/s]Extractor Estimating: 321it [03:24,  1.51it/s]Extractor Estimating: 322it [03:25,  1.44it/s]Extractor Estimating: 323it [03:26,  1.50it/s]Extractor Estimating: 324it [03:26,  1.53it/s]Extractor Estimating: 325it [03:27,  1.59it/s]Extractor Estimating: 326it [03:27,  1.60it/s]Extractor Estimating: 327it [03:28,  1.64it/s]Extractor Estimating: 328it [03:29,  1.69it/s]Extractor Estimating: 329it [03:29,  1.79it/s]Extractor Estimating: 330it [03:30,  1.76it/s]Extractor Estimating: 331it [03:30,  1.74it/s]Extractor Estimating: 332it [03:31,  1.77it/s]Extractor Estimating: 333it [03:31,  1.72it/s]Extractor Estimating: 334it [03:32,  1.74it/s]Extractor Estimating: 335it [03:33,  1.72it/s]Extractor Estimating: 336it [03:33,  1.74it/s]Extractor Estimating: 337it [03:34,  1.69it/s]Extractor Estimating: 338it [03:34,  1.65it/s]Extractor Estimating: 339it [03:35,  1.61it/s]Extractor Estimating: 340it [03:36,  1.56it/s]Extractor Estimating: 341it [03:36,  1.58it/s]Extractor Estimating: 342it [03:37,  1.62it/s]Extractor Estimating: 343it [03:38,  1.61it/s]Extractor Estimating: 344it [03:38,  1.62it/s]Extractor Estimating: 345it [03:39,  1.72it/s]Extractor Estimating: 346it [03:39,  1.70it/s]Extractor Estimating: 347it [03:40,  1.72it/s]Extractor Estimating: 348it [03:41,  1.55it/s]Extractor Estimating: 349it [03:41,  1.55it/s]Extractor Estimating: 350it [03:42,  1.60it/s]Extractor Estimating: 351it [03:42,  1.63it/s]Extractor Estimating: 352it [03:43,  1.64it/s]Extractor Estimating: 353it [03:44,  1.65it/s]Extractor Estimating: 354it [03:44,  1.63it/s]Extractor Estimating: 355it [03:45,  1.60it/s]Extractor Estimating: 356it [03:46,  1.57it/s]Extractor Estimating: 357it [03:46,  1.52it/s]Extractor Estimating: 358it [03:47,  1.53it/s]Extractor Estimating: 359it [03:47,  1.59it/s]Extractor Estimating: 360it [03:48,  1.56it/s]Extractor Estimating: 361it [03:49,  1.62it/s]Extractor Estimating: 362it [03:49,  1.61it/s]Extractor Estimating: 363it [03:50,  1.63it/s]Extractor Estimating: 364it [03:51,  1.67it/s]Extractor Estimating: 365it [03:51,  1.69it/s]Extractor Estimating: 366it [03:52,  1.66it/s]Extractor Estimating: 367it [03:52,  1.62it/s]Extractor Estimating: 368it [03:53,  1.58it/s]Extractor Estimating: 369it [03:54,  1.61it/s]Extractor Estimating: 370it [03:54,  1.64it/s]Extractor Estimating: 371it [03:55,  1.64it/s]Extractor Estimating: 372it [03:56,  1.59it/s]Extractor Estimating: 373it [03:56,  1.53it/s]Extractor Estimating: 374it [03:57,  1.52it/s]Extractor Estimating: 375it [03:57,  1.71it/s]Extractor Estimating: 375it [03:57,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:09,099 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:09,103 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:09,103 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:09,103 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:09,103 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:00:09,702 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:00:09,703 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:00:10,259 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:00:11,321 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:00:11,321 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:14,241 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:14,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:14,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:14,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:00:14,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:00:14,907 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:00:14,908 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:00:15,480 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:00:15,651 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:00:15,651 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 04:20:34,095 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 04:20:34,124 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7818 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 26637
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26737, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26737, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.105, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.047, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.037, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 74, avg_time 1.089, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 174, avg_time 1.044, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 274, avg_time 2.214, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 48, avg_time 1.037, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 148, avg_time 1.051, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 248, avg_time 1.049, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 22, avg_time 1.074, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 122, avg_time 2.221, loss:nan
g_step 1200, step 222, avg_time 1.073, loss:nan
g_step 1300, step 322, avg_time 1.065, loss:nan
g_step 1400, step 96, avg_time 1.047, loss:nan
g_step 1500, step 196, avg_time 1.070, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 296, avg_time 2.223, loss:nan
g_step 1700, step 70, avg_time 1.052, loss:nan
g_step 1800, step 170, avg_time 1.054, loss:nan
g_step 1900, step 270, avg_time 1.059, loss:nan
g_step 2000, step 44, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 144, avg_time 2.242, loss:nan
g_step 2200, step 244, avg_time 1.075, loss:nan
g_step 2300, step 18, avg_time 1.037, loss:nan
g_step 2400, step 118, avg_time 1.077, loss:nan
g_step 2500, step 218, avg_time 1.058, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 318, avg_time 2.226, loss:nan
g_step 2700, step 92, avg_time 1.038, loss:nan
g_step 2800, step 192, avg_time 1.070, loss:nan
g_step 2900, step 292, avg_time 1.054, loss:nan
g_step 3000, step 66, avg_time 1.072, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 166, avg_time 2.224, loss:nan
g_step 3200, step 266, avg_time 1.050, loss:nan
g_step 3300, step 40, avg_time 1.073, loss:nan
g_step 3400, step 140, avg_time 1.061, loss:nan
g_step 3500, step 240, avg_time 1.047, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 14, avg_time 2.222, loss:nan
g_step 3700, step 114, avg_time 1.059, loss:nan
g_step 3800, step 214, avg_time 1.051, loss:nan
g_step 3900, step 314, avg_time 1.060, loss:nan
g_step 4000, step 88, avg_time 1.051, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 188, avg_time 2.224, loss:nan
g_step 4200, step 288, avg_time 1.066, loss:nan
g_step 4300, step 62, avg_time 1.039, loss:nan
g_step 4400, step 162, avg_time 1.051, loss:nan
g_step 4500, step 262, avg_time 1.067, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 36, avg_time 2.243, loss:nan
g_step 4700, step 136, avg_time 1.043, loss:nan
g_step 4800, step 236, avg_time 1.066, loss:nan
g_step 4900, step 10, avg_time 1.039, loss:nan
g_step 5000, step 110, avg_time 1.062, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 210, avg_time 2.236, loss:nan
g_step 5200, step 310, avg_time 1.049, loss:nan
g_step 5300, step 84, avg_time 1.055, loss:nan
g_step 5400, step 184, avg_time 1.055, loss:nan
g_step 5500, step 284, avg_time 1.038, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 58, avg_time 2.239, loss:nan
g_step 5700, step 158, avg_time 1.057, loss:nan
g_step 5800, step 258, avg_time 1.059, loss:nan
g_step 5900, step 32, avg_time 1.052, loss:nan
g_step 6000, step 132, avg_time 1.055, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 232, avg_time 2.237, loss:nan
g_step 6200, step 6, avg_time 1.054, loss:nan
g_step 6300, step 106, avg_time 1.077, loss:nan
g_step 6400, step 206, avg_time 1.045, loss:nan
g_step 6500, step 306, avg_time 1.054, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 04:20:34 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 04:20:34 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_04-20-34_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 04:20:35 - WARNING - datasets.builder -   Using custom data configuration default-8734a947c1129f6f
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8734a947c1129f6f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 04:20:35,444 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:20:35,445 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:20:35,446 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:20:35,447 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:20:35,460 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:20:35,465 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:20:35,466 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:20:35,466 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:20:35,466 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:20:35,466 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:20:35,466 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 04:20:35,598 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:20:38,703 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 04:20:38,706 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8734a947c1129f6f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  3.72ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.25ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.40ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.48ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.52ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.54ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.52ba/s]100%|██████████| 8/8 [00:01<00:00,  4.69ba/s]100%|██████████| 8/8 [00:01<00:00,  4.52ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.10ba/s] 40%|████      | 2/5 [00:00<00:00,  4.33ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.41ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.58ba/s]100%|██████████| 5/5 [00:01<00:00,  4.44ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.56ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.37ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.67ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.82ba/s]100%|██████████| 8/8 [00:00<00:00, 10.83ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.62ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.54ba/s]100%|██████████| 5/5 [00:00<00:00, 12.98ba/s]100%|██████████| 5/5 [00:00<00:00, 12.26ba/s]
[INFO|trainer.py:414] 2023-08-28 04:20:43,192 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 04:20:43,204 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 04:20:43,204 >>   Num examples = 7900
[INFO|trainer.py:1149] 2023-08-28 04:20:43,204 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 04:20:43,204 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 04:20:43,204 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 04:20:43,204 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 04:20:43,204 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<02:56,  3.48it/s]  0%|          | 2/615 [00:00<02:52,  3.56it/s]  0%|          | 3/615 [00:00<02:50,  3.59it/s]  1%|          | 4/615 [00:01<02:49,  3.60it/s]  1%|          | 5/615 [00:01<02:48,  3.61it/s]  1%|          | 6/615 [00:01<02:48,  3.60it/s]  1%|          | 7/615 [00:01<02:50,  3.57it/s]  1%|▏         | 8/615 [00:02<02:49,  3.57it/s]  1%|▏         | 9/615 [00:02<02:49,  3.57it/s]  2%|▏         | 10/615 [00:02<02:49,  3.57it/s]  2%|▏         | 11/615 [00:03<02:49,  3.57it/s]  2%|▏         | 12/615 [00:03<02:48,  3.57it/s]  2%|▏         | 13/615 [00:03<02:48,  3.57it/s]  2%|▏         | 14/615 [00:03<02:48,  3.56it/s]  2%|▏         | 15/615 [00:04<02:48,  3.57it/s]  3%|▎         | 16/615 [00:04<02:48,  3.56it/s]  3%|▎         | 17/615 [00:04<02:47,  3.56it/s]  3%|▎         | 18/615 [00:05<02:48,  3.54it/s]  3%|▎         | 19/615 [00:05<02:47,  3.55it/s]  3%|▎         | 20/615 [00:05<02:46,  3.57it/s]  3%|▎         | 21/615 [00:05<02:45,  3.59it/s]  4%|▎         | 22/615 [00:06<02:44,  3.60it/s]  4%|▎         | 23/615 [00:06<02:44,  3.61it/s]  4%|▍         | 24/615 [00:06<02:43,  3.61it/s]  4%|▍         | 25/615 [00:06<02:43,  3.61it/s]  4%|▍         | 26/615 [00:07<02:43,  3.61it/s]  4%|▍         | 27/615 [00:07<02:42,  3.61it/s]  5%|▍         | 28/615 [00:07<02:42,  3.62it/s]  5%|▍         | 29/615 [00:08<02:43,  3.59it/s]  5%|▍         | 30/615 [00:08<02:42,  3.60it/s]  5%|▌         | 31/615 [00:08<02:41,  3.61it/s]  5%|▌         | 32/615 [00:08<02:41,  3.61it/s]  5%|▌         | 33/615 [00:09<02:41,  3.61it/s]  6%|▌         | 34/615 [00:09<02:40,  3.61it/s]  6%|▌         | 35/615 [00:09<02:40,  3.61it/s]  6%|▌         | 36/615 [00:10<02:40,  3.62it/s]  6%|▌         | 37/615 [00:10<02:39,  3.62it/s]  6%|▌         | 38/615 [00:10<02:39,  3.62it/s]  6%|▋         | 39/615 [00:10<02:39,  3.61it/s]  7%|▋         | 40/615 [00:11<02:39,  3.60it/s]  7%|▋         | 41/615 [00:11<02:39,  3.60it/s]  7%|▋         | 42/615 [00:11<02:38,  3.61it/s]  7%|▋         | 43/615 [00:11<02:38,  3.62it/s]  7%|▋         | 44/615 [00:12<02:37,  3.62it/s]  7%|▋         | 45/615 [00:12<02:37,  3.61it/s]  7%|▋         | 46/615 [00:12<02:37,  3.61it/s]  8%|▊         | 47/615 [00:13<02:37,  3.61it/s]  8%|▊         | 48/615 [00:13<02:36,  3.61it/s]  8%|▊         | 49/615 [00:13<02:36,  3.62it/s]  8%|▊         | 50/615 [00:13<02:36,  3.62it/s]  8%|▊         | 51/615 [00:14<02:36,  3.61it/s]  8%|▊         | 52/615 [00:14<02:35,  3.61it/s]  9%|▊         | 53/615 [00:14<02:35,  3.61it/s]  9%|▉         | 54/615 [00:15<02:35,  3.61it/s]  9%|▉         | 55/615 [00:15<02:35,  3.61it/s]  9%|▉         | 56/615 [00:15<02:34,  3.61it/s]  9%|▉         | 57/615 [00:15<02:34,  3.61it/s]  9%|▉         | 58/615 [00:16<02:34,  3.61it/s] 10%|▉         | 59/615 [00:16<02:33,  3.61it/s] 10%|▉         | 60/615 [00:16<02:33,  3.61it/s] 10%|▉         | 61/615 [00:16<02:33,  3.62it/s] 10%|█         | 62/615 [00:17<02:33,  3.60it/s] 10%|█         | 63/615 [00:17<02:32,  3.61it/s] 10%|█         | 64/615 [00:17<02:32,  3.61it/s] 11%|█         | 65/615 [00:18<02:32,  3.61it/s] 11%|█         | 66/615 [00:18<02:32,  3.61it/s] 11%|█         | 67/615 [00:18<02:31,  3.61it/s] 11%|█         | 68/615 [00:18<02:31,  3.61it/s] 11%|█         | 69/615 [00:19<02:31,  3.61it/s] 11%|█▏        | 70/615 [00:19<02:30,  3.62it/s] 12%|█▏        | 71/615 [00:19<02:30,  3.62it/s] 12%|█▏        | 72/615 [00:19<02:30,  3.62it/s] 12%|█▏        | 73/615 [00:20<02:30,  3.60it/s] 12%|█▏        | 74/615 [00:20<02:30,  3.60it/s] 12%|█▏        | 75/615 [00:20<02:29,  3.61it/s] 12%|█▏        | 76/615 [00:21<02:29,  3.61it/s] 13%|█▎        | 77/615 [00:21<02:28,  3.61it/s] 13%|█▎        | 78/615 [00:21<02:28,  3.61it/s] 13%|█▎        | 79/615 [00:21<02:28,  3.61it/s] 13%|█▎        | 80/615 [00:22<02:28,  3.60it/s] 13%|█▎        | 81/615 [00:22<02:27,  3.61it/s] 13%|█▎        | 82/615 [00:22<02:27,  3.61it/s] 13%|█▎        | 83/615 [00:23<02:27,  3.62it/s] 14%|█▎        | 84/615 [00:23<02:27,  3.60it/s] 14%|█▍        | 85/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 86/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 87/615 [00:24<02:26,  3.60it/s] 14%|█▍        | 88/615 [00:24<02:26,  3.60it/s] 14%|█▍        | 89/615 [00:24<02:25,  3.61it/s] 15%|█▍        | 90/615 [00:24<02:25,  3.61it/s] 15%|█▍        | 91/615 [00:25<02:25,  3.61it/s] 15%|█▍        | 92/615 [00:25<02:24,  3.61it/s] 15%|█▌        | 93/615 [00:25<02:24,  3.61it/s] 15%|█▌        | 94/615 [00:26<02:24,  3.61it/s] 15%|█▌        | 95/615 [00:26<02:23,  3.61it/s] 16%|█▌        | 96/615 [00:26<02:23,  3.61it/s] 16%|█▌        | 97/615 [00:26<02:23,  3.62it/s] 16%|█▌        | 98/615 [00:27<02:23,  3.61it/s] 16%|█▌        | 99/615 [00:27<02:22,  3.61it/s] 16%|█▋        | 100/615 [00:27<02:22,  3.61it/s] 16%|█▋        | 101/615 [00:28<02:22,  3.61it/s] 17%|█▋        | 102/615 [00:28<02:22,  3.61it/s] 17%|█▋        | 103/615 [00:28<02:22,  3.60it/s] 17%|█▋        | 104/615 [00:28<02:21,  3.60it/s] 17%|█▋        | 105/615 [00:29<02:22,  3.58it/s] 17%|█▋        | 106/615 [00:29<02:21,  3.59it/s] 17%|█▋        | 107/615 [00:29<02:21,  3.60it/s] 18%|█▊        | 108/615 [00:29<02:20,  3.61it/s] 18%|█▊        | 109/615 [00:30<02:20,  3.61it/s] 18%|█▊        | 110/615 [00:30<02:20,  3.61it/s] 18%|█▊        | 111/615 [00:30<02:19,  3.60it/s] 18%|█▊        | 112/615 [00:31<02:19,  3.61it/s] 18%|█▊        | 113/615 [00:31<02:19,  3.61it/s] 19%|█▊        | 114/615 [00:31<02:18,  3.61it/s] 19%|█▊        | 115/615 [00:31<02:18,  3.61it/s] 19%|█▉        | 116/615 [00:32<02:18,  3.59it/s] 19%|█▉        | 117/615 [00:32<02:18,  3.60it/s] 19%|█▉        | 118/615 [00:32<02:18,  3.60it/s] 19%|█▉        | 119/615 [00:33<02:17,  3.60it/s] 20%|█▉        | 120/615 [00:33<02:17,  3.60it/s] 20%|█▉        | 121/615 [00:33<02:17,  3.60it/s] 20%|█▉        | 122/615 [00:33<02:16,  3.60it/s] 20%|██        | 123/615 [00:34<02:16,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 04:21:17,468 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:21:17,468 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 04:21:17,468 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.97it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.05it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.25it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.38it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.85it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.46it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.32it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.26it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.22it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.35it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.33it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.29it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.19it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.12it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.01it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.98it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.01it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.12it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.16it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.16it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.16it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.04it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.05it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.99it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.93it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.09it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.12it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.19it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.24it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.13it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.07it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.97it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.99it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.92it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.07it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.10it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.15it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.18it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.05it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.05it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.93it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.93it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.03it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.06it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.31it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.17it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.23it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.20it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.94it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.91it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.97it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.06it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.25it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.17it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.20it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.26it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.12it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.01it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.86it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.89it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.06it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.08it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 44.23it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.23it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.19it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.04it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.92it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.98it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.08it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.08it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.19it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.08it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.20it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.12it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.01it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.94it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.02it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.15it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.07it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.16it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.22it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.03it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.08it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.98it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.95it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.14it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.17it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.14it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.23it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.18it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.08it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.05it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.02it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.90it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.12it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.11it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.18it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.28it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.15it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.08it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.03it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.01it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.96it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.06it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.19it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.16it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.21it/s][A                                                 
                                                 [A 20%|██        | 123/615 [00:46<02:16,  3.61it/s]
100%|██████████| 543/543 [00:12<00:00, 44.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:21:29,802 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-28 04:21:29,826 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:21:31,478 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:21:31,523 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:21:31,531 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123/special_tokens_map.json
 20%|██        | 124/615 [00:48<37:46,  4.62s/it] 20%|██        | 125/615 [00:49<27:04,  3.31s/it] 20%|██        | 126/615 [00:49<19:36,  2.41s/it] 21%|██        | 127/615 [00:49<14:22,  1.77s/it] 21%|██        | 128/615 [00:50<10:43,  1.32s/it] 21%|██        | 129/615 [00:50<08:10,  1.01s/it] 21%|██        | 130/615 [00:50<06:23,  1.26it/s] 21%|██▏       | 131/615 [00:50<05:08,  1.57it/s] 21%|██▏       | 132/615 [00:51<04:19,  1.86it/s] 22%|██▏       | 133/615 [00:51<03:42,  2.17it/s] 22%|██▏       | 134/615 [00:51<03:16,  2.44it/s] 22%|██▏       | 135/615 [00:51<02:57,  2.70it/s] 22%|██▏       | 136/615 [00:52<02:44,  2.91it/s] 22%|██▏       | 137/615 [00:52<02:35,  3.08it/s] 22%|██▏       | 138/615 [00:52<02:28,  3.21it/s] 23%|██▎       | 139/615 [00:53<02:24,  3.30it/s] 23%|██▎       | 140/615 [00:53<02:20,  3.37it/s] 23%|██▎       | 141/615 [00:53<02:18,  3.42it/s] 23%|██▎       | 142/615 [00:53<02:16,  3.46it/s] 23%|██▎       | 143/615 [00:54<02:15,  3.49it/s] 23%|██▎       | 144/615 [00:54<02:14,  3.51it/s] 24%|██▎       | 145/615 [00:54<02:12,  3.54it/s] 24%|██▎       | 146/615 [00:55<02:11,  3.56it/s] 24%|██▍       | 147/615 [00:55<02:10,  3.57it/s] 24%|██▍       | 148/615 [00:55<02:10,  3.58it/s] 24%|██▍       | 149/615 [00:55<02:09,  3.59it/s] 24%|██▍       | 150/615 [00:56<02:09,  3.60it/s] 25%|██▍       | 151/615 [00:56<02:08,  3.60it/s] 25%|██▍       | 152/615 [00:56<02:08,  3.61it/s] 25%|██▍       | 153/615 [00:57<02:08,  3.61it/s] 25%|██▌       | 154/615 [00:57<02:08,  3.60it/s] 25%|██▌       | 155/615 [00:57<02:07,  3.60it/s] 25%|██▌       | 156/615 [00:57<02:07,  3.61it/s] 26%|██▌       | 157/615 [00:58<02:06,  3.61it/s] 26%|██▌       | 158/615 [00:58<02:06,  3.61it/s] 26%|██▌       | 159/615 [00:58<02:06,  3.61it/s] 26%|██▌       | 160/615 [00:58<02:06,  3.61it/s] 26%|██▌       | 161/615 [00:59<02:05,  3.61it/s] 26%|██▋       | 162/615 [00:59<02:05,  3.61it/s] 27%|██▋       | 163/615 [00:59<02:05,  3.59it/s] 27%|██▋       | 164/615 [01:00<02:06,  3.58it/s] 27%|██▋       | 165/615 [01:00<02:05,  3.58it/s] 27%|██▋       | 166/615 [01:00<02:05,  3.59it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/615 [01:00<02:04,  3.59it/s] 27%|██▋       | 168/615 [01:01<02:04,  3.60it/s] 27%|██▋       | 169/615 [01:01<02:03,  3.60it/s] 28%|██▊       | 170/615 [01:01<02:03,  3.61it/s] 28%|██▊       | 171/615 [01:02<02:03,  3.61it/s] 28%|██▊       | 172/615 [01:02<02:02,  3.61it/s] 28%|██▊       | 173/615 [01:02<02:02,  3.61it/s] 28%|██▊       | 174/615 [01:02<02:02,  3.60it/s] 28%|██▊       | 175/615 [01:03<02:02,  3.60it/s] 29%|██▊       | 176/615 [01:03<02:01,  3.61it/s] 29%|██▉       | 177/615 [01:03<02:01,  3.61it/s] 29%|██▉       | 178/615 [01:03<02:01,  3.61it/s] 29%|██▉       | 179/615 [01:04<02:00,  3.61it/s] 29%|██▉       | 180/615 [01:04<02:00,  3.60it/s] 29%|██▉       | 181/615 [01:04<02:00,  3.60it/s] 30%|██▉       | 182/615 [01:05<02:00,  3.61it/s] 30%|██▉       | 183/615 [01:05<01:59,  3.61it/s] 30%|██▉       | 184/615 [01:05<01:59,  3.61it/s] 30%|███       | 185/615 [01:05<01:59,  3.60it/s] 30%|███       | 186/615 [01:06<01:59,  3.60it/s] 30%|███       | 187/615 [01:06<01:58,  3.60it/s] 31%|███       | 188/615 [01:06<01:58,  3.61it/s] 31%|███       | 189/615 [01:07<01:58,  3.61it/s] 31%|███       | 190/615 [01:07<01:57,  3.61it/s] 31%|███       | 191/615 [01:07<01:57,  3.61it/s] 31%|███       | 192/615 [01:07<01:57,  3.61it/s] 31%|███▏      | 193/615 [01:08<01:56,  3.61it/s] 32%|███▏      | 194/615 [01:08<01:56,  3.61it/s] 32%|███▏      | 195/615 [01:08<01:56,  3.61it/s] 32%|███▏      | 196/615 [01:08<01:56,  3.59it/s] 32%|███▏      | 197/615 [01:09<01:56,  3.59it/s] 32%|███▏      | 198/615 [01:09<01:55,  3.60it/s] 32%|███▏      | 199/615 [01:09<01:55,  3.60it/s] 33%|███▎      | 200/615 [01:10<01:55,  3.61it/s] 33%|███▎      | 201/615 [01:10<01:54,  3.61it/s] 33%|███▎      | 202/615 [01:10<01:54,  3.61it/s] 33%|███▎      | 203/615 [01:10<01:54,  3.61it/s] 33%|███▎      | 204/615 [01:11<01:53,  3.61it/s] 33%|███▎      | 205/615 [01:11<01:53,  3.61it/s] 33%|███▎      | 206/615 [01:11<01:53,  3.61it/s] 34%|███▎      | 207/615 [01:12<01:53,  3.60it/s] 34%|███▍      | 208/615 [01:12<01:52,  3.60it/s] 34%|███▍      | 209/615 [01:12<01:52,  3.61it/s] 34%|███▍      | 210/615 [01:12<01:52,  3.60it/s] 34%|███▍      | 211/615 [01:13<01:52,  3.61it/s] 34%|███▍      | 212/615 [01:13<01:51,  3.61it/s] 35%|███▍      | 213/615 [01:13<01:51,  3.60it/s] 35%|███▍      | 214/615 [01:13<01:51,  3.60it/s] 35%|███▍      | 215/615 [01:14<01:51,  3.60it/s] 35%|███▌      | 216/615 [01:14<01:50,  3.61it/s] 35%|███▌      | 217/615 [01:14<01:50,  3.61it/s] 35%|███▌      | 218/615 [01:15<01:50,  3.60it/s] 36%|███▌      | 219/615 [01:15<01:49,  3.60it/s] 36%|███▌      | 220/615 [01:15<01:49,  3.60it/s] 36%|███▌      | 221/615 [01:15<01:49,  3.60it/s] 36%|███▌      | 222/615 [01:16<01:49,  3.60it/s] 36%|███▋      | 223/615 [01:16<01:48,  3.60it/s] 36%|███▋      | 224/615 [01:16<01:48,  3.60it/s] 37%|███▋      | 225/615 [01:17<01:48,  3.60it/s] 37%|███▋      | 226/615 [01:17<01:48,  3.60it/s] 37%|███▋      | 227/615 [01:17<01:47,  3.60it/s] 37%|███▋      | 228/615 [01:17<01:47,  3.60it/s] 37%|███▋      | 229/615 [01:18<01:47,  3.59it/s] 37%|███▋      | 230/615 [01:18<01:47,  3.59it/s] 38%|███▊      | 231/615 [01:18<01:46,  3.59it/s] 38%|███▊      | 232/615 [01:18<01:46,  3.59it/s] 38%|███▊      | 233/615 [01:19<01:46,  3.59it/s] 38%|███▊      | 234/615 [01:19<01:45,  3.60it/s] 38%|███▊      | 235/615 [01:19<01:45,  3.60it/s] 38%|███▊      | 236/615 [01:20<01:45,  3.60it/s] 39%|███▊      | 237/615 [01:20<01:44,  3.61it/s] 39%|███▊      | 238/615 [01:20<01:44,  3.61it/s] 39%|███▉      | 239/615 [01:20<01:44,  3.61it/s] 39%|███▉      | 240/615 [01:21<01:44,  3.60it/s] 39%|███▉      | 241/615 [01:21<01:43,  3.60it/s] 39%|███▉      | 242/615 [01:21<01:43,  3.60it/s] 40%|███▉      | 243/615 [01:22<01:43,  3.60it/s] 40%|███▉      | 244/615 [01:22<01:42,  3.61it/s] 40%|███▉      | 245/615 [01:22<01:42,  3.61it/s] 40%|████      | 246/615 [01:22<01:42,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 04:22:06,169 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:22:06,169 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 04:22:06,169 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3115, 'eval_samples_per_second': 352.678, 'eval_steps_per_second': 44.105, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.77it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.13it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.04it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.22it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.66it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.43it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.18it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.96it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.03it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.20it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.31it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.27it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 43.91it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.92it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.90it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.91it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.92it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.01it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.03it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.21it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.17it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.97it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.93it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.86it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.87it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.93it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.01it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.05it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.17it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.12it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.95it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.02it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.94it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.91it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 43.89it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.99it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.12it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.16it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.09it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.02it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.98it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.02it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.94it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.89it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.07it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.14it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.08it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.91it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.00it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.97it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.96it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.03it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.99it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.05it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.09it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.07it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.04it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.92it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.88it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.94it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.02it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.08it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.06it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.05it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.87it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.87it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.93it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.92it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.93it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.06it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.04it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.12it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.04it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.95it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.90it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.94it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.90it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.01it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.97it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.02it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.10it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.00it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.02it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.90it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.98it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.05it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.10it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.78it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.87it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.92it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.98it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.06it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.96it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.94it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.97it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.04it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.08it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.97it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.93it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.97it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.99it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.01it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.90it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.98it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.01it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.08it/s][A                                                 
                                                 [A 40%|████      | 246/615 [01:35<01:42,  3.61it/s]
100%|██████████| 543/543 [00:12<00:00, 44.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:22:18,531 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 04:22:18,560 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:22:20,250 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:22:20,262 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:22:20,269 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246/special_tokens_map.json
 40%|████      | 247/615 [01:37<28:22,  4.63s/it] 40%|████      | 248/615 [01:37<20:19,  3.32s/it] 40%|████      | 249/615 [01:38<14:42,  2.41s/it] 41%|████      | 250/615 [01:38<10:46,  1.77s/it] 41%|████      | 251/615 [01:38<08:02,  1.32s/it] 41%|████      | 252/615 [01:39<06:07,  1.01s/it] 41%|████      | 253/615 [01:39<04:46,  1.26it/s] 41%|████▏     | 254/615 [01:39<03:50,  1.56it/s] 41%|████▏     | 255/615 [01:39<03:11,  1.88it/s] 42%|████▏     | 256/615 [01:40<02:43,  2.19it/s] 42%|████▏     | 257/615 [01:40<02:25,  2.46it/s] 42%|████▏     | 258/615 [01:40<02:11,  2.71it/s] 42%|████▏     | 259/615 [01:40<02:01,  2.93it/s] 42%|████▏     | 260/615 [01:41<01:54,  3.11it/s] 42%|████▏     | 261/615 [01:41<01:49,  3.24it/s] 43%|████▎     | 262/615 [01:41<01:45,  3.35it/s] 43%|████▎     | 263/615 [01:42<01:42,  3.42it/s] 43%|████▎     | 264/615 [01:42<01:40,  3.48it/s] 43%|████▎     | 265/615 [01:42<01:39,  3.52it/s] 43%|████▎     | 266/615 [01:42<01:38,  3.54it/s] 43%|████▎     | 267/615 [01:43<01:37,  3.56it/s] 44%|████▎     | 268/615 [01:43<01:38,  3.54it/s] 44%|████▎     | 269/615 [01:43<01:37,  3.56it/s] 44%|████▍     | 270/615 [01:44<01:36,  3.57it/s] 44%|████▍     | 271/615 [01:44<01:36,  3.57it/s] 44%|████▍     | 272/615 [01:44<01:35,  3.58it/s] 44%|████▍     | 273/615 [01:44<01:35,  3.59it/s] 45%|████▍     | 274/615 [01:45<01:34,  3.60it/s] 45%|████▍     | 275/615 [01:45<01:34,  3.60it/s] 45%|████▍     | 276/615 [01:45<01:34,  3.60it/s] 45%|████▌     | 277/615 [01:45<01:33,  3.61it/s] 45%|████▌     | 278/615 [01:46<01:33,  3.61it/s] 45%|████▌     | 279/615 [01:46<01:33,  3.59it/s] 46%|████▌     | 280/615 [01:46<01:33,  3.60it/s] 46%|████▌     | 281/615 [01:47<01:32,  3.60it/s] 46%|████▌     | 282/615 [01:47<01:32,  3.61it/s] 46%|████▌     | 283/615 [01:47<01:32,  3.61it/s] 46%|████▌     | 284/615 [01:47<01:31,  3.61it/s] 46%|████▋     | 285/615 [01:48<01:31,  3.61it/s] 47%|████▋     | 286/615 [01:48<01:31,  3.61it/s] 47%|████▋     | 287/615 [01:48<01:30,  3.61it/s] 47%|████▋     | 288/615 [01:49<01:30,  3.61it/s] 47%|████▋     | 289/615 [01:49<01:30,  3.61it/s] 47%|████▋     | 290/615 [01:49<01:30,  3.59it/s] 47%|████▋     | 291/615 [01:49<01:30,  3.59it/s] 47%|████▋     | 292/615 [01:50<01:29,  3.60it/s] 48%|████▊     | 293/615 [01:50<01:29,  3.60it/s] 48%|████▊     | 294/615 [01:50<01:29,  3.60it/s] 48%|████▊     | 295/615 [01:50<01:28,  3.61it/s] 48%|████▊     | 296/615 [01:51<01:30,  3.53it/s] 48%|████▊     | 297/615 [01:51<01:29,  3.54it/s] 48%|████▊     | 298/615 [01:51<01:28,  3.56it/s] 49%|████▊     | 299/615 [01:52<01:28,  3.58it/s] 49%|████▉     | 300/615 [01:52<01:27,  3.59it/s] 49%|████▉     | 301/615 [01:52<01:27,  3.58it/s] 49%|████▉     | 302/615 [01:52<01:27,  3.59it/s] 49%|████▉     | 303/615 [01:53<01:26,  3.60it/s] 49%|████▉     | 304/615 [01:53<01:26,  3.59it/s] 50%|████▉     | 305/615 [01:53<01:26,  3.60it/s] 50%|████▉     | 306/615 [01:54<01:25,  3.60it/s] 50%|████▉     | 307/615 [01:54<01:25,  3.60it/s] 50%|█████     | 308/615 [01:54<01:25,  3.61it/s] 50%|█████     | 309/615 [01:54<01:24,  3.61it/s] 50%|█████     | 310/615 [01:55<01:24,  3.61it/s] 51%|█████     | 311/615 [01:55<01:24,  3.61it/s] 51%|█████     | 312/615 [01:55<01:24,  3.58it/s] 51%|█████     | 313/615 [01:56<01:24,  3.59it/s] 51%|█████     | 314/615 [01:56<01:23,  3.60it/s] 51%|█████     | 315/615 [01:56<01:23,  3.60it/s] 51%|█████▏    | 316/615 [01:56<01:22,  3.60it/s] 52%|█████▏    | 317/615 [01:57<01:22,  3.61it/s] 52%|█████▏    | 318/615 [01:57<01:22,  3.61it/s] 52%|█████▏    | 319/615 [01:57<01:22,  3.60it/s] 52%|█████▏    | 320/615 [01:57<01:21,  3.61it/s] 52%|█████▏    | 321/615 [01:58<01:21,  3.60it/s] 52%|█████▏    | 322/615 [01:58<01:21,  3.60it/s] 53%|█████▎    | 323/615 [01:58<01:21,  3.60it/s] 53%|█████▎    | 324/615 [01:59<01:20,  3.61it/s] 53%|█████▎    | 325/615 [01:59<01:20,  3.61it/s] 53%|█████▎    | 326/615 [01:59<01:20,  3.61it/s] 53%|█████▎    | 327/615 [01:59<01:19,  3.61it/s] 53%|█████▎    | 328/615 [02:00<01:19,  3.60it/s] 53%|█████▎    | 329/615 [02:00<01:19,  3.60it/s] 54%|█████▎    | 330/615 [02:00<01:19,  3.60it/s] 54%|█████▍    | 331/615 [02:01<01:18,  3.60it/s] 54%|█████▍    | 332/615 [02:01<01:18,  3.58it/s] 54%|█████▍    | 333/615 [02:01<01:18,  3.59it/s] 54%|█████▍    | 334/615 [02:01<01:18,  3.59it/s] 54%|█████▍    | 335/615 [02:02<01:17,  3.60it/s] 55%|█████▍    | 336/615 [02:02<01:17,  3.60it/s] 55%|█████▍    | 337/615 [02:02<01:17,  3.60it/s] 55%|█████▍    | 338/615 [02:02<01:16,  3.60it/s] 55%|█████▌    | 339/615 [02:03<01:16,  3.60it/s] 55%|█████▌    | 340/615 [02:03<01:16,  3.60it/s] 55%|█████▌    | 341/615 [02:03<01:15,  3.61it/s] 56%|█████▌    | 342/615 [02:04<01:15,  3.61it/s] 56%|█████▌    | 343/615 [02:04<01:15,  3.59it/s] 56%|█████▌    | 344/615 [02:04<01:15,  3.60it/s] 56%|█████▌    | 345/615 [02:04<01:14,  3.60it/s] 56%|█████▋    | 346/615 [02:05<01:14,  3.60it/s] 56%|█████▋    | 347/615 [02:05<01:14,  3.60it/s] 57%|█████▋    | 348/615 [02:05<01:14,  3.61it/s] 57%|█████▋    | 349/615 [02:05<01:13,  3.61it/s] 57%|█████▋    | 350/615 [02:06<01:13,  3.61it/s] 57%|█████▋    | 351/615 [02:06<01:13,  3.61it/s] 57%|█████▋    | 352/615 [02:06<01:12,  3.61it/s] 57%|█████▋    | 353/615 [02:07<01:12,  3.61it/s] 58%|█████▊    | 354/615 [02:07<01:12,  3.59it/s] 58%|█████▊    | 355/615 [02:07<01:12,  3.60it/s] 58%|█████▊    | 356/615 [02:07<01:11,  3.60it/s] 58%|█████▊    | 357/615 [02:08<01:11,  3.60it/s] 58%|█████▊    | 358/615 [02:08<01:11,  3.60it/s] 58%|█████▊    | 359/615 [02:08<01:11,  3.60it/s] 59%|█████▊    | 360/615 [02:09<01:10,  3.61it/s] 59%|█████▊    | 361/615 [02:09<01:10,  3.61it/s] 59%|█████▉    | 362/615 [02:09<01:10,  3.61it/s] 59%|█████▉    | 363/615 [02:09<01:09,  3.61it/s] 59%|█████▉    | 364/615 [02:10<01:09,  3.61it/s] 59%|█████▉    | 365/615 [02:10<01:09,  3.59it/s] 60%|█████▉    | 366/615 [02:10<01:09,  3.59it/s] 60%|█████▉    | 367/615 [02:10<01:08,  3.60it/s] 60%|█████▉    | 368/615 [02:11<01:08,  3.60it/s] 60%|██████    | 369/615 [02:11<01:08,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 04:22:54,881 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:22:54,881 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 04:22:54,881 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3376, 'eval_samples_per_second': 351.932, 'eval_steps_per_second': 44.012, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.65it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.83it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.11it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.16it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.78it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.38it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.20it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.27it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.29it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.26it/s][A
 10%|█         | 57/543 [00:01<00:11, 44.17it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.15it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.02it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.02it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.92it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.97it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.95it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.15it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.15it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.00it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.09it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.98it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.04it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.96it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.87it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.92it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.13it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.03it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.02it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.93it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.97it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.94it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.96it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.99it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.01it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.10it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.17it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.09it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.06it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.93it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.92it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.86it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.91it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.04it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.16it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.14it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.11it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.01it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.01it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.01it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.81it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.99it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.09it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.03it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.11it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.10it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.00it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.01it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.91it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.86it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.02it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.07it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.07it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.08it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.96it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.04it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.84it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.97it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.00it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.13it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.12it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.16it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.98it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.96it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.96it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.02it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.99it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.94it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.11it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.13it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.09it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.05it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.93it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.91it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.93it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.00it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.01it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.98it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.08it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.12it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.04it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.95it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.93it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.94it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.03it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.04it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.07it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.13it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.06it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.91it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.01it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.95it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.84it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.98it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.07it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.13it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.15it/s][A                                                 
                                                 [A 60%|██████    | 369/615 [02:24<01:08,  3.60it/s]
100%|██████████| 543/543 [00:12<00:00, 44.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:23:07,229 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-28 04:23:07,248 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:23:08,777 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:23:08,794 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:23:08,807 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369/special_tokens_map.json
 60%|██████    | 370/615 [02:26<18:41,  4.58s/it] 60%|██████    | 371/615 [02:26<13:22,  3.29s/it] 60%|██████    | 372/615 [02:26<09:40,  2.39s/it] 61%|██████    | 373/615 [02:27<07:04,  1.76s/it] 61%|██████    | 374/615 [02:27<05:16,  1.31s/it] 61%|██████    | 375/615 [02:27<04:00,  1.00s/it] 61%|██████    | 376/615 [02:27<03:08,  1.27it/s] 61%|██████▏   | 377/615 [02:28<02:31,  1.57it/s] 61%|██████▏   | 378/615 [02:28<02:05,  1.89it/s] 62%|██████▏   | 379/615 [02:28<01:47,  2.20it/s] 62%|██████▏   | 380/615 [02:28<01:34,  2.48it/s] 62%|██████▏   | 381/615 [02:29<01:25,  2.73it/s] 62%|██████▏   | 382/615 [02:29<01:19,  2.93it/s] 62%|██████▏   | 383/615 [02:29<01:14,  3.10it/s] 62%|██████▏   | 384/615 [02:30<01:11,  3.22it/s] 63%|██████▎   | 385/615 [02:30<01:09,  3.32it/s] 63%|██████▎   | 386/615 [02:30<01:07,  3.39it/s] 63%|██████▎   | 387/615 [02:30<01:06,  3.44it/s] 63%|██████▎   | 388/615 [02:31<01:05,  3.48it/s] 63%|██████▎   | 389/615 [02:31<01:04,  3.50it/s] 63%|██████▎   | 390/615 [02:31<01:03,  3.52it/s] 64%|██████▎   | 391/615 [02:32<01:03,  3.52it/s] 64%|██████▎   | 392/615 [02:32<01:03,  3.53it/s] 64%|██████▍   | 393/615 [02:32<01:02,  3.55it/s] 64%|██████▍   | 394/615 [02:32<01:01,  3.57it/s] 64%|██████▍   | 395/615 [02:33<01:01,  3.59it/s] 64%|██████▍   | 396/615 [02:33<01:00,  3.59it/s] 65%|██████▍   | 397/615 [02:33<01:00,  3.60it/s] 65%|██████▍   | 398/615 [02:34<01:00,  3.60it/s] 65%|██████▍   | 399/615 [02:34<00:59,  3.61it/s] 65%|██████▌   | 400/615 [02:34<00:59,  3.61it/s] 65%|██████▌   | 401/615 [02:34<00:59,  3.61it/s] 65%|██████▌   | 402/615 [02:35<00:59,  3.60it/s] 66%|██████▌   | 403/615 [02:35<00:58,  3.61it/s] 66%|██████▌   | 404/615 [02:35<00:58,  3.61it/s] 66%|██████▌   | 405/615 [02:35<00:58,  3.61it/s] 66%|██████▌   | 406/615 [02:36<00:57,  3.61it/s] 66%|██████▌   | 407/615 [02:36<00:57,  3.60it/s] 66%|██████▋   | 408/615 [02:36<00:57,  3.61it/s] 67%|██████▋   | 409/615 [02:37<00:57,  3.61it/s] 67%|██████▋   | 410/615 [02:37<00:56,  3.61it/s] 67%|██████▋   | 411/615 [02:37<00:56,  3.61it/s] 67%|██████▋   | 412/615 [02:37<00:56,  3.61it/s] 67%|██████▋   | 413/615 [02:38<00:56,  3.59it/s] 67%|██████▋   | 414/615 [02:38<00:55,  3.59it/s] 67%|██████▋   | 415/615 [02:38<00:55,  3.60it/s] 68%|██████▊   | 416/615 [02:39<00:55,  3.60it/s] 68%|██████▊   | 417/615 [02:39<00:54,  3.60it/s] 68%|██████▊   | 418/615 [02:39<00:54,  3.60it/s] 68%|██████▊   | 419/615 [02:39<00:54,  3.61it/s] 68%|██████▊   | 420/615 [02:40<00:54,  3.61it/s] 68%|██████▊   | 421/615 [02:40<00:53,  3.61it/s] 69%|██████▊   | 422/615 [02:40<00:53,  3.61it/s] 69%|██████▉   | 423/615 [02:40<00:53,  3.61it/s] 69%|██████▉   | 424/615 [02:41<00:53,  3.60it/s] 69%|██████▉   | 425/615 [02:41<00:52,  3.60it/s] 69%|██████▉   | 426/615 [02:41<00:52,  3.60it/s] 69%|██████▉   | 427/615 [02:42<00:52,  3.61it/s] 70%|██████▉   | 428/615 [02:42<00:51,  3.61it/s] 70%|██████▉   | 429/615 [02:42<00:51,  3.61it/s] 70%|██████▉   | 430/615 [02:42<00:51,  3.61it/s] 70%|███████   | 431/615 [02:43<00:51,  3.61it/s] 70%|███████   | 432/615 [02:43<00:50,  3.61it/s] 70%|███████   | 433/615 [02:43<00:50,  3.61it/s] 71%|███████   | 434/615 [02:43<00:50,  3.61it/s] 71%|███████   | 435/615 [02:44<00:50,  3.60it/s] 71%|███████   | 436/615 [02:44<00:49,  3.60it/s] 71%|███████   | 437/615 [02:44<00:49,  3.60it/s] 71%|███████   | 438/615 [02:45<00:49,  3.60it/s] 71%|███████▏  | 439/615 [02:45<00:48,  3.60it/s] 72%|███████▏  | 440/615 [02:45<00:48,  3.59it/s] 72%|███████▏  | 441/615 [02:45<00:48,  3.59it/s] 72%|███████▏  | 442/615 [02:46<00:48,  3.60it/s] 72%|███████▏  | 443/615 [02:46<00:47,  3.60it/s] 72%|███████▏  | 444/615 [02:46<00:47,  3.60it/s] 72%|███████▏  | 445/615 [02:47<00:47,  3.60it/s] 73%|███████▎  | 446/615 [02:47<00:47,  3.58it/s] 73%|███████▎  | 447/615 [02:47<00:46,  3.59it/s] 73%|███████▎  | 448/615 [02:47<00:46,  3.59it/s] 73%|███████▎  | 449/615 [02:48<00:46,  3.59it/s] 73%|███████▎  | 450/615 [02:48<00:45,  3.60it/s] 73%|███████▎  | 451/615 [02:48<00:45,  3.60it/s] 73%|███████▎  | 452/615 [02:49<00:45,  3.60it/s] 74%|███████▎  | 453/615 [02:49<00:44,  3.60it/s] 74%|███████▍  | 454/615 [02:49<00:44,  3.61it/s] 74%|███████▍  | 455/615 [02:49<00:44,  3.60it/s] 74%|███████▍  | 456/615 [02:50<00:44,  3.59it/s] 74%|███████▍  | 457/615 [02:50<00:44,  3.59it/s] 74%|███████▍  | 458/615 [02:50<00:43,  3.59it/s] 75%|███████▍  | 459/615 [02:50<00:43,  3.60it/s] 75%|███████▍  | 460/615 [02:51<00:43,  3.60it/s] 75%|███████▍  | 461/615 [02:51<00:44,  3.49it/s] 75%|███████▌  | 462/615 [02:51<00:43,  3.51it/s] 75%|███████▌  | 463/615 [02:52<00:42,  3.54it/s] 75%|███████▌  | 464/615 [02:52<00:42,  3.56it/s] 76%|███████▌  | 465/615 [02:52<00:41,  3.57it/s] 76%|███████▌  | 466/615 [02:52<00:41,  3.58it/s] 76%|███████▌  | 467/615 [02:53<00:41,  3.59it/s] 76%|███████▌  | 468/615 [02:53<00:41,  3.58it/s] 76%|███████▋  | 469/615 [02:53<00:40,  3.59it/s] 76%|███████▋  | 470/615 [02:54<00:40,  3.60it/s] 77%|███████▋  | 471/615 [02:54<00:39,  3.60it/s] 77%|███████▋  | 472/615 [02:54<00:39,  3.60it/s] 77%|███████▋  | 473/615 [02:54<00:39,  3.61it/s] 77%|███████▋  | 474/615 [02:55<00:39,  3.61it/s] 77%|███████▋  | 475/615 [02:55<00:38,  3.61it/s] 77%|███████▋  | 476/615 [02:55<00:38,  3.61it/s] 78%|███████▊  | 477/615 [02:55<00:38,  3.61it/s] 78%|███████▊  | 478/615 [02:56<00:37,  3.61it/s] 78%|███████▊  | 479/615 [02:56<00:37,  3.58it/s] 78%|███████▊  | 480/615 [02:56<00:37,  3.59it/s] 78%|███████▊  | 481/615 [02:57<00:37,  3.60it/s] 78%|███████▊  | 482/615 [02:57<00:36,  3.60it/s] 79%|███████▊  | 483/615 [02:57<00:36,  3.60it/s] 79%|███████▊  | 484/615 [02:57<00:36,  3.60it/s] 79%|███████▉  | 485/615 [02:58<00:36,  3.61it/s] 79%|███████▉  | 486/615 [02:58<00:35,  3.61it/s] 79%|███████▉  | 487/615 [02:58<00:35,  3.61it/s] 79%|███████▉  | 488/615 [02:59<00:35,  3.61it/s] 80%|███████▉  | 489/615 [02:59<00:34,  3.61it/s] 80%|███████▉  | 490/615 [02:59<00:34,  3.59it/s] 80%|███████▉  | 491/615 [02:59<00:34,  3.59it/s] 80%|████████  | 492/615 [03:00<00:34,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 04:23:43,482 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:23:43,482 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 04:23:43,482 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3319, 'eval_samples_per_second': 352.095, 'eval_steps_per_second': 44.032, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.90it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.78it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.56it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.47it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.80it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.51it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.31it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.09it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.14it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.13it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.28it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.28it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.10it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.07it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.91it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.92it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.92it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.08it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.16it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.10it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.08it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.00it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.99it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.98it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.96it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.10it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.21it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.24it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.13it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.96it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.94it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.98it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.94it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.02it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.09it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.11it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.15it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.03it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.99it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.00it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.91it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.00it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.02it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.06it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.10it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.10it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.98it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.94it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.00it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.03it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.04it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.03it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.10it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.08it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.10it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.02it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.93it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.97it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.97it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.98it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.13it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.04it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.96it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.03it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.03it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.96it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.02it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.00it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.04it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.05it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.95it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.06it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.09it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.08it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.09it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.02it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.03it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 43.99it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.01it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.97it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.03it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.07it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.06it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.07it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.08it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.11it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.00it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.00it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.01it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.06it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.07it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.01it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.02it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.00it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.05it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.99it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.92it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.02it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.02it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.00it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.03it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.04it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.94it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.95it/s][A                                                 
                                                 [A 80%|████████  | 492/615 [03:12<00:34,  3.60it/s]
100%|██████████| 543/543 [00:12<00:00, 43.95it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:23:55,830 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 04:23:55,855 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:23:57,552 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:23:57,563 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:23:57,576 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492/special_tokens_map.json
 80%|████████  | 493/615 [03:14<09:25,  4.63s/it] 80%|████████  | 494/615 [03:15<06:42,  3.33s/it] 80%|████████  | 495/615 [03:15<04:49,  2.41s/it] 81%|████████  | 496/615 [03:15<03:30,  1.77s/it] 81%|████████  | 497/615 [03:16<02:36,  1.32s/it] 81%|████████  | 498/615 [03:16<01:58,  1.01s/it] 81%|████████  | 499/615 [03:16<01:31,  1.27it/s] 81%|████████▏ | 500/615 [03:16<01:13,  1.57it/s]                                                  81%|████████▏ | 500/615 [03:16<01:13,  1.57it/s] 81%|████████▏ | 501/615 [03:17<01:00,  1.89it/s] 82%|████████▏ | 502/615 [03:17<00:51,  2.21it/s] 82%|████████▏ | 503/615 [03:17<00:44,  2.50it/s] 82%|████████▏ | 504/615 [03:17<00:40,  2.75it/s] 82%|████████▏ | 505/615 [03:18<00:37,  2.96it/s] 82%|████████▏ | 506/615 [03:18<00:34,  3.13it/s] 82%|████████▏ | 507/615 [03:18<00:33,  3.26it/s] 83%|████████▎ | 508/615 [03:19<00:31,  3.36it/s] 83%|████████▎ | 509/615 [03:19<00:30,  3.43it/s] 83%|████████▎ | 510/615 [03:19<00:30,  3.48it/s] 83%|████████▎ | 511/615 [03:19<00:29,  3.51it/s] 83%|████████▎ | 512/615 [03:20<00:29,  3.54it/s] 83%|████████▎ | 513/615 [03:20<00:28,  3.56it/s] 84%|████████▎ | 514/615 [03:20<00:28,  3.58it/s] 84%|████████▎ | 515/615 [03:21<00:27,  3.59it/s] 84%|████████▍ | 516/615 [03:21<00:27,  3.54it/s] 84%|████████▍ | 517/615 [03:21<00:27,  3.56it/s] 84%|████████▍ | 518/615 [03:21<00:27,  3.58it/s] 84%|████████▍ | 519/615 [03:22<00:26,  3.58it/s] 85%|████████▍ | 520/615 [03:22<00:26,  3.59it/s] 85%|████████▍ | 521/615 [03:22<00:26,  3.60it/s] 85%|████████▍ | 522/615 [03:22<00:25,  3.60it/s] 85%|████████▌ | 523/615 [03:23<00:25,  3.61it/s] 85%|████████▌ | 524/615 [03:23<00:25,  3.61it/s] 85%|████████▌ | 525/615 [03:23<00:24,  3.61it/s] 86%|████████▌ | 526/615 [03:24<00:24,  3.61it/s] 86%|████████▌ | 527/615 [03:24<00:24,  3.60it/s] 86%|████████▌ | 528/615 [03:24<00:24,  3.60it/s] 86%|████████▌ | 529/615 [03:24<00:23,  3.61it/s] 86%|████████▌ | 530/615 [03:25<00:23,  3.61it/s] 86%|████████▋ | 531/615 [03:25<00:23,  3.61it/s] 87%|████████▋ | 532/615 [03:25<00:22,  3.61it/s] 87%|████████▋ | 533/615 [03:26<00:22,  3.61it/s] 87%|████████▋ | 534/615 [03:26<00:22,  3.61it/s] 87%|████████▋ | 535/615 [03:26<00:22,  3.61it/s] 87%|████████▋ | 536/615 [03:26<00:21,  3.62it/s] 87%|████████▋ | 537/615 [03:27<00:21,  3.62it/s] 87%|████████▋ | 538/615 [03:27<00:21,  3.60it/s] 88%|████████▊ | 539/615 [03:27<00:21,  3.61it/s] 88%|████████▊ | 540/615 [03:27<00:20,  3.61it/s] 88%|████████▊ | 541/615 [03:28<00:20,  3.61it/s] 88%|████████▊ | 542/615 [03:28<00:20,  3.61it/s] 88%|████████▊ | 543/615 [03:28<00:19,  3.61it/s] 88%|████████▊ | 544/615 [03:29<00:19,  3.61it/s] 89%|████████▊ | 545/615 [03:29<00:19,  3.61it/s] 89%|████████▉ | 546/615 [03:29<00:19,  3.61it/s] 89%|████████▉ | 547/615 [03:29<00:18,  3.61it/s] 89%|████████▉ | 548/615 [03:30<00:18,  3.61it/s] 89%|████████▉ | 549/615 [03:30<00:18,  3.61it/s] 89%|████████▉ | 550/615 [03:30<00:18,  3.61it/s] 90%|████████▉ | 551/615 [03:31<00:17,  3.61it/s] 90%|████████▉ | 552/615 [03:31<00:17,  3.61it/s] 90%|████████▉ | 553/615 [03:31<00:17,  3.61it/s] 90%|█████████ | 554/615 [03:31<00:16,  3.61it/s] 90%|█████████ | 555/615 [03:32<00:16,  3.61it/s] 90%|█████████ | 556/615 [03:32<00:16,  3.61it/s] 91%|█████████ | 557/615 [03:32<00:16,  3.61it/s] 91%|█████████ | 558/615 [03:32<00:15,  3.61it/s] 91%|█████████ | 559/615 [03:33<00:15,  3.61it/s] 91%|█████████ | 560/615 [03:33<00:15,  3.60it/s] 91%|█████████ | 561/615 [03:33<00:14,  3.61it/s] 91%|█████████▏| 562/615 [03:34<00:14,  3.61it/s] 92%|█████████▏| 563/615 [03:34<00:14,  3.61it/s] 92%|█████████▏| 564/615 [03:34<00:14,  3.61it/s] 92%|█████████▏| 565/615 [03:34<00:13,  3.61it/s] 92%|█████████▏| 566/615 [03:35<00:13,  3.61it/s] 92%|█████████▏| 567/615 [03:35<00:13,  3.61it/s] 92%|█████████▏| 568/615 [03:35<00:13,  3.61it/s] 93%|█████████▎| 569/615 [03:36<00:12,  3.61it/s] 93%|█████████▎| 570/615 [03:36<00:12,  3.61it/s] 93%|█████████▎| 571/615 [03:36<00:12,  3.58it/s] 93%|█████████▎| 572/615 [03:36<00:11,  3.59it/s] 93%|█████████▎| 573/615 [03:37<00:11,  3.60it/s] 93%|█████████▎| 574/615 [03:37<00:11,  3.60it/s] 93%|█████████▎| 575/615 [03:37<00:11,  3.61it/s] 94%|█████████▎| 576/615 [03:37<00:10,  3.61it/s] 94%|█████████▍| 577/615 [03:38<00:10,  3.61it/s] 94%|█████████▍| 578/615 [03:38<00:10,  3.61it/s] 94%|█████████▍| 579/615 [03:38<00:09,  3.61it/s] 94%|█████████▍| 580/615 [03:39<00:09,  3.61it/s] 94%|█████████▍| 581/615 [03:39<00:09,  3.61it/s] 95%|█████████▍| 582/615 [03:39<00:09,  3.60it/s] 95%|█████████▍| 583/615 [03:39<00:08,  3.61it/s] 95%|█████████▍| 584/615 [03:40<00:08,  3.61it/s] 95%|█████████▌| 585/615 [03:40<00:08,  3.61it/s] 95%|█████████▌| 586/615 [03:40<00:08,  3.61it/s] 95%|█████████▌| 587/615 [03:41<00:07,  3.61it/s] 96%|█████████▌| 588/615 [03:41<00:07,  3.61it/s] 96%|█████████▌| 589/615 [03:41<00:07,  3.61it/s] 96%|█████████▌| 590/615 [03:41<00:06,  3.61it/s] 96%|█████████▌| 591/615 [03:42<00:06,  3.61it/s] 96%|█████████▋| 592/615 [03:42<00:06,  3.61it/s] 96%|█████████▋| 593/615 [03:42<00:06,  3.59it/s] 97%|█████████▋| 594/615 [03:42<00:05,  3.60it/s] 97%|█████████▋| 595/615 [03:43<00:05,  3.60it/s] 97%|█████████▋| 596/615 [03:43<00:05,  3.60it/s] 97%|█████████▋| 597/615 [03:43<00:04,  3.61it/s] 97%|█████████▋| 598/615 [03:44<00:04,  3.61it/s] 97%|█████████▋| 599/615 [03:44<00:04,  3.61it/s] 98%|█████████▊| 600/615 [03:44<00:04,  3.61it/s] 98%|█████████▊| 601/615 [03:44<00:03,  3.61it/s] 98%|█████████▊| 602/615 [03:45<00:03,  3.61it/s] 98%|█████████▊| 603/615 [03:45<00:03,  3.61it/s] 98%|█████████▊| 604/615 [03:45<00:03,  3.58it/s] 98%|█████████▊| 605/615 [03:46<00:02,  3.59it/s] 99%|█████████▊| 606/615 [03:46<00:02,  3.60it/s] 99%|█████████▊| 607/615 [03:46<00:02,  3.61it/s] 99%|█████████▉| 608/615 [03:46<00:01,  3.61it/s] 99%|█████████▉| 609/615 [03:47<00:01,  3.61it/s] 99%|█████████▉| 610/615 [03:47<00:01,  3.61it/s] 99%|█████████▉| 611/615 [03:47<00:01,  3.61it/s]100%|█████████▉| 612/615 [03:47<00:00,  3.61it/s]100%|█████████▉| 613/615 [03:48<00:00,  3.61it/s]100%|█████████▉| 614/615 [03:48<00:00,  3.61it/s]100%|██████████| 615/615 [03:48<00:00,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 04:24:31,987 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:24:31,987 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 04:24:31,987 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3273, 'eval_samples_per_second': 352.227, 'eval_steps_per_second': 44.049, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.28it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.21it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.18it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.39it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.93it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.28it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.24it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.16it/s][A
  9%|▊         | 47/543 [00:01<00:11, 43.56it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.56it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.47it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.41it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.24it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.08it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.96it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.97it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.03it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.16it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.35it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.32it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.22it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.04it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.08it/s][A
 22%|██▏       | 122/543 [00:02<00:10, 41.73it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 42.97it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.39it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.71it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 43.96it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.00it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.94it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.91it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.87it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.63it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.90it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 44.05it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.16it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.24it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.23it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.13it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.00it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.99it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.80it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.87it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.89it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.18it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.30it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.17it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.19it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.09it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.05it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.91it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.99it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.00it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.13it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.16it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.17it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.14it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.04it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.98it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.98it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.14it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.14it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.17it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.20it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.14it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.00it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.95it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.90it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.02it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.05it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.18it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.23it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.12it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.04it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.95it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.01it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.06it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.04it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.06it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.20it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.18it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.12it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.01it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.99it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.00it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.99it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.06it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.07it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.17it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.12it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.04it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.05it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.03it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.03it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.05it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.16it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.53it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.29it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.20it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.08it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.09it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.10it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.09it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.10it/s][A                                                 
                                                 [A100%|██████████| 615/615 [04:01<00:00,  3.58it/s]
100%|██████████| 543/543 [00:12<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:24:44,339 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-28 04:24:44,355 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:24:46,312 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:24:46,323 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:24:46,333 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 04:24:46,628 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 04:24:46,628 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123 (score: 1.0794728994369507).
                                                 100%|██████████| 615/615 [04:05<00:00,  3.58it/s]100%|██████████| 615/615 [04:05<00:00,  2.51it/s]
[INFO|trainer.py:1894] 2023-08-28 04:24:48,291 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 04:24:48,310 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:24:49,930 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:24:49,946 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:24:49,956 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:24:50,136 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:24:50,136 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:24:50,136 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:24:50,136 >>   train_runtime            = 0:04:05.08
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:24:50,136 >>   train_samples            =       7900
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:24:50,136 >>   train_samples_per_second =    161.171
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:24:50,137 >>   train_steps_per_second   =      2.509
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3294, 'eval_samples_per_second': 352.168, 'eval_steps_per_second': 44.041, 'epoch': 5.0}
{'train_runtime': 245.0818, 'train_samples_per_second': 161.171, 'train_steps_per_second': 2.509, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 04:24:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 04:24:50,182 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:24:50,183 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 04:24:50,183 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.44it/s]  2%|▏         | 12/543 [00:00<00:10, 48.55it/s]  3%|▎         | 17/543 [00:00<00:11, 46.94it/s]  4%|▍         | 22/543 [00:00<00:11, 46.15it/s]  5%|▍         | 27/543 [00:00<00:11, 45.73it/s]  6%|▌         | 32/543 [00:00<00:11, 45.43it/s]  7%|▋         | 37/543 [00:00<00:11, 45.19it/s]  8%|▊         | 42/543 [00:00<00:11, 44.38it/s]  9%|▊         | 47/543 [00:01<00:11, 43.97it/s] 10%|▉         | 52/543 [00:01<00:11, 43.75it/s] 10%|█         | 57/543 [00:01<00:11, 43.93it/s] 11%|█▏        | 62/543 [00:01<00:10, 44.07it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.19it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.39it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.37it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.32it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.09it/s] 17%|█▋        | 92/543 [00:02<00:10, 43.88it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.75it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.92it/s] 20%|█▉        | 107/543 [00:02<00:09, 44.12it/s] 21%|██        | 112/543 [00:02<00:09, 44.26it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.33it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.19it/s] 23%|██▎       | 127/543 [00:02<00:09, 44.13it/s] 24%|██▍       | 132/543 [00:02<00:09, 43.92it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.77it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.74it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.94it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.10it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.17it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.22it/s] 31%|███       | 167/543 [00:03<00:08, 44.15it/s] 32%|███▏      | 172/543 [00:03<00:08, 43.97it/s] 33%|███▎      | 177/543 [00:03<00:08, 43.86it/s] 34%|███▎      | 182/543 [00:04<00:08, 43.81it/s] 34%|███▍      | 187/543 [00:04<00:08, 43.75it/s] 35%|███▌      | 192/543 [00:04<00:07, 43.98it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.11it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.21it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.30it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.16it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.14it/s] 41%|████      | 222/543 [00:05<00:07, 44.06it/s] 42%|████▏     | 227/543 [00:05<00:07, 43.93it/s] 43%|████▎     | 232/543 [00:05<00:07, 44.04it/s] 44%|████▎     | 237/543 [00:05<00:06, 44.23it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.32it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.45it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.36it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.25it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.25it/s] 49%|████▉     | 267/543 [00:06<00:06, 44.08it/s] 50%|█████     | 272/543 [00:06<00:06, 44.02it/s] 51%|█████     | 277/543 [00:06<00:06, 43.99it/s] 52%|█████▏    | 282/543 [00:06<00:05, 44.14it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.23it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.34it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.39it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.30it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.23it/s] 57%|█████▋    | 312/543 [00:07<00:05, 44.14it/s] 58%|█████▊    | 317/543 [00:07<00:05, 44.13it/s] 59%|█████▉    | 322/543 [00:07<00:05, 44.13it/s] 60%|██████    | 327/543 [00:07<00:04, 44.12it/s] 61%|██████    | 332/543 [00:07<00:04, 44.18it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.29it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.29it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.24it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.22it/s] 66%|██████▌   | 357/543 [00:08<00:04, 44.20it/s] 67%|██████▋   | 362/543 [00:08<00:04, 44.08it/s] 68%|██████▊   | 367/543 [00:08<00:03, 44.19it/s] 69%|██████▊   | 372/543 [00:08<00:03, 44.20it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.22it/s] 70%|███████   | 382/543 [00:08<00:03, 44.32it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.36it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.19it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.13it/s] 74%|███████▍  | 402/543 [00:09<00:03, 44.18it/s] 75%|███████▍  | 407/543 [00:09<00:03, 44.13it/s] 76%|███████▌  | 412/543 [00:09<00:02, 44.16it/s] 77%|███████▋  | 417/543 [00:09<00:02, 44.07it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.22it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.27it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.24it/s] 80%|████████  | 437/543 [00:09<00:02, 44.23it/s] 81%|████████▏ | 442/543 [00:09<00:02, 44.20it/s] 82%|████████▏ | 447/543 [00:10<00:02, 44.16it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.18it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.17it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.19it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.16it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.09it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.19it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.20it/s] 90%|████████▉ | 487/543 [00:11<00:01, 44.21it/s] 91%|█████████ | 492/543 [00:11<00:01, 44.08it/s] 92%|█████████▏| 497/543 [00:11<00:01, 44.21it/s] 92%|█████████▏| 502/543 [00:11<00:00, 44.26it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.19it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.27it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.22it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.23it/s] 97%|█████████▋| 527/543 [00:11<00:00, 44.25it/s] 98%|█████████▊| 532/543 [00:12<00:00, 44.15it/s] 99%|█████████▉| 537/543 [00:12<00:00, 44.14it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.20it/s]100%|██████████| 543/543 [00:12<00:00, 44.23it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:25:02,475 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:25:02,475 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:25:02,475 >>   eval_loss               =     1.0795
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:25:02,475 >>   eval_runtime            = 0:00:12.29
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:25:02,475 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:25:02,475 >>   eval_samples_per_second =    353.239
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:25:02,475 >>   eval_steps_per_second   =     44.175
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:25:02,475 >>   perplexity              =     2.9431
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:09,618 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:09,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:09,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:09,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:09,625 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:25:10,396 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:25:10,396 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:25:11,107 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:25:12,169 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:25:12,169 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:15,186 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:15,188 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:15,188 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:15,188 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:25:15,188 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:25:15,837 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:25:15,837 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:25:16,410 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:25:16,580 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:25:16,580 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.84it/s]Extractor Predicting: 2it [00:01,  1.81it/s]Extractor Predicting: 3it [00:01,  1.84it/s]Extractor Predicting: 4it [00:02,  1.92it/s]Extractor Predicting: 5it [00:02,  1.88it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 7it [00:03,  1.85it/s]Extractor Predicting: 8it [00:04,  1.87it/s]Extractor Predicting: 9it [00:04,  1.95it/s]Extractor Predicting: 10it [00:05,  1.96it/s]Extractor Predicting: 11it [00:05,  1.98it/s]Extractor Predicting: 12it [00:06,  1.98it/s]Extractor Predicting: 13it [00:06,  1.88it/s]Extractor Predicting: 14it [00:07,  1.83it/s]Extractor Predicting: 15it [00:08,  1.78it/s]Extractor Predicting: 16it [00:08,  1.78it/s]Extractor Predicting: 17it [00:09,  1.77it/s]Extractor Predicting: 18it [00:09,  1.74it/s]Extractor Predicting: 19it [00:10,  1.68it/s]Extractor Predicting: 20it [00:11,  1.64it/s]Extractor Predicting: 21it [00:11,  1.67it/s]Extractor Predicting: 22it [00:12,  1.72it/s]Extractor Predicting: 23it [00:12,  1.73it/s]Extractor Predicting: 24it [00:13,  1.72it/s]Extractor Predicting: 25it [00:13,  1.72it/s]Extractor Predicting: 26it [00:14,  1.71it/s]Extractor Predicting: 27it [00:15,  1.69it/s]Extractor Predicting: 28it [00:15,  1.72it/s]Extractor Predicting: 29it [00:16,  1.69it/s]Extractor Predicting: 30it [00:17,  1.57it/s]Extractor Predicting: 31it [00:17,  1.61it/s]Extractor Predicting: 32it [00:18,  1.65it/s]Extractor Predicting: 33it [00:18,  1.68it/s]Extractor Predicting: 34it [00:19,  1.67it/s]Extractor Predicting: 35it [00:19,  1.66it/s]Extractor Predicting: 36it [00:20,  1.64it/s]Extractor Predicting: 37it [00:21,  1.64it/s]Extractor Predicting: 38it [00:21,  1.65it/s]Extractor Predicting: 39it [00:22,  1.69it/s]Extractor Predicting: 40it [00:22,  1.70it/s]Extractor Predicting: 41it [00:23,  1.69it/s]Extractor Predicting: 42it [00:24,  1.67it/s]Extractor Predicting: 43it [00:24,  1.71it/s]Extractor Predicting: 44it [00:25,  1.72it/s]Extractor Predicting: 45it [00:25,  1.73it/s]Extractor Predicting: 46it [00:26,  1.75it/s]Extractor Predicting: 47it [00:26,  1.74it/s]Extractor Predicting: 48it [00:27,  1.72it/s]Extractor Predicting: 49it [00:28,  1.71it/s]Extractor Predicting: 50it [00:28,  1.73it/s]Extractor Predicting: 51it [00:29,  1.73it/s]Extractor Predicting: 52it [00:29,  1.71it/s]Extractor Predicting: 53it [00:30,  1.68it/s]Extractor Predicting: 54it [00:31,  1.72it/s]Extractor Predicting: 55it [00:31,  1.75it/s]Extractor Predicting: 56it [00:32,  1.71it/s]Extractor Predicting: 57it [00:32,  1.69it/s]Extractor Predicting: 58it [00:33,  1.74it/s]Extractor Predicting: 59it [00:34,  1.68it/s]Extractor Predicting: 60it [00:34,  1.70it/s]Extractor Predicting: 61it [00:35,  1.71it/s]Extractor Predicting: 62it [00:35,  1.71it/s]Extractor Predicting: 63it [00:36,  1.72it/s]Extractor Predicting: 64it [00:36,  1.72it/s]Extractor Predicting: 65it [00:37,  1.78it/s]Extractor Predicting: 66it [00:38,  1.77it/s]Extractor Predicting: 67it [00:38,  1.76it/s]Extractor Predicting: 68it [00:39,  1.72it/s]Extractor Predicting: 69it [00:39,  1.74it/s]Extractor Predicting: 70it [00:40,  1.73it/s]Extractor Predicting: 71it [00:40,  1.71it/s]Extractor Predicting: 72it [00:41,  1.72it/s]Extractor Predicting: 73it [00:42,  1.70it/s]Extractor Predicting: 74it [00:42,  1.68it/s]Extractor Predicting: 75it [00:43,  1.63it/s]Extractor Predicting: 76it [00:43,  1.66it/s]Extractor Predicting: 77it [00:44,  1.70it/s]Extractor Predicting: 78it [00:45,  1.69it/s]Extractor Predicting: 79it [00:45,  1.68it/s]Extractor Predicting: 80it [00:46,  1.70it/s]Extractor Predicting: 81it [00:46,  1.71it/s]Extractor Predicting: 82it [00:47,  1.71it/s]Extractor Predicting: 83it [00:48,  1.73it/s]Extractor Predicting: 84it [00:48,  1.73it/s]Extractor Predicting: 85it [00:49,  1.74it/s]Extractor Predicting: 86it [00:49,  1.71it/s]Extractor Predicting: 87it [00:50,  1.72it/s]Extractor Predicting: 88it [00:50,  1.75it/s]Extractor Predicting: 89it [00:51,  1.77it/s]Extractor Predicting: 90it [00:52,  1.73it/s]Extractor Predicting: 91it [00:52,  1.70it/s]Extractor Predicting: 92it [00:53,  1.68it/s]Extractor Predicting: 93it [00:53,  1.74it/s]Extractor Predicting: 94it [00:54,  1.75it/s]Extractor Predicting: 95it [00:54,  1.75it/s]Extractor Predicting: 96it [00:55,  1.76it/s]Extractor Predicting: 97it [00:56,  1.78it/s]Extractor Predicting: 98it [00:56,  1.75it/s]Extractor Predicting: 99it [00:57,  1.73it/s]Extractor Predicting: 100it [00:57,  1.74it/s]Extractor Predicting: 101it [00:58,  1.75it/s]Extractor Predicting: 102it [00:58,  1.74it/s]Extractor Predicting: 103it [00:59,  1.74it/s]Extractor Predicting: 104it [01:00,  1.76it/s]Extractor Predicting: 105it [01:00,  1.73it/s]Extractor Predicting: 106it [01:01,  1.75it/s]Extractor Predicting: 107it [01:01,  1.75it/s]Extractor Predicting: 108it [01:02,  1.73it/s]Extractor Predicting: 109it [01:02,  1.74it/s]Extractor Predicting: 110it [01:03,  1.72it/s]Extractor Predicting: 111it [01:04,  1.74it/s]Extractor Predicting: 112it [01:04,  1.78it/s]Extractor Predicting: 113it [01:05,  1.79it/s]Extractor Predicting: 114it [01:05,  1.77it/s]Extractor Predicting: 115it [01:06,  1.76it/s]Extractor Predicting: 116it [01:06,  1.75it/s]Extractor Predicting: 117it [01:07,  1.77it/s]Extractor Predicting: 118it [01:08,  1.72it/s]Extractor Predicting: 119it [01:08,  1.73it/s]Extractor Predicting: 120it [01:09,  1.71it/s]Extractor Predicting: 121it [01:09,  1.68it/s]Extractor Predicting: 122it [01:10,  1.68it/s]Extractor Predicting: 123it [01:11,  1.73it/s]Extractor Predicting: 124it [01:11,  1.75it/s]Extractor Predicting: 125it [01:12,  1.77it/s]Extractor Predicting: 126it [01:12,  1.70it/s]Extractor Predicting: 127it [01:13,  1.70it/s]Extractor Predicting: 128it [01:13,  1.73it/s]Extractor Predicting: 129it [01:14,  1.75it/s]Extractor Predicting: 130it [01:14,  1.81it/s]Extractor Predicting: 131it [01:15,  1.76it/s]Extractor Predicting: 132it [01:16,  1.74it/s]Extractor Predicting: 133it [01:16,  1.75it/s]Extractor Predicting: 134it [01:17,  1.76it/s]Extractor Predicting: 135it [01:17,  1.78it/s]Extractor Predicting: 136it [01:18,  1.77it/s]Extractor Predicting: 137it [01:18,  1.79it/s]Extractor Predicting: 138it [01:20,  1.16it/s]Extractor Predicting: 139it [01:21,  1.28it/s]Extractor Predicting: 140it [01:21,  1.41it/s]Extractor Predicting: 141it [01:22,  1.51it/s]Extractor Predicting: 142it [01:22,  1.60it/s]Extractor Predicting: 143it [01:23,  1.65it/s]Extractor Predicting: 144it [01:23,  1.69it/s]Extractor Predicting: 145it [01:24,  1.69it/s]Extractor Predicting: 146it [01:25,  1.73it/s]Extractor Predicting: 147it [01:25,  1.58it/s]Extractor Predicting: 148it [01:26,  1.63it/s]Extractor Predicting: 149it [01:26,  1.68it/s]Extractor Predicting: 150it [01:27,  1.66it/s]Extractor Predicting: 151it [01:28,  1.69it/s]Extractor Predicting: 152it [01:28,  1.71it/s]Extractor Predicting: 153it [01:29,  1.69it/s]Extractor Predicting: 154it [01:29,  1.70it/s]Extractor Predicting: 155it [01:30,  1.68it/s]Extractor Predicting: 156it [01:31,  1.67it/s]Extractor Predicting: 157it [01:31,  1.67it/s]Extractor Predicting: 158it [01:32,  1.69it/s]Extractor Predicting: 159it [01:32,  1.72it/s]Extractor Predicting: 160it [01:33,  1.75it/s]Extractor Predicting: 161it [01:33,  1.77it/s]Extractor Predicting: 162it [01:34,  1.77it/s]Extractor Predicting: 163it [01:35,  1.66it/s]Extractor Predicting: 163it [01:35,  1.71it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:01,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:01,416 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:01,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:01,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:01,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:27:02,089 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:27:02,089 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:27:02,849 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:27:03,904 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:27:03,904 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:06,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:06,760 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:06,760 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:06,760 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:27:06,760 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:27:07,424 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:27:07,425 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:27:08,013 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:27:08,183 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:27:08,184 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.74it/s]Extractor Predicting: 3it [00:01,  1.78it/s]Extractor Predicting: 4it [00:02,  1.80it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:03,  1.81it/s]Extractor Predicting: 8it [00:04,  1.81it/s]Extractor Predicting: 9it [00:04,  1.84it/s]Extractor Predicting: 10it [00:05,  1.85it/s]Extractor Predicting: 11it [00:06,  1.87it/s]Extractor Predicting: 12it [00:06,  1.88it/s]Extractor Predicting: 13it [00:07,  1.86it/s]Extractor Predicting: 14it [00:07,  1.82it/s]Extractor Predicting: 15it [00:08,  1.81it/s]Extractor Predicting: 16it [00:08,  1.81it/s]Extractor Predicting: 17it [00:09,  1.78it/s]Extractor Predicting: 18it [00:09,  1.78it/s]Extractor Predicting: 19it [00:10,  1.84it/s]Extractor Predicting: 20it [00:11,  1.81it/s]Extractor Predicting: 21it [00:11,  1.82it/s]Extractor Predicting: 22it [00:12,  1.80it/s]Extractor Predicting: 23it [00:12,  1.79it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:13,  1.78it/s]Extractor Predicting: 26it [00:14,  1.79it/s]Extractor Predicting: 27it [00:14,  1.83it/s]Extractor Predicting: 28it [00:15,  1.82it/s]Extractor Predicting: 29it [00:16,  1.82it/s]Extractor Predicting: 30it [00:16,  1.84it/s]Extractor Predicting: 31it [00:17,  1.84it/s]Extractor Predicting: 32it [00:17,  1.84it/s]Extractor Predicting: 33it [00:18,  1.82it/s]Extractor Predicting: 34it [00:18,  1.81it/s]Extractor Predicting: 35it [00:19,  1.85it/s]Extractor Predicting: 36it [00:19,  1.67it/s]Extractor Predicting: 37it [00:20,  1.71it/s]Extractor Predicting: 38it [00:21,  1.71it/s]Extractor Predicting: 39it [00:21,  1.78it/s]Extractor Predicting: 40it [00:22,  1.78it/s]Extractor Predicting: 41it [00:22,  1.71it/s]Extractor Predicting: 42it [00:23,  1.72it/s]Extractor Predicting: 43it [00:24,  1.71it/s]Extractor Predicting: 44it [00:24,  1.71it/s]Extractor Predicting: 45it [00:25,  1.70it/s]Extractor Predicting: 46it [00:25,  1.67it/s]Extractor Predicting: 47it [00:26,  1.65it/s]Extractor Predicting: 48it [00:27,  1.68it/s]Extractor Predicting: 49it [00:27,  1.67it/s]Extractor Predicting: 50it [00:28,  1.67it/s]Extractor Predicting: 51it [00:28,  1.68it/s]Extractor Predicting: 52it [00:29,  1.67it/s]Extractor Predicting: 53it [00:30,  1.66it/s]Extractor Predicting: 54it [00:30,  1.61it/s]Extractor Predicting: 55it [00:31,  1.64it/s]Extractor Predicting: 56it [00:31,  1.70it/s]Extractor Predicting: 57it [00:32,  1.69it/s]Extractor Predicting: 58it [00:32,  1.69it/s]Extractor Predicting: 59it [00:33,  1.66it/s]Extractor Predicting: 60it [00:34,  1.66it/s]Extractor Predicting: 61it [00:34,  1.70it/s]Extractor Predicting: 62it [00:35,  1.71it/s]Extractor Predicting: 63it [00:36,  1.65it/s]Extractor Predicting: 64it [00:36,  1.67it/s]Extractor Predicting: 65it [00:37,  1.68it/s]Extractor Predicting: 66it [00:37,  1.65it/s]Extractor Predicting: 67it [00:38,  1.67it/s]Extractor Predicting: 68it [00:39,  1.66it/s]Extractor Predicting: 69it [00:39,  1.67it/s]Extractor Predicting: 70it [00:40,  1.69it/s]Extractor Predicting: 71it [00:40,  1.71it/s]Extractor Predicting: 72it [00:41,  1.70it/s]Extractor Predicting: 73it [00:41,  1.69it/s]Extractor Predicting: 74it [00:42,  1.71it/s]Extractor Predicting: 75it [00:43,  1.71it/s]Extractor Predicting: 76it [00:43,  1.71it/s]Extractor Predicting: 77it [00:44,  1.74it/s]Extractor Predicting: 78it [00:44,  1.71it/s]Extractor Predicting: 79it [00:45,  1.68it/s]Extractor Predicting: 80it [00:46,  1.71it/s]Extractor Predicting: 81it [00:46,  1.59it/s]Extractor Predicting: 82it [00:47,  1.64it/s]Extractor Predicting: 83it [00:47,  1.68it/s]Extractor Predicting: 84it [00:48,  1.64it/s]Extractor Predicting: 85it [00:49,  1.65it/s]Extractor Predicting: 86it [00:49,  1.68it/s]Extractor Predicting: 87it [00:50,  1.72it/s]Extractor Predicting: 88it [00:50,  1.69it/s]Extractor Predicting: 89it [00:51,  1.74it/s]Extractor Predicting: 90it [00:51,  1.75it/s]Extractor Predicting: 91it [00:52,  1.77it/s]Extractor Predicting: 92it [00:53,  1.76it/s]Extractor Predicting: 93it [00:53,  1.77it/s]Extractor Predicting: 94it [00:54,  1.75it/s]Extractor Predicting: 95it [00:54,  1.77it/s]Extractor Predicting: 96it [00:55,  1.79it/s]Extractor Predicting: 97it [00:55,  1.78it/s]Extractor Predicting: 98it [00:56,  1.71it/s]Extractor Predicting: 99it [00:57,  1.72it/s]Extractor Predicting: 100it [00:57,  1.72it/s]Extractor Predicting: 101it [00:58,  1.71it/s]Extractor Predicting: 102it [00:58,  1.73it/s]Extractor Predicting: 103it [00:59,  1.70it/s]Extractor Predicting: 104it [00:59,  1.73it/s]Extractor Predicting: 105it [01:00,  1.72it/s]Extractor Predicting: 106it [01:01,  1.67it/s]Extractor Predicting: 107it [01:01,  1.65it/s]Extractor Predicting: 108it [01:02,  1.68it/s]Extractor Predicting: 109it [01:03,  1.61it/s]Extractor Predicting: 110it [01:03,  1.65it/s]Extractor Predicting: 111it [01:04,  1.64it/s]Extractor Predicting: 112it [01:04,  1.68it/s]Extractor Predicting: 113it [01:05,  1.66it/s]Extractor Predicting: 114it [01:06,  1.68it/s]Extractor Predicting: 115it [01:06,  1.67it/s]Extractor Predicting: 116it [01:07,  1.65it/s]Extractor Predicting: 117it [01:07,  1.66it/s]Extractor Predicting: 118it [01:08,  1.71it/s]Extractor Predicting: 119it [01:08,  1.71it/s]Extractor Predicting: 120it [01:09,  1.76it/s]Extractor Predicting: 121it [01:10,  1.74it/s]Extractor Predicting: 122it [01:10,  1.77it/s]Extractor Predicting: 123it [01:11,  1.79it/s]Extractor Predicting: 124it [01:11,  1.83it/s]Extractor Predicting: 125it [01:12,  1.80it/s]Extractor Predicting: 126it [01:12,  1.75it/s]Extractor Predicting: 127it [01:13,  1.75it/s]Extractor Predicting: 128it [01:14,  1.73it/s]Extractor Predicting: 129it [01:14,  1.73it/s]Extractor Predicting: 130it [01:15,  1.72it/s]Extractor Predicting: 131it [01:15,  1.79it/s]Extractor Predicting: 132it [01:16,  1.76it/s]Extractor Predicting: 133it [01:16,  1.76it/s]Extractor Predicting: 134it [01:17,  1.79it/s]Extractor Predicting: 135it [01:18,  1.78it/s]Extractor Predicting: 136it [01:18,  1.79it/s]Extractor Predicting: 137it [01:19,  1.83it/s]Extractor Predicting: 138it [01:19,  1.81it/s]Extractor Predicting: 139it [01:20,  1.81it/s]Extractor Predicting: 140it [01:20,  1.77it/s]Extractor Predicting: 141it [01:21,  1.79it/s]Extractor Predicting: 142it [01:21,  1.77it/s]Extractor Predicting: 143it [01:22,  1.74it/s]Extractor Predicting: 144it [01:23,  1.74it/s]Extractor Predicting: 145it [01:23,  1.69it/s]Extractor Predicting: 146it [01:24,  1.64it/s]Extractor Predicting: 147it [01:24,  1.67it/s]Extractor Predicting: 148it [01:25,  1.67it/s]Extractor Predicting: 149it [01:26,  1.51it/s]Extractor Predicting: 150it [01:26,  1.56it/s]Extractor Predicting: 151it [01:27,  1.60it/s]Extractor Predicting: 152it [01:28,  1.63it/s]Extractor Predicting: 153it [01:28,  1.68it/s]Extractor Predicting: 154it [01:29,  1.71it/s]Extractor Predicting: 155it [01:29,  1.76it/s]Extractor Predicting: 156it [01:30,  1.70it/s]Extractor Predicting: 157it [01:30,  1.71it/s]Extractor Predicting: 158it [01:31,  1.72it/s]Extractor Predicting: 159it [01:32,  1.72it/s]Extractor Predicting: 160it [01:32,  1.76it/s]Extractor Predicting: 161it [01:33,  1.67it/s]Extractor Predicting: 162it [01:33,  1.64it/s]Extractor Predicting: 163it [01:34,  1.69it/s]Extractor Predicting: 164it [01:35,  1.69it/s]Extractor Predicting: 165it [01:35,  1.77it/s]Extractor Predicting: 166it [01:36,  1.78it/s]Extractor Predicting: 167it [01:36,  1.77it/s]Extractor Predicting: 168it [01:37,  1.83it/s]Extractor Predicting: 169it [01:37,  1.85it/s]Extractor Predicting: 170it [01:38,  1.81it/s]Extractor Predicting: 171it [01:38,  1.79it/s]Extractor Predicting: 172it [01:39,  1.80it/s]Extractor Predicting: 173it [01:40,  1.75it/s]Extractor Predicting: 174it [01:40,  1.79it/s]Extractor Predicting: 175it [01:41,  1.79it/s]Extractor Predicting: 176it [01:41,  1.77it/s]Extractor Predicting: 177it [01:42,  1.74it/s]Extractor Predicting: 178it [01:42,  1.74it/s]Extractor Predicting: 179it [01:43,  1.83it/s]Extractor Predicting: 180it [01:43,  1.80it/s]Extractor Predicting: 181it [01:44,  1.80it/s]Extractor Predicting: 182it [01:45,  1.78it/s]Extractor Predicting: 183it [01:45,  1.77it/s]Extractor Predicting: 184it [01:46,  1.76it/s]Extractor Predicting: 185it [01:46,  1.80it/s]Extractor Predicting: 186it [01:47,  1.76it/s]Extractor Predicting: 187it [01:47,  1.75it/s]Extractor Predicting: 188it [01:48,  1.73it/s]Extractor Predicting: 189it [01:49,  1.72it/s]Extractor Predicting: 190it [01:49,  1.69it/s]Extractor Predicting: 191it [01:50,  1.69it/s]Extractor Predicting: 192it [01:50,  1.72it/s]Extractor Predicting: 193it [01:51,  1.76it/s]Extractor Predicting: 194it [01:51,  1.77it/s]Extractor Predicting: 195it [01:52,  1.74it/s]Extractor Predicting: 196it [01:53,  1.75it/s]Extractor Predicting: 197it [01:53,  1.75it/s]Extractor Predicting: 198it [01:54,  1.76it/s]Extractor Predicting: 199it [01:54,  1.75it/s]Extractor Predicting: 200it [01:55,  1.74it/s]Extractor Predicting: 201it [01:56,  1.75it/s]Extractor Predicting: 202it [01:56,  1.75it/s]Extractor Predicting: 203it [01:57,  1.75it/s]Extractor Predicting: 204it [01:57,  1.75it/s]Extractor Predicting: 205it [01:58,  1.75it/s]Extractor Predicting: 206it [01:58,  1.74it/s]Extractor Predicting: 207it [01:59,  1.77it/s]Extractor Predicting: 208it [01:59,  1.76it/s]Extractor Predicting: 209it [02:00,  1.76it/s]Extractor Predicting: 210it [02:01,  1.75it/s]Extractor Predicting: 211it [02:01,  1.74it/s]Extractor Predicting: 212it [02:02,  1.76it/s]Extractor Predicting: 213it [02:02,  1.74it/s]Extractor Predicting: 214it [02:03,  1.75it/s]Extractor Predicting: 215it [02:04,  1.70it/s]Extractor Predicting: 216it [02:04,  1.72it/s]Extractor Predicting: 217it [02:05,  1.75it/s]Extractor Predicting: 218it [02:05,  1.75it/s]Extractor Predicting: 219it [02:06,  1.73it/s]Extractor Predicting: 220it [02:06,  1.75it/s]Extractor Predicting: 221it [02:07,  1.73it/s]Extractor Predicting: 222it [02:08,  1.73it/s]Extractor Predicting: 223it [02:08,  1.66it/s]Extractor Predicting: 224it [02:09,  1.67it/s]Extractor Predicting: 225it [02:09,  1.65it/s]Extractor Predicting: 226it [02:10,  1.67it/s]Extractor Predicting: 227it [02:11,  1.62it/s]Extractor Predicting: 228it [02:11,  1.57it/s]Extractor Predicting: 229it [02:12,  1.60it/s]Extractor Predicting: 230it [02:13,  1.62it/s]Extractor Predicting: 231it [02:13,  1.63it/s]Extractor Predicting: 232it [02:14,  1.64it/s]Extractor Predicting: 233it [02:14,  1.65it/s]Extractor Predicting: 234it [02:15,  1.67it/s]Extractor Predicting: 235it [02:16,  1.68it/s]Extractor Predicting: 236it [02:16,  1.70it/s]Extractor Predicting: 237it [02:17,  1.69it/s]Extractor Predicting: 238it [02:17,  1.76it/s]Extractor Predicting: 239it [02:18,  1.79it/s]Extractor Predicting: 240it [02:18,  1.85it/s]Extractor Predicting: 241it [02:19,  1.84it/s]Extractor Predicting: 242it [02:19,  1.83it/s]Extractor Predicting: 243it [02:20,  1.78it/s]Extractor Predicting: 244it [02:21,  1.78it/s]Extractor Predicting: 245it [02:21,  1.82it/s]Extractor Predicting: 246it [02:22,  1.79it/s]Extractor Predicting: 247it [02:22,  1.81it/s]Extractor Predicting: 248it [02:23,  1.82it/s]Extractor Predicting: 249it [02:23,  1.88it/s]Extractor Predicting: 250it [02:24,  1.88it/s]Extractor Predicting: 251it [02:24,  1.69it/s]Extractor Predicting: 252it [02:25,  1.77it/s]Extractor Predicting: 253it [02:25,  1.78it/s]Extractor Predicting: 254it [02:26,  1.83it/s]Extractor Predicting: 255it [02:27,  1.79it/s]Extractor Predicting: 256it [02:27,  1.86it/s]Extractor Predicting: 257it [02:28,  1.83it/s]Extractor Predicting: 258it [02:28,  1.78it/s]Extractor Predicting: 259it [02:29,  1.73it/s]Extractor Predicting: 260it [02:29,  1.80it/s]Extractor Predicting: 261it [02:30,  1.77it/s]Extractor Predicting: 262it [02:31,  1.79it/s]Extractor Predicting: 263it [02:31,  1.84it/s]Extractor Predicting: 264it [02:32,  1.86it/s]Extractor Predicting: 265it [02:32,  1.85it/s]Extractor Predicting: 266it [02:33,  1.87it/s]Extractor Predicting: 267it [02:33,  1.90it/s]Extractor Predicting: 268it [02:34,  1.89it/s]Extractor Predicting: 269it [02:34,  1.84it/s]Extractor Predicting: 270it [02:35,  1.86it/s]Extractor Predicting: 271it [02:35,  1.85it/s]Extractor Predicting: 272it [02:36,  1.81it/s]Extractor Predicting: 273it [02:36,  1.88it/s]Extractor Predicting: 274it [02:37,  1.88it/s]Extractor Predicting: 275it [02:37,  1.86it/s]Extractor Predicting: 276it [02:38,  1.83it/s]Extractor Predicting: 277it [02:39,  1.83it/s]Extractor Predicting: 278it [02:39,  1.86it/s]Extractor Predicting: 279it [02:40,  1.81it/s]Extractor Predicting: 280it [02:40,  1.88it/s]Extractor Predicting: 281it [02:41,  1.89it/s]Extractor Predicting: 282it [02:41,  1.91it/s]Extractor Predicting: 283it [02:42,  1.90it/s]Extractor Predicting: 284it [02:42,  1.90it/s]Extractor Predicting: 285it [02:43,  1.91it/s]Extractor Predicting: 286it [02:43,  1.90it/s]Extractor Predicting: 287it [02:44,  1.90it/s]Extractor Predicting: 288it [02:44,  1.85it/s]Extractor Predicting: 289it [02:45,  1.85it/s]Extractor Predicting: 290it [02:46,  1.76it/s]Extractor Predicting: 291it [02:46,  1.80it/s]Extractor Predicting: 292it [02:47,  1.84it/s]Extractor Predicting: 293it [02:47,  1.86it/s]Extractor Predicting: 294it [02:48,  1.87it/s]Extractor Predicting: 295it [02:48,  1.88it/s]Extractor Predicting: 296it [02:49,  1.78it/s]Extractor Predicting: 297it [02:49,  1.76it/s]Extractor Predicting: 298it [02:50,  1.70it/s]Extractor Predicting: 299it [02:51,  1.70it/s]Extractor Predicting: 300it [02:51,  1.73it/s]Extractor Predicting: 301it [02:52,  1.74it/s]Extractor Predicting: 302it [02:52,  1.74it/s]Extractor Predicting: 303it [02:53,  1.70it/s]Extractor Predicting: 304it [02:54,  1.70it/s]Extractor Predicting: 305it [02:54,  1.75it/s]Extractor Predicting: 306it [02:55,  1.69it/s]Extractor Predicting: 307it [02:55,  1.65it/s]Extractor Predicting: 308it [02:56,  1.66it/s]Extractor Predicting: 309it [02:56,  1.70it/s]Extractor Predicting: 310it [02:57,  1.73it/s]Extractor Predicting: 311it [02:58,  1.74it/s]Extractor Predicting: 312it [02:58,  1.72it/s]Extractor Predicting: 313it [02:59,  1.68it/s]Extractor Predicting: 314it [02:59,  1.66it/s]Extractor Predicting: 315it [03:00,  1.66it/s]Extractor Predicting: 316it [03:01,  1.66it/s]Extractor Predicting: 317it [03:01,  1.66it/s]Extractor Predicting: 318it [03:02,  1.67it/s]Extractor Predicting: 319it [03:03,  1.55it/s]Extractor Predicting: 320it [03:03,  1.59it/s]Extractor Predicting: 321it [03:04,  1.68it/s]Extractor Predicting: 322it [03:04,  1.71it/s]Extractor Predicting: 323it [03:05,  1.70it/s]Extractor Predicting: 324it [03:05,  1.70it/s]Extractor Predicting: 325it [03:06,  1.70it/s]Extractor Predicting: 326it [03:07,  1.69it/s]Extractor Predicting: 327it [03:07,  1.66it/s]Extractor Predicting: 328it [03:08,  1.68it/s]Extractor Predicting: 329it [03:08,  1.73it/s]Extractor Predicting: 330it [03:09,  1.76it/s]Extractor Predicting: 331it [03:09,  1.99it/s]Extractor Predicting: 331it [03:09,  1.74it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:27,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:27,440 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:27,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:27,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:27,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:30:28,060 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:30:28,061 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:30:28,656 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:30:29,704 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:30:29,704 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:32,578 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:32,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:32,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:32,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:30:32,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:30:33,357 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:30:33,358 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:30:33,975 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:30:34,147 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:30:34,147 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.65it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.57it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:20,  1.67it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:22,  1.68it/s]Extractor Predicting: 37it [00:22,  1.69it/s]Extractor Predicting: 38it [00:23,  1.69it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.65it/s]Extractor Predicting: 42it [00:25,  1.67it/s]Extractor Predicting: 43it [00:26,  1.65it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:27,  1.66it/s]Extractor Predicting: 46it [00:28,  1.69it/s]Extractor Predicting: 47it [00:28,  1.67it/s]Extractor Predicting: 48it [00:29,  1.71it/s]Extractor Predicting: 49it [00:29,  1.73it/s]Extractor Predicting: 50it [00:30,  1.76it/s]Extractor Predicting: 51it [00:30,  1.76it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:32,  1.65it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:33,  1.71it/s]Extractor Predicting: 56it [00:33,  1.79it/s]Extractor Predicting: 57it [00:34,  1.84it/s]Extractor Predicting: 58it [00:34,  1.90it/s]Extractor Predicting: 59it [00:35,  1.99it/s]Extractor Predicting: 60it [00:35,  2.08it/s]Extractor Predicting: 61it [00:36,  2.14it/s]Extractor Predicting: 62it [00:36,  2.12it/s]Extractor Predicting: 63it [00:37,  2.14it/s]Extractor Predicting: 64it [00:37,  2.12it/s]Extractor Predicting: 65it [00:38,  2.11it/s]Extractor Predicting: 66it [00:38,  2.11it/s]Extractor Predicting: 67it [00:39,  2.09it/s]Extractor Predicting: 68it [00:39,  2.10it/s]Extractor Predicting: 69it [00:39,  2.15it/s]Extractor Predicting: 70it [00:40,  2.10it/s]Extractor Predicting: 71it [00:40,  2.10it/s]Extractor Predicting: 72it [00:41,  2.13it/s]Extractor Predicting: 73it [00:41,  2.18it/s]Extractor Predicting: 74it [00:42,  2.20it/s]Extractor Predicting: 75it [00:42,  2.18it/s]Extractor Predicting: 76it [00:43,  2.15it/s]Extractor Predicting: 77it [00:43,  2.22it/s]Extractor Predicting: 78it [00:44,  2.13it/s]Extractor Predicting: 79it [00:44,  2.13it/s]Extractor Predicting: 80it [00:45,  2.11it/s]Extractor Predicting: 81it [00:45,  2.10it/s]Extractor Predicting: 82it [00:45,  2.13it/s]Extractor Predicting: 83it [00:46,  2.14it/s]Extractor Predicting: 84it [00:46,  2.14it/s]Extractor Predicting: 85it [00:47,  2.15it/s]Extractor Predicting: 86it [00:47,  1.98it/s]Extractor Predicting: 87it [00:48,  1.92it/s]Extractor Predicting: 88it [00:49,  1.88it/s]Extractor Predicting: 89it [00:49,  1.86it/s]Extractor Predicting: 90it [00:50,  1.87it/s]Extractor Predicting: 91it [00:50,  1.84it/s]Extractor Predicting: 92it [00:51,  1.83it/s]Extractor Predicting: 93it [00:51,  1.83it/s]Extractor Predicting: 94it [00:52,  1.81it/s]Extractor Predicting: 95it [00:53,  1.65it/s]Extractor Predicting: 96it [00:53,  1.71it/s]Extractor Predicting: 97it [00:54,  1.75it/s]Extractor Predicting: 98it [00:54,  1.78it/s]Extractor Predicting: 99it [00:55,  1.77it/s]Extractor Predicting: 100it [00:55,  1.71it/s]Extractor Predicting: 101it [00:56,  1.75it/s]Extractor Predicting: 102it [00:57,  1.75it/s]Extractor Predicting: 103it [00:57,  1.71it/s]Extractor Predicting: 104it [00:58,  1.69it/s]Extractor Predicting: 105it [00:58,  1.69it/s]Extractor Predicting: 106it [00:59,  1.66it/s]Extractor Predicting: 107it [01:00,  1.63it/s]Extractor Predicting: 108it [01:00,  1.59it/s]Extractor Predicting: 108it [01:00,  1.78it/s]
[INFO|configuration_utils.py:515] 2023-08-28 04:31:36,136 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:31:36,137 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:31:36,144 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:31:36,145 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 04:31:36,147 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:31:39,264 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 04:31:39,270 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 04:31:39,287 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:31:39,288 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:31:39,293 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:31:39,296 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:31:39,296 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:31:39,296 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:31:39,296 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:31:39,296 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:31:39,296 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 04:31:39,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:40,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:40,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:41,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:42,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:43,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:43,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:44,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:45,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:46,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:46,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:47,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:48,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:48,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:49,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:50,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:50,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:51,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:52,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:52,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:53,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:53,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:54,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:41, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-28 04:31:55,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:55,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:56,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:57,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:57,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:58,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:58,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:31:59,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:00,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:00,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:01,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:02,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:02,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:03,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:04,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:04,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:05,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:05,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:06,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:07,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:08,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:08,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:09,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:09,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:10,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:27, 15.93s/it][WARNING|generation_utils.py:914] 2023-08-28 04:32:11,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:12,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:12,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:13,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:14,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:14,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:15,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:15,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:16,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:17,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:17,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:18,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:19,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:19,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:20,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:20,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:21,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:22,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:22,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:23,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:24,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:24,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:25,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:25,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:26,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:47<03:10, 15.90s/it][WARNING|generation_utils.py:914] 2023-08-28 04:32:27,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:28,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:28,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:29,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:30,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:31,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:31,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:32,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:32,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:33,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:34,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:35,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:35,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:36,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:37,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:37,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:38,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:39,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:39,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:40,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:41,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:41,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:42,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:43,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:44,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:05<03:02, 16.56s/it][WARNING|generation_utils.py:914] 2023-08-28 04:32:44,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:45,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:46,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:46,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:47,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:47,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:48,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:49,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:49,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:50,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:50,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:51,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:52,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:53,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:53,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:54,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:54,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:55,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:56,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:56,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:58,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:58,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:59,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:32:59,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:00,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:21<02:45, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 04:33:01,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:02,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:02,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:03,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:03,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:04,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:05,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:05,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:06,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:06,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:07,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:08,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:08,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:09,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:10,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:10,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:11,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:11,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:12,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:13,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:13,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:14,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:15,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:15,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:36<02:23, 15.99s/it][WARNING|generation_utils.py:914] 2023-08-28 04:33:16,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:16,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:17,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:18,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:18,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:19,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:20,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:20,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:21,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:22,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:22,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:23,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:24,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:25,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:25,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:26,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:27,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:27,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:28,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:29,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:29,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:30,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:31,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:31,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:32,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:32,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:54<02:11, 16.47s/it][WARNING|generation_utils.py:914] 2023-08-28 04:33:33,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:34,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:34,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:35,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:36,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:36,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:37,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:37,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:38,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:39,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:39,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:40,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:40,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:41,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:41,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:42,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:42,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:43,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:44,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:44,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:45,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:46,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:46,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:47,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:47,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:48,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:48,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:49,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:50,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:50,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:11<01:57, 16.81s/it][WARNING|generation_utils.py:914] 2023-08-28 04:33:51,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:51,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:52,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:53,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:54,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:54,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:55,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:56,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:56,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:57,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:57,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:58,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:59,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:33:59,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:00,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:01,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:01,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:02,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:03,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:03,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:04,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:04,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:05,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:05,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:26<01:38, 16.34s/it][WARNING|generation_utils.py:914] 2023-08-28 04:34:06,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:07,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:07,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:08,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:08,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:09,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:10,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:10,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:11,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:11,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:12,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:12,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:13,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:14,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:14,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:15,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:15,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:16,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:16,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:17,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:18,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:18,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:19,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:19,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:20,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:41<01:19, 15.84s/it][WARNING|generation_utils.py:914] 2023-08-28 04:34:21,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:21,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:22,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:23,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:23,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:24,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:24,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:25,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:26,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:26,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:27,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:28,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:28,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:29,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:29,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:30,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:31,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:31,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:32,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:32,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:33,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:34,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:34,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:35,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:56<01:02, 15.54s/it][WARNING|generation_utils.py:914] 2023-08-28 04:34:36,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:36,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:37,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:38,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:38,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:39,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:40,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:40,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:41,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:42,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:43,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:43,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:44,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:44,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:45,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:46,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:46,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:47,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:48,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:48,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:49,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:50,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:50,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:51,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:52,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:52,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:53,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:54,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:15<00:49, 16.58s/it][WARNING|generation_utils.py:914] 2023-08-28 04:34:55,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:55,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:56,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:56,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:57,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:58,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:58,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:59,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:34:59,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:00,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:01,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:01,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:02,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:03,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:04,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:04,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:05,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:06,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:06,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:07,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:08,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:08,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:09,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:10,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:10,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:11,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:32<00:33, 16.73s/it][WARNING|generation_utils.py:914] 2023-08-28 04:35:12,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:12,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:13,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:14,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:14,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:15,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:15,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:16,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:16,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:17,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:18,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:18,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:19,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:19,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:20,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:21,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:21,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:22,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:23,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:23,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:24,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:25,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:25,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:26,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:26,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:27,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:48<00:16, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 04:35:28,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:28,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:29,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:29,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:30,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:31,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:32,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:33,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:33,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:34,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:35,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:35,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:36,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:36,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:37,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:38,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:38,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:39,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:40,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:40,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:41,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:42,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:42,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:43,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:44,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:35:44,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:05<00:00, 16.77s/it]Generating: 100%|██████████| 15/15 [04:05<00:00, 16.40s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:52,406 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:52,411 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:52,411 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:52,411 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:52,411 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:35:53,004 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:35:53,005 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:35:53,586 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:35:54,677 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:35:54,677 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:57,545 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:57,550 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:57,550 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:57,550 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:35:57,550 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:35:58,190 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:35:58,191 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:35:58,760 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:35:58,933 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:35:58,933 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in Life , he played the title character , a young princess of the family at the end of the third season of HBO s Game of Thrones . Head Entity : Game of Thrones , Tail Entity : Princess of the family .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.75375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.7525, 'errors': {'', "('pharaoh Amenhotep II', 'made from material', '', 'He is best remembered for the painting of the pharaoh Amenhotep II , composed by Alfred Wohl in 1827 for a French conservatory .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : cast member .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Billboard Pop Hits', 'cast member', '', 'On October 25 , 1978 , the band released the track The Lads , the debut single by the British band The Riveters ( now defunct ) , on Columbia Records , along with several appearances on the Billboard Pop Hits chart for June .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : follows .', 'success_rate': 0.7283653846153846, 'errors': {'', "('Nigeria', 'follows', '', 'In 2013 , a second poll showed Zaire had a 7 % approval rating , while in 2014 , Nigeria had a 24 % approval rating .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 281, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 366, 'raw': 576}
{'target': 600, 'success': 391, 'raw': 608}
{'target': 600, 'success': 410, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 452, 'raw': 704}
{'target': 600, 'success': 470, 'raw': 736}
{'target': 600, 'success': 490, 'raw': 768}
{'target': 600, 'success': 511, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 590, 'raw': 928}
{'target': 600, 'success': 608, 'raw': 960}
{'prompt': 'Relation : league .', 'success_rate': 0.6333333333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.765, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in Life , he studied with a group of Latin philologists and other thinkers at the Renaissance in Italy , in Naples from 1635 to 1645 , and from 1654 back to 1701 . Head Entity : Naples , Tail Entity : Rome .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 262, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 305, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 341, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 412, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 501, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 584, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : mother .', 'success_rate': 0.6729910714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : residence .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1453 the head of the province of Ghent and of neighboring districts at Neuromö , in Burgundesland , were also under the Ottoman Empire . Head Entity : Burgundesland , Tail Entity : Ottomans .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 15273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.34it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.48it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.53it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:04,  1.52it/s]Extractor Estimating: 8it [00:05,  1.49it/s]Extractor Estimating: 9it [00:06,  1.53it/s]Extractor Estimating: 10it [00:06,  1.52it/s]Extractor Estimating: 11it [00:07,  1.43it/s]Extractor Estimating: 12it [00:08,  1.49it/s]Extractor Estimating: 13it [00:08,  1.50it/s]Extractor Estimating: 14it [00:09,  1.46it/s]Extractor Estimating: 15it [00:10,  1.48it/s]Extractor Estimating: 16it [00:10,  1.50it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.54it/s]Extractor Estimating: 19it [00:12,  1.52it/s]Extractor Estimating: 20it [00:13,  1.56it/s]Extractor Estimating: 21it [00:13,  1.56it/s]Extractor Estimating: 22it [00:14,  1.63it/s]Extractor Estimating: 23it [00:15,  1.56it/s]Extractor Estimating: 24it [00:15,  1.48it/s]Extractor Estimating: 25it [00:16,  1.50it/s]Extractor Estimating: 26it [00:17,  1.55it/s]Extractor Estimating: 27it [00:17,  1.55it/s]Extractor Estimating: 28it [00:18,  1.61it/s]Extractor Estimating: 29it [00:19,  1.60it/s]Extractor Estimating: 30it [00:19,  1.62it/s]Extractor Estimating: 31it [00:20,  1.58it/s]Extractor Estimating: 32it [00:20,  1.60it/s]Extractor Estimating: 33it [00:21,  1.52it/s]Extractor Estimating: 34it [00:22,  1.51it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:23,  1.51it/s]Extractor Estimating: 37it [00:24,  1.53it/s]Extractor Estimating: 38it [00:24,  1.55it/s]Extractor Estimating: 39it [00:25,  1.52it/s]Extractor Estimating: 40it [00:26,  1.52it/s]Extractor Estimating: 41it [00:26,  1.54it/s]Extractor Estimating: 42it [00:27,  1.55it/s]Extractor Estimating: 43it [00:28,  1.46it/s]Extractor Estimating: 44it [00:29,  1.37it/s]Extractor Estimating: 45it [00:29,  1.40it/s]Extractor Estimating: 46it [00:30,  1.46it/s]Extractor Estimating: 47it [00:31,  1.48it/s]Extractor Estimating: 48it [00:31,  1.50it/s]Extractor Estimating: 49it [00:32,  1.49it/s]Extractor Estimating: 50it [00:33,  1.46it/s]Extractor Estimating: 51it [00:33,  1.46it/s]Extractor Estimating: 52it [00:34,  1.55it/s]Extractor Estimating: 53it [00:34,  1.56it/s]Extractor Estimating: 54it [00:35,  1.55it/s]Extractor Estimating: 55it [00:36,  1.53it/s]Extractor Estimating: 56it [00:36,  1.56it/s]Extractor Estimating: 57it [00:37,  1.53it/s]Extractor Estimating: 58it [00:38,  1.55it/s]Extractor Estimating: 59it [00:38,  1.55it/s]Extractor Estimating: 60it [00:39,  1.56it/s]Extractor Estimating: 61it [00:40,  1.56it/s]Extractor Estimating: 62it [00:40,  1.58it/s]Extractor Estimating: 63it [00:41,  1.61it/s]Extractor Estimating: 64it [00:41,  1.62it/s]Extractor Estimating: 65it [00:42,  1.63it/s]Extractor Estimating: 66it [00:43,  1.59it/s]Extractor Estimating: 67it [00:43,  1.60it/s]Extractor Estimating: 68it [00:44,  1.63it/s]Extractor Estimating: 69it [00:45,  1.63it/s]Extractor Estimating: 70it [00:45,  1.61it/s]Extractor Estimating: 71it [00:46,  1.54it/s]Extractor Estimating: 72it [00:47,  1.57it/s]Extractor Estimating: 73it [00:47,  1.60it/s]Extractor Estimating: 74it [00:48,  1.58it/s]Extractor Estimating: 75it [00:48,  1.57it/s]Extractor Estimating: 76it [00:49,  1.54it/s]Extractor Estimating: 77it [00:50,  1.56it/s]Extractor Estimating: 78it [00:50,  1.57it/s]Extractor Estimating: 79it [00:51,  1.50it/s]Extractor Estimating: 80it [00:52,  1.44it/s]Extractor Estimating: 81it [00:52,  1.49it/s]Extractor Estimating: 82it [00:53,  1.45it/s]Extractor Estimating: 83it [00:54,  1.52it/s]Extractor Estimating: 84it [00:54,  1.52it/s]Extractor Estimating: 85it [00:55,  1.55it/s]Extractor Estimating: 86it [00:56,  1.58it/s]Extractor Estimating: 87it [00:56,  1.59it/s]Extractor Estimating: 88it [00:57,  1.63it/s]Extractor Estimating: 89it [00:58,  1.60it/s]Extractor Estimating: 90it [00:58,  1.57it/s]Extractor Estimating: 91it [00:59,  1.59it/s]Extractor Estimating: 92it [00:59,  1.56it/s]Extractor Estimating: 93it [01:00,  1.56it/s]Extractor Estimating: 94it [01:01,  1.55it/s]Extractor Estimating: 95it [01:01,  1.54it/s]Extractor Estimating: 96it [01:02,  1.56it/s]Extractor Estimating: 97it [01:03,  1.57it/s]Extractor Estimating: 98it [01:03,  1.54it/s]Extractor Estimating: 99it [01:04,  1.39it/s]Extractor Estimating: 100it [01:05,  1.46it/s]Extractor Estimating: 101it [01:06,  1.45it/s]Extractor Estimating: 102it [01:06,  1.48it/s]Extractor Estimating: 103it [01:07,  1.53it/s]Extractor Estimating: 104it [01:07,  1.55it/s]Extractor Estimating: 105it [01:08,  1.55it/s]Extractor Estimating: 106it [01:09,  1.57it/s]Extractor Estimating: 107it [01:09,  1.59it/s]Extractor Estimating: 108it [01:10,  1.63it/s]Extractor Estimating: 109it [01:10,  1.64it/s]Extractor Estimating: 110it [01:11,  1.58it/s]Extractor Estimating: 111it [01:12,  1.61it/s]Extractor Estimating: 112it [01:12,  1.60it/s]Extractor Estimating: 113it [01:13,  1.62it/s]Extractor Estimating: 114it [01:14,  1.62it/s]Extractor Estimating: 115it [01:14,  1.62it/s]Extractor Estimating: 116it [01:15,  1.56it/s]Extractor Estimating: 117it [01:16,  1.46it/s]Extractor Estimating: 118it [01:16,  1.46it/s]Extractor Estimating: 119it [01:17,  1.51it/s]Extractor Estimating: 120it [01:18,  1.47it/s]Extractor Estimating: 121it [01:18,  1.46it/s]Extractor Estimating: 122it [01:19,  1.48it/s]Extractor Estimating: 123it [01:20,  1.53it/s]Extractor Estimating: 124it [01:20,  1.52it/s]Extractor Estimating: 125it [01:21,  1.43it/s]Extractor Estimating: 126it [01:22,  1.44it/s]Extractor Estimating: 127it [01:22,  1.46it/s]Extractor Estimating: 128it [01:23,  1.53it/s]Extractor Estimating: 129it [01:24,  1.60it/s]Extractor Estimating: 130it [01:24,  1.55it/s]Extractor Estimating: 131it [01:25,  1.62it/s]Extractor Estimating: 132it [01:26,  1.58it/s]Extractor Estimating: 133it [01:26,  1.58it/s]Extractor Estimating: 134it [01:27,  1.55it/s]Extractor Estimating: 135it [01:27,  1.53it/s]Extractor Estimating: 136it [01:28,  1.48it/s]Extractor Estimating: 137it [01:29,  1.46it/s]Extractor Estimating: 138it [01:30,  1.51it/s]Extractor Estimating: 139it [01:30,  1.44it/s]Extractor Estimating: 140it [01:31,  1.50it/s]Extractor Estimating: 141it [01:31,  1.58it/s]Extractor Estimating: 142it [01:32,  1.55it/s]Extractor Estimating: 143it [01:33,  1.60it/s]Extractor Estimating: 144it [01:33,  1.63it/s]Extractor Estimating: 145it [01:34,  1.55it/s]Extractor Estimating: 146it [01:35,  1.53it/s]Extractor Estimating: 147it [01:35,  1.56it/s]Extractor Estimating: 148it [01:36,  1.61it/s]Extractor Estimating: 149it [01:37,  1.58it/s]Extractor Estimating: 150it [01:37,  1.57it/s]Extractor Estimating: 151it [01:38,  1.56it/s]Extractor Estimating: 152it [01:39,  1.54it/s]Extractor Estimating: 153it [01:39,  1.53it/s]Extractor Estimating: 154it [01:40,  1.55it/s]Extractor Estimating: 155it [01:40,  1.56it/s]Extractor Estimating: 156it [01:41,  1.58it/s]Extractor Estimating: 157it [01:42,  1.53it/s]Extractor Estimating: 158it [01:42,  1.54it/s]Extractor Estimating: 159it [01:43,  1.53it/s]Extractor Estimating: 160it [01:44,  1.52it/s]Extractor Estimating: 161it [01:44,  1.53it/s]Extractor Estimating: 162it [01:45,  1.57it/s]Extractor Estimating: 163it [01:46,  1.53it/s]Extractor Estimating: 164it [01:46,  1.54it/s]Extractor Estimating: 165it [01:47,  1.58it/s]Extractor Estimating: 166it [01:48,  1.56it/s]Extractor Estimating: 167it [01:48,  1.56it/s]Extractor Estimating: 168it [01:49,  1.41it/s]Extractor Estimating: 169it [01:50,  1.42it/s]Extractor Estimating: 170it [01:50,  1.47it/s]Extractor Estimating: 171it [01:51,  1.48it/s]Extractor Estimating: 172it [01:52,  1.52it/s]Extractor Estimating: 173it [01:52,  1.53it/s]Extractor Estimating: 174it [01:53,  1.54it/s]Extractor Estimating: 175it [01:54,  1.52it/s]Extractor Estimating: 176it [01:54,  1.57it/s]Extractor Estimating: 177it [01:55,  1.52it/s]Extractor Estimating: 178it [01:55,  1.56it/s]Extractor Estimating: 179it [01:56,  1.57it/s]Extractor Estimating: 180it [01:57,  1.58it/s]Extractor Estimating: 181it [01:57,  1.58it/s]Extractor Estimating: 182it [01:58,  1.61it/s]Extractor Estimating: 183it [01:59,  1.62it/s]Extractor Estimating: 184it [01:59,  1.61it/s]Extractor Estimating: 185it [02:00,  1.60it/s]Extractor Estimating: 186it [02:01,  1.52it/s]Extractor Estimating: 187it [02:01,  1.58it/s]Extractor Estimating: 188it [02:02,  1.61it/s]Extractor Estimating: 189it [02:02,  1.61it/s]Extractor Estimating: 190it [02:03,  1.44it/s]Extractor Estimating: 191it [02:04,  1.49it/s]Extractor Estimating: 192it [02:05,  1.48it/s]Extractor Estimating: 193it [02:05,  1.57it/s]Extractor Estimating: 194it [02:06,  1.55it/s]Extractor Estimating: 195it [02:06,  1.55it/s]Extractor Estimating: 196it [02:07,  1.55it/s]Extractor Estimating: 197it [02:08,  1.60it/s]Extractor Estimating: 198it [02:08,  1.59it/s]Extractor Estimating: 199it [02:09,  1.59it/s]Extractor Estimating: 200it [02:10,  1.59it/s]Extractor Estimating: 201it [02:10,  1.58it/s]Extractor Estimating: 202it [02:11,  1.63it/s]Extractor Estimating: 203it [02:11,  1.63it/s]Extractor Estimating: 204it [02:12,  1.65it/s]Extractor Estimating: 205it [02:12,  1.69it/s]Extractor Estimating: 206it [02:13,  1.67it/s]Extractor Estimating: 207it [02:14,  1.66it/s]Extractor Estimating: 208it [02:14,  1.70it/s]Extractor Estimating: 209it [02:15,  1.66it/s]Extractor Estimating: 210it [02:16,  1.61it/s]Extractor Estimating: 211it [02:16,  1.57it/s]Extractor Estimating: 212it [02:17,  1.62it/s]Extractor Estimating: 213it [02:17,  1.59it/s]Extractor Estimating: 214it [02:18,  1.65it/s]Extractor Estimating: 215it [02:19,  1.68it/s]Extractor Estimating: 216it [02:19,  1.67it/s]Extractor Estimating: 217it [02:20,  1.71it/s]Extractor Estimating: 218it [02:20,  1.68it/s]Extractor Estimating: 219it [02:21,  1.68it/s]Extractor Estimating: 220it [02:22,  1.64it/s]Extractor Estimating: 221it [02:22,  1.68it/s]Extractor Estimating: 222it [02:23,  1.69it/s]Extractor Estimating: 223it [02:23,  1.67it/s]Extractor Estimating: 224it [02:24,  1.68it/s]Extractor Estimating: 225it [02:25,  1.72it/s]Extractor Estimating: 226it [02:25,  1.74it/s]Extractor Estimating: 227it [02:26,  1.77it/s]Extractor Estimating: 228it [02:26,  1.79it/s]Extractor Estimating: 229it [02:27,  1.83it/s]Extractor Estimating: 230it [02:27,  1.84it/s]Extractor Estimating: 231it [02:28,  1.81it/s]Extractor Estimating: 232it [02:28,  1.74it/s]Extractor Estimating: 233it [02:29,  1.74it/s]Extractor Estimating: 234it [02:29,  1.80it/s]Extractor Estimating: 235it [02:30,  1.75it/s]Extractor Estimating: 236it [02:31,  1.73it/s]Extractor Estimating: 237it [02:31,  1.74it/s]Extractor Estimating: 238it [02:32,  1.72it/s]Extractor Estimating: 239it [02:32,  1.69it/s]Extractor Estimating: 240it [02:33,  1.71it/s]Extractor Estimating: 241it [02:34,  1.77it/s]Extractor Estimating: 242it [02:34,  1.75it/s]Extractor Estimating: 243it [02:35,  1.75it/s]Extractor Estimating: 244it [02:35,  1.71it/s]Extractor Estimating: 245it [02:36,  1.71it/s]Extractor Estimating: 246it [02:37,  1.72it/s]Extractor Estimating: 247it [02:37,  1.67it/s]Extractor Estimating: 248it [02:38,  1.66it/s]Extractor Estimating: 249it [02:38,  1.70it/s]Extractor Estimating: 250it [02:39,  1.71it/s]Extractor Estimating: 251it [02:40,  1.68it/s]Extractor Estimating: 252it [02:40,  1.66it/s]Extractor Estimating: 253it [02:41,  1.63it/s]Extractor Estimating: 254it [02:41,  1.63it/s]Extractor Estimating: 255it [02:42,  1.60it/s]Extractor Estimating: 256it [02:43,  1.60it/s]Extractor Estimating: 257it [02:43,  1.55it/s]Extractor Estimating: 258it [02:44,  1.57it/s]Extractor Estimating: 259it [02:45,  1.60it/s]Extractor Estimating: 260it [02:45,  1.60it/s]Extractor Estimating: 261it [02:46,  1.59it/s]Extractor Estimating: 262it [02:46,  1.59it/s]Extractor Estimating: 263it [02:47,  1.60it/s]Extractor Estimating: 264it [02:48,  1.62it/s]Extractor Estimating: 265it [02:48,  1.62it/s]Extractor Estimating: 266it [02:49,  1.63it/s]Extractor Estimating: 267it [02:50,  1.62it/s]Extractor Estimating: 268it [02:50,  1.59it/s]Extractor Estimating: 269it [02:51,  1.62it/s]Extractor Estimating: 270it [02:51,  1.65it/s]Extractor Estimating: 271it [02:52,  1.66it/s]Extractor Estimating: 272it [02:53,  1.66it/s]Extractor Estimating: 273it [02:53,  1.46it/s]Extractor Estimating: 274it [02:54,  1.50it/s]Extractor Estimating: 275it [02:55,  1.53it/s]Extractor Estimating: 276it [02:55,  1.51it/s]Extractor Estimating: 277it [02:56,  1.46it/s]Extractor Estimating: 278it [02:57,  1.45it/s]Extractor Estimating: 279it [02:57,  1.46it/s]Extractor Estimating: 280it [02:58,  1.51it/s]Extractor Estimating: 281it [02:59,  1.48it/s]Extractor Estimating: 282it [02:59,  1.49it/s]Extractor Estimating: 283it [03:00,  1.49it/s]Extractor Estimating: 284it [03:01,  1.44it/s]Extractor Estimating: 285it [03:01,  1.50it/s]Extractor Estimating: 286it [03:02,  1.52it/s]Extractor Estimating: 287it [03:03,  1.48it/s]Extractor Estimating: 288it [03:03,  1.50it/s]Extractor Estimating: 289it [03:04,  1.47it/s]Extractor Estimating: 290it [03:05,  1.49it/s]Extractor Estimating: 291it [03:05,  1.51it/s]Extractor Estimating: 292it [03:06,  1.50it/s]Extractor Estimating: 293it [03:07,  1.56it/s]Extractor Estimating: 294it [03:07,  1.54it/s]Extractor Estimating: 295it [03:08,  1.51it/s]Extractor Estimating: 296it [03:09,  1.53it/s]Extractor Estimating: 297it [03:09,  1.52it/s]Extractor Estimating: 298it [03:10,  1.50it/s]Extractor Estimating: 299it [03:11,  1.50it/s]Extractor Estimating: 300it [03:11,  1.52it/s]Extractor Estimating: 301it [03:12,  1.51it/s]Extractor Estimating: 302it [03:13,  1.55it/s]Extractor Estimating: 303it [03:13,  1.49it/s]Extractor Estimating: 304it [03:14,  1.51it/s]Extractor Estimating: 305it [03:15,  1.57it/s]Extractor Estimating: 306it [03:15,  1.61it/s]Extractor Estimating: 307it [03:16,  1.60it/s]Extractor Estimating: 308it [03:16,  1.59it/s]Extractor Estimating: 309it [03:17,  1.59it/s]Extractor Estimating: 310it [03:18,  1.61it/s]Extractor Estimating: 311it [03:18,  1.53it/s]Extractor Estimating: 312it [03:19,  1.52it/s]Extractor Estimating: 313it [03:20,  1.53it/s]Extractor Estimating: 314it [03:20,  1.57it/s]Extractor Estimating: 315it [03:21,  1.60it/s]Extractor Estimating: 316it [03:22,  1.50it/s]Extractor Estimating: 317it [03:22,  1.49it/s]Extractor Estimating: 318it [03:23,  1.51it/s]Extractor Estimating: 319it [03:24,  1.49it/s]Extractor Estimating: 320it [03:24,  1.52it/s]Extractor Estimating: 321it [03:25,  1.50it/s]Extractor Estimating: 322it [03:26,  1.44it/s]Extractor Estimating: 323it [03:26,  1.50it/s]Extractor Estimating: 324it [03:27,  1.53it/s]Extractor Estimating: 325it [03:28,  1.58it/s]Extractor Estimating: 326it [03:28,  1.61it/s]Extractor Estimating: 327it [03:29,  1.64it/s]Extractor Estimating: 328it [03:29,  1.70it/s]Extractor Estimating: 329it [03:30,  1.79it/s]Extractor Estimating: 330it [03:30,  1.76it/s]Extractor Estimating: 331it [03:31,  1.73it/s]Extractor Estimating: 332it [03:32,  1.77it/s]Extractor Estimating: 333it [03:32,  1.72it/s]Extractor Estimating: 334it [03:33,  1.74it/s]Extractor Estimating: 335it [03:33,  1.72it/s]Extractor Estimating: 336it [03:34,  1.73it/s]Extractor Estimating: 337it [03:35,  1.68it/s]Extractor Estimating: 338it [03:35,  1.64it/s]Extractor Estimating: 339it [03:36,  1.59it/s]Extractor Estimating: 340it [03:37,  1.55it/s]Extractor Estimating: 341it [03:37,  1.57it/s]Extractor Estimating: 342it [03:38,  1.62it/s]Extractor Estimating: 343it [03:38,  1.60it/s]Extractor Estimating: 344it [03:39,  1.63it/s]Extractor Estimating: 345it [03:40,  1.56it/s]Extractor Estimating: 346it [03:40,  1.58it/s]Extractor Estimating: 347it [03:41,  1.62it/s]Extractor Estimating: 348it [03:41,  1.63it/s]Extractor Estimating: 349it [03:42,  1.61it/s]Extractor Estimating: 350it [03:43,  1.64it/s]Extractor Estimating: 351it [03:43,  1.66it/s]Extractor Estimating: 352it [03:44,  1.65it/s]Extractor Estimating: 353it [03:44,  1.65it/s]Extractor Estimating: 354it [03:45,  1.64it/s]Extractor Estimating: 355it [03:46,  1.60it/s]Extractor Estimating: 356it [03:46,  1.56it/s]Extractor Estimating: 357it [03:47,  1.50it/s]Extractor Estimating: 358it [03:48,  1.51it/s]Extractor Estimating: 359it [03:48,  1.58it/s]Extractor Estimating: 360it [03:49,  1.54it/s]Extractor Estimating: 361it [03:50,  1.59it/s]Extractor Estimating: 362it [03:50,  1.59it/s]Extractor Estimating: 363it [03:51,  1.60it/s]Extractor Estimating: 364it [03:51,  1.64it/s]Extractor Estimating: 365it [03:52,  1.67it/s]Extractor Estimating: 366it [03:53,  1.64it/s]Extractor Estimating: 367it [03:53,  1.60it/s]Extractor Estimating: 368it [03:54,  1.55it/s]Extractor Estimating: 369it [03:55,  1.58it/s]Extractor Estimating: 370it [03:55,  1.62it/s]Extractor Estimating: 371it [03:56,  1.62it/s]Extractor Estimating: 372it [03:57,  1.57it/s]Extractor Estimating: 373it [03:57,  1.52it/s]Extractor Estimating: 374it [03:58,  1.51it/s]Extractor Estimating: 375it [03:58,  1.69it/s]Extractor Estimating: 375it [03:58,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:15,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:15,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:15,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:15,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:15,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:40:16,013 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:40:16,014 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:40:16,601 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:40:17,688 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:40:17,688 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:20,553 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:20,558 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:20,559 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:20,559 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:40:20,559 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:40:21,210 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:40:21,211 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:40:21,763 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:40:21,948 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:40:21,948 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 07:03:56,798 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 07:03:57,250 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7872 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 25250
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25350, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25350, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.090, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.072, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.074, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 72, avg_time 1.076, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 172, avg_time 1.081, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 272, avg_time 2.263, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 44, avg_time 1.076, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 144, avg_time 1.075, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 244, avg_time 1.092, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 16, avg_time 1.079, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 116, avg_time 2.244, loss:nan
g_step 1200, step 216, avg_time 1.086, loss:nan
g_step 1300, step 316, avg_time 1.091, loss:nan
g_step 1400, step 88, avg_time 1.070, loss:nan
g_step 1500, step 188, avg_time 1.060, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 288, avg_time 2.306, loss:nan
g_step 1700, step 60, avg_time 1.070, loss:nan
g_step 1800, step 160, avg_time 1.098, loss:nan
g_step 1900, step 260, avg_time 1.080, loss:nan
g_step 2000, step 32, avg_time 1.070, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 132, avg_time 2.244, loss:nan
g_step 2200, step 232, avg_time 1.082, loss:nan
g_step 2300, step 4, avg_time 1.088, loss:nan
g_step 2400, step 104, avg_time 1.075, loss:nan
g_step 2500, step 204, avg_time 1.085, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 304, avg_time 2.253, loss:nan
g_step 2700, step 76, avg_time 1.076, loss:nan
g_step 2800, step 176, avg_time 1.069, loss:nan
g_step 2900, step 276, avg_time 1.073, loss:nan
g_step 3000, step 48, avg_time 1.097, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 148, avg_time 2.269, loss:nan
g_step 3200, step 248, avg_time 1.077, loss:nan
g_step 3300, step 20, avg_time 1.071, loss:nan
g_step 3400, step 120, avg_time 1.074, loss:nan
g_step 3500, step 220, avg_time 1.071, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 320, avg_time 2.266, loss:nan
g_step 3700, step 92, avg_time 1.088, loss:nan
g_step 3800, step 192, avg_time 1.079, loss:nan
g_step 3900, step 292, avg_time 1.074, loss:nan
g_step 4000, step 64, avg_time 1.070, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 164, avg_time 2.254, loss:nan
g_step 4200, step 264, avg_time 1.093, loss:nan
g_step 4300, step 36, avg_time 1.072, loss:nan
g_step 4400, step 136, avg_time 1.091, loss:nan
g_step 4500, step 236, avg_time 1.087, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 8, avg_time 2.250, loss:nan
g_step 4700, step 108, avg_time 1.079, loss:nan
g_step 4800, step 208, avg_time 1.062, loss:nan
g_step 4900, step 308, avg_time 1.087, loss:nan
g_step 5000, step 80, avg_time 1.092, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 180, avg_time 2.238, loss:nan
g_step 5200, step 280, avg_time 1.091, loss:nan
g_step 5300, step 52, avg_time 1.066, loss:nan
g_step 5400, step 152, avg_time 1.084, loss:nan
g_step 5500, step 252, avg_time 1.080, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 24, avg_time 2.233, loss:nan
g_step 5700, step 124, avg_time 1.082, loss:nan
g_step 5800, step 224, avg_time 1.078, loss:nan
g_step 5900, step 324, avg_time 1.089, loss:nan
g_step 6000, step 96, avg_time 1.089, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 196, avg_time 2.252, loss:nan
g_step 6200, step 296, avg_time 1.069, loss:nan
g_step 6300, step 68, avg_time 1.084, loss:nan
g_step 6400, step 168, avg_time 1.059, loss:nan
g_step 6500, step 268, avg_time 1.098, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 07:03:57 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 07:03:57 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_07-03-56_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 07:03:58 - WARNING - datasets.builder -   Using custom data configuration default-b1b72cf746f233b3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b1b72cf746f233b3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 07:03:58,879 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:03:58,881 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:03:58,881 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:03:58,882 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:03:58,895 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:03:58,904 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:03:58,904 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:03:58,904 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:03:58,904 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:03:58,904 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:03:58,904 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 07:03:59,050 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:04:02,112 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 07:04:02,115 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b1b72cf746f233b3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.15ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.92ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.20ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.35ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.43ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.47ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.51ba/s]100%|██████████| 8/8 [00:01<00:00,  4.63ba/s]100%|██████████| 8/8 [00:01<00:00,  4.39ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.19ba/s] 40%|████      | 2/5 [00:00<00:00,  4.38ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.43ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.47ba/s]100%|██████████| 5/5 [00:00<00:00,  5.09ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.41ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.88ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.43ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.55ba/s]100%|██████████| 8/8 [00:00<00:00, 10.46ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.16ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.40ba/s]100%|██████████| 5/5 [00:00<00:00, 12.78ba/s]100%|██████████| 5/5 [00:00<00:00, 12.04ba/s]
[INFO|trainer.py:414] 2023-08-28 07:04:06,495 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 07:04:06,508 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 07:04:06,508 >>   Num examples = 7920
[INFO|trainer.py:1149] 2023-08-28 07:04:06,508 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 07:04:06,508 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 07:04:06,508 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 07:04:06,508 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 07:04:06,509 >>   Total optimization steps = 620
  0%|          | 0/620 [00:00<?, ?it/s]  0%|          | 1/620 [00:00<02:54,  3.54it/s]  0%|          | 2/620 [00:00<02:51,  3.60it/s]  0%|          | 3/620 [00:00<02:50,  3.61it/s]  1%|          | 4/620 [00:01<02:50,  3.62it/s]  1%|          | 5/620 [00:01<02:49,  3.62it/s]  1%|          | 6/620 [00:01<02:49,  3.63it/s]  1%|          | 7/620 [00:01<02:49,  3.61it/s]  1%|▏         | 8/620 [00:02<02:49,  3.62it/s]  1%|▏         | 9/620 [00:02<02:48,  3.62it/s]  2%|▏         | 10/620 [00:02<02:48,  3.63it/s]  2%|▏         | 11/620 [00:03<02:47,  3.63it/s]  2%|▏         | 12/620 [00:03<02:47,  3.63it/s]  2%|▏         | 13/620 [00:03<02:47,  3.63it/s]  2%|▏         | 14/620 [00:03<02:47,  3.63it/s]  2%|▏         | 15/620 [00:04<02:46,  3.63it/s]  3%|▎         | 16/620 [00:04<02:46,  3.63it/s]  3%|▎         | 17/620 [00:04<02:46,  3.63it/s]  3%|▎         | 18/620 [00:04<02:46,  3.61it/s]  3%|▎         | 19/620 [00:05<02:46,  3.62it/s]  3%|▎         | 20/620 [00:05<02:45,  3.62it/s]  3%|▎         | 21/620 [00:05<02:45,  3.62it/s]  4%|▎         | 22/620 [00:06<02:44,  3.63it/s]  4%|▎         | 23/620 [00:06<02:44,  3.63it/s]  4%|▍         | 24/620 [00:06<02:44,  3.63it/s]  4%|▍         | 25/620 [00:06<02:43,  3.63it/s]  4%|▍         | 26/620 [00:07<02:43,  3.63it/s]  4%|▍         | 27/620 [00:07<02:43,  3.63it/s]  5%|▍         | 28/620 [00:07<02:43,  3.63it/s]  5%|▍         | 29/620 [00:08<02:43,  3.60it/s]  5%|▍         | 30/620 [00:08<02:43,  3.61it/s]  5%|▌         | 31/620 [00:08<02:42,  3.62it/s]  5%|▌         | 32/620 [00:08<02:42,  3.62it/s]  5%|▌         | 33/620 [00:09<02:41,  3.63it/s]  5%|▌         | 34/620 [00:09<02:41,  3.62it/s]  6%|▌         | 35/620 [00:09<02:41,  3.63it/s]  6%|▌         | 36/620 [00:09<02:41,  3.62it/s]  6%|▌         | 37/620 [00:10<02:40,  3.62it/s]  6%|▌         | 38/620 [00:10<02:40,  3.62it/s]  6%|▋         | 39/620 [00:10<02:40,  3.62it/s]  6%|▋         | 40/620 [00:11<02:40,  3.61it/s]  7%|▋         | 41/620 [00:11<02:40,  3.61it/s]  7%|▋         | 42/620 [00:11<02:39,  3.62it/s]  7%|▋         | 43/620 [00:11<02:39,  3.62it/s]  7%|▋         | 44/620 [00:12<02:38,  3.62it/s]  7%|▋         | 45/620 [00:12<02:38,  3.62it/s]  7%|▋         | 46/620 [00:12<02:38,  3.62it/s]  8%|▊         | 47/620 [00:12<02:38,  3.62it/s]  8%|▊         | 48/620 [00:13<02:37,  3.63it/s]  8%|▊         | 49/620 [00:13<02:37,  3.62it/s]  8%|▊         | 50/620 [00:13<02:37,  3.62it/s]  8%|▊         | 51/620 [00:14<02:37,  3.61it/s]  8%|▊         | 52/620 [00:14<02:37,  3.62it/s]  9%|▊         | 53/620 [00:14<02:36,  3.62it/s]  9%|▊         | 54/620 [00:14<02:36,  3.62it/s]  9%|▉         | 55/620 [00:15<02:36,  3.62it/s]  9%|▉         | 56/620 [00:15<02:35,  3.62it/s]  9%|▉         | 57/620 [00:15<02:35,  3.62it/s]  9%|▉         | 58/620 [00:16<02:35,  3.62it/s] 10%|▉         | 59/620 [00:16<02:34,  3.62it/s] 10%|▉         | 60/620 [00:16<02:34,  3.62it/s] 10%|▉         | 61/620 [00:16<02:34,  3.62it/s] 10%|█         | 62/620 [00:17<02:34,  3.61it/s] 10%|█         | 63/620 [00:17<02:34,  3.61it/s] 10%|█         | 64/620 [00:17<02:33,  3.61it/s] 10%|█         | 65/620 [00:17<02:33,  3.62it/s] 11%|█         | 66/620 [00:18<02:33,  3.62it/s] 11%|█         | 67/620 [00:18<02:32,  3.62it/s] 11%|█         | 68/620 [00:18<02:32,  3.62it/s] 11%|█         | 69/620 [00:19<02:32,  3.62it/s] 11%|█▏        | 70/620 [00:19<02:31,  3.62it/s] 11%|█▏        | 71/620 [00:19<02:31,  3.62it/s] 12%|█▏        | 72/620 [00:19<02:31,  3.62it/s] 12%|█▏        | 73/620 [00:20<02:32,  3.59it/s] 12%|█▏        | 74/620 [00:20<02:31,  3.60it/s] 12%|█▏        | 75/620 [00:20<02:31,  3.60it/s] 12%|█▏        | 76/620 [00:20<02:30,  3.61it/s] 12%|█▏        | 77/620 [00:21<02:30,  3.62it/s] 13%|█▎        | 78/620 [00:21<02:29,  3.62it/s] 13%|█▎        | 79/620 [00:21<02:29,  3.62it/s] 13%|█▎        | 80/620 [00:22<02:29,  3.62it/s] 13%|█▎        | 81/620 [00:22<02:28,  3.62it/s] 13%|█▎        | 82/620 [00:22<02:28,  3.62it/s] 13%|█▎        | 83/620 [00:22<02:28,  3.62it/s] 14%|█▎        | 84/620 [00:23<02:28,  3.60it/s] 14%|█▎        | 85/620 [00:23<02:28,  3.60it/s] 14%|█▍        | 86/620 [00:23<02:27,  3.61it/s] 14%|█▍        | 87/620 [00:24<02:27,  3.61it/s] 14%|█▍        | 88/620 [00:24<02:27,  3.62it/s] 14%|█▍        | 89/620 [00:24<02:26,  3.62it/s] 15%|█▍        | 90/620 [00:24<02:26,  3.62it/s] 15%|█▍        | 91/620 [00:25<02:26,  3.62it/s] 15%|█▍        | 92/620 [00:25<02:25,  3.62it/s] 15%|█▌        | 93/620 [00:25<02:25,  3.62it/s] 15%|█▌        | 94/620 [00:25<02:25,  3.62it/s] 15%|█▌        | 95/620 [00:26<02:24,  3.62it/s] 15%|█▌        | 96/620 [00:26<02:24,  3.63it/s] 16%|█▌        | 97/620 [00:26<02:24,  3.62it/s] 16%|█▌        | 98/620 [00:27<02:24,  3.62it/s] 16%|█▌        | 99/620 [00:27<02:23,  3.62it/s] 16%|█▌        | 100/620 [00:27<02:23,  3.62it/s] 16%|█▋        | 101/620 [00:27<02:23,  3.62it/s] 16%|█▋        | 102/620 [00:28<02:23,  3.62it/s] 17%|█▋        | 103/620 [00:28<02:23,  3.61it/s] 17%|█▋        | 104/620 [00:28<02:22,  3.61it/s] 17%|█▋        | 105/620 [00:29<02:22,  3.62it/s] 17%|█▋        | 106/620 [00:29<02:22,  3.62it/s] 17%|█▋        | 107/620 [00:29<02:21,  3.62it/s] 17%|█▋        | 108/620 [00:29<02:21,  3.62it/s] 18%|█▊        | 109/620 [00:30<02:21,  3.62it/s] 18%|█▊        | 110/620 [00:30<02:20,  3.62it/s] 18%|█▊        | 111/620 [00:30<02:20,  3.62it/s] 18%|█▊        | 112/620 [00:30<02:20,  3.62it/s] 18%|█▊        | 113/620 [00:31<02:20,  3.62it/s] 18%|█▊        | 114/620 [00:31<02:20,  3.60it/s] 19%|█▊        | 115/620 [00:31<02:20,  3.61it/s] 19%|█▊        | 116/620 [00:32<02:19,  3.61it/s] 19%|█▉        | 117/620 [00:32<02:19,  3.61it/s] 19%|█▉        | 118/620 [00:32<02:18,  3.62it/s] 19%|█▉        | 119/620 [00:32<02:18,  3.62it/s] 19%|█▉        | 120/620 [00:33<02:18,  3.62it/s] 20%|█▉        | 121/620 [00:33<02:18,  3.61it/s] 20%|█▉        | 122/620 [00:33<02:17,  3.62it/s] 20%|█▉        | 123/620 [00:33<02:17,  3.62it/s] 20%|██        | 124/620 [00:34<02:07,  3.88it/s][INFO|trainer.py:2140] 2023-08-28 07:04:40,716 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:04:40,717 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 07:04:40,717 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.69it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.04it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.29it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.39it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.79it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.54it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.35it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.19it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.26it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.33it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.36it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.33it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.29it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.09it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.11it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.02it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.98it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.14it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.22it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.20it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.29it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.12it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.12it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.09it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.99it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.96it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.12it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.11it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.11it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.19it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.07it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.16it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.04it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.03it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.06it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.19it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.23it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.13it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.10it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.11it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.06it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.08it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.05it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.00it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.16it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.19it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.14it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.13it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.03it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.10it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.12it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.14it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.14it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.19it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.19it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.07it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.10it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.05it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.11it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.14it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.14it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.17it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.08it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.15it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.13it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.09it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.03it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.07it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.05it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.17it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.12it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.10it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.18it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.16it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.13it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.03it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.11it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.12it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.17it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.09it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.07it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.14it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.09it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.02it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.02it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.00it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.04it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.07it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.14it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.16it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.17it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.10it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.04it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.01it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.01it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.94it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.98it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.04it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.02it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.16it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.15it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.05it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.07it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.18it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.13it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.12it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.11it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.17it/s][A                                                 
                                                 [A 20%|██        | 124/620 [00:46<02:07,  3.88it/s]
100%|██████████| 543/543 [00:12<00:00, 44.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:04:53,057 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-28 07:04:53,083 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:04:54,815 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:04:54,835 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:04:54,845 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124/special_tokens_map.json
 20%|██        | 125/620 [00:48<37:48,  4.58s/it] 20%|██        | 126/620 [00:49<27:06,  3.29s/it] 20%|██        | 127/620 [00:49<19:37,  2.39s/it] 21%|██        | 128/620 [00:49<14:24,  1.76s/it] 21%|██        | 129/620 [00:50<10:45,  1.31s/it] 21%|██        | 130/620 [00:50<08:12,  1.00s/it] 21%|██        | 131/620 [00:50<06:27,  1.26it/s] 21%|██▏       | 132/620 [00:50<05:12,  1.56it/s] 21%|██▏       | 133/620 [00:51<04:19,  1.88it/s] 22%|██▏       | 134/620 [00:51<03:42,  2.19it/s] 22%|██▏       | 135/620 [00:51<03:15,  2.47it/s] 22%|██▏       | 136/620 [00:51<02:57,  2.72it/s] 22%|██▏       | 137/620 [00:52<02:44,  2.93it/s] 22%|██▏       | 138/620 [00:52<02:35,  3.09it/s] 22%|██▏       | 139/620 [00:52<02:29,  3.21it/s] 23%|██▎       | 140/620 [00:53<02:25,  3.30it/s] 23%|██▎       | 141/620 [00:53<02:21,  3.38it/s] 23%|██▎       | 142/620 [00:53<02:19,  3.43it/s] 23%|██▎       | 143/620 [00:53<02:17,  3.47it/s] 23%|██▎       | 144/620 [00:54<02:15,  3.50it/s] 23%|██▎       | 145/620 [00:54<02:14,  3.52it/s] 24%|██▎       | 146/620 [00:54<02:13,  3.54it/s] 24%|██▎       | 147/620 [00:55<02:13,  3.55it/s] 24%|██▍       | 148/620 [00:55<02:12,  3.55it/s] 24%|██▍       | 149/620 [00:55<02:12,  3.56it/s] 24%|██▍       | 150/620 [00:55<02:11,  3.57it/s] 24%|██▍       | 151/620 [00:56<02:16,  3.42it/s] 25%|██▍       | 152/620 [00:56<02:14,  3.47it/s] 25%|██▍       | 153/620 [00:56<02:13,  3.50it/s] 25%|██▍       | 154/620 [00:57<02:12,  3.52it/s] 25%|██▌       | 155/620 [00:57<02:11,  3.53it/s] 25%|██▌       | 156/620 [00:57<02:11,  3.54it/s] 25%|██▌       | 157/620 [00:57<02:10,  3.55it/s] 25%|██▌       | 158/620 [00:58<02:10,  3.55it/s] 26%|██▌       | 159/620 [00:58<02:09,  3.55it/s] 26%|██▌       | 160/620 [00:58<02:09,  3.55it/s] 26%|██▌       | 161/620 [00:59<02:09,  3.55it/s] 26%|██▌       | 162/620 [00:59<02:13,  3.42it/s] 26%|██▋       | 163/620 [00:59<02:11,  3.47it/s] 26%|██▋       | 164/620 [00:59<02:10,  3.50it/s] 27%|██▋       | 165/620 [01:00<02:09,  3.52it/s] 27%|██▋       | 166/620 [01:00<02:08,  3.52it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/620 [01:00<02:08,  3.53it/s] 27%|██▋       | 168/620 [01:01<02:07,  3.54it/s] 27%|██▋       | 169/620 [01:01<02:07,  3.55it/s] 27%|██▋       | 170/620 [01:01<02:06,  3.55it/s] 28%|██▊       | 171/620 [01:01<02:06,  3.55it/s] 28%|██▊       | 172/620 [01:02<02:06,  3.55it/s] 28%|██▊       | 173/620 [01:02<02:06,  3.53it/s] 28%|██▊       | 174/620 [01:02<02:05,  3.54it/s] 28%|██▊       | 175/620 [01:03<02:05,  3.55it/s] 28%|██▊       | 176/620 [01:03<02:04,  3.56it/s] 29%|██▊       | 177/620 [01:03<02:04,  3.56it/s] 29%|██▊       | 178/620 [01:03<02:03,  3.56it/s] 29%|██▉       | 179/620 [01:04<02:03,  3.56it/s] 29%|██▉       | 180/620 [01:04<02:03,  3.56it/s] 29%|██▉       | 181/620 [01:04<02:03,  3.56it/s] 29%|██▉       | 182/620 [01:04<02:03,  3.56it/s] 30%|██▉       | 183/620 [01:05<02:02,  3.56it/s] 30%|██▉       | 184/620 [01:05<02:03,  3.54it/s] 30%|██▉       | 185/620 [01:05<02:02,  3.54it/s] 30%|███       | 186/620 [01:06<02:02,  3.55it/s] 30%|███       | 187/620 [01:06<02:02,  3.55it/s] 30%|███       | 188/620 [01:06<02:01,  3.55it/s] 30%|███       | 189/620 [01:06<02:01,  3.56it/s] 31%|███       | 190/620 [01:07<02:00,  3.56it/s] 31%|███       | 191/620 [01:07<02:00,  3.55it/s] 31%|███       | 192/620 [01:07<02:00,  3.55it/s] 31%|███       | 193/620 [01:08<02:00,  3.55it/s] 31%|███▏      | 194/620 [01:08<02:00,  3.55it/s] 31%|███▏      | 195/620 [01:08<01:59,  3.54it/s] 32%|███▏      | 196/620 [01:08<01:59,  3.55it/s] 32%|███▏      | 197/620 [01:09<01:58,  3.57it/s] 32%|███▏      | 198/620 [01:09<01:57,  3.58it/s] 32%|███▏      | 199/620 [01:09<01:57,  3.59it/s] 32%|███▏      | 200/620 [01:10<01:56,  3.60it/s] 32%|███▏      | 201/620 [01:10<01:56,  3.61it/s] 33%|███▎      | 202/620 [01:10<01:55,  3.61it/s] 33%|███▎      | 203/620 [01:10<01:55,  3.61it/s] 33%|███▎      | 204/620 [01:11<01:55,  3.61it/s] 33%|███▎      | 205/620 [01:11<01:54,  3.61it/s] 33%|███▎      | 206/620 [01:11<01:56,  3.55it/s] 33%|███▎      | 207/620 [01:11<01:55,  3.57it/s] 34%|███▎      | 208/620 [01:12<01:54,  3.58it/s] 34%|███▎      | 209/620 [01:12<01:54,  3.59it/s] 34%|███▍      | 210/620 [01:12<01:53,  3.60it/s] 34%|███▍      | 211/620 [01:13<01:53,  3.60it/s] 34%|███▍      | 212/620 [01:13<01:53,  3.60it/s] 34%|███▍      | 213/620 [01:13<01:52,  3.61it/s] 35%|███▍      | 214/620 [01:13<01:52,  3.61it/s] 35%|███▍      | 215/620 [01:14<01:52,  3.61it/s] 35%|███▍      | 216/620 [01:14<01:51,  3.61it/s] 35%|███▌      | 217/620 [01:14<01:51,  3.60it/s] 35%|███▌      | 218/620 [01:15<01:51,  3.60it/s] 35%|███▌      | 219/620 [01:15<01:51,  3.61it/s] 35%|███▌      | 220/620 [01:15<01:50,  3.61it/s] 36%|███▌      | 221/620 [01:15<01:50,  3.61it/s] 36%|███▌      | 222/620 [01:16<01:50,  3.60it/s] 36%|███▌      | 223/620 [01:16<01:50,  3.60it/s] 36%|███▌      | 224/620 [01:16<01:49,  3.60it/s] 36%|███▋      | 225/620 [01:16<01:49,  3.61it/s] 36%|███▋      | 226/620 [01:17<01:49,  3.61it/s] 37%|███▋      | 227/620 [01:17<01:48,  3.61it/s] 37%|███▋      | 228/620 [01:17<01:49,  3.58it/s] 37%|███▋      | 229/620 [01:18<01:49,  3.59it/s] 37%|███▋      | 230/620 [01:18<01:48,  3.60it/s] 37%|███▋      | 231/620 [01:18<01:48,  3.60it/s] 37%|███▋      | 232/620 [01:18<01:47,  3.61it/s] 38%|███▊      | 233/620 [01:19<01:47,  3.61it/s] 38%|███▊      | 234/620 [01:19<01:46,  3.61it/s] 38%|███▊      | 235/620 [01:19<01:46,  3.61it/s] 38%|███▊      | 236/620 [01:20<01:46,  3.61it/s] 38%|███▊      | 237/620 [01:20<01:45,  3.61it/s] 38%|███▊      | 238/620 [01:20<01:45,  3.61it/s] 39%|███▊      | 239/620 [01:20<01:46,  3.59it/s] 39%|███▊      | 240/620 [01:21<01:45,  3.60it/s] 39%|███▉      | 241/620 [01:21<01:45,  3.60it/s] 39%|███▉      | 242/620 [01:21<01:45,  3.60it/s] 39%|███▉      | 243/620 [01:21<01:44,  3.60it/s] 39%|███▉      | 244/620 [01:22<01:44,  3.60it/s] 40%|███▉      | 245/620 [01:22<01:43,  3.61it/s] 40%|███▉      | 246/620 [01:22<01:43,  3.61it/s] 40%|███▉      | 247/620 [01:23<01:43,  3.61it/s] 40%|████      | 248/620 [01:23<01:36,  3.87it/s][INFO|trainer.py:2140] 2023-08-28 07:05:29,819 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:05:29,819 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 07:05:29,819 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3065, 'eval_samples_per_second': 352.822, 'eval_steps_per_second': 44.123, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.00it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.17it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.18it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.29it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.83it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.49it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.20it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.89it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.13it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.20it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.31it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.29it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.16it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.06it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.87it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.84it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.87it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.98it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.13it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.18it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.21it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.00it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.89it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.84it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.90it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.08it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.09it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.16it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.14it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.06it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.04it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.78it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.90it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.00it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.12it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.16it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.13it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.09it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.11it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.01it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.93it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.93it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.97it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.07it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.07it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.05it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.95it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.05it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.96it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.97it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.95it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.02it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.05it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.07it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 43.97it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.97it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.07it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.93it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.86it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.00it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.08it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.02it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.16it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.05it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.09it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.00it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.90it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.94it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.12it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.12it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.13it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.07it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.16it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.08it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.95it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.97it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.82it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.12it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.15it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.09it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.06it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.09it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.01it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.06it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.04it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.03it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.99it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.07it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.09it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.03it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.97it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.97it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.02it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.99it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.01it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.07it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.09it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.05it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.00it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.02it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.00it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.00it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.96it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.98it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.13it/s][A                                                 
                                                 [A 40%|████      | 248/620 [01:35<01:36,  3.87it/s]
100%|██████████| 543/543 [00:12<00:00, 44.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:05:42,163 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248
[INFO|configuration_utils.py:351] 2023-08-28 07:05:42,181 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:05:43,853 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:05:43,870 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:05:43,881 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248/special_tokens_map.json
 40%|████      | 249/620 [01:37<28:16,  4.57s/it] 40%|████      | 250/620 [01:38<20:15,  3.29s/it] 40%|████      | 251/620 [01:38<14:39,  2.38s/it] 41%|████      | 252/620 [01:38<10:45,  1.75s/it] 41%|████      | 253/620 [01:39<08:01,  1.31s/it] 41%|████      | 254/620 [01:39<06:06,  1.00s/it] 41%|████      | 255/620 [01:39<04:46,  1.27it/s] 41%|████▏     | 256/620 [01:39<03:50,  1.58it/s] 41%|████▏     | 257/620 [01:40<03:10,  1.90it/s] 42%|████▏     | 258/620 [01:40<02:43,  2.22it/s] 42%|████▏     | 259/620 [01:40<02:24,  2.50it/s] 42%|████▏     | 260/620 [01:41<02:10,  2.76it/s] 42%|████▏     | 261/620 [01:41<02:00,  2.97it/s] 42%|████▏     | 262/620 [01:41<01:54,  3.14it/s] 42%|████▏     | 263/620 [01:41<01:49,  3.27it/s] 43%|████▎     | 264/620 [01:42<01:46,  3.36it/s] 43%|████▎     | 265/620 [01:42<01:43,  3.43it/s] 43%|████▎     | 266/620 [01:42<01:41,  3.48it/s] 43%|████▎     | 267/620 [01:42<01:40,  3.52it/s] 43%|████▎     | 268/620 [01:43<01:39,  3.55it/s] 43%|████▎     | 269/620 [01:43<01:38,  3.57it/s] 44%|████▎     | 270/620 [01:43<01:38,  3.57it/s] 44%|████▎     | 271/620 [01:44<01:37,  3.58it/s] 44%|████▍     | 272/620 [01:44<01:36,  3.59it/s] 44%|████▍     | 273/620 [01:44<01:36,  3.60it/s] 44%|████▍     | 274/620 [01:44<01:35,  3.60it/s] 44%|████▍     | 275/620 [01:45<01:35,  3.60it/s] 45%|████▍     | 276/620 [01:45<01:35,  3.61it/s] 45%|████▍     | 277/620 [01:45<01:35,  3.61it/s] 45%|████▍     | 278/620 [01:46<01:34,  3.61it/s] 45%|████▌     | 279/620 [01:46<01:34,  3.61it/s] 45%|████▌     | 280/620 [01:46<01:34,  3.61it/s] 45%|████▌     | 281/620 [01:46<01:34,  3.57it/s] 45%|████▌     | 282/620 [01:47<01:34,  3.57it/s] 46%|████▌     | 283/620 [01:47<01:34,  3.56it/s] 46%|████▌     | 284/620 [01:47<01:34,  3.56it/s] 46%|████▌     | 285/620 [01:47<01:34,  3.56it/s] 46%|████▌     | 286/620 [01:48<01:33,  3.56it/s] 46%|████▋     | 287/620 [01:48<01:33,  3.55it/s] 46%|████▋     | 288/620 [01:48<01:33,  3.55it/s] 47%|████▋     | 289/620 [01:49<01:33,  3.56it/s] 47%|████▋     | 290/620 [01:49<01:32,  3.56it/s] 47%|████▋     | 291/620 [01:49<01:32,  3.56it/s] 47%|████▋     | 292/620 [01:49<01:32,  3.55it/s] 47%|████▋     | 293/620 [01:50<01:32,  3.55it/s] 47%|████▋     | 294/620 [01:50<01:31,  3.55it/s] 48%|████▊     | 295/620 [01:50<01:33,  3.48it/s] 48%|████▊     | 296/620 [01:51<01:33,  3.48it/s] 48%|████▊     | 297/620 [01:51<01:32,  3.50it/s] 48%|████▊     | 298/620 [01:51<01:31,  3.52it/s] 48%|████▊     | 299/620 [01:51<01:30,  3.53it/s] 48%|████▊     | 300/620 [01:52<01:30,  3.54it/s] 49%|████▊     | 301/620 [01:52<01:30,  3.54it/s] 49%|████▊     | 302/620 [01:52<01:29,  3.55it/s] 49%|████▉     | 303/620 [01:53<01:29,  3.54it/s] 49%|████▉     | 304/620 [01:53<01:29,  3.55it/s] 49%|████▉     | 305/620 [01:53<01:28,  3.55it/s] 49%|████▉     | 306/620 [01:53<01:28,  3.55it/s] 50%|████▉     | 307/620 [01:54<01:28,  3.55it/s] 50%|████▉     | 308/620 [01:54<01:27,  3.56it/s] 50%|████▉     | 309/620 [01:54<01:27,  3.56it/s] 50%|█████     | 310/620 [01:55<01:27,  3.56it/s] 50%|█████     | 311/620 [01:55<01:26,  3.57it/s] 50%|█████     | 312/620 [01:55<01:26,  3.57it/s] 50%|█████     | 313/620 [01:55<01:26,  3.56it/s] 51%|█████     | 314/620 [01:56<01:26,  3.54it/s] 51%|█████     | 315/620 [01:56<01:25,  3.55it/s] 51%|█████     | 316/620 [01:56<01:25,  3.55it/s] 51%|█████     | 317/620 [01:56<01:25,  3.55it/s] 51%|█████▏    | 318/620 [01:57<01:25,  3.55it/s] 51%|█████▏    | 319/620 [01:57<01:24,  3.55it/s] 52%|█████▏    | 320/620 [01:57<01:24,  3.55it/s] 52%|█████▏    | 321/620 [01:58<01:24,  3.55it/s] 52%|█████▏    | 322/620 [01:58<01:23,  3.56it/s] 52%|█████▏    | 323/620 [01:58<01:23,  3.56it/s] 52%|█████▏    | 324/620 [01:58<01:23,  3.56it/s] 52%|█████▏    | 325/620 [01:59<01:22,  3.56it/s] 53%|█████▎    | 326/620 [01:59<01:22,  3.56it/s] 53%|█████▎    | 327/620 [01:59<01:22,  3.56it/s] 53%|█████▎    | 328/620 [02:00<01:22,  3.56it/s] 53%|█████▎    | 329/620 [02:00<01:21,  3.56it/s] 53%|█████▎    | 330/620 [02:00<01:22,  3.54it/s] 53%|█████▎    | 331/620 [02:00<01:21,  3.54it/s] 54%|█████▎    | 332/620 [02:01<01:20,  3.56it/s] 54%|█████▎    | 333/620 [02:01<01:20,  3.57it/s] 54%|█████▍    | 334/620 [02:01<01:19,  3.58it/s] 54%|█████▍    | 335/620 [02:02<01:19,  3.59it/s] 54%|█████▍    | 336/620 [02:02<01:18,  3.60it/s] 54%|█████▍    | 337/620 [02:02<01:18,  3.60it/s] 55%|█████▍    | 338/620 [02:02<01:18,  3.60it/s] 55%|█████▍    | 339/620 [02:03<01:17,  3.61it/s] 55%|█████▍    | 340/620 [02:03<01:17,  3.61it/s] 55%|█████▌    | 341/620 [02:03<01:17,  3.60it/s] 55%|█████▌    | 342/620 [02:03<01:17,  3.60it/s] 55%|█████▌    | 343/620 [02:04<01:16,  3.61it/s] 55%|█████▌    | 344/620 [02:04<01:16,  3.60it/s] 56%|█████▌    | 345/620 [02:04<01:16,  3.60it/s] 56%|█████▌    | 346/620 [02:05<01:16,  3.60it/s] 56%|█████▌    | 347/620 [02:05<01:15,  3.60it/s] 56%|█████▌    | 348/620 [02:05<01:15,  3.60it/s] 56%|█████▋    | 349/620 [02:05<01:15,  3.60it/s] 56%|█████▋    | 350/620 [02:06<01:14,  3.60it/s] 57%|█████▋    | 351/620 [02:06<01:14,  3.61it/s] 57%|█████▋    | 352/620 [02:06<01:14,  3.60it/s] 57%|█████▋    | 353/620 [02:07<01:14,  3.60it/s] 57%|█████▋    | 354/620 [02:07<01:13,  3.60it/s] 57%|█████▋    | 355/620 [02:07<01:13,  3.60it/s] 57%|█████▋    | 356/620 [02:07<01:13,  3.60it/s] 58%|█████▊    | 357/620 [02:08<01:12,  3.60it/s] 58%|█████▊    | 358/620 [02:08<01:12,  3.60it/s] 58%|█████▊    | 359/620 [02:08<01:12,  3.61it/s] 58%|█████▊    | 360/620 [02:08<01:11,  3.61it/s] 58%|█████▊    | 361/620 [02:09<01:11,  3.61it/s] 58%|█████▊    | 362/620 [02:09<01:11,  3.61it/s] 59%|█████▊    | 363/620 [02:09<01:11,  3.59it/s] 59%|█████▊    | 364/620 [02:10<01:11,  3.60it/s] 59%|█████▉    | 365/620 [02:10<01:10,  3.60it/s] 59%|█████▉    | 366/620 [02:10<01:10,  3.60it/s] 59%|█████▉    | 367/620 [02:10<01:10,  3.60it/s] 59%|█████▉    | 368/620 [02:11<01:09,  3.61it/s] 60%|█████▉    | 369/620 [02:11<01:09,  3.61it/s] 60%|█████▉    | 370/620 [02:11<01:09,  3.61it/s] 60%|█████▉    | 371/620 [02:12<01:09,  3.61it/s] 60%|██████    | 372/620 [02:12<01:04,  3.87it/s][INFO|trainer.py:2140] 2023-08-28 07:06:18,762 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:06:18,762 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 07:06:18,762 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3277, 'eval_samples_per_second': 352.216, 'eval_steps_per_second': 44.047, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.10it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.33it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.28it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.14it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.72it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.37it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.16it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.09it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.05it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.14it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.35it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.30it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.10it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.11it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.96it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.81it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.83it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.04it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.16it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.19it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.22it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.03it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.07it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.92it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.67it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.79it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.98it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.24it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.26it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.24it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.09it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.03it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.89it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.79it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.83it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.99it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.09it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.21it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.17it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.10it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.98it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.91it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.86it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.91it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.07it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.16it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.19it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.12it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.06it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.96it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.95it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.94it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.94it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.05it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.08it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.12it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.10it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.04it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.94it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.94it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.91it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.82it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.96it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.14it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.07it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.07it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.98it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.97it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.97it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.91it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.88it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.06it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.17it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.09it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.20it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.01it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.93it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.95it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.92it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.96it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.09it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.03it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.12it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.13it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.99it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.97it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.87it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.99it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.05it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.07it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.03it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.06it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.10it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.02it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.98it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.06it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.08it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.06it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.07it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.04it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.03it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.94it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.98it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.99it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.07it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.12it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.08it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.93it/s][A                                                 
                                                 [A 60%|██████    | 372/620 [02:24<01:04,  3.87it/s]
100%|██████████| 543/543 [00:12<00:00, 43.93it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:06:31,123 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372
[INFO|configuration_utils.py:351] 2023-08-28 07:06:31,143 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:06:32,867 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:06:32,885 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:06:32,897 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372/special_tokens_map.json
 60%|██████    | 373/620 [02:26<18:54,  4.59s/it] 60%|██████    | 374/620 [02:27<13:31,  3.30s/it] 60%|██████    | 375/620 [02:27<09:46,  2.39s/it] 61%|██████    | 376/620 [02:27<07:09,  1.76s/it] 61%|██████    | 377/620 [02:28<05:19,  1.32s/it] 61%|██████    | 378/620 [02:28<04:03,  1.01s/it] 61%|██████    | 379/620 [02:28<03:09,  1.27it/s] 61%|██████▏   | 380/620 [02:28<02:32,  1.57it/s] 61%|██████▏   | 381/620 [02:29<02:06,  1.89it/s] 62%|██████▏   | 382/620 [02:29<01:48,  2.20it/s] 62%|██████▏   | 383/620 [02:29<01:35,  2.48it/s] 62%|██████▏   | 384/620 [02:30<01:26,  2.73it/s] 62%|██████▏   | 385/620 [02:30<01:20,  2.94it/s] 62%|██████▏   | 386/620 [02:30<01:15,  3.10it/s] 62%|██████▏   | 387/620 [02:30<01:12,  3.22it/s] 63%|██████▎   | 388/620 [02:31<01:09,  3.32it/s] 63%|██████▎   | 389/620 [02:31<01:08,  3.38it/s] 63%|██████▎   | 390/620 [02:31<01:07,  3.43it/s] 63%|██████▎   | 391/620 [02:32<01:06,  3.47it/s] 63%|██████▎   | 392/620 [02:32<01:05,  3.50it/s] 63%|██████▎   | 393/620 [02:32<01:04,  3.51it/s] 64%|██████▎   | 394/620 [02:32<01:04,  3.53it/s] 64%|██████▎   | 395/620 [02:33<01:03,  3.54it/s] 64%|██████▍   | 396/620 [02:33<01:03,  3.55it/s] 64%|██████▍   | 397/620 [02:33<01:02,  3.55it/s] 64%|██████▍   | 398/620 [02:33<01:02,  3.56it/s] 64%|██████▍   | 399/620 [02:34<01:02,  3.56it/s] 65%|██████▍   | 400/620 [02:34<01:02,  3.54it/s] 65%|██████▍   | 401/620 [02:34<01:01,  3.54it/s] 65%|██████▍   | 402/620 [02:35<01:01,  3.54it/s] 65%|██████▌   | 403/620 [02:35<01:01,  3.55it/s] 65%|██████▌   | 404/620 [02:35<01:00,  3.55it/s] 65%|██████▌   | 405/620 [02:35<01:00,  3.55it/s] 65%|██████▌   | 406/620 [02:36<01:00,  3.55it/s] 66%|██████▌   | 407/620 [02:36<00:59,  3.56it/s] 66%|██████▌   | 408/620 [02:36<00:59,  3.56it/s] 66%|██████▌   | 409/620 [02:37<00:59,  3.56it/s] 66%|██████▌   | 410/620 [02:37<00:59,  3.56it/s] 66%|██████▋   | 411/620 [02:37<00:58,  3.54it/s] 66%|██████▋   | 412/620 [02:37<00:58,  3.55it/s] 67%|██████▋   | 413/620 [02:38<00:58,  3.55it/s] 67%|██████▋   | 414/620 [02:38<00:58,  3.55it/s] 67%|██████▋   | 415/620 [02:38<00:57,  3.55it/s] 67%|██████▋   | 416/620 [02:39<00:57,  3.55it/s] 67%|██████▋   | 417/620 [02:39<00:57,  3.55it/s] 67%|██████▋   | 418/620 [02:39<00:56,  3.55it/s] 68%|██████▊   | 419/620 [02:39<00:56,  3.56it/s] 68%|██████▊   | 420/620 [02:40<00:56,  3.56it/s] 68%|██████▊   | 421/620 [02:40<00:55,  3.56it/s] 68%|██████▊   | 422/620 [02:40<00:55,  3.55it/s] 68%|██████▊   | 423/620 [02:41<00:55,  3.55it/s] 68%|██████▊   | 424/620 [02:41<00:55,  3.55it/s] 69%|██████▊   | 425/620 [02:41<00:54,  3.56it/s] 69%|██████▊   | 426/620 [02:41<00:54,  3.56it/s] 69%|██████▉   | 427/620 [02:42<00:54,  3.56it/s] 69%|██████▉   | 428/620 [02:42<00:53,  3.56it/s] 69%|██████▉   | 429/620 [02:42<00:53,  3.56it/s] 69%|██████▉   | 430/620 [02:42<00:53,  3.56it/s] 70%|██████▉   | 431/620 [02:43<00:53,  3.56it/s] 70%|██████▉   | 432/620 [02:43<00:52,  3.56it/s] 70%|██████▉   | 433/620 [02:43<00:52,  3.55it/s] 70%|███████   | 434/620 [02:44<00:52,  3.55it/s] 70%|███████   | 435/620 [02:44<00:52,  3.56it/s] 70%|███████   | 436/620 [02:44<00:51,  3.56it/s] 70%|███████   | 437/620 [02:44<00:51,  3.56it/s] 71%|███████   | 438/620 [02:45<00:51,  3.56it/s] 71%|███████   | 439/620 [02:45<00:50,  3.56it/s] 71%|███████   | 440/620 [02:45<00:50,  3.56it/s] 71%|███████   | 441/620 [02:46<00:50,  3.56it/s] 71%|███████▏  | 442/620 [02:46<00:49,  3.56it/s] 71%|███████▏  | 443/620 [02:46<00:49,  3.57it/s] 72%|███████▏  | 444/620 [02:46<00:49,  3.55it/s] 72%|███████▏  | 445/620 [02:47<00:49,  3.56it/s] 72%|███████▏  | 446/620 [02:47<00:48,  3.56it/s] 72%|███████▏  | 447/620 [02:47<00:48,  3.56it/s] 72%|███████▏  | 448/620 [02:48<00:48,  3.56it/s] 72%|███████▏  | 449/620 [02:48<00:48,  3.56it/s] 73%|███████▎  | 450/620 [02:48<00:47,  3.55it/s] 73%|███████▎  | 451/620 [02:48<00:47,  3.55it/s] 73%|███████▎  | 452/620 [02:49<00:47,  3.55it/s] 73%|███████▎  | 453/620 [02:49<00:46,  3.56it/s] 73%|███████▎  | 454/620 [02:49<00:46,  3.56it/s] 73%|███████▎  | 455/620 [02:50<00:46,  3.54it/s] 74%|███████▎  | 456/620 [02:50<00:46,  3.55it/s] 74%|███████▎  | 457/620 [02:50<00:45,  3.55it/s] 74%|███████▍  | 458/620 [02:50<00:46,  3.48it/s] 74%|███████▍  | 459/620 [02:51<00:46,  3.49it/s] 74%|███████▍  | 460/620 [02:51<00:45,  3.51it/s] 74%|███████▍  | 461/620 [02:51<00:45,  3.52it/s] 75%|███████▍  | 462/620 [02:52<00:44,  3.53it/s] 75%|███████▍  | 463/620 [02:52<00:44,  3.54it/s] 75%|███████▍  | 464/620 [02:52<00:44,  3.55it/s] 75%|███████▌  | 465/620 [02:52<00:43,  3.55it/s] 75%|███████▌  | 466/620 [02:53<00:43,  3.53it/s] 75%|███████▌  | 467/620 [02:53<00:43,  3.54it/s] 75%|███████▌  | 468/620 [02:53<00:42,  3.54it/s] 76%|███████▌  | 469/620 [02:53<00:42,  3.55it/s] 76%|███████▌  | 470/620 [02:54<00:42,  3.55it/s] 76%|███████▌  | 471/620 [02:54<00:41,  3.55it/s] 76%|███████▌  | 472/620 [02:54<00:41,  3.55it/s] 76%|███████▋  | 473/620 [02:55<00:41,  3.55it/s] 76%|███████▋  | 474/620 [02:55<00:41,  3.55it/s] 77%|███████▋  | 475/620 [02:55<00:40,  3.56it/s] 77%|███████▋  | 476/620 [02:55<00:40,  3.56it/s] 77%|███████▋  | 477/620 [02:56<00:40,  3.54it/s] 77%|███████▋  | 478/620 [02:56<00:40,  3.55it/s] 77%|███████▋  | 479/620 [02:56<00:39,  3.55it/s] 77%|███████▋  | 480/620 [02:57<00:39,  3.57it/s] 78%|███████▊  | 481/620 [02:57<00:38,  3.59it/s] 78%|███████▊  | 482/620 [02:57<00:38,  3.59it/s] 78%|███████▊  | 483/620 [02:57<00:38,  3.60it/s] 78%|███████▊  | 484/620 [02:58<00:37,  3.60it/s] 78%|███████▊  | 485/620 [02:58<00:37,  3.61it/s] 78%|███████▊  | 486/620 [02:58<00:37,  3.61it/s] 79%|███████▊  | 487/620 [02:59<00:36,  3.61it/s] 79%|███████▊  | 488/620 [02:59<00:36,  3.61it/s] 79%|███████▉  | 489/620 [02:59<00:36,  3.61it/s] 79%|███████▉  | 490/620 [02:59<00:36,  3.61it/s] 79%|███████▉  | 491/620 [03:00<00:35,  3.61it/s] 79%|███████▉  | 492/620 [03:00<00:35,  3.61it/s] 80%|███████▉  | 493/620 [03:00<00:35,  3.61it/s] 80%|███████▉  | 494/620 [03:00<00:34,  3.61it/s] 80%|███████▉  | 495/620 [03:01<00:34,  3.61it/s] 80%|████████  | 496/620 [03:01<00:32,  3.87it/s][INFO|trainer.py:2140] 2023-08-28 07:07:07,949 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:07:07,949 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 07:07:07,949 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3302, 'eval_samples_per_second': 352.143, 'eval_steps_per_second': 44.038, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.94it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.25it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.25it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.39it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.84it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.30it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.08it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.99it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.16it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.27it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.41it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.31it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.17it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.03it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.95it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.87it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.83it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.03it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.98it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.22it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.22it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.06it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.95it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.91it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.79it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.87it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.02it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.22it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.24it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.12it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.08it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.98it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.79it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.80it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.86it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.01it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.20it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.24it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.25it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.15it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.90it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.65it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.79it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.95it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.05it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.20it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.23it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.20it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.09it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.93it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.92it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.87it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.04it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.96it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.16it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.25it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.17it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.01it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.92it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.90it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.00it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.09it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.16it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.10it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.13it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.06it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.01it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.92it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.93it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.02it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.13it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.17it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.02it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.91it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.99it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.99it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.06it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.95it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.07it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.08it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.12it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.09it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.97it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.81it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.02it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.09it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.12it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.12it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.07it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.08it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.04it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.03it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.95it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.86it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.10it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.13it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.16it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.08it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.00it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.98it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.95it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.05it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.08it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.11it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.11it/s][A                                                 
                                                 [A 80%|████████  | 496/620 [03:13<00:32,  3.87it/s]
100%|██████████| 543/543 [00:12<00:00, 44.11it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:07:20,291 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496
[INFO|configuration_utils.py:351] 2023-08-28 07:07:20,308 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:07:22,021 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:07:22,038 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:07:22,045 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496/special_tokens_map.json
 80%|████████  | 497/620 [03:16<09:32,  4.66s/it] 80%|████████  | 498/620 [03:16<06:48,  3.34s/it] 80%|████████  | 499/620 [03:16<04:53,  2.43s/it] 81%|████████  | 500/620 [03:17<03:33,  1.78s/it]                                                  81%|████████  | 500/620 [03:17<03:33,  1.78s/it] 81%|████████  | 501/620 [03:17<02:38,  1.33s/it] 81%|████████  | 502/620 [03:17<01:59,  1.02s/it] 81%|████████  | 503/620 [03:18<01:33,  1.26it/s] 81%|████████▏ | 504/620 [03:18<01:14,  1.56it/s] 81%|████████▏ | 505/620 [03:18<01:01,  1.88it/s] 82%|████████▏ | 506/620 [03:18<00:52,  2.19it/s] 82%|████████▏ | 507/620 [03:19<00:45,  2.47it/s] 82%|████████▏ | 508/620 [03:19<00:41,  2.72it/s] 82%|████████▏ | 509/620 [03:19<00:37,  2.92it/s] 82%|████████▏ | 510/620 [03:20<00:35,  3.09it/s] 82%|████████▏ | 511/620 [03:20<00:33,  3.21it/s] 83%|████████▎ | 512/620 [03:20<00:32,  3.28it/s] 83%|████████▎ | 513/620 [03:20<00:31,  3.36it/s] 83%|████████▎ | 514/620 [03:21<00:31,  3.42it/s] 83%|████████▎ | 515/620 [03:21<00:30,  3.46it/s] 83%|████████▎ | 516/620 [03:21<00:29,  3.49it/s] 83%|████████▎ | 517/620 [03:21<00:29,  3.51it/s] 84%|████████▎ | 518/620 [03:22<00:28,  3.52it/s] 84%|████████▎ | 519/620 [03:22<00:28,  3.53it/s] 84%|████████▍ | 520/620 [03:22<00:28,  3.54it/s] 84%|████████▍ | 521/620 [03:23<00:27,  3.55it/s] 84%|████████▍ | 522/620 [03:23<00:27,  3.55it/s] 84%|████████▍ | 523/620 [03:23<00:27,  3.55it/s] 85%|████████▍ | 524/620 [03:23<00:26,  3.57it/s] 85%|████████▍ | 525/620 [03:24<00:26,  3.58it/s] 85%|████████▍ | 526/620 [03:24<00:26,  3.58it/s] 85%|████████▌ | 527/620 [03:24<00:25,  3.59it/s] 85%|████████▌ | 528/620 [03:25<00:25,  3.60it/s] 85%|████████▌ | 529/620 [03:25<00:25,  3.61it/s] 85%|████████▌ | 530/620 [03:25<00:24,  3.60it/s] 86%|████████▌ | 531/620 [03:25<00:24,  3.60it/s] 86%|████████▌ | 532/620 [03:26<00:24,  3.60it/s] 86%|████████▌ | 533/620 [03:26<00:24,  3.60it/s] 86%|████████▌ | 534/620 [03:26<00:23,  3.59it/s] 86%|████████▋ | 535/620 [03:27<00:23,  3.59it/s] 86%|████████▋ | 536/620 [03:27<00:23,  3.59it/s] 87%|████████▋ | 537/620 [03:27<00:23,  3.60it/s] 87%|████████▋ | 538/620 [03:27<00:22,  3.60it/s] 87%|████████▋ | 539/620 [03:28<00:22,  3.61it/s] 87%|████████▋ | 540/620 [03:28<00:22,  3.61it/s] 87%|████████▋ | 541/620 [03:28<00:21,  3.61it/s] 87%|████████▋ | 542/620 [03:28<00:21,  3.61it/s] 88%|████████▊ | 543/620 [03:29<00:21,  3.61it/s] 88%|████████▊ | 544/620 [03:29<00:21,  3.61it/s] 88%|████████▊ | 545/620 [03:29<00:20,  3.59it/s] 88%|████████▊ | 546/620 [03:30<00:20,  3.60it/s] 88%|████████▊ | 547/620 [03:30<00:20,  3.60it/s] 88%|████████▊ | 548/620 [03:30<00:19,  3.60it/s] 89%|████████▊ | 549/620 [03:30<00:19,  3.60it/s] 89%|████████▊ | 550/620 [03:31<00:19,  3.60it/s] 89%|████████▉ | 551/620 [03:31<00:19,  3.61it/s] 89%|████████▉ | 552/620 [03:31<00:18,  3.61it/s] 89%|████████▉ | 553/620 [03:32<00:18,  3.61it/s] 89%|████████▉ | 554/620 [03:32<00:18,  3.61it/s] 90%|████████▉ | 555/620 [03:32<00:18,  3.61it/s] 90%|████████▉ | 556/620 [03:32<00:17,  3.59it/s] 90%|████████▉ | 557/620 [03:33<00:17,  3.60it/s] 90%|█████████ | 558/620 [03:33<00:17,  3.60it/s] 90%|█████████ | 559/620 [03:33<00:16,  3.61it/s] 90%|█████████ | 560/620 [03:33<00:16,  3.61it/s] 90%|█████████ | 561/620 [03:34<00:16,  3.61it/s] 91%|█████████ | 562/620 [03:34<00:16,  3.61it/s] 91%|█████████ | 563/620 [03:34<00:15,  3.61it/s] 91%|█████████ | 564/620 [03:35<00:15,  3.61it/s] 91%|█████████ | 565/620 [03:35<00:15,  3.61it/s] 91%|█████████▏| 566/620 [03:35<00:14,  3.61it/s] 91%|█████████▏| 567/620 [03:35<00:14,  3.60it/s] 92%|█████████▏| 568/620 [03:36<00:14,  3.60it/s] 92%|█████████▏| 569/620 [03:36<00:14,  3.61it/s] 92%|█████████▏| 570/620 [03:36<00:13,  3.61it/s] 92%|█████████▏| 571/620 [03:36<00:13,  3.61it/s] 92%|█████████▏| 572/620 [03:37<00:13,  3.61it/s] 92%|█████████▏| 573/620 [03:37<00:13,  3.61it/s] 93%|█████████▎| 574/620 [03:37<00:12,  3.61it/s] 93%|█████████▎| 575/620 [03:38<00:12,  3.61it/s] 93%|█████████▎| 576/620 [03:38<00:12,  3.61it/s] 93%|█████████▎| 577/620 [03:38<00:11,  3.61it/s] 93%|█████████▎| 578/620 [03:38<00:11,  3.60it/s] 93%|█████████▎| 579/620 [03:39<00:11,  3.61it/s] 94%|█████████▎| 580/620 [03:39<00:11,  3.61it/s] 94%|█████████▎| 581/620 [03:39<00:10,  3.61it/s] 94%|█████████▍| 582/620 [03:40<00:10,  3.61it/s] 94%|█████████▍| 583/620 [03:40<00:10,  3.61it/s] 94%|█████████▍| 584/620 [03:40<00:09,  3.61it/s] 94%|█████████▍| 585/620 [03:40<00:09,  3.61it/s] 95%|█████████▍| 586/620 [03:41<00:09,  3.61it/s] 95%|█████████▍| 587/620 [03:41<00:09,  3.61it/s] 95%|█████████▍| 588/620 [03:41<00:08,  3.61it/s] 95%|█████████▌| 589/620 [03:41<00:08,  3.60it/s] 95%|█████████▌| 590/620 [03:42<00:08,  3.61it/s] 95%|█████████▌| 591/620 [03:42<00:08,  3.61it/s] 95%|█████████▌| 592/620 [03:42<00:07,  3.61it/s] 96%|█████████▌| 593/620 [03:43<00:07,  3.60it/s] 96%|█████████▌| 594/620 [03:43<00:07,  3.61it/s] 96%|█████████▌| 595/620 [03:43<00:06,  3.61it/s] 96%|█████████▌| 596/620 [03:43<00:06,  3.61it/s] 96%|█████████▋| 597/620 [03:44<00:06,  3.61it/s] 96%|█████████▋| 598/620 [03:44<00:06,  3.61it/s] 97%|█████████▋| 599/620 [03:44<00:05,  3.61it/s] 97%|█████████▋| 600/620 [03:45<00:05,  3.60it/s] 97%|█████████▋| 601/620 [03:45<00:05,  3.60it/s] 97%|█████████▋| 602/620 [03:45<00:04,  3.60it/s] 97%|█████████▋| 603/620 [03:45<00:04,  3.60it/s] 97%|█████████▋| 604/620 [03:46<00:04,  3.60it/s] 98%|█████████▊| 605/620 [03:46<00:04,  3.61it/s] 98%|█████████▊| 606/620 [03:46<00:03,  3.61it/s] 98%|█████████▊| 607/620 [03:46<00:03,  3.61it/s] 98%|█████████▊| 608/620 [03:47<00:03,  3.61it/s] 98%|█████████▊| 609/620 [03:47<00:03,  3.61it/s] 98%|█████████▊| 610/620 [03:47<00:02,  3.61it/s] 99%|█████████▊| 611/620 [03:48<00:02,  3.61it/s] 99%|█████████▊| 612/620 [03:48<00:02,  3.59it/s] 99%|█████████▉| 613/620 [03:48<00:01,  3.58it/s] 99%|█████████▉| 614/620 [03:48<00:01,  3.58it/s] 99%|█████████▉| 615/620 [03:49<00:01,  3.59it/s] 99%|█████████▉| 616/620 [03:49<00:01,  3.60it/s]100%|█████████▉| 617/620 [03:49<00:00,  3.60it/s]100%|█████████▉| 618/620 [03:50<00:00,  3.61it/s]100%|█████████▉| 619/620 [03:50<00:00,  3.60it/s]100%|██████████| 620/620 [03:50<00:00,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 07:07:57,036 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:07:57,036 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 07:07:57,036 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3253, 'eval_samples_per_second': 352.283, 'eval_steps_per_second': 44.056, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7298387096774197e-05, 'epoch': 4.03}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.77it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.35it/s][A
  3%|▎         | 17/543 [00:00<00:11, 44.67it/s][A
  4%|▍         | 22/543 [00:00<00:11, 44.96it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.49it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.05it/s][A
  7%|▋         | 37/543 [00:00<00:11, 43.99it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.92it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.02it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.06it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.23it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.29it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.23it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.18it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.95it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.86it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.84it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.97it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.03it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.28it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.18it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.27it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.14it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.95it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.85it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.80it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.95it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.03it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.27it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.28it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.15it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.05it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.90it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.84it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.97it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.03it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.04it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.20it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.27it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.16it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.01it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.95it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.86it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.98it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.10it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.09it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.26it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.11it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.06it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.03it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.98it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.93it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.93it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.00it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.15it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.20it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.13it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.93it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.00it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.02it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.12it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.17it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.13it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.02it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.10it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.91it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.02it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.06it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.06it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.18it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.96it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.04it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.13it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.10it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.99it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.05it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.03it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.16it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.21it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.17it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.06it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.10it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.83it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.95it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.03it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.09it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.08it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.20it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.04it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.07it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.10it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.00it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.00it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.83it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.19it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.24it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.14it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.17it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.16it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.95it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.92it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.92it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.06it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.19it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.16it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.05it/s][A                                                 
                                                 [A100%|██████████| 620/620 [04:02<00:00,  3.86it/s]
100%|██████████| 543/543 [00:12<00:00, 44.05it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:08:09,387 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620
[INFO|configuration_utils.py:351] 2023-08-28 07:08:09,404 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:08:12,267 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:08:12,295 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:08:12,303 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 07:08:12,579 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 07:08:12,579 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124 (score: 1.0794728994369507).
                                                 100%|██████████| 620/620 [04:07<00:00,  3.86it/s]100%|██████████| 620/620 [04:07<00:00,  2.50it/s]
[INFO|trainer.py:1894] 2023-08-28 07:08:14,303 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 07:08:14,322 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 07:08:16,138 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 07:08:16,150 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 07:08:16,162 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:08:16,356 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:16,356 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:16,357 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:16,357 >>   train_runtime            = 0:04:07.78
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:16,357 >>   train_samples            =       7920
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:16,357 >>   train_samples_per_second =    159.813
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:16,357 >>   train_steps_per_second   =      2.502
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3263, 'eval_samples_per_second': 352.255, 'eval_steps_per_second': 44.052, 'epoch': 5.0}
{'train_runtime': 247.7893, 'train_samples_per_second': 159.813, 'train_steps_per_second': 2.502, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 07:08:16 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 07:08:16,394 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:08:16,395 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 07:08:16,395 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 56.67it/s]  2%|▏         | 12/543 [00:00<00:10, 49.01it/s]  3%|▎         | 17/543 [00:00<00:11, 47.22it/s]  4%|▍         | 22/543 [00:00<00:11, 46.32it/s]  5%|▍         | 27/543 [00:00<00:11, 45.73it/s]  6%|▌         | 32/543 [00:00<00:11, 45.43it/s]  7%|▋         | 37/543 [00:00<00:11, 45.17it/s]  8%|▊         | 42/543 [00:00<00:11, 44.67it/s]  9%|▊         | 47/543 [00:01<00:11, 43.93it/s] 10%|▉         | 52/543 [00:01<00:11, 43.67it/s] 10%|█         | 57/543 [00:01<00:11, 43.85it/s] 11%|█▏        | 62/543 [00:01<00:10, 43.97it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.23it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.34it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.44it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.38it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.05it/s] 17%|█▋        | 92/543 [00:02<00:10, 43.58it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.60it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.72it/s] 20%|█▉        | 107/543 [00:02<00:09, 43.99it/s] 21%|██        | 112/543 [00:02<00:09, 44.18it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.25it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.28it/s] 23%|██▎       | 127/543 [00:02<00:09, 44.13it/s] 24%|██▍       | 132/543 [00:02<00:09, 44.03it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.84it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.78it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.75it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.04it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.19it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.23it/s] 31%|███       | 167/543 [00:03<00:08, 44.18it/s] 32%|███▏      | 172/543 [00:03<00:08, 44.06it/s] 33%|███▎      | 177/543 [00:03<00:08, 43.85it/s] 34%|███▎      | 182/543 [00:04<00:08, 43.75it/s] 34%|███▍      | 187/543 [00:04<00:08, 43.74it/s] 35%|███▌      | 192/543 [00:04<00:07, 43.88it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.03it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.13it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.21it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.11it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.02it/s] 41%|████      | 222/543 [00:05<00:07, 43.86it/s] 42%|████▏     | 227/543 [00:05<00:07, 43.54it/s] 43%|████▎     | 232/543 [00:05<00:07, 43.72it/s] 44%|████▎     | 237/543 [00:05<00:06, 43.92it/s] 45%|████▍     | 242/543 [00:05<00:06, 43.99it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.17it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.26it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.20it/s] 48%|████▊     | 262/543 [00:05<00:06, 43.98it/s] 49%|████▉     | 267/543 [00:06<00:06, 43.88it/s] 50%|█████     | 272/543 [00:06<00:06, 43.84it/s] 51%|█████     | 277/543 [00:06<00:06, 43.81it/s] 52%|█████▏    | 282/543 [00:06<00:05, 43.95it/s] 53%|█████▎    | 287/543 [00:06<00:05, 43.98it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.14it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.19it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.16it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.15it/s] 57%|█████▋    | 312/543 [00:07<00:05, 43.94it/s] 58%|█████▊    | 317/543 [00:07<00:05, 43.94it/s] 59%|█████▉    | 322/543 [00:07<00:05, 43.98it/s] 60%|██████    | 327/543 [00:07<00:04, 44.14it/s] 61%|██████    | 332/543 [00:07<00:04, 44.28it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.33it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.31it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.27it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.23it/s] 66%|██████▌   | 357/543 [00:08<00:04, 44.15it/s] 67%|██████▋   | 362/543 [00:08<00:04, 44.12it/s] 68%|██████▊   | 367/543 [00:08<00:03, 44.06it/s] 69%|██████▊   | 372/543 [00:08<00:03, 44.13it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.22it/s] 70%|███████   | 382/543 [00:08<00:03, 44.33it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.35it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.10it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.14it/s] 74%|███████▍  | 402/543 [00:09<00:03, 44.11it/s] 75%|███████▍  | 407/543 [00:09<00:03, 43.97it/s] 76%|███████▌  | 412/543 [00:09<00:02, 44.17it/s] 77%|███████▋  | 417/543 [00:09<00:02, 44.25it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.25it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.33it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.32it/s] 80%|████████  | 437/543 [00:09<00:02, 44.21it/s] 81%|████████▏ | 442/543 [00:09<00:02, 44.16it/s] 82%|████████▏ | 447/543 [00:10<00:02, 44.10it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.05it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.05it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.16it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.30it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.19it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.19it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.13it/s] 90%|████████▉ | 487/543 [00:11<00:01, 44.08it/s] 91%|█████████ | 492/543 [00:11<00:01, 44.13it/s] 92%|█████████▏| 497/543 [00:11<00:01, 44.09it/s] 92%|█████████▏| 502/543 [00:11<00:00, 44.05it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.12it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.25it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.20it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.15it/s] 97%|█████████▋| 527/543 [00:11<00:00, 43.99it/s] 98%|█████████▊| 532/543 [00:12<00:00, 44.08it/s] 99%|█████████▉| 537/543 [00:12<00:00, 44.15it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.11it/s]100%|██████████| 543/543 [00:12<00:00, 44.17it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 07:08:28,705 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:28,705 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:28,705 >>   eval_loss               =     1.0795
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:28,705 >>   eval_runtime            = 0:00:12.31
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:28,705 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:28,705 >>   eval_samples_per_second =    352.722
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:28,705 >>   eval_steps_per_second   =     44.111
[INFO|trainer_pt_utils.py:913] 2023-08-28 07:08:28,705 >>   perplexity              =     2.9431
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:35,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:35,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:35,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:35,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:35,240 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:08:35,560 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:08:35,561 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:08:35,830 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:08:36,891 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:08:36,891 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:39,057 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:39,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:39,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:39,061 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:08:39,062 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:08:39,414 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:08:39,415 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:08:40,152 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:08:40,316 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:08:40,317 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.81it/s]Extractor Predicting: 2it [00:01,  1.80it/s]Extractor Predicting: 3it [00:01,  1.82it/s]Extractor Predicting: 4it [00:02,  1.91it/s]Extractor Predicting: 5it [00:02,  1.87it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 7it [00:03,  1.85it/s]Extractor Predicting: 8it [00:04,  1.88it/s]Extractor Predicting: 9it [00:04,  1.94it/s]Extractor Predicting: 10it [00:05,  1.96it/s]Extractor Predicting: 11it [00:05,  1.98it/s]Extractor Predicting: 12it [00:06,  1.98it/s]Extractor Predicting: 13it [00:06,  1.88it/s]Extractor Predicting: 14it [00:07,  1.82it/s]Extractor Predicting: 15it [00:08,  1.76it/s]Extractor Predicting: 16it [00:08,  1.76it/s]Extractor Predicting: 17it [00:09,  1.76it/s]Extractor Predicting: 18it [00:09,  1.73it/s]Extractor Predicting: 19it [00:10,  1.67it/s]Extractor Predicting: 20it [00:11,  1.65it/s]Extractor Predicting: 21it [00:11,  1.68it/s]Extractor Predicting: 22it [00:12,  1.72it/s]Extractor Predicting: 23it [00:12,  1.73it/s]Extractor Predicting: 24it [00:13,  1.72it/s]Extractor Predicting: 25it [00:13,  1.72it/s]Extractor Predicting: 26it [00:14,  1.70it/s]Extractor Predicting: 27it [00:15,  1.69it/s]Extractor Predicting: 28it [00:15,  1.71it/s]Extractor Predicting: 29it [00:16,  1.69it/s]Extractor Predicting: 30it [00:16,  1.69it/s]Extractor Predicting: 31it [00:17,  1.71it/s]Extractor Predicting: 32it [00:18,  1.71it/s]Extractor Predicting: 33it [00:18,  1.72it/s]Extractor Predicting: 34it [00:19,  1.69it/s]Extractor Predicting: 35it [00:19,  1.67it/s]Extractor Predicting: 36it [00:20,  1.64it/s]Extractor Predicting: 37it [00:21,  1.64it/s]Extractor Predicting: 38it [00:21,  1.62it/s]Extractor Predicting: 39it [00:22,  1.67it/s]Extractor Predicting: 40it [00:23,  1.57it/s]Extractor Predicting: 41it [00:23,  1.59it/s]Extractor Predicting: 42it [00:24,  1.60it/s]Extractor Predicting: 43it [00:24,  1.66it/s]Extractor Predicting: 44it [00:25,  1.69it/s]Extractor Predicting: 45it [00:25,  1.71it/s]Extractor Predicting: 46it [00:26,  1.74it/s]Extractor Predicting: 47it [00:27,  1.75it/s]Extractor Predicting: 48it [00:27,  1.73it/s]Extractor Predicting: 49it [00:28,  1.70it/s]Extractor Predicting: 50it [00:28,  1.72it/s]Extractor Predicting: 51it [00:29,  1.74it/s]Extractor Predicting: 52it [00:29,  1.72it/s]Extractor Predicting: 53it [00:30,  1.68it/s]Extractor Predicting: 54it [00:31,  1.72it/s]Extractor Predicting: 55it [00:31,  1.76it/s]Extractor Predicting: 56it [00:32,  1.71it/s]Extractor Predicting: 57it [00:32,  1.69it/s]Extractor Predicting: 58it [00:33,  1.72it/s]Extractor Predicting: 59it [00:34,  1.68it/s]Extractor Predicting: 60it [00:34,  1.69it/s]Extractor Predicting: 61it [00:35,  1.72it/s]Extractor Predicting: 62it [00:35,  1.72it/s]Extractor Predicting: 63it [00:36,  1.72it/s]Extractor Predicting: 64it [00:36,  1.73it/s]Extractor Predicting: 65it [00:37,  1.79it/s]Extractor Predicting: 66it [00:38,  1.78it/s]Extractor Predicting: 67it [00:38,  1.78it/s]Extractor Predicting: 68it [00:39,  1.74it/s]Extractor Predicting: 69it [00:39,  1.76it/s]Extractor Predicting: 70it [00:40,  1.73it/s]Extractor Predicting: 71it [00:40,  1.72it/s]Extractor Predicting: 72it [00:41,  1.72it/s]Extractor Predicting: 73it [00:42,  1.70it/s]Extractor Predicting: 74it [00:42,  1.68it/s]Extractor Predicting: 75it [00:43,  1.65it/s]Extractor Predicting: 76it [00:43,  1.67it/s]Extractor Predicting: 77it [00:44,  1.71it/s]Extractor Predicting: 78it [00:45,  1.71it/s]Extractor Predicting: 79it [00:45,  1.68it/s]Extractor Predicting: 80it [00:46,  1.72it/s]Extractor Predicting: 81it [00:46,  1.72it/s]Extractor Predicting: 82it [00:47,  1.73it/s]Extractor Predicting: 83it [00:48,  1.74it/s]Extractor Predicting: 84it [00:48,  1.75it/s]Extractor Predicting: 85it [00:49,  1.76it/s]Extractor Predicting: 86it [00:49,  1.73it/s]Extractor Predicting: 87it [00:50,  1.74it/s]Extractor Predicting: 88it [00:50,  1.77it/s]Extractor Predicting: 89it [00:51,  1.79it/s]Extractor Predicting: 90it [00:51,  1.75it/s]Extractor Predicting: 91it [00:52,  1.72it/s]Extractor Predicting: 92it [00:53,  1.70it/s]Extractor Predicting: 93it [00:53,  1.76it/s]Extractor Predicting: 94it [00:54,  1.77it/s]Extractor Predicting: 95it [00:54,  1.77it/s]Extractor Predicting: 96it [00:55,  1.78it/s]Extractor Predicting: 97it [00:55,  1.79it/s]Extractor Predicting: 98it [00:56,  1.59it/s]Extractor Predicting: 99it [00:57,  1.62it/s]Extractor Predicting: 100it [00:57,  1.65it/s]Extractor Predicting: 101it [00:58,  1.68it/s]Extractor Predicting: 102it [00:59,  1.68it/s]Extractor Predicting: 103it [00:59,  1.70it/s]Extractor Predicting: 104it [01:00,  1.72it/s]Extractor Predicting: 105it [01:00,  1.70it/s]Extractor Predicting: 106it [01:01,  1.73it/s]Extractor Predicting: 107it [01:01,  1.73it/s]Extractor Predicting: 108it [01:02,  1.72it/s]Extractor Predicting: 109it [01:03,  1.73it/s]Extractor Predicting: 110it [01:03,  1.71it/s]Extractor Predicting: 111it [01:04,  1.73it/s]Extractor Predicting: 112it [01:04,  1.76it/s]Extractor Predicting: 113it [01:05,  1.77it/s]Extractor Predicting: 114it [01:05,  1.75it/s]Extractor Predicting: 115it [01:06,  1.73it/s]Extractor Predicting: 116it [01:07,  1.73it/s]Extractor Predicting: 117it [01:07,  1.74it/s]Extractor Predicting: 118it [01:08,  1.70it/s]Extractor Predicting: 119it [01:08,  1.71it/s]Extractor Predicting: 120it [01:09,  1.69it/s]Extractor Predicting: 121it [01:10,  1.67it/s]Extractor Predicting: 122it [01:10,  1.66it/s]Extractor Predicting: 123it [01:11,  1.71it/s]Extractor Predicting: 124it [01:11,  1.73it/s]Extractor Predicting: 125it [01:12,  1.75it/s]Extractor Predicting: 126it [01:13,  1.69it/s]Extractor Predicting: 127it [01:13,  1.69it/s]Extractor Predicting: 128it [01:14,  1.71it/s]Extractor Predicting: 129it [01:14,  1.74it/s]Extractor Predicting: 130it [01:15,  1.79it/s]Extractor Predicting: 131it [01:15,  1.74it/s]Extractor Predicting: 132it [01:16,  1.74it/s]Extractor Predicting: 133it [01:17,  1.74it/s]Extractor Predicting: 134it [01:17,  1.75it/s]Extractor Predicting: 135it [01:18,  1.76it/s]Extractor Predicting: 136it [01:18,  1.76it/s]Extractor Predicting: 137it [01:19,  1.77it/s]Extractor Predicting: 138it [01:19,  1.74it/s]Extractor Predicting: 139it [01:20,  1.71it/s]Extractor Predicting: 140it [01:21,  1.75it/s]Extractor Predicting: 141it [01:21,  1.75it/s]Extractor Predicting: 142it [01:22,  1.79it/s]Extractor Predicting: 143it [01:22,  1.80it/s]Extractor Predicting: 144it [01:23,  1.79it/s]Extractor Predicting: 145it [01:23,  1.76it/s]Extractor Predicting: 146it [01:24,  1.78it/s]Extractor Predicting: 147it [01:24,  1.78it/s]Extractor Predicting: 148it [01:25,  1.79it/s]Extractor Predicting: 149it [01:26,  1.80it/s]Extractor Predicting: 150it [01:26,  1.74it/s]Extractor Predicting: 151it [01:27,  1.75it/s]Extractor Predicting: 152it [01:27,  1.75it/s]Extractor Predicting: 153it [01:28,  1.72it/s]Extractor Predicting: 154it [01:28,  1.73it/s]Extractor Predicting: 155it [01:29,  1.70it/s]Extractor Predicting: 156it [01:30,  1.67it/s]Extractor Predicting: 157it [01:30,  1.68it/s]Extractor Predicting: 158it [01:31,  1.69it/s]Extractor Predicting: 159it [01:31,  1.73it/s]Extractor Predicting: 160it [01:32,  1.77it/s]Extractor Predicting: 161it [01:32,  1.79it/s]Extractor Predicting: 162it [01:33,  1.79it/s]Extractor Predicting: 163it [01:34,  1.67it/s]Extractor Predicting: 163it [01:34,  1.73it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:23,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:23,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:23,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:23,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:23,707 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:10:24,293 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:10:24,294 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:10:24,859 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:10:25,888 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:10:25,888 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:28,753 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:28,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:28,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:28,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:10:28,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:10:29,402 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:10:29,403 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:10:29,975 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:10:30,137 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:10:30,137 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.76it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.78it/s]Extractor Predicting: 4it [00:02,  1.80it/s]Extractor Predicting: 5it [00:02,  1.79it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:03,  1.81it/s]Extractor Predicting: 8it [00:04,  1.81it/s]Extractor Predicting: 9it [00:04,  1.83it/s]Extractor Predicting: 10it [00:05,  1.85it/s]Extractor Predicting: 11it [00:06,  1.87it/s]Extractor Predicting: 12it [00:06,  1.88it/s]Extractor Predicting: 13it [00:07,  1.75it/s]Extractor Predicting: 14it [00:07,  1.76it/s]Extractor Predicting: 15it [00:08,  1.77it/s]Extractor Predicting: 16it [00:08,  1.77it/s]Extractor Predicting: 17it [00:09,  1.77it/s]Extractor Predicting: 18it [00:10,  1.78it/s]Extractor Predicting: 19it [00:10,  1.83it/s]Extractor Predicting: 20it [00:11,  1.80it/s]Extractor Predicting: 21it [00:11,  1.81it/s]Extractor Predicting: 22it [00:12,  1.80it/s]Extractor Predicting: 23it [00:12,  1.79it/s]Extractor Predicting: 24it [00:13,  1.77it/s]Extractor Predicting: 25it [00:13,  1.78it/s]Extractor Predicting: 26it [00:14,  1.79it/s]Extractor Predicting: 27it [00:14,  1.84it/s]Extractor Predicting: 28it [00:15,  1.82it/s]Extractor Predicting: 29it [00:16,  1.82it/s]Extractor Predicting: 30it [00:16,  1.83it/s]Extractor Predicting: 31it [00:17,  1.83it/s]Extractor Predicting: 32it [00:17,  1.83it/s]Extractor Predicting: 33it [00:18,  1.82it/s]Extractor Predicting: 34it [00:18,  1.81it/s]Extractor Predicting: 35it [00:19,  1.84it/s]Extractor Predicting: 36it [00:19,  1.81it/s]Extractor Predicting: 37it [00:20,  1.79it/s]Extractor Predicting: 38it [00:21,  1.77it/s]Extractor Predicting: 39it [00:21,  1.82it/s]Extractor Predicting: 40it [00:22,  1.81it/s]Extractor Predicting: 41it [00:22,  1.71it/s]Extractor Predicting: 42it [00:23,  1.72it/s]Extractor Predicting: 43it [00:23,  1.71it/s]Extractor Predicting: 44it [00:24,  1.71it/s]Extractor Predicting: 45it [00:25,  1.69it/s]Extractor Predicting: 46it [00:25,  1.66it/s]Extractor Predicting: 47it [00:26,  1.64it/s]Extractor Predicting: 48it [00:26,  1.68it/s]Extractor Predicting: 49it [00:27,  1.65it/s]Extractor Predicting: 50it [00:28,  1.67it/s]Extractor Predicting: 51it [00:28,  1.68it/s]Extractor Predicting: 52it [00:29,  1.68it/s]Extractor Predicting: 53it [00:29,  1.67it/s]Extractor Predicting: 54it [00:30,  1.62it/s]Extractor Predicting: 55it [00:31,  1.67it/s]Extractor Predicting: 56it [00:31,  1.73it/s]Extractor Predicting: 57it [00:32,  1.71it/s]Extractor Predicting: 58it [00:32,  1.70it/s]Extractor Predicting: 59it [00:33,  1.67it/s]Extractor Predicting: 60it [00:34,  1.66it/s]Extractor Predicting: 61it [00:34,  1.70it/s]Extractor Predicting: 62it [00:35,  1.69it/s]Extractor Predicting: 63it [00:35,  1.64it/s]Extractor Predicting: 64it [00:36,  1.66it/s]Extractor Predicting: 65it [00:37,  1.66it/s]Extractor Predicting: 66it [00:37,  1.64it/s]Extractor Predicting: 67it [00:38,  1.66it/s]Extractor Predicting: 68it [00:38,  1.65it/s]Extractor Predicting: 69it [00:39,  1.66it/s]Extractor Predicting: 70it [00:40,  1.67it/s]Extractor Predicting: 71it [00:40,  1.70it/s]Extractor Predicting: 72it [00:41,  1.69it/s]Extractor Predicting: 73it [00:41,  1.68it/s]Extractor Predicting: 74it [00:42,  1.72it/s]Extractor Predicting: 75it [00:43,  1.72it/s]Extractor Predicting: 76it [00:43,  1.73it/s]Extractor Predicting: 77it [00:44,  1.76it/s]Extractor Predicting: 78it [00:44,  1.74it/s]Extractor Predicting: 79it [00:45,  1.70it/s]Extractor Predicting: 80it [00:45,  1.75it/s]Extractor Predicting: 81it [00:46,  1.44it/s]Extractor Predicting: 82it [00:47,  1.52it/s]Extractor Predicting: 83it [00:48,  1.59it/s]Extractor Predicting: 84it [00:48,  1.58it/s]Extractor Predicting: 85it [00:49,  1.61it/s]Extractor Predicting: 86it [00:49,  1.64it/s]Extractor Predicting: 87it [00:50,  1.68it/s]Extractor Predicting: 88it [00:51,  1.66it/s]Extractor Predicting: 89it [00:51,  1.72it/s]Extractor Predicting: 90it [00:52,  1.73it/s]Extractor Predicting: 91it [00:52,  1.76it/s]Extractor Predicting: 92it [00:53,  1.76it/s]Extractor Predicting: 93it [00:53,  1.75it/s]Extractor Predicting: 94it [00:54,  1.73it/s]Extractor Predicting: 95it [00:54,  1.75it/s]Extractor Predicting: 96it [00:55,  1.78it/s]Extractor Predicting: 97it [00:56,  1.78it/s]Extractor Predicting: 98it [00:56,  1.70it/s]Extractor Predicting: 99it [00:57,  1.71it/s]Extractor Predicting: 100it [00:57,  1.70it/s]Extractor Predicting: 101it [00:58,  1.69it/s]Extractor Predicting: 102it [00:59,  1.72it/s]Extractor Predicting: 103it [00:59,  1.69it/s]Extractor Predicting: 104it [01:00,  1.72it/s]Extractor Predicting: 105it [01:00,  1.71it/s]Extractor Predicting: 106it [01:01,  1.66it/s]Extractor Predicting: 107it [01:02,  1.64it/s]Extractor Predicting: 108it [01:02,  1.68it/s]Extractor Predicting: 109it [01:03,  1.61it/s]Extractor Predicting: 110it [01:03,  1.65it/s]Extractor Predicting: 111it [01:04,  1.64it/s]Extractor Predicting: 112it [01:05,  1.66it/s]Extractor Predicting: 113it [01:05,  1.66it/s]Extractor Predicting: 114it [01:06,  1.67it/s]Extractor Predicting: 115it [01:06,  1.67it/s]Extractor Predicting: 116it [01:07,  1.64it/s]Extractor Predicting: 117it [01:08,  1.65it/s]Extractor Predicting: 118it [01:08,  1.71it/s]Extractor Predicting: 119it [01:09,  1.71it/s]Extractor Predicting: 120it [01:09,  1.76it/s]Extractor Predicting: 121it [01:10,  1.74it/s]Extractor Predicting: 122it [01:10,  1.77it/s]Extractor Predicting: 123it [01:11,  1.79it/s]Extractor Predicting: 124it [01:11,  1.82it/s]Extractor Predicting: 125it [01:12,  1.80it/s]Extractor Predicting: 126it [01:13,  1.76it/s]Extractor Predicting: 127it [01:13,  1.75it/s]Extractor Predicting: 128it [01:14,  1.73it/s]Extractor Predicting: 129it [01:14,  1.73it/s]Extractor Predicting: 130it [01:15,  1.72it/s]Extractor Predicting: 131it [01:16,  1.78it/s]Extractor Predicting: 132it [01:16,  1.76it/s]Extractor Predicting: 133it [01:17,  1.76it/s]Extractor Predicting: 134it [01:17,  1.79it/s]Extractor Predicting: 135it [01:18,  1.79it/s]Extractor Predicting: 136it [01:18,  1.79it/s]Extractor Predicting: 137it [01:19,  1.84it/s]Extractor Predicting: 138it [01:19,  1.81it/s]Extractor Predicting: 139it [01:20,  1.81it/s]Extractor Predicting: 140it [01:21,  1.79it/s]Extractor Predicting: 141it [01:21,  1.80it/s]Extractor Predicting: 142it [01:22,  1.77it/s]Extractor Predicting: 143it [01:22,  1.75it/s]Extractor Predicting: 144it [01:23,  1.76it/s]Extractor Predicting: 145it [01:23,  1.70it/s]Extractor Predicting: 146it [01:24,  1.65it/s]Extractor Predicting: 147it [01:25,  1.67it/s]Extractor Predicting: 148it [01:25,  1.67it/s]Extractor Predicting: 149it [01:26,  1.71it/s]Extractor Predicting: 150it [01:26,  1.71it/s]Extractor Predicting: 151it [01:27,  1.72it/s]Extractor Predicting: 152it [01:28,  1.72it/s]Extractor Predicting: 153it [01:28,  1.76it/s]Extractor Predicting: 154it [01:29,  1.77it/s]Extractor Predicting: 155it [01:29,  1.81it/s]Extractor Predicting: 156it [01:30,  1.73it/s]Extractor Predicting: 157it [01:30,  1.75it/s]Extractor Predicting: 158it [01:31,  1.75it/s]Extractor Predicting: 159it [01:32,  1.76it/s]Extractor Predicting: 160it [01:32,  1.79it/s]Extractor Predicting: 161it [01:33,  1.69it/s]Extractor Predicting: 162it [01:33,  1.76it/s]Extractor Predicting: 163it [01:34,  1.79it/s]Extractor Predicting: 164it [01:34,  1.76it/s]Extractor Predicting: 165it [01:35,  1.83it/s]Extractor Predicting: 166it [01:35,  1.83it/s]Extractor Predicting: 167it [01:36,  1.81it/s]Extractor Predicting: 168it [01:36,  1.88it/s]Extractor Predicting: 169it [01:37,  1.89it/s]Extractor Predicting: 170it [01:38,  1.61it/s]Extractor Predicting: 171it [01:38,  1.65it/s]Extractor Predicting: 172it [01:39,  1.69it/s]Extractor Predicting: 173it [01:40,  1.68it/s]Extractor Predicting: 174it [01:40,  1.74it/s]Extractor Predicting: 175it [01:41,  1.75it/s]Extractor Predicting: 176it [01:41,  1.74it/s]Extractor Predicting: 177it [01:42,  1.73it/s]Extractor Predicting: 178it [01:42,  1.72it/s]Extractor Predicting: 179it [01:43,  1.82it/s]Extractor Predicting: 180it [01:43,  1.79it/s]Extractor Predicting: 181it [01:44,  1.79it/s]Extractor Predicting: 182it [01:45,  1.77it/s]Extractor Predicting: 183it [01:45,  1.76it/s]Extractor Predicting: 184it [01:46,  1.75it/s]Extractor Predicting: 185it [01:46,  1.80it/s]Extractor Predicting: 186it [01:47,  1.76it/s]Extractor Predicting: 187it [01:47,  1.75it/s]Extractor Predicting: 188it [01:48,  1.73it/s]Extractor Predicting: 189it [01:49,  1.71it/s]Extractor Predicting: 190it [01:49,  1.68it/s]Extractor Predicting: 191it [01:50,  1.68it/s]Extractor Predicting: 192it [01:50,  1.71it/s]Extractor Predicting: 193it [01:51,  1.75it/s]Extractor Predicting: 194it [01:51,  1.77it/s]Extractor Predicting: 195it [01:52,  1.73it/s]Extractor Predicting: 196it [01:53,  1.75it/s]Extractor Predicting: 197it [01:53,  1.75it/s]Extractor Predicting: 198it [01:54,  1.76it/s]Extractor Predicting: 199it [01:54,  1.75it/s]Extractor Predicting: 200it [01:55,  1.74it/s]Extractor Predicting: 201it [01:56,  1.75it/s]Extractor Predicting: 202it [01:56,  1.75it/s]Extractor Predicting: 203it [01:57,  1.75it/s]Extractor Predicting: 204it [01:57,  1.76it/s]Extractor Predicting: 205it [01:58,  1.76it/s]Extractor Predicting: 206it [01:58,  1.75it/s]Extractor Predicting: 207it [01:59,  1.77it/s]Extractor Predicting: 208it [01:59,  1.77it/s]Extractor Predicting: 209it [02:00,  1.76it/s]Extractor Predicting: 210it [02:01,  1.76it/s]Extractor Predicting: 211it [02:01,  1.75it/s]Extractor Predicting: 212it [02:02,  1.77it/s]Extractor Predicting: 213it [02:02,  1.76it/s]Extractor Predicting: 214it [02:03,  1.76it/s]Extractor Predicting: 215it [02:04,  1.70it/s]Extractor Predicting: 216it [02:04,  1.74it/s]Extractor Predicting: 217it [02:05,  1.77it/s]Extractor Predicting: 218it [02:05,  1.76it/s]Extractor Predicting: 219it [02:06,  1.74it/s]Extractor Predicting: 220it [02:06,  1.75it/s]Extractor Predicting: 221it [02:07,  1.73it/s]Extractor Predicting: 222it [02:08,  1.74it/s]Extractor Predicting: 223it [02:08,  1.67it/s]Extractor Predicting: 224it [02:09,  1.68it/s]Extractor Predicting: 225it [02:09,  1.67it/s]Extractor Predicting: 226it [02:10,  1.67it/s]Extractor Predicting: 227it [02:11,  1.63it/s]Extractor Predicting: 228it [02:11,  1.58it/s]Extractor Predicting: 229it [02:12,  1.61it/s]Extractor Predicting: 230it [02:12,  1.64it/s]Extractor Predicting: 231it [02:13,  1.64it/s]Extractor Predicting: 232it [02:14,  1.65it/s]Extractor Predicting: 233it [02:14,  1.66it/s]Extractor Predicting: 234it [02:15,  1.68it/s]Extractor Predicting: 235it [02:15,  1.69it/s]Extractor Predicting: 236it [02:16,  1.71it/s]Extractor Predicting: 237it [02:17,  1.70it/s]Extractor Predicting: 238it [02:17,  1.77it/s]Extractor Predicting: 239it [02:18,  1.80it/s]Extractor Predicting: 240it [02:18,  1.86it/s]Extractor Predicting: 241it [02:19,  1.85it/s]Extractor Predicting: 242it [02:19,  1.84it/s]Extractor Predicting: 243it [02:20,  1.78it/s]Extractor Predicting: 244it [02:20,  1.79it/s]Extractor Predicting: 245it [02:21,  1.83it/s]Extractor Predicting: 246it [02:21,  1.80it/s]Extractor Predicting: 247it [02:22,  1.82it/s]Extractor Predicting: 248it [02:23,  1.83it/s]Extractor Predicting: 249it [02:23,  1.89it/s]Extractor Predicting: 250it [02:24,  1.88it/s]Extractor Predicting: 251it [02:24,  1.95it/s]Extractor Predicting: 252it [02:25,  1.97it/s]Extractor Predicting: 253it [02:25,  1.93it/s]Extractor Predicting: 254it [02:26,  1.95it/s]Extractor Predicting: 255it [02:26,  1.88it/s]Extractor Predicting: 256it [02:27,  1.93it/s]Extractor Predicting: 257it [02:27,  1.89it/s]Extractor Predicting: 258it [02:28,  1.83it/s]Extractor Predicting: 259it [02:28,  1.78it/s]Extractor Predicting: 260it [02:29,  1.84it/s]Extractor Predicting: 261it [02:29,  1.81it/s]Extractor Predicting: 262it [02:30,  1.82it/s]Extractor Predicting: 263it [02:31,  1.88it/s]Extractor Predicting: 264it [02:31,  1.89it/s]Extractor Predicting: 265it [02:32,  1.88it/s]Extractor Predicting: 266it [02:32,  1.66it/s]Extractor Predicting: 267it [02:33,  1.75it/s]Extractor Predicting: 268it [02:33,  1.78it/s]Extractor Predicting: 269it [02:34,  1.76it/s]Extractor Predicting: 270it [02:34,  1.81it/s]Extractor Predicting: 271it [02:35,  1.80it/s]Extractor Predicting: 272it [02:36,  1.79it/s]Extractor Predicting: 273it [02:36,  1.86it/s]Extractor Predicting: 274it [02:37,  1.87it/s]Extractor Predicting: 275it [02:37,  1.86it/s]Extractor Predicting: 276it [02:38,  1.83it/s]Extractor Predicting: 277it [02:38,  1.84it/s]Extractor Predicting: 278it [02:39,  1.87it/s]Extractor Predicting: 279it [02:39,  1.81it/s]Extractor Predicting: 280it [02:40,  1.88it/s]Extractor Predicting: 281it [02:40,  1.90it/s]Extractor Predicting: 282it [02:41,  1.91it/s]Extractor Predicting: 283it [02:41,  1.92it/s]Extractor Predicting: 284it [02:42,  1.91it/s]Extractor Predicting: 285it [02:42,  1.91it/s]Extractor Predicting: 286it [02:43,  1.91it/s]Extractor Predicting: 287it [02:43,  1.90it/s]Extractor Predicting: 288it [02:44,  1.86it/s]Extractor Predicting: 289it [02:45,  1.85it/s]Extractor Predicting: 290it [02:45,  1.77it/s]Extractor Predicting: 291it [02:46,  1.80it/s]Extractor Predicting: 292it [02:46,  1.84it/s]Extractor Predicting: 293it [02:47,  1.85it/s]Extractor Predicting: 294it [02:47,  1.86it/s]Extractor Predicting: 295it [02:48,  1.88it/s]Extractor Predicting: 296it [02:49,  1.77it/s]Extractor Predicting: 297it [02:49,  1.76it/s]Extractor Predicting: 298it [02:50,  1.67it/s]Extractor Predicting: 299it [02:50,  1.68it/s]Extractor Predicting: 300it [02:51,  1.71it/s]Extractor Predicting: 301it [02:51,  1.72it/s]Extractor Predicting: 302it [02:52,  1.73it/s]Extractor Predicting: 303it [02:53,  1.68it/s]Extractor Predicting: 304it [02:53,  1.69it/s]Extractor Predicting: 305it [02:54,  1.73it/s]Extractor Predicting: 306it [02:54,  1.69it/s]Extractor Predicting: 307it [02:55,  1.64it/s]Extractor Predicting: 308it [02:56,  1.65it/s]Extractor Predicting: 309it [02:56,  1.69it/s]Extractor Predicting: 310it [02:57,  1.72it/s]Extractor Predicting: 311it [02:57,  1.74it/s]Extractor Predicting: 312it [02:58,  1.72it/s]Extractor Predicting: 313it [02:59,  1.68it/s]Extractor Predicting: 314it [02:59,  1.66it/s]Extractor Predicting: 315it [03:00,  1.66it/s]Extractor Predicting: 316it [03:00,  1.66it/s]Extractor Predicting: 317it [03:01,  1.66it/s]Extractor Predicting: 318it [03:02,  1.67it/s]Extractor Predicting: 319it [03:02,  1.69it/s]Extractor Predicting: 320it [03:03,  1.69it/s]Extractor Predicting: 321it [03:03,  1.76it/s]Extractor Predicting: 322it [03:04,  1.76it/s]Extractor Predicting: 323it [03:04,  1.73it/s]Extractor Predicting: 324it [03:05,  1.72it/s]Extractor Predicting: 325it [03:06,  1.71it/s]Extractor Predicting: 326it [03:06,  1.70it/s]Extractor Predicting: 327it [03:07,  1.67it/s]Extractor Predicting: 328it [03:07,  1.69it/s]Extractor Predicting: 329it [03:08,  1.73it/s]Extractor Predicting: 330it [03:09,  1.77it/s]Extractor Predicting: 331it [03:09,  1.98it/s]Extractor Predicting: 331it [03:09,  1.75it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:49,020 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:49,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:49,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:49,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:49,025 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:13:49,629 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:13:49,630 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:13:50,199 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:13:51,247 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:13:51,247 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:54,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:54,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:54,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:54,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:13:54,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:13:54,735 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:13:54,735 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:13:55,297 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:13:55,468 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:13:55,469 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.65it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.69it/s]Extractor Predicting: 15it [00:09,  1.64it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:18,  1.65it/s]Extractor Predicting: 31it [00:18,  1.63it/s]Extractor Predicting: 32it [00:19,  1.52it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:20,  1.60it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.65it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.68it/s]Extractor Predicting: 39it [00:23,  1.62it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.65it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.67it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:28,  1.69it/s]Extractor Predicting: 47it [00:28,  1.67it/s]Extractor Predicting: 48it [00:29,  1.71it/s]Extractor Predicting: 49it [00:29,  1.72it/s]Extractor Predicting: 50it [00:30,  1.75it/s]Extractor Predicting: 51it [00:30,  1.76it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:32,  1.64it/s]Extractor Predicting: 54it [00:32,  1.69it/s]Extractor Predicting: 55it [00:33,  1.71it/s]Extractor Predicting: 56it [00:33,  1.79it/s]Extractor Predicting: 57it [00:34,  1.84it/s]Extractor Predicting: 58it [00:34,  1.91it/s]Extractor Predicting: 59it [00:35,  1.99it/s]Extractor Predicting: 60it [00:35,  2.08it/s]Extractor Predicting: 61it [00:36,  2.14it/s]Extractor Predicting: 62it [00:36,  2.12it/s]Extractor Predicting: 63it [00:37,  2.15it/s]Extractor Predicting: 64it [00:37,  2.12it/s]Extractor Predicting: 65it [00:38,  2.10it/s]Extractor Predicting: 66it [00:38,  2.10it/s]Extractor Predicting: 67it [00:38,  2.09it/s]Extractor Predicting: 68it [00:39,  2.11it/s]Extractor Predicting: 69it [00:39,  2.15it/s]Extractor Predicting: 70it [00:40,  2.10it/s]Extractor Predicting: 71it [00:40,  2.11it/s]Extractor Predicting: 72it [00:41,  2.13it/s]Extractor Predicting: 73it [00:41,  2.18it/s]Extractor Predicting: 74it [00:42,  2.20it/s]Extractor Predicting: 75it [00:42,  2.19it/s]Extractor Predicting: 76it [00:43,  2.16it/s]Extractor Predicting: 77it [00:43,  2.23it/s]Extractor Predicting: 78it [00:44,  2.15it/s]Extractor Predicting: 79it [00:44,  2.13it/s]Extractor Predicting: 80it [00:45,  2.12it/s]Extractor Predicting: 81it [00:45,  2.10it/s]Extractor Predicting: 82it [00:46,  1.91it/s]Extractor Predicting: 83it [00:46,  1.97it/s]Extractor Predicting: 84it [00:47,  2.02it/s]Extractor Predicting: 85it [00:47,  2.04it/s]Extractor Predicting: 86it [00:48,  1.91it/s]Extractor Predicting: 87it [00:48,  1.87it/s]Extractor Predicting: 88it [00:49,  1.84it/s]Extractor Predicting: 89it [00:49,  1.83it/s]Extractor Predicting: 90it [00:50,  1.85it/s]Extractor Predicting: 91it [00:50,  1.81it/s]Extractor Predicting: 92it [00:51,  1.81it/s]Extractor Predicting: 93it [00:52,  1.81it/s]Extractor Predicting: 94it [00:52,  1.79it/s]Extractor Predicting: 95it [00:53,  1.83it/s]Extractor Predicting: 96it [00:53,  1.84it/s]Extractor Predicting: 97it [00:54,  1.84it/s]Extractor Predicting: 98it [00:54,  1.84it/s]Extractor Predicting: 99it [00:55,  1.81it/s]Extractor Predicting: 100it [00:55,  1.74it/s]Extractor Predicting: 101it [00:56,  1.78it/s]Extractor Predicting: 102it [00:57,  1.78it/s]Extractor Predicting: 103it [00:57,  1.72it/s]Extractor Predicting: 104it [00:58,  1.71it/s]Extractor Predicting: 105it [00:58,  1.69it/s]Extractor Predicting: 106it [00:59,  1.68it/s]Extractor Predicting: 107it [01:00,  1.65it/s]Extractor Predicting: 108it [01:00,  1.60it/s]Extractor Predicting: 108it [01:00,  1.78it/s]
[INFO|configuration_utils.py:515] 2023-08-28 07:14:57,794 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:14:57,795 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:14:57,800 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:14:57,801 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 07:14:57,804 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:15:00,761 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 07:15:00,763 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 07:15:00,773 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:15:00,774 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:15:00,779 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:15:00,783 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:15:00,783 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:15:00,783 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:15:00,783 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:15:00,783 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:15:00,783 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 07:15:01,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:01,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:02,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:02,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:03,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:04,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:05,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:05,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:06,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:07,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:08,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:08,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:09,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:10,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:10,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:11,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:12,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:12,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:13,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:13,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:14,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:15,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:16,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:40, 15.76s/it][WARNING|generation_utils.py:914] 2023-08-28 07:15:16,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:17,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:17,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:18,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:19,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:19,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:20,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:20,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:21,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:22,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:22,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:23,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:24,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:24,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:25,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:26,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:26,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:27,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:28,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:28,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:29,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:30,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:30,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:31,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:31,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:25, 15.80s/it][WARNING|generation_utils.py:914] 2023-08-28 07:15:32,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:33,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:34,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:34,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:35,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:35,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:36,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:37,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:37,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:38,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:39,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:39,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:40,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:40,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:41,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:42,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:42,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:43,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:43,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:44,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:45,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:45,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:46,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:47,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:47,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:47<03:09, 15.78s/it][WARNING|generation_utils.py:914] 2023-08-28 07:15:48,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:49,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:49,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:50,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:51,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:52,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:52,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:53,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:54,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:54,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:55,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:56,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:56,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:57,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:58,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:58,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:15:59,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:00,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:00,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:01,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:02,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:02,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:03,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:04,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:05,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:04<03:00, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-28 07:16:05,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:06,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:07,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:07,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:08,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:08,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:09,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:10,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:10,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:11,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:11,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:12,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:13,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:13,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:14,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:15,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:15,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:16,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:17,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:17,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:18,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:19,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:20,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:20,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:21,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:21<02:43, 16.37s/it][WARNING|generation_utils.py:914] 2023-08-28 07:16:22,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:22,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:23,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:24,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:24,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:25,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:25,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:26,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:27,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:27,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:28,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:28,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:29,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:30,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:30,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:31,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:32,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:32,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:33,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:33,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:34,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:35,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:35,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:36,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:35<02:22, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 07:16:36,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:37,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:38,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:38,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:39,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:40,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:40,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:41,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:42,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:42,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:43,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:44,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:45,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:45,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:46,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:47,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:47,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:48,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:48,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:49,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:50,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:51,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:51,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:52,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:52,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:53,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:53<02:10, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 07:16:54,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:54,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:55,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:55,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:56,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:57,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:57,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:58,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:58,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:16:59,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:00,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:00,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:01,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:01,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:02,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:02,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:03,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:03,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:04,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:05,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:05,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:06,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:07,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:07,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:08,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:08,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:09,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:09,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:10,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:11,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:10<01:56, 16.66s/it][WARNING|generation_utils.py:914] 2023-08-28 07:17:11,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:12,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:12,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:13,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:14,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:14,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:15,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:16,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:16,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:17,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:17,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:18,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:19,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:19,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:20,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:21,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:21,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:22,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:23,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:23,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:24,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:24,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:25,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:26,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:25<01:36, 16.15s/it][WARNING|generation_utils.py:914] 2023-08-28 07:17:26,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:27,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:27,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:28,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:28,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:29,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:30,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:30,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:31,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:31,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:32,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:33,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:33,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:34,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:34,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:35,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:35,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:36,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:37,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:37,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:38,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:38,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:39,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:39,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:40,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:40<01:18, 15.67s/it][WARNING|generation_utils.py:914] 2023-08-28 07:17:41,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:41,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:42,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:43,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:43,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:44,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:44,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:45,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:46,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:46,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:47,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:47,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:48,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:49,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:49,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:50,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:50,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:51,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:52,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:52,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:53,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:53,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:54,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:55,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:54<01:01, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-28 07:17:55,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:56,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:57,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:57,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:58,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:59,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:17:59,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:00,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:01,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:02,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:02,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:03,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:04,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:04,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:05,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:06,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:06,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:07,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:08,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:08,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:09,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:09,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:10,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:11,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:12,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:12,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:13,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:14,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:13<00:49, 16.43s/it][WARNING|generation_utils.py:914] 2023-08-28 07:18:14,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:15,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:15,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:16,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:17,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:17,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:18,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:19,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:19,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:20,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:20,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:21,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:22,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:23,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:23,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:24,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:25,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:25,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:26,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:27,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:27,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:28,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:29,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:29,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:30,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:31,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:30<00:33, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-28 07:18:31,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:32,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:33,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:33,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:34,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:34,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:35,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:35,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:36,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:37,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:37,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:38,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:38,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:39,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:39,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:40,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:41,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:41,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:42,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:43,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:43,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:44,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:44,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:45,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:46,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:46,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:46<00:16, 16.31s/it][WARNING|generation_utils.py:914] 2023-08-28 07:18:47,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:47,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:48,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:49,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:49,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:50,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:51,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:52,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:52,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:53,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:54,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:54,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:55,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:56,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:56,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:57,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:57,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:58,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:18:59,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:00,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:00,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:01,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:01,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:02,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:03,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 07:19:03,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:03<00:00, 16.60s/it]Generating: 100%|██████████| 15/15 [04:03<00:00, 16.24s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:09,655 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:09,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:09,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:09,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:09,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:19:09,959 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:19:09,960 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:19:10,220 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:19:11,295 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:19:11,295 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:13,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:13,356 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:13,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:13,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:19:13,357 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:19:13,999 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:19:14,000 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:19:14,546 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:19:14,723 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:19:14,723 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in Life , he played the title character , a young princess of the family at the end of the third season of HBO s Game of Thrones . Head Entity : Game of Thrones , Tail Entity : Princess of the family .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.75375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.7525, 'errors': {'', "('pharaoh Amenhotep II', 'made from material', '', 'He is best remembered for the painting of the pharaoh Amenhotep II , composed by Alfred Wohl in 1827 for a French conservatory .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : cast member .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Billboard Pop Hits', 'cast member', '', 'On October 25 , 1978 , the band released the track The Lads , the debut single by the British band The Riveters ( now defunct ) , on Columbia Records , along with several appearances on the Billboard Pop Hits chart for June .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : follows .', 'success_rate': 0.7283653846153846, 'errors': {'', "('Nigeria', 'follows', '', 'In 2013 , a second poll showed Zaire had a 7 % approval rating , while in 2014 , Nigeria had a 24 % approval rating .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 281, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 366, 'raw': 576}
{'target': 600, 'success': 391, 'raw': 608}
{'target': 600, 'success': 410, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 452, 'raw': 704}
{'target': 600, 'success': 470, 'raw': 736}
{'target': 600, 'success': 490, 'raw': 768}
{'target': 600, 'success': 511, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 590, 'raw': 928}
{'target': 600, 'success': 608, 'raw': 960}
{'prompt': 'Relation : league .', 'success_rate': 0.6333333333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.765, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in Life , he studied with a group of Latin philologists and other thinkers at the Renaissance in Italy , in Naples from 1635 to 1645 , and from 1654 back to 1701 . Head Entity : Naples , Tail Entity : Rome .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 262, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 305, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 341, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 412, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 501, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 584, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : mother .', 'success_rate': 0.6729910714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : residence .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1453 the head of the province of Ghent and of neighboring districts at Neuromö , in Burgundesland , were also under the Ottoman Empire . Head Entity : Burgundesland , Tail Entity : Ottomans .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 15273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.34it/s]Extractor Estimating: 2it [00:01,  1.41it/s]Extractor Estimating: 3it [00:02,  1.48it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:04,  1.48it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.49it/s]Extractor Estimating: 9it [00:05,  1.53it/s]Extractor Estimating: 10it [00:06,  1.52it/s]Extractor Estimating: 11it [00:07,  1.44it/s]Extractor Estimating: 12it [00:08,  1.50it/s]Extractor Estimating: 13it [00:08,  1.51it/s]Extractor Estimating: 14it [00:09,  1.46it/s]Extractor Estimating: 15it [00:10,  1.49it/s]Extractor Estimating: 16it [00:10,  1.50it/s]Extractor Estimating: 17it [00:11,  1.51it/s]Extractor Estimating: 18it [00:11,  1.54it/s]Extractor Estimating: 19it [00:12,  1.52it/s]Extractor Estimating: 20it [00:13,  1.56it/s]Extractor Estimating: 21it [00:13,  1.56it/s]Extractor Estimating: 22it [00:14,  1.63it/s]Extractor Estimating: 23it [00:15,  1.56it/s]Extractor Estimating: 24it [00:15,  1.48it/s]Extractor Estimating: 25it [00:16,  1.50it/s]Extractor Estimating: 26it [00:17,  1.54it/s]Extractor Estimating: 27it [00:17,  1.55it/s]Extractor Estimating: 28it [00:18,  1.60it/s]Extractor Estimating: 29it [00:19,  1.60it/s]Extractor Estimating: 30it [00:19,  1.62it/s]Extractor Estimating: 31it [00:20,  1.59it/s]Extractor Estimating: 32it [00:20,  1.60it/s]Extractor Estimating: 33it [00:21,  1.52it/s]Extractor Estimating: 34it [00:22,  1.50it/s]Extractor Estimating: 35it [00:22,  1.52it/s]Extractor Estimating: 36it [00:23,  1.51it/s]Extractor Estimating: 37it [00:24,  1.54it/s]Extractor Estimating: 38it [00:24,  1.55it/s]Extractor Estimating: 39it [00:25,  1.51it/s]Extractor Estimating: 40it [00:26,  1.52it/s]Extractor Estimating: 41it [00:26,  1.54it/s]Extractor Estimating: 42it [00:27,  1.55it/s]Extractor Estimating: 43it [00:28,  1.44it/s]Extractor Estimating: 44it [00:28,  1.46it/s]Extractor Estimating: 45it [00:29,  1.48it/s]Extractor Estimating: 46it [00:30,  1.42it/s]Extractor Estimating: 47it [00:31,  1.46it/s]Extractor Estimating: 48it [00:31,  1.48it/s]Extractor Estimating: 49it [00:32,  1.48it/s]Extractor Estimating: 50it [00:33,  1.45it/s]Extractor Estimating: 51it [00:33,  1.45it/s]Extractor Estimating: 52it [00:34,  1.54it/s]Extractor Estimating: 53it [00:34,  1.56it/s]Extractor Estimating: 54it [00:35,  1.53it/s]Extractor Estimating: 55it [00:36,  1.51it/s]Extractor Estimating: 56it [00:36,  1.55it/s]Extractor Estimating: 57it [00:37,  1.53it/s]Extractor Estimating: 58it [00:38,  1.54it/s]Extractor Estimating: 59it [00:38,  1.55it/s]Extractor Estimating: 60it [00:39,  1.56it/s]Extractor Estimating: 61it [00:40,  1.56it/s]Extractor Estimating: 62it [00:40,  1.58it/s]Extractor Estimating: 63it [00:41,  1.60it/s]Extractor Estimating: 64it [00:41,  1.61it/s]Extractor Estimating: 65it [00:42,  1.62it/s]Extractor Estimating: 66it [00:43,  1.59it/s]Extractor Estimating: 67it [00:43,  1.59it/s]Extractor Estimating: 68it [00:44,  1.61it/s]Extractor Estimating: 69it [00:45,  1.62it/s]Extractor Estimating: 70it [00:45,  1.60it/s]Extractor Estimating: 71it [00:46,  1.53it/s]Extractor Estimating: 72it [00:47,  1.56it/s]Extractor Estimating: 73it [00:47,  1.60it/s]Extractor Estimating: 74it [00:48,  1.58it/s]Extractor Estimating: 75it [00:48,  1.56it/s]Extractor Estimating: 76it [00:49,  1.54it/s]Extractor Estimating: 77it [00:50,  1.56it/s]Extractor Estimating: 78it [00:50,  1.56it/s]Extractor Estimating: 79it [00:51,  1.50it/s]Extractor Estimating: 80it [00:52,  1.44it/s]Extractor Estimating: 81it [00:52,  1.48it/s]Extractor Estimating: 82it [00:53,  1.46it/s]Extractor Estimating: 83it [00:54,  1.53it/s]Extractor Estimating: 84it [00:54,  1.53it/s]Extractor Estimating: 85it [00:55,  1.56it/s]Extractor Estimating: 86it [00:56,  1.58it/s]Extractor Estimating: 87it [00:56,  1.58it/s]Extractor Estimating: 88it [00:57,  1.62it/s]Extractor Estimating: 89it [00:58,  1.59it/s]Extractor Estimating: 90it [00:58,  1.56it/s]Extractor Estimating: 91it [00:59,  1.58it/s]Extractor Estimating: 92it [00:59,  1.56it/s]Extractor Estimating: 93it [01:00,  1.56it/s]Extractor Estimating: 94it [01:01,  1.55it/s]Extractor Estimating: 95it [01:01,  1.54it/s]Extractor Estimating: 96it [01:02,  1.56it/s]Extractor Estimating: 97it [01:03,  1.57it/s]Extractor Estimating: 98it [01:03,  1.55it/s]Extractor Estimating: 99it [01:04,  1.47it/s]Extractor Estimating: 100it [01:05,  1.52it/s]Extractor Estimating: 101it [01:05,  1.48it/s]Extractor Estimating: 102it [01:06,  1.51it/s]Extractor Estimating: 103it [01:07,  1.55it/s]Extractor Estimating: 104it [01:07,  1.56it/s]Extractor Estimating: 105it [01:08,  1.56it/s]Extractor Estimating: 106it [01:09,  1.58it/s]Extractor Estimating: 107it [01:09,  1.60it/s]Extractor Estimating: 108it [01:10,  1.63it/s]Extractor Estimating: 109it [01:10,  1.64it/s]Extractor Estimating: 110it [01:11,  1.58it/s]Extractor Estimating: 111it [01:12,  1.61it/s]Extractor Estimating: 112it [01:12,  1.59it/s]Extractor Estimating: 113it [01:13,  1.62it/s]Extractor Estimating: 114it [01:13,  1.63it/s]Extractor Estimating: 115it [01:14,  1.62it/s]Extractor Estimating: 116it [01:15,  1.56it/s]Extractor Estimating: 117it [01:15,  1.61it/s]Extractor Estimating: 118it [01:16,  1.43it/s]Extractor Estimating: 119it [01:17,  1.48it/s]Extractor Estimating: 120it [01:18,  1.45it/s]Extractor Estimating: 121it [01:18,  1.45it/s]Extractor Estimating: 122it [01:19,  1.47it/s]Extractor Estimating: 123it [01:20,  1.52it/s]Extractor Estimating: 124it [01:20,  1.51it/s]Extractor Estimating: 125it [01:21,  1.42it/s]Extractor Estimating: 126it [01:22,  1.44it/s]Extractor Estimating: 127it [01:22,  1.45it/s]Extractor Estimating: 128it [01:23,  1.53it/s]Extractor Estimating: 129it [01:24,  1.60it/s]Extractor Estimating: 130it [01:24,  1.55it/s]Extractor Estimating: 131it [01:25,  1.61it/s]Extractor Estimating: 132it [01:25,  1.57it/s]Extractor Estimating: 133it [01:26,  1.57it/s]Extractor Estimating: 134it [01:27,  1.55it/s]Extractor Estimating: 135it [01:27,  1.52it/s]Extractor Estimating: 136it [01:28,  1.48it/s]Extractor Estimating: 137it [01:29,  1.46it/s]Extractor Estimating: 138it [01:29,  1.50it/s]Extractor Estimating: 139it [01:30,  1.44it/s]Extractor Estimating: 140it [01:31,  1.49it/s]Extractor Estimating: 141it [01:31,  1.56it/s]Extractor Estimating: 142it [01:32,  1.55it/s]Extractor Estimating: 143it [01:33,  1.59it/s]Extractor Estimating: 144it [01:33,  1.62it/s]Extractor Estimating: 145it [01:34,  1.54it/s]Extractor Estimating: 146it [01:35,  1.53it/s]Extractor Estimating: 147it [01:35,  1.56it/s]Extractor Estimating: 148it [01:36,  1.60it/s]Extractor Estimating: 149it [01:36,  1.58it/s]Extractor Estimating: 150it [01:37,  1.56it/s]Extractor Estimating: 151it [01:38,  1.56it/s]Extractor Estimating: 152it [01:38,  1.54it/s]Extractor Estimating: 153it [01:39,  1.53it/s]Extractor Estimating: 154it [01:40,  1.55it/s]Extractor Estimating: 155it [01:40,  1.56it/s]Extractor Estimating: 156it [01:41,  1.58it/s]Extractor Estimating: 157it [01:42,  1.53it/s]Extractor Estimating: 158it [01:42,  1.54it/s]Extractor Estimating: 159it [01:43,  1.53it/s]Extractor Estimating: 160it [01:44,  1.52it/s]Extractor Estimating: 161it [01:44,  1.53it/s]Extractor Estimating: 162it [01:45,  1.57it/s]Extractor Estimating: 163it [01:46,  1.54it/s]Extractor Estimating: 164it [01:46,  1.55it/s]Extractor Estimating: 165it [01:47,  1.57it/s]Extractor Estimating: 166it [01:47,  1.56it/s]Extractor Estimating: 167it [01:48,  1.56it/s]Extractor Estimating: 168it [01:49,  1.41it/s]Extractor Estimating: 169it [01:50,  1.42it/s]Extractor Estimating: 170it [01:50,  1.47it/s]Extractor Estimating: 171it [01:51,  1.49it/s]Extractor Estimating: 172it [01:52,  1.52it/s]Extractor Estimating: 173it [01:52,  1.53it/s]Extractor Estimating: 174it [01:53,  1.54it/s]Extractor Estimating: 175it [01:54,  1.52it/s]Extractor Estimating: 176it [01:54,  1.57it/s]Extractor Estimating: 177it [01:55,  1.52it/s]Extractor Estimating: 178it [01:55,  1.56it/s]Extractor Estimating: 179it [01:56,  1.57it/s]Extractor Estimating: 180it [01:57,  1.57it/s]Extractor Estimating: 181it [01:57,  1.58it/s]Extractor Estimating: 182it [01:58,  1.61it/s]Extractor Estimating: 183it [01:59,  1.61it/s]Extractor Estimating: 184it [01:59,  1.61it/s]Extractor Estimating: 185it [02:00,  1.59it/s]Extractor Estimating: 186it [02:01,  1.52it/s]Extractor Estimating: 187it [02:01,  1.58it/s]Extractor Estimating: 188it [02:02,  1.61it/s]Extractor Estimating: 189it [02:02,  1.61it/s]Extractor Estimating: 190it [02:03,  1.58it/s]Extractor Estimating: 191it [02:04,  1.60it/s]Extractor Estimating: 192it [02:04,  1.42it/s]Extractor Estimating: 193it [02:05,  1.52it/s]Extractor Estimating: 194it [02:06,  1.52it/s]Extractor Estimating: 195it [02:06,  1.53it/s]Extractor Estimating: 196it [02:07,  1.53it/s]Extractor Estimating: 197it [02:08,  1.58it/s]Extractor Estimating: 198it [02:08,  1.58it/s]Extractor Estimating: 199it [02:09,  1.58it/s]Extractor Estimating: 200it [02:09,  1.58it/s]Extractor Estimating: 201it [02:10,  1.57it/s]Extractor Estimating: 202it [02:11,  1.63it/s]Extractor Estimating: 203it [02:11,  1.63it/s]Extractor Estimating: 204it [02:12,  1.65it/s]Extractor Estimating: 205it [02:12,  1.68it/s]Extractor Estimating: 206it [02:13,  1.66it/s]Extractor Estimating: 207it [02:14,  1.65it/s]Extractor Estimating: 208it [02:14,  1.69it/s]Extractor Estimating: 209it [02:15,  1.65it/s]Extractor Estimating: 210it [02:16,  1.61it/s]Extractor Estimating: 211it [02:16,  1.56it/s]Extractor Estimating: 212it [02:17,  1.61it/s]Extractor Estimating: 213it [02:17,  1.58it/s]Extractor Estimating: 214it [02:18,  1.64it/s]Extractor Estimating: 215it [02:19,  1.68it/s]Extractor Estimating: 216it [02:19,  1.67it/s]Extractor Estimating: 217it [02:20,  1.71it/s]Extractor Estimating: 218it [02:20,  1.68it/s]Extractor Estimating: 219it [02:21,  1.68it/s]Extractor Estimating: 220it [02:22,  1.65it/s]Extractor Estimating: 221it [02:22,  1.68it/s]Extractor Estimating: 222it [02:23,  1.69it/s]Extractor Estimating: 223it [02:23,  1.67it/s]Extractor Estimating: 224it [02:24,  1.67it/s]Extractor Estimating: 225it [02:25,  1.72it/s]Extractor Estimating: 226it [02:25,  1.74it/s]Extractor Estimating: 227it [02:26,  1.77it/s]Extractor Estimating: 228it [02:26,  1.79it/s]Extractor Estimating: 229it [02:27,  1.83it/s]Extractor Estimating: 230it [02:27,  1.85it/s]Extractor Estimating: 231it [02:28,  1.82it/s]Extractor Estimating: 232it [02:28,  1.75it/s]Extractor Estimating: 233it [02:29,  1.75it/s]Extractor Estimating: 234it [02:29,  1.80it/s]Extractor Estimating: 235it [02:30,  1.76it/s]Extractor Estimating: 236it [02:31,  1.74it/s]Extractor Estimating: 237it [02:31,  1.75it/s]Extractor Estimating: 238it [02:32,  1.73it/s]Extractor Estimating: 239it [02:32,  1.70it/s]Extractor Estimating: 240it [02:33,  1.71it/s]Extractor Estimating: 241it [02:34,  1.77it/s]Extractor Estimating: 242it [02:34,  1.76it/s]Extractor Estimating: 243it [02:35,  1.75it/s]Extractor Estimating: 244it [02:35,  1.70it/s]Extractor Estimating: 245it [02:36,  1.72it/s]Extractor Estimating: 246it [02:36,  1.72it/s]Extractor Estimating: 247it [02:37,  1.67it/s]Extractor Estimating: 248it [02:38,  1.68it/s]Extractor Estimating: 249it [02:38,  1.71it/s]Extractor Estimating: 250it [02:39,  1.71it/s]Extractor Estimating: 251it [02:39,  1.69it/s]Extractor Estimating: 252it [02:40,  1.67it/s]Extractor Estimating: 253it [02:41,  1.58it/s]Extractor Estimating: 254it [02:41,  1.60it/s]Extractor Estimating: 255it [02:42,  1.57it/s]Extractor Estimating: 256it [02:43,  1.58it/s]Extractor Estimating: 257it [02:43,  1.55it/s]Extractor Estimating: 258it [02:44,  1.56it/s]Extractor Estimating: 259it [02:45,  1.60it/s]Extractor Estimating: 260it [02:45,  1.59it/s]Extractor Estimating: 261it [02:46,  1.58it/s]Extractor Estimating: 262it [02:46,  1.59it/s]Extractor Estimating: 263it [02:47,  1.60it/s]Extractor Estimating: 264it [02:48,  1.62it/s]Extractor Estimating: 265it [02:48,  1.63it/s]Extractor Estimating: 266it [02:49,  1.63it/s]Extractor Estimating: 267it [02:50,  1.62it/s]Extractor Estimating: 268it [02:50,  1.44it/s]Extractor Estimating: 269it [02:51,  1.51it/s]Extractor Estimating: 270it [02:52,  1.56it/s]Extractor Estimating: 271it [02:52,  1.59it/s]Extractor Estimating: 272it [02:53,  1.60it/s]Extractor Estimating: 273it [02:53,  1.56it/s]Extractor Estimating: 274it [02:54,  1.56it/s]Extractor Estimating: 275it [02:55,  1.58it/s]Extractor Estimating: 276it [02:55,  1.54it/s]Extractor Estimating: 277it [02:56,  1.49it/s]Extractor Estimating: 278it [02:57,  1.47it/s]Extractor Estimating: 279it [02:58,  1.47it/s]Extractor Estimating: 280it [02:58,  1.52it/s]Extractor Estimating: 281it [02:59,  1.48it/s]Extractor Estimating: 282it [02:59,  1.49it/s]Extractor Estimating: 283it [03:00,  1.49it/s]Extractor Estimating: 284it [03:01,  1.44it/s]Extractor Estimating: 285it [03:02,  1.50it/s]Extractor Estimating: 286it [03:02,  1.52it/s]Extractor Estimating: 287it [03:03,  1.47it/s]Extractor Estimating: 288it [03:04,  1.50it/s]Extractor Estimating: 289it [03:04,  1.46it/s]Extractor Estimating: 290it [03:05,  1.47it/s]Extractor Estimating: 291it [03:06,  1.50it/s]Extractor Estimating: 292it [03:06,  1.49it/s]Extractor Estimating: 293it [03:07,  1.55it/s]Extractor Estimating: 294it [03:07,  1.53it/s]Extractor Estimating: 295it [03:08,  1.50it/s]Extractor Estimating: 296it [03:09,  1.52it/s]Extractor Estimating: 297it [03:10,  1.46it/s]Extractor Estimating: 298it [03:10,  1.45it/s]Extractor Estimating: 299it [03:11,  1.46it/s]Extractor Estimating: 300it [03:12,  1.49it/s]Extractor Estimating: 301it [03:12,  1.49it/s]Extractor Estimating: 302it [03:13,  1.54it/s]Extractor Estimating: 303it [03:14,  1.48it/s]Extractor Estimating: 304it [03:14,  1.50it/s]Extractor Estimating: 305it [03:15,  1.56it/s]Extractor Estimating: 306it [03:15,  1.60it/s]Extractor Estimating: 307it [03:16,  1.59it/s]Extractor Estimating: 308it [03:17,  1.58it/s]Extractor Estimating: 309it [03:17,  1.59it/s]Extractor Estimating: 310it [03:18,  1.61it/s]Extractor Estimating: 311it [03:19,  1.52it/s]Extractor Estimating: 312it [03:19,  1.51it/s]Extractor Estimating: 313it [03:20,  1.52it/s]Extractor Estimating: 314it [03:21,  1.56it/s]Extractor Estimating: 315it [03:21,  1.60it/s]Extractor Estimating: 316it [03:22,  1.51it/s]Extractor Estimating: 317it [03:23,  1.51it/s]Extractor Estimating: 318it [03:23,  1.52it/s]Extractor Estimating: 319it [03:24,  1.50it/s]Extractor Estimating: 320it [03:25,  1.53it/s]Extractor Estimating: 321it [03:25,  1.51it/s]Extractor Estimating: 322it [03:26,  1.44it/s]Extractor Estimating: 323it [03:27,  1.50it/s]Extractor Estimating: 324it [03:27,  1.53it/s]Extractor Estimating: 325it [03:28,  1.58it/s]Extractor Estimating: 326it [03:28,  1.61it/s]Extractor Estimating: 327it [03:29,  1.64it/s]Extractor Estimating: 328it [03:30,  1.69it/s]Extractor Estimating: 329it [03:30,  1.79it/s]Extractor Estimating: 330it [03:31,  1.76it/s]Extractor Estimating: 331it [03:31,  1.74it/s]Extractor Estimating: 332it [03:32,  1.78it/s]Extractor Estimating: 333it [03:32,  1.73it/s]Extractor Estimating: 334it [03:33,  1.75it/s]Extractor Estimating: 335it [03:33,  1.72it/s]Extractor Estimating: 336it [03:34,  1.74it/s]Extractor Estimating: 337it [03:35,  1.68it/s]Extractor Estimating: 338it [03:35,  1.64it/s]Extractor Estimating: 339it [03:36,  1.59it/s]Extractor Estimating: 340it [03:37,  1.55it/s]Extractor Estimating: 341it [03:37,  1.57it/s]Extractor Estimating: 342it [03:38,  1.62it/s]Extractor Estimating: 343it [03:39,  1.60it/s]Extractor Estimating: 344it [03:39,  1.63it/s]Extractor Estimating: 345it [03:40,  1.72it/s]Extractor Estimating: 346it [03:40,  1.70it/s]Extractor Estimating: 347it [03:41,  1.72it/s]Extractor Estimating: 348it [03:42,  1.54it/s]Extractor Estimating: 349it [03:42,  1.55it/s]Extractor Estimating: 350it [03:43,  1.60it/s]Extractor Estimating: 351it [03:43,  1.62it/s]Extractor Estimating: 352it [03:44,  1.62it/s]Extractor Estimating: 353it [03:45,  1.64it/s]Extractor Estimating: 354it [03:45,  1.54it/s]Extractor Estimating: 355it [03:46,  1.54it/s]Extractor Estimating: 356it [03:47,  1.51it/s]Extractor Estimating: 357it [03:47,  1.48it/s]Extractor Estimating: 358it [03:48,  1.49it/s]Extractor Estimating: 359it [03:49,  1.56it/s]Extractor Estimating: 360it [03:49,  1.52it/s]Extractor Estimating: 361it [03:50,  1.57it/s]Extractor Estimating: 362it [03:51,  1.57it/s]Extractor Estimating: 363it [03:51,  1.60it/s]Extractor Estimating: 364it [03:52,  1.64it/s]Extractor Estimating: 365it [03:52,  1.66it/s]Extractor Estimating: 366it [03:53,  1.63it/s]Extractor Estimating: 367it [03:54,  1.59it/s]Extractor Estimating: 368it [03:54,  1.55it/s]Extractor Estimating: 369it [03:55,  1.58it/s]Extractor Estimating: 370it [03:55,  1.61it/s]Extractor Estimating: 371it [03:56,  1.61it/s]Extractor Estimating: 372it [03:57,  1.56it/s]Extractor Estimating: 373it [03:58,  1.50it/s]Extractor Estimating: 374it [03:58,  1.50it/s]Extractor Estimating: 375it [03:59,  1.68it/s]Extractor Estimating: 375it [03:59,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:30,961 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:30,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:30,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:30,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:30,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 07:23:31,789 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 07:23:31,790 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:23:32,377 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 07:23:33,438 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:23:33,438 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:36,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:36,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:36,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:36,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 07:23:36,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 07:23:37,332 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 07:23:37,333 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 07:23:37,891 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 07:23:38,069 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 07:23:38,069 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 09:49:38,445 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 09:49:38,484 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7880 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 23533
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23633, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23633, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.069, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.093, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.122, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 71, avg_time 1.109, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 171, avg_time 1.096, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 271, avg_time 2.267, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 42, avg_time 1.090, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 142, avg_time 1.106, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 242, avg_time 1.098, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.099, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 113, avg_time 2.291, loss:nan
g_step 1200, step 213, avg_time 1.079, loss:nan
g_step 1300, step 313, avg_time 1.106, loss:nan
g_step 1400, step 84, avg_time 1.073, loss:nan
g_step 1500, step 184, avg_time 1.110, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 284, avg_time 2.261, loss:nan
g_step 1700, step 55, avg_time 1.116, loss:nan
g_step 1800, step 155, avg_time 1.102, loss:nan
g_step 1900, step 255, avg_time 1.106, loss:nan
g_step 2000, step 26, avg_time 1.096, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 126, avg_time 2.281, loss:nan
g_step 2200, step 226, avg_time 1.074, loss:nan
g_step 2300, step 326, avg_time 1.113, loss:nan
g_step 2400, step 97, avg_time 1.096, loss:nan
g_step 2500, step 197, avg_time 1.111, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 297, avg_time 2.268, loss:nan
g_step 2700, step 68, avg_time 1.106, loss:nan
g_step 2800, step 168, avg_time 1.090, loss:nan
g_step 2900, step 268, avg_time 1.098, loss:nan
g_step 3000, step 39, avg_time 1.128, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 139, avg_time 2.254, loss:nan
g_step 3200, step 239, avg_time 1.119, loss:nan
g_step 3300, step 10, avg_time 1.098, loss:nan
g_step 3400, step 110, avg_time 1.115, loss:nan
g_step 3500, step 210, avg_time 1.099, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 310, avg_time 2.258, loss:nan
g_step 3700, step 81, avg_time 1.086, loss:nan
g_step 3800, step 181, avg_time 1.089, loss:nan
g_step 3900, step 281, avg_time 1.116, loss:nan
g_step 4000, step 52, avg_time 1.081, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 152, avg_time 2.296, loss:nan
g_step 4200, step 252, avg_time 1.090, loss:nan
g_step 4300, step 23, avg_time 1.089, loss:nan
g_step 4400, step 123, avg_time 1.087, loss:nan
g_step 4500, step 223, avg_time 1.122, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 323, avg_time 2.273, loss:nan
g_step 4700, step 94, avg_time 1.099, loss:nan
g_step 4800, step 194, avg_time 1.097, loss:nan
g_step 4900, step 294, avg_time 1.105, loss:nan
g_step 5000, step 65, avg_time 1.109, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 165, avg_time 2.248, loss:nan
g_step 5200, step 265, avg_time 1.111, loss:nan
g_step 5300, step 36, avg_time 1.103, loss:nan
g_step 5400, step 136, avg_time 1.102, loss:nan
g_step 5500, step 236, avg_time 1.079, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 7, avg_time 2.271, loss:nan
g_step 5700, step 107, avg_time 1.104, loss:nan
g_step 5800, step 207, avg_time 1.109, loss:nan
g_step 5900, step 307, avg_time 1.096, loss:nan
g_step 6000, step 78, avg_time 1.098, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 178, avg_time 2.288, loss:nan
g_step 6200, step 278, avg_time 1.085, loss:nan
g_step 6300, step 49, avg_time 1.108, loss:nan
g_step 6400, step 149, avg_time 1.096, loss:nan
g_step 6500, step 249, avg_time 1.075, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 09:49:38 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 09:49:38 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_09-49-38_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 09:49:39 - WARNING - datasets.builder -   Using custom data configuration default-1b1301327a4d3bd7
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1b1301327a4d3bd7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 09:49:39,706 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:49:39,707 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:49:39,707 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:49:39,708 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:49:39,716 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:49:39,722 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:49:39,722 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:49:39,722 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:49:39,722 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:49:39,722 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:49:39,722 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 09:49:39,867 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:49:42,923 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 09:49:42,932 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1b1301327a4d3bd7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.14ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.92ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.23ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.36ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.42ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.49ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.53ba/s]100%|██████████| 8/8 [00:01<00:00,  4.68ba/s]100%|██████████| 8/8 [00:01<00:00,  4.41ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.12ba/s] 40%|████      | 2/5 [00:00<00:00,  3.28ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.71ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.97ba/s]100%|██████████| 5/5 [00:01<00:00,  4.46ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.05ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.40ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.71ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.80ba/s]100%|██████████| 8/8 [00:00<00:00, 10.83ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.09ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.62ba/s]100%|██████████| 5/5 [00:00<00:00, 13.04ba/s]100%|██████████| 5/5 [00:00<00:00, 12.26ba/s]
[INFO|trainer.py:414] 2023-08-28 09:49:47,412 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 09:49:47,428 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 09:49:47,428 >>   Num examples = 7899
[INFO|trainer.py:1149] 2023-08-28 09:49:47,428 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 09:49:47,428 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 09:49:47,428 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 09:49:47,428 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 09:49:47,428 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<02:56,  3.49it/s]  0%|          | 2/615 [00:00<02:52,  3.56it/s]  0%|          | 3/615 [00:00<02:50,  3.58it/s]  1%|          | 4/615 [00:01<02:49,  3.60it/s]  1%|          | 5/615 [00:01<02:49,  3.60it/s]  1%|          | 6/615 [00:01<02:48,  3.61it/s]  1%|          | 7/615 [00:01<02:49,  3.60it/s]  1%|▏         | 8/615 [00:02<02:48,  3.60it/s]  1%|▏         | 9/615 [00:02<02:47,  3.61it/s]  2%|▏         | 10/615 [00:02<02:47,  3.61it/s]  2%|▏         | 11/615 [00:03<02:46,  3.62it/s]  2%|▏         | 12/615 [00:03<02:46,  3.62it/s]  2%|▏         | 13/615 [00:03<02:46,  3.62it/s]  2%|▏         | 14/615 [00:03<02:46,  3.62it/s]  2%|▏         | 15/615 [00:04<02:45,  3.62it/s]  3%|▎         | 16/615 [00:04<02:45,  3.62it/s]  3%|▎         | 17/615 [00:04<02:45,  3.62it/s]  3%|▎         | 18/615 [00:04<02:45,  3.61it/s]  3%|▎         | 19/615 [00:05<02:44,  3.62it/s]  3%|▎         | 20/615 [00:05<02:44,  3.62it/s]  3%|▎         | 21/615 [00:05<02:43,  3.62it/s]  4%|▎         | 22/615 [00:06<02:43,  3.62it/s]  4%|▎         | 23/615 [00:06<02:43,  3.62it/s]  4%|▍         | 24/615 [00:06<02:43,  3.62it/s]  4%|▍         | 25/615 [00:06<02:42,  3.62it/s]  4%|▍         | 26/615 [00:07<02:42,  3.62it/s]  4%|▍         | 27/615 [00:07<02:42,  3.62it/s]  5%|▍         | 28/615 [00:07<02:42,  3.62it/s]  5%|▍         | 29/615 [00:08<02:42,  3.60it/s]  5%|▍         | 30/615 [00:08<02:42,  3.61it/s]  5%|▌         | 31/615 [00:08<02:41,  3.61it/s]  5%|▌         | 32/615 [00:08<02:41,  3.61it/s]  5%|▌         | 33/615 [00:09<02:41,  3.61it/s]  6%|▌         | 34/615 [00:09<02:40,  3.61it/s]  6%|▌         | 35/615 [00:09<02:40,  3.62it/s]  6%|▌         | 36/615 [00:09<02:40,  3.61it/s]  6%|▌         | 37/615 [00:10<02:39,  3.62it/s]  6%|▌         | 38/615 [00:10<02:39,  3.62it/s]  6%|▋         | 39/615 [00:10<02:39,  3.62it/s]  7%|▋         | 40/615 [00:11<02:38,  3.62it/s]  7%|▋         | 41/615 [00:11<02:38,  3.62it/s]  7%|▋         | 42/615 [00:11<02:38,  3.62it/s]  7%|▋         | 43/615 [00:11<02:38,  3.62it/s]  7%|▋         | 44/615 [00:12<02:37,  3.62it/s]  7%|▋         | 45/615 [00:12<02:37,  3.62it/s]  7%|▋         | 46/615 [00:12<02:37,  3.62it/s]  8%|▊         | 47/615 [00:13<02:36,  3.62it/s]  8%|▊         | 48/615 [00:13<02:36,  3.62it/s]  8%|▊         | 49/615 [00:13<02:36,  3.61it/s]  8%|▊         | 50/615 [00:13<02:36,  3.61it/s]  8%|▊         | 51/615 [00:14<02:35,  3.62it/s]  8%|▊         | 52/615 [00:14<02:35,  3.62it/s]  9%|▊         | 53/615 [00:14<02:35,  3.62it/s]  9%|▉         | 54/615 [00:14<02:34,  3.63it/s]  9%|▉         | 55/615 [00:15<02:34,  3.63it/s]  9%|▉         | 56/615 [00:15<02:34,  3.63it/s]  9%|▉         | 57/615 [00:15<02:33,  3.63it/s]  9%|▉         | 58/615 [00:16<02:33,  3.62it/s] 10%|▉         | 59/615 [00:16<02:33,  3.63it/s] 10%|▉         | 60/615 [00:16<02:33,  3.61it/s] 10%|▉         | 61/615 [00:16<02:33,  3.61it/s] 10%|█         | 62/615 [00:17<02:33,  3.61it/s] 10%|█         | 63/615 [00:17<02:32,  3.61it/s] 10%|█         | 64/615 [00:17<02:32,  3.62it/s] 11%|█         | 65/615 [00:17<02:32,  3.62it/s] 11%|█         | 66/615 [00:18<02:31,  3.62it/s] 11%|█         | 67/615 [00:18<02:31,  3.62it/s] 11%|█         | 68/615 [00:18<02:31,  3.61it/s] 11%|█         | 69/615 [00:19<02:31,  3.61it/s] 11%|█▏        | 70/615 [00:19<02:30,  3.61it/s] 12%|█▏        | 71/615 [00:19<02:31,  3.60it/s] 12%|█▏        | 72/615 [00:19<02:30,  3.61it/s] 12%|█▏        | 73/615 [00:20<02:30,  3.61it/s] 12%|█▏        | 74/615 [00:20<02:29,  3.62it/s] 12%|█▏        | 75/615 [00:20<02:29,  3.62it/s] 12%|█▏        | 76/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 77/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 78/615 [00:21<02:28,  3.62it/s] 13%|█▎        | 79/615 [00:21<02:27,  3.62it/s] 13%|█▎        | 80/615 [00:22<02:27,  3.62it/s] 13%|█▎        | 81/615 [00:22<02:27,  3.62it/s] 13%|█▎        | 82/615 [00:22<02:28,  3.60it/s] 13%|█▎        | 83/615 [00:22<02:27,  3.60it/s] 14%|█▎        | 84/615 [00:23<02:27,  3.61it/s] 14%|█▍        | 85/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 86/615 [00:23<02:26,  3.61it/s] 14%|█▍        | 87/615 [00:24<02:26,  3.61it/s] 14%|█▍        | 88/615 [00:24<02:25,  3.61it/s] 14%|█▍        | 89/615 [00:24<02:25,  3.61it/s] 15%|█▍        | 90/615 [00:24<02:25,  3.60it/s] 15%|█▍        | 91/615 [00:25<02:25,  3.60it/s] 15%|█▍        | 92/615 [00:25<02:25,  3.61it/s] 15%|█▌        | 93/615 [00:25<02:25,  3.59it/s] 15%|█▌        | 94/615 [00:26<02:24,  3.59it/s] 15%|█▌        | 95/615 [00:26<02:24,  3.59it/s] 16%|█▌        | 96/615 [00:26<02:24,  3.58it/s] 16%|█▌        | 97/615 [00:26<02:24,  3.58it/s] 16%|█▌        | 98/615 [00:27<02:24,  3.57it/s] 16%|█▌        | 99/615 [00:27<02:24,  3.57it/s] 16%|█▋        | 100/615 [00:27<02:24,  3.57it/s] 16%|█▋        | 101/615 [00:27<02:24,  3.56it/s] 17%|█▋        | 102/615 [00:28<02:24,  3.56it/s] 17%|█▋        | 103/615 [00:28<02:23,  3.56it/s] 17%|█▋        | 104/615 [00:28<02:24,  3.54it/s] 17%|█▋        | 105/615 [00:29<02:23,  3.54it/s] 17%|█▋        | 106/615 [00:29<02:23,  3.55it/s] 17%|█▋        | 107/615 [00:29<02:23,  3.55it/s] 18%|█▊        | 108/615 [00:29<02:22,  3.56it/s] 18%|█▊        | 109/615 [00:30<02:22,  3.56it/s] 18%|█▊        | 110/615 [00:30<02:21,  3.56it/s] 18%|█▊        | 111/615 [00:30<02:21,  3.56it/s] 18%|█▊        | 112/615 [00:31<02:21,  3.56it/s] 18%|█▊        | 113/615 [00:31<02:20,  3.56it/s] 19%|█▊        | 114/615 [00:31<02:20,  3.56it/s] 19%|█▊        | 115/615 [00:31<02:20,  3.56it/s] 19%|█▉        | 116/615 [00:32<02:20,  3.56it/s] 19%|█▉        | 117/615 [00:32<02:20,  3.55it/s] 19%|█▉        | 118/615 [00:32<02:19,  3.55it/s] 19%|█▉        | 119/615 [00:33<02:19,  3.55it/s] 20%|█▉        | 120/615 [00:33<02:19,  3.55it/s] 20%|█▉        | 121/615 [00:33<02:18,  3.55it/s] 20%|█▉        | 122/615 [00:33<02:18,  3.56it/s] 20%|██        | 123/615 [00:34<02:18,  3.54it/s][INFO|trainer.py:2140] 2023-08-28 09:50:21,722 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:50:21,722 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 09:50:21,722 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.73it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.29it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.30it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.48it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.82it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.49it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.19it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.09it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.19it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.32it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.38it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.35it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.13it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.06it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.99it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.80it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.96it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.98it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.15it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.30it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.20it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.12it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.03it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.05it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.99it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.92it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.02it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.18it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.27it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.21it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.03it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.07it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.05it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.97it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.99it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.07it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.18it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.20it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.18it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.79it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.86it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.87it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.89it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.81it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.99it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.13it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.06it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.07it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.07it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.90it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.93it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.00it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.08it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.16it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.07it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.10it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.12it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.09it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.00it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.94it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.99it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.12it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.06it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.03it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.11it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.09it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.07it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.91it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.90it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.05it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.05it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.17it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.96it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.09it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.16it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.00it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.92it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.08it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.10it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.12it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.09it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.04it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.13it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.05it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.92it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.90it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.05it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.14it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.10it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.12it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.11it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.10it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.08it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.95it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.96it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.00it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.22it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.23it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.19it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.05it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.12it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.98it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.93it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.00it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.11it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.17it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.08it/s][A                                                 
                                                 [A 20%|██        | 123/615 [00:46<02:18,  3.54it/s]
100%|██████████| 543/543 [00:12<00:00, 44.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:50:34,060 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-28 09:50:34,077 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:50:36,190 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:50:36,205 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:50:36,215 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123/special_tokens_map.json
 20%|██        | 124/615 [00:49<38:47,  4.74s/it] 20%|██        | 125/615 [00:49<27:47,  3.40s/it] 20%|██        | 126/615 [00:49<20:06,  2.47s/it] 21%|██        | 127/615 [00:50<14:44,  1.81s/it] 21%|██        | 128/615 [00:50<10:58,  1.35s/it] 21%|██        | 129/615 [00:50<08:20,  1.03s/it] 21%|██        | 130/615 [00:51<06:30,  1.24it/s] 21%|██▏       | 131/615 [00:51<05:17,  1.53it/s] 21%|██▏       | 132/615 [00:51<04:22,  1.84it/s] 22%|██▏       | 133/615 [00:51<03:44,  2.15it/s] 22%|██▏       | 134/615 [00:52<03:17,  2.44it/s] 22%|██▏       | 135/615 [00:52<02:58,  2.69it/s] 22%|██▏       | 136/615 [00:52<02:45,  2.90it/s] 22%|██▏       | 137/615 [00:53<02:35,  3.07it/s] 22%|██▏       | 138/615 [00:53<02:29,  3.20it/s] 23%|██▎       | 139/615 [00:53<02:24,  3.30it/s] 23%|██▎       | 140/615 [00:53<02:20,  3.37it/s] 23%|██▎       | 141/615 [00:54<02:18,  3.42it/s] 23%|██▎       | 142/615 [00:54<02:16,  3.47it/s] 23%|██▎       | 143/615 [00:54<02:15,  3.50it/s] 23%|██▎       | 144/615 [00:54<02:13,  3.52it/s] 24%|██▎       | 145/615 [00:55<02:13,  3.53it/s] 24%|██▎       | 146/615 [00:55<02:12,  3.54it/s] 24%|██▍       | 147/615 [00:55<02:14,  3.49it/s] 24%|██▍       | 148/615 [00:56<02:13,  3.51it/s] 24%|██▍       | 149/615 [00:56<02:12,  3.53it/s] 24%|██▍       | 150/615 [00:56<02:11,  3.54it/s] 25%|██▍       | 151/615 [00:56<02:11,  3.54it/s] 25%|██▍       | 152/615 [00:57<02:10,  3.55it/s] 25%|██▍       | 153/615 [00:57<02:10,  3.55it/s] 25%|██▌       | 154/615 [00:57<02:09,  3.55it/s] 25%|██▌       | 155/615 [00:58<02:09,  3.55it/s] 25%|██▌       | 156/615 [00:58<02:09,  3.55it/s] 26%|██▌       | 157/615 [00:58<02:08,  3.55it/s] 26%|██▌       | 158/615 [00:58<02:08,  3.54it/s] 26%|██▌       | 159/615 [00:59<02:08,  3.56it/s] 26%|██▌       | 160/615 [00:59<02:07,  3.57it/s] 26%|██▌       | 161/615 [00:59<02:06,  3.58it/s] 26%|██▋       | 162/615 [01:00<02:06,  3.58it/s] 27%|██▋       | 163/615 [01:00<02:05,  3.59it/s] 27%|██▋       | 164/615 [01:00<02:05,  3.59it/s] 27%|██▋       | 165/615 [01:00<02:04,  3.60it/s] 27%|██▋       | 166/615 [01:01<02:04,  3.61it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/615 [01:01<02:04,  3.60it/s] 27%|██▋       | 168/615 [01:01<02:03,  3.61it/s] 27%|██▋       | 169/615 [01:01<02:04,  3.59it/s] 28%|██▊       | 170/615 [01:02<02:03,  3.60it/s] 28%|██▊       | 171/615 [01:02<02:03,  3.60it/s] 28%|██▊       | 172/615 [01:02<02:03,  3.60it/s] 28%|██▊       | 173/615 [01:03<02:02,  3.60it/s] 28%|██▊       | 174/615 [01:03<02:02,  3.60it/s] 28%|██▊       | 175/615 [01:03<02:01,  3.61it/s] 29%|██▊       | 176/615 [01:03<02:01,  3.61it/s] 29%|██▉       | 177/615 [01:04<02:01,  3.61it/s] 29%|██▉       | 178/615 [01:04<02:01,  3.60it/s] 29%|██▉       | 179/615 [01:04<02:00,  3.61it/s] 29%|██▉       | 180/615 [01:05<02:01,  3.59it/s] 29%|██▉       | 181/615 [01:05<02:00,  3.59it/s] 30%|██▉       | 182/615 [01:05<02:00,  3.60it/s] 30%|██▉       | 183/615 [01:05<01:59,  3.60it/s] 30%|██▉       | 184/615 [01:06<01:59,  3.61it/s] 30%|███       | 185/615 [01:06<01:59,  3.60it/s] 30%|███       | 186/615 [01:06<01:58,  3.61it/s] 30%|███       | 187/615 [01:06<01:58,  3.61it/s] 31%|███       | 188/615 [01:07<01:58,  3.60it/s] 31%|███       | 189/615 [01:07<01:58,  3.60it/s] 31%|███       | 190/615 [01:07<01:57,  3.61it/s] 31%|███       | 191/615 [01:08<01:57,  3.60it/s] 31%|███       | 192/615 [01:08<01:57,  3.60it/s] 31%|███▏      | 193/615 [01:08<01:56,  3.61it/s] 32%|███▏      | 194/615 [01:08<01:56,  3.61it/s] 32%|███▏      | 195/615 [01:09<01:56,  3.61it/s] 32%|███▏      | 196/615 [01:09<01:56,  3.61it/s] 32%|███▏      | 197/615 [01:09<01:55,  3.61it/s] 32%|███▏      | 198/615 [01:10<01:55,  3.61it/s] 32%|███▏      | 199/615 [01:10<01:55,  3.61it/s] 33%|███▎      | 200/615 [01:10<01:54,  3.61it/s] 33%|███▎      | 201/615 [01:10<01:54,  3.61it/s] 33%|███▎      | 202/615 [01:11<01:54,  3.60it/s] 33%|███▎      | 203/615 [01:11<01:54,  3.60it/s] 33%|███▎      | 204/615 [01:11<01:54,  3.60it/s] 33%|███▎      | 205/615 [01:11<01:53,  3.60it/s] 33%|███▎      | 206/615 [01:12<01:53,  3.61it/s] 34%|███▎      | 207/615 [01:12<01:53,  3.60it/s] 34%|███▍      | 208/615 [01:12<01:52,  3.61it/s] 34%|███▍      | 209/615 [01:13<01:52,  3.61it/s] 34%|███▍      | 210/615 [01:13<01:52,  3.61it/s] 34%|███▍      | 211/615 [01:13<01:52,  3.61it/s] 34%|███▍      | 212/615 [01:13<01:51,  3.61it/s] 35%|███▍      | 213/615 [01:14<01:51,  3.59it/s] 35%|███▍      | 214/615 [01:14<01:51,  3.60it/s] 35%|███▍      | 215/615 [01:14<01:51,  3.60it/s] 35%|███▌      | 216/615 [01:15<01:50,  3.60it/s] 35%|███▌      | 217/615 [01:15<01:50,  3.61it/s] 35%|███▌      | 218/615 [01:15<01:50,  3.60it/s] 36%|███▌      | 219/615 [01:15<01:49,  3.60it/s] 36%|███▌      | 220/615 [01:16<01:49,  3.60it/s] 36%|███▌      | 221/615 [01:16<01:49,  3.61it/s] 36%|███▌      | 222/615 [01:16<01:48,  3.61it/s] 36%|███▋      | 223/615 [01:16<01:48,  3.61it/s] 36%|███▋      | 224/615 [01:17<01:48,  3.61it/s] 37%|███▋      | 225/615 [01:17<01:47,  3.61it/s] 37%|███▋      | 226/615 [01:17<01:47,  3.61it/s] 37%|███▋      | 227/615 [01:18<01:47,  3.61it/s] 37%|███▋      | 228/615 [01:18<01:47,  3.61it/s] 37%|███▋      | 229/615 [01:18<01:46,  3.61it/s] 37%|███▋      | 230/615 [01:18<01:46,  3.61it/s] 38%|███▊      | 231/615 [01:19<01:46,  3.61it/s] 38%|███▊      | 232/615 [01:19<01:46,  3.61it/s] 38%|███▊      | 233/615 [01:19<01:45,  3.61it/s] 38%|███▊      | 234/615 [01:20<01:46,  3.57it/s] 38%|███▊      | 235/615 [01:20<01:46,  3.58it/s] 38%|███▊      | 236/615 [01:20<01:45,  3.59it/s] 39%|███▊      | 237/615 [01:20<01:45,  3.60it/s] 39%|███▊      | 238/615 [01:21<01:44,  3.60it/s] 39%|███▉      | 239/615 [01:21<01:44,  3.60it/s] 39%|███▉      | 240/615 [01:21<01:44,  3.60it/s] 39%|███▉      | 241/615 [01:21<01:43,  3.60it/s] 39%|███▉      | 242/615 [01:22<01:43,  3.60it/s] 40%|███▉      | 243/615 [01:22<01:43,  3.60it/s] 40%|███▉      | 244/615 [01:22<01:42,  3.61it/s] 40%|███▉      | 245/615 [01:23<01:43,  3.59it/s] 40%|████      | 246/615 [01:23<01:42,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 09:51:10,892 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:51:10,892 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 09:51:10,892 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3216, 'eval_samples_per_second': 352.389, 'eval_steps_per_second': 44.069, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.39it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.53it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.37it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.38it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.72it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.45it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.05it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.00it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.11it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.21it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.38it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.39it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.12it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.03it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.99it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.81it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.85it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.04it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.19it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.32it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.23it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.06it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.00it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.93it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.89it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.98it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.12it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.23it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.28it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.14it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.03it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.97it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.96it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.92it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.97it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.09it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.16it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.18it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.05it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.99it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.97it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.99it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.94it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.02it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.94it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.03it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.07it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.98it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.97it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.97it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.97it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.95it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.01it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.12it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.15it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.11it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.05it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.92it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.99it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.91it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.98it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.08it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.13it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.09it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.03it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.00it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.96it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.87it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.98it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.04it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.00it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.15it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.08it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.05it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.95it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.98it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.95it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.03it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.11it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.09it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.02it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.99it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.01it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.99it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.99it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.94it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.02it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.10it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.09it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.99it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.09it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.02it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.94it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.03it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.90it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.94it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.16it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.08it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.12it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.04it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.01it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.04it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.99it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.95it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.10it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.06it/s][A                                                 
                                                 [A 40%|████      | 246/615 [01:35<01:42,  3.60it/s]
100%|██████████| 543/543 [00:12<00:00, 44.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:51:23,264 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 09:51:23,291 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:51:25,150 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:51:25,168 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:51:25,177 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246/special_tokens_map.json
 40%|████      | 247/615 [01:38<28:45,  4.69s/it] 40%|████      | 248/615 [01:38<20:35,  3.37s/it] 40%|████      | 249/615 [01:38<14:52,  2.44s/it] 41%|████      | 250/615 [01:39<10:53,  1.79s/it] 41%|████      | 251/615 [01:39<08:06,  1.34s/it] 41%|████      | 252/615 [01:39<06:09,  1.02s/it] 41%|████      | 253/615 [01:39<04:48,  1.26it/s] 41%|████▏     | 254/615 [01:40<03:51,  1.56it/s] 41%|████▏     | 255/615 [01:40<03:11,  1.88it/s] 42%|████▏     | 256/615 [01:40<02:43,  2.20it/s] 42%|████▏     | 257/615 [01:41<02:23,  2.49it/s] 42%|████▏     | 258/615 [01:41<02:09,  2.75it/s] 42%|████▏     | 259/615 [01:41<02:01,  2.93it/s] 42%|████▏     | 260/615 [01:41<01:54,  3.11it/s] 42%|████▏     | 261/615 [01:42<01:49,  3.24it/s] 43%|████▎     | 262/615 [01:42<01:45,  3.35it/s] 43%|████▎     | 263/615 [01:42<01:42,  3.42it/s] 43%|████▎     | 264/615 [01:43<01:41,  3.47it/s] 43%|████▎     | 265/615 [01:43<01:39,  3.52it/s] 43%|████▎     | 266/615 [01:43<01:38,  3.54it/s] 43%|████▎     | 267/615 [01:43<01:37,  3.56it/s] 44%|████▎     | 268/615 [01:44<01:36,  3.58it/s] 44%|████▎     | 269/615 [01:44<01:36,  3.59it/s] 44%|████▍     | 270/615 [01:44<01:36,  3.58it/s] 44%|████▍     | 271/615 [01:44<01:35,  3.59it/s] 44%|████▍     | 272/615 [01:45<01:35,  3.60it/s] 44%|████▍     | 273/615 [01:45<01:35,  3.60it/s] 45%|████▍     | 274/615 [01:45<01:34,  3.60it/s] 45%|████▍     | 275/615 [01:46<01:34,  3.60it/s] 45%|████▍     | 276/615 [01:46<01:34,  3.60it/s] 45%|████▌     | 277/615 [01:46<01:33,  3.61it/s] 45%|████▌     | 278/615 [01:46<01:33,  3.61it/s] 45%|████▌     | 279/615 [01:47<01:33,  3.61it/s] 46%|████▌     | 280/615 [01:47<01:32,  3.61it/s] 46%|████▌     | 281/615 [01:47<01:32,  3.61it/s] 46%|████▌     | 282/615 [01:48<01:32,  3.61it/s] 46%|████▌     | 283/615 [01:48<01:31,  3.61it/s] 46%|████▌     | 284/615 [01:48<01:31,  3.61it/s] 46%|████▋     | 285/615 [01:48<01:31,  3.62it/s] 47%|████▋     | 286/615 [01:49<01:31,  3.61it/s] 47%|████▋     | 287/615 [01:49<01:30,  3.61it/s] 47%|████▋     | 288/615 [01:49<01:30,  3.61it/s] 47%|████▋     | 289/615 [01:49<01:30,  3.59it/s] 47%|████▋     | 290/615 [01:50<01:30,  3.59it/s] 47%|████▋     | 291/615 [01:50<01:30,  3.59it/s] 47%|████▋     | 292/615 [01:50<01:30,  3.59it/s] 48%|████▊     | 293/615 [01:51<01:29,  3.59it/s] 48%|████▊     | 294/615 [01:51<01:32,  3.49it/s] 48%|████▊     | 295/615 [01:51<01:31,  3.51it/s] 48%|████▊     | 296/615 [01:51<01:30,  3.54it/s] 48%|████▊     | 297/615 [01:52<01:29,  3.56it/s] 48%|████▊     | 298/615 [01:52<01:28,  3.58it/s] 49%|████▊     | 299/615 [01:52<01:28,  3.59it/s] 49%|████▉     | 300/615 [01:53<01:27,  3.60it/s] 49%|████▉     | 301/615 [01:53<01:27,  3.60it/s] 49%|████▉     | 302/615 [01:53<01:26,  3.60it/s] 49%|████▉     | 303/615 [01:53<01:26,  3.59it/s] 49%|████▉     | 304/615 [01:54<01:26,  3.59it/s] 50%|████▉     | 305/615 [01:54<01:26,  3.60it/s] 50%|████▉     | 306/615 [01:54<01:25,  3.60it/s] 50%|████▉     | 307/615 [01:55<01:25,  3.60it/s] 50%|█████     | 308/615 [01:55<01:25,  3.61it/s] 50%|█████     | 309/615 [01:55<01:24,  3.61it/s] 50%|█████     | 310/615 [01:55<01:24,  3.61it/s] 51%|█████     | 311/615 [01:56<01:24,  3.61it/s] 51%|█████     | 312/615 [01:56<01:24,  3.61it/s] 51%|█████     | 313/615 [01:56<01:23,  3.61it/s] 51%|█████     | 314/615 [01:56<01:23,  3.60it/s] 51%|█████     | 315/615 [01:57<01:23,  3.60it/s] 51%|█████▏    | 316/615 [01:57<01:22,  3.61it/s] 52%|█████▏    | 317/615 [01:57<01:22,  3.61it/s] 52%|█████▏    | 318/615 [01:58<01:22,  3.60it/s] 52%|█████▏    | 319/615 [01:58<01:22,  3.60it/s] 52%|█████▏    | 320/615 [01:58<01:21,  3.60it/s] 52%|█████▏    | 321/615 [01:58<01:21,  3.60it/s] 52%|█████▏    | 322/615 [01:59<01:21,  3.61it/s] 53%|█████▎    | 323/615 [01:59<01:20,  3.61it/s] 53%|█████▎    | 324/615 [01:59<01:20,  3.61it/s] 53%|█████▎    | 325/615 [02:00<01:21,  3.57it/s] 53%|█████▎    | 326/615 [02:00<01:20,  3.58it/s] 53%|█████▎    | 327/615 [02:00<01:20,  3.59it/s] 53%|█████▎    | 328/615 [02:00<01:20,  3.59it/s] 53%|█████▎    | 329/615 [02:01<01:19,  3.59it/s] 54%|█████▎    | 330/615 [02:01<01:19,  3.59it/s] 54%|█████▍    | 331/615 [02:01<01:18,  3.60it/s] 54%|█████▍    | 332/615 [02:01<01:18,  3.60it/s] 54%|█████▍    | 333/615 [02:02<01:18,  3.60it/s] 54%|█████▍    | 334/615 [02:02<01:17,  3.60it/s] 54%|█████▍    | 335/615 [02:02<01:17,  3.61it/s] 55%|█████▍    | 336/615 [02:03<01:17,  3.59it/s] 55%|█████▍    | 337/615 [02:03<01:17,  3.60it/s] 55%|█████▍    | 338/615 [02:03<01:16,  3.60it/s] 55%|█████▌    | 339/615 [02:03<01:16,  3.60it/s] 55%|█████▌    | 340/615 [02:04<01:16,  3.60it/s] 55%|█████▌    | 341/615 [02:04<01:16,  3.60it/s] 56%|█████▌    | 342/615 [02:04<01:15,  3.61it/s] 56%|█████▌    | 343/615 [02:05<01:15,  3.61it/s] 56%|█████▌    | 344/615 [02:05<01:15,  3.61it/s] 56%|█████▌    | 345/615 [02:05<01:14,  3.60it/s] 56%|█████▋    | 346/615 [02:05<01:14,  3.61it/s] 56%|█████▋    | 347/615 [02:06<01:14,  3.59it/s] 57%|█████▋    | 348/615 [02:06<01:14,  3.60it/s] 57%|█████▋    | 349/615 [02:06<01:13,  3.60it/s] 57%|█████▋    | 350/615 [02:06<01:13,  3.60it/s] 57%|█████▋    | 351/615 [02:07<01:13,  3.61it/s] 57%|█████▋    | 352/615 [02:07<01:12,  3.61it/s] 57%|█████▋    | 353/615 [02:07<01:12,  3.61it/s] 58%|█████▊    | 354/615 [02:08<01:12,  3.61it/s] 58%|█████▊    | 355/615 [02:08<01:12,  3.61it/s] 58%|█████▊    | 356/615 [02:08<01:11,  3.61it/s] 58%|█████▊    | 357/615 [02:08<01:11,  3.61it/s] 58%|█████▊    | 358/615 [02:09<01:11,  3.58it/s] 58%|█████▊    | 359/615 [02:09<01:11,  3.59it/s] 59%|█████▊    | 360/615 [02:09<01:10,  3.59it/s] 59%|█████▊    | 361/615 [02:09<01:10,  3.60it/s] 59%|█████▉    | 362/615 [02:10<01:10,  3.61it/s] 59%|█████▉    | 363/615 [02:10<01:09,  3.60it/s] 59%|█████▉    | 364/615 [02:10<01:09,  3.61it/s] 59%|█████▉    | 365/615 [02:11<01:09,  3.61it/s] 60%|█████▉    | 366/615 [02:11<01:09,  3.61it/s] 60%|█████▉    | 367/615 [02:11<01:08,  3.61it/s] 60%|█████▉    | 368/615 [02:11<01:08,  3.62it/s] 60%|██████    | 369/615 [02:12<01:08,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 09:51:59,766 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:51:59,766 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 09:51:59,766 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3266, 'eval_samples_per_second': 352.248, 'eval_steps_per_second': 44.051, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.58it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.84it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.14it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.22it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.77it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.38it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.31it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.16it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.26it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.37it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.25it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.21it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.12it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.96it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.89it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.84it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.95it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.08it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.25it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.31it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.21it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.11it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.94it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.90it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.69it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.87it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.96it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.12it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.21it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.15it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.12it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.06it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.81it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.85it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.97it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.14it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.19it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.23it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.19it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.11it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.02it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.76it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.83it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.02it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.11it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.19it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.10it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.11it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.07it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.01it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.87it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.96it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.07it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.12it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.19it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.13it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.09it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.03it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.02it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.87it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.93it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.02it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.07it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.15it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.11it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.08it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.04it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.01it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.89it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.96it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.04it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.17it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.11it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.14it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.07it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.03it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.90it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.99it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.02it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.15it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.19it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.19it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.11it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.06it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.08it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.90it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.88it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.96it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.11it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.20it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.20it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.10it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.10it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.09it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.95it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.91it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.86it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.16it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.10it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.18it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.15it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.10it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.00it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.93it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.94it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.03it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.07it/s][A                                                 
                                                 [A 60%|██████    | 369/615 [02:24<01:08,  3.60it/s]
100%|██████████| 543/543 [00:12<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:52:12,101 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-28 09:52:12,123 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:52:14,091 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:52:14,117 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:52:14,135 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369/special_tokens_map.json
 60%|██████    | 370/615 [02:27<19:16,  4.72s/it] 60%|██████    | 371/615 [02:27<13:47,  3.39s/it] 60%|██████    | 372/615 [02:27<09:56,  2.46s/it] 61%|██████    | 373/615 [02:28<07:16,  1.80s/it] 61%|██████    | 374/615 [02:28<05:24,  1.34s/it] 61%|██████    | 375/615 [02:28<04:05,  1.02s/it] 61%|██████    | 376/615 [02:28<03:11,  1.25it/s] 61%|██████▏   | 377/615 [02:29<02:32,  1.56it/s] 61%|██████▏   | 378/615 [02:29<02:06,  1.88it/s] 62%|██████▏   | 379/615 [02:29<01:47,  2.19it/s] 62%|██████▏   | 380/615 [02:30<01:34,  2.48it/s] 62%|██████▏   | 381/615 [02:30<01:25,  2.74it/s] 62%|██████▏   | 382/615 [02:30<01:19,  2.94it/s] 62%|██████▏   | 383/615 [02:30<01:14,  3.12it/s] 62%|██████▏   | 384/615 [02:31<01:10,  3.25it/s] 63%|██████▎   | 385/615 [02:31<01:08,  3.36it/s] 63%|██████▎   | 386/615 [02:31<01:06,  3.43it/s] 63%|██████▎   | 387/615 [02:32<01:05,  3.48it/s] 63%|██████▎   | 388/615 [02:32<01:04,  3.52it/s] 63%|██████▎   | 389/615 [02:32<01:03,  3.55it/s] 63%|██████▎   | 390/615 [02:32<01:03,  3.57it/s] 64%|██████▎   | 391/615 [02:33<01:02,  3.58it/s] 64%|██████▎   | 392/615 [02:33<01:02,  3.59it/s] 64%|██████▍   | 393/615 [02:33<01:01,  3.58it/s] 64%|██████▍   | 394/615 [02:33<01:01,  3.59it/s] 64%|██████▍   | 395/615 [02:34<01:01,  3.59it/s] 64%|██████▍   | 396/615 [02:34<01:00,  3.60it/s] 65%|██████▍   | 397/615 [02:34<01:00,  3.60it/s] 65%|██████▍   | 398/615 [02:35<01:00,  3.61it/s] 65%|██████▍   | 399/615 [02:35<00:59,  3.61it/s] 65%|██████▌   | 400/615 [02:35<00:59,  3.61it/s] 65%|██████▌   | 401/615 [02:35<00:59,  3.61it/s] 65%|██████▌   | 402/615 [02:36<00:58,  3.62it/s] 66%|██████▌   | 403/615 [02:36<00:58,  3.61it/s] 66%|██████▌   | 404/615 [02:36<00:58,  3.60it/s] 66%|██████▌   | 405/615 [02:37<00:58,  3.60it/s] 66%|██████▌   | 406/615 [02:37<00:57,  3.61it/s] 66%|██████▌   | 407/615 [02:37<00:57,  3.61it/s] 66%|██████▋   | 408/615 [02:37<00:57,  3.61it/s] 67%|██████▋   | 409/615 [02:38<00:57,  3.61it/s] 67%|██████▋   | 410/615 [02:38<00:56,  3.61it/s] 67%|██████▋   | 411/615 [02:38<00:56,  3.61it/s] 67%|██████▋   | 412/615 [02:38<00:56,  3.61it/s] 67%|██████▋   | 413/615 [02:39<00:55,  3.61it/s] 67%|██████▋   | 414/615 [02:39<00:55,  3.61it/s] 67%|██████▋   | 415/615 [02:39<00:55,  3.60it/s] 68%|██████▊   | 416/615 [02:40<00:55,  3.60it/s] 68%|██████▊   | 417/615 [02:40<00:54,  3.61it/s] 68%|██████▊   | 418/615 [02:40<00:54,  3.61it/s] 68%|██████▊   | 419/615 [02:40<00:54,  3.61it/s] 68%|██████▊   | 420/615 [02:41<00:53,  3.61it/s] 68%|██████▊   | 421/615 [02:41<00:53,  3.61it/s] 69%|██████▊   | 422/615 [02:41<00:53,  3.61it/s] 69%|██████▉   | 423/615 [02:42<00:53,  3.61it/s] 69%|██████▉   | 424/615 [02:42<00:52,  3.61it/s] 69%|██████▉   | 425/615 [02:42<00:52,  3.61it/s] 69%|██████▉   | 426/615 [02:42<00:52,  3.58it/s] 69%|██████▉   | 427/615 [02:43<00:52,  3.59it/s] 70%|██████▉   | 428/615 [02:43<00:51,  3.60it/s] 70%|██████▉   | 429/615 [02:43<00:51,  3.60it/s] 70%|██████▉   | 430/615 [02:43<00:51,  3.61it/s] 70%|███████   | 431/615 [02:44<00:51,  3.61it/s] 70%|███████   | 432/615 [02:44<00:50,  3.61it/s] 70%|███████   | 433/615 [02:44<00:50,  3.61it/s] 71%|███████   | 434/615 [02:45<00:50,  3.61it/s] 71%|███████   | 435/615 [02:45<00:49,  3.61it/s] 71%|███████   | 436/615 [02:45<00:49,  3.61it/s] 71%|███████   | 437/615 [02:45<00:49,  3.60it/s] 71%|███████   | 438/615 [02:46<00:49,  3.60it/s] 71%|███████▏  | 439/615 [02:46<00:48,  3.60it/s] 72%|███████▏  | 440/615 [02:46<00:48,  3.60it/s] 72%|███████▏  | 441/615 [02:46<00:48,  3.60it/s] 72%|███████▏  | 442/615 [02:47<00:47,  3.60it/s] 72%|███████▏  | 443/615 [02:47<00:47,  3.61it/s] 72%|███████▏  | 444/615 [02:47<00:47,  3.61it/s] 72%|███████▏  | 445/615 [02:48<00:47,  3.61it/s] 73%|███████▎  | 446/615 [02:48<00:46,  3.61it/s] 73%|███████▎  | 447/615 [02:48<00:46,  3.61it/s] 73%|███████▎  | 448/615 [02:48<00:46,  3.59it/s] 73%|███████▎  | 449/615 [02:49<00:46,  3.59it/s] 73%|███████▎  | 450/615 [02:49<00:45,  3.60it/s] 73%|███████▎  | 451/615 [02:49<00:45,  3.60it/s] 73%|███████▎  | 452/615 [02:50<00:45,  3.57it/s] 74%|███████▎  | 453/615 [02:50<00:45,  3.57it/s] 74%|███████▍  | 454/615 [02:50<00:45,  3.57it/s] 74%|███████▍  | 455/615 [02:50<00:44,  3.58it/s] 74%|███████▍  | 456/615 [02:51<00:44,  3.58it/s] 74%|███████▍  | 457/615 [02:51<00:45,  3.47it/s] 74%|███████▍  | 458/615 [02:51<00:44,  3.49it/s] 75%|███████▍  | 459/615 [02:52<00:44,  3.53it/s] 75%|███████▍  | 460/615 [02:52<00:43,  3.54it/s] 75%|███████▍  | 461/615 [02:52<00:43,  3.56it/s] 75%|███████▌  | 462/615 [02:52<00:42,  3.57it/s] 75%|███████▌  | 463/615 [02:53<00:42,  3.58it/s] 75%|███████▌  | 464/615 [02:53<00:42,  3.59it/s] 76%|███████▌  | 465/615 [02:53<00:41,  3.60it/s] 76%|███████▌  | 466/615 [02:53<00:41,  3.61it/s] 76%|███████▌  | 467/615 [02:54<00:41,  3.61it/s] 76%|███████▌  | 468/615 [02:54<00:40,  3.61it/s] 76%|███████▋  | 469/615 [02:54<00:40,  3.61it/s] 76%|███████▋  | 470/615 [02:55<00:40,  3.61it/s] 77%|███████▋  | 471/615 [02:55<00:40,  3.59it/s] 77%|███████▋  | 472/615 [02:55<00:39,  3.60it/s] 77%|███████▋  | 473/615 [02:55<00:39,  3.60it/s] 77%|███████▋  | 474/615 [02:56<00:39,  3.61it/s] 77%|███████▋  | 475/615 [02:56<00:38,  3.61it/s] 77%|███████▋  | 476/615 [02:56<00:38,  3.61it/s] 78%|███████▊  | 477/615 [02:57<00:38,  3.61it/s] 78%|███████▊  | 478/615 [02:57<00:37,  3.61it/s] 78%|███████▊  | 479/615 [02:57<00:37,  3.61it/s] 78%|███████▊  | 480/615 [02:57<00:37,  3.61it/s] 78%|███████▊  | 481/615 [02:58<00:37,  3.61it/s] 78%|███████▊  | 482/615 [02:58<00:36,  3.60it/s] 79%|███████▊  | 483/615 [02:58<00:36,  3.60it/s] 79%|███████▊  | 484/615 [02:58<00:36,  3.60it/s] 79%|███████▉  | 485/615 [02:59<00:36,  3.60it/s] 79%|███████▉  | 486/615 [02:59<00:35,  3.60it/s] 79%|███████▉  | 487/615 [02:59<00:35,  3.60it/s] 79%|███████▉  | 488/615 [03:00<00:35,  3.60it/s] 80%|███████▉  | 489/615 [03:00<00:34,  3.60it/s] 80%|███████▉  | 490/615 [03:00<00:34,  3.61it/s] 80%|███████▉  | 491/615 [03:00<00:34,  3.61it/s] 80%|████████  | 492/615 [03:01<00:34,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 09:52:48,749 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:52:48,749 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 09:52:48,749 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3238, 'eval_samples_per_second': 352.327, 'eval_steps_per_second': 44.061, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.76it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.34it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.39it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.36it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.74it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.49it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.15it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.97it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.09it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.25it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.36it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.27it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.20it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.05it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.98it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.85it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.85it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.91it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.14it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.25it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.25it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.17it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.03it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.97it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.80it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.92it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.95it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.16it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.21it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.23it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.23it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.04it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.99it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.96it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.96it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.99it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.11it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.16it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.19it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.14it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.03it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.95it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.92it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.96it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.94it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.10it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.18it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.20it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.08it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.06it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.02it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.98it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.03it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.08it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.12it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.15it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.20it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.09it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.05it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.97it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.12it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.14it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.17it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.15it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.04it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.03it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.00it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.01it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.05it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.13it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.09it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.19it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.15it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.06it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.02it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.94it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.02it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.07it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.12it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.02it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.13it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.12it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.04it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.02it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.02it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.02it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.12it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.15it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.05it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.13it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.08it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.02it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.08it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.03it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.05it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.14it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.12it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.06it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.06it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.02it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.00it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.94it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.99it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.12it/s][A                                                 
                                                 [A 80%|████████  | 492/615 [03:13<00:34,  3.61it/s]
100%|██████████| 543/543 [00:12<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:53:01,091 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 09:53:01,109 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:53:02,785 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:53:02,797 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:53:02,809 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492/special_tokens_map.json
 80%|████████  | 493/615 [03:15<09:24,  4.63s/it] 80%|████████  | 494/615 [03:16<06:42,  3.33s/it] 80%|████████  | 495/615 [03:16<04:49,  2.41s/it] 81%|████████  | 496/615 [03:16<03:31,  1.77s/it] 81%|████████  | 497/615 [03:17<02:36,  1.33s/it] 81%|████████  | 498/615 [03:17<01:58,  1.01s/it] 81%|████████  | 499/615 [03:17<01:31,  1.26it/s] 81%|████████▏ | 500/615 [03:17<01:13,  1.56it/s]                                                  81%|████████▏ | 500/615 [03:17<01:13,  1.56it/s] 81%|████████▏ | 501/615 [03:18<01:00,  1.88it/s] 82%|████████▏ | 502/615 [03:18<00:51,  2.19it/s] 82%|████████▏ | 503/615 [03:18<00:45,  2.49it/s] 82%|████████▏ | 504/615 [03:19<00:40,  2.74it/s] 82%|████████▏ | 505/615 [03:19<00:37,  2.96it/s] 82%|████████▏ | 506/615 [03:19<00:34,  3.13it/s] 82%|████████▏ | 507/615 [03:19<00:33,  3.26it/s] 83%|████████▎ | 508/615 [03:20<00:31,  3.36it/s] 83%|████████▎ | 509/615 [03:20<00:30,  3.43it/s] 83%|████████▎ | 510/615 [03:20<00:30,  3.48it/s] 83%|████████▎ | 511/615 [03:21<00:29,  3.52it/s] 83%|████████▎ | 512/615 [03:21<00:29,  3.55it/s] 83%|████████▎ | 513/615 [03:21<00:28,  3.57it/s] 84%|████████▎ | 514/615 [03:21<00:28,  3.58it/s] 84%|████████▎ | 515/615 [03:22<00:27,  3.59it/s] 84%|████████▍ | 516/615 [03:22<00:27,  3.60it/s] 84%|████████▍ | 517/615 [03:22<00:27,  3.60it/s] 84%|████████▍ | 518/615 [03:22<00:27,  3.58it/s] 84%|████████▍ | 519/615 [03:23<00:26,  3.59it/s] 85%|████████▍ | 520/615 [03:23<00:26,  3.59it/s] 85%|████████▍ | 521/615 [03:23<00:26,  3.60it/s] 85%|████████▍ | 522/615 [03:24<00:25,  3.61it/s] 85%|████████▌ | 523/615 [03:24<00:25,  3.61it/s] 85%|████████▌ | 524/615 [03:24<00:25,  3.61it/s] 85%|████████▌ | 525/615 [03:24<00:24,  3.61it/s] 86%|████████▌ | 526/615 [03:25<00:24,  3.61it/s] 86%|████████▌ | 527/615 [03:25<00:24,  3.61it/s] 86%|████████▌ | 528/615 [03:25<00:24,  3.62it/s] 86%|████████▌ | 529/615 [03:25<00:23,  3.60it/s] 86%|████████▌ | 530/615 [03:26<00:23,  3.61it/s] 86%|████████▋ | 531/615 [03:26<00:23,  3.61it/s] 87%|████████▋ | 532/615 [03:26<00:23,  3.61it/s] 87%|████████▋ | 533/615 [03:27<00:22,  3.61it/s] 87%|████████▋ | 534/615 [03:27<00:22,  3.61it/s] 87%|████████▋ | 535/615 [03:27<00:22,  3.61it/s] 87%|████████▋ | 536/615 [03:27<00:21,  3.61it/s] 87%|████████▋ | 537/615 [03:28<00:21,  3.61it/s] 87%|████████▋ | 538/615 [03:28<00:21,  3.61it/s] 88%|████████▊ | 539/615 [03:28<00:21,  3.61it/s] 88%|████████▊ | 540/615 [03:29<00:20,  3.60it/s] 88%|████████▊ | 541/615 [03:29<00:20,  3.60it/s] 88%|████████▊ | 542/615 [03:29<00:20,  3.61it/s] 88%|████████▊ | 543/615 [03:29<00:19,  3.61it/s] 88%|████████▊ | 544/615 [03:30<00:19,  3.61it/s] 89%|████████▊ | 545/615 [03:30<00:19,  3.61it/s] 89%|████████▉ | 546/615 [03:30<00:19,  3.61it/s] 89%|████████▉ | 547/615 [03:30<00:18,  3.61it/s] 89%|████████▉ | 548/615 [03:31<00:18,  3.61it/s] 89%|████████▉ | 549/615 [03:31<00:18,  3.61it/s] 89%|████████▉ | 550/615 [03:31<00:17,  3.61it/s] 90%|████████▉ | 551/615 [03:32<00:17,  3.60it/s] 90%|████████▉ | 552/615 [03:32<00:17,  3.61it/s] 90%|████████▉ | 553/615 [03:32<00:17,  3.61it/s] 90%|█████████ | 554/615 [03:32<00:16,  3.61it/s] 90%|█████████ | 555/615 [03:33<00:16,  3.61it/s] 90%|█████████ | 556/615 [03:33<00:16,  3.61it/s] 91%|█████████ | 557/615 [03:33<00:16,  3.62it/s] 91%|█████████ | 558/615 [03:34<00:15,  3.62it/s] 91%|█████████ | 559/615 [03:34<00:15,  3.61it/s] 91%|█████████ | 560/615 [03:34<00:15,  3.61it/s] 91%|█████████ | 561/615 [03:34<00:14,  3.61it/s] 91%|█████████▏| 562/615 [03:35<00:14,  3.60it/s] 92%|█████████▏| 563/615 [03:35<00:14,  3.60it/s] 92%|█████████▏| 564/615 [03:35<00:14,  3.60it/s] 92%|█████████▏| 565/615 [03:35<00:13,  3.61it/s] 92%|█████████▏| 566/615 [03:36<00:13,  3.60it/s] 92%|█████████▏| 567/615 [03:36<00:13,  3.61it/s] 92%|█████████▏| 568/615 [03:36<00:13,  3.61it/s] 93%|█████████▎| 569/615 [03:37<00:12,  3.61it/s] 93%|█████████▎| 570/615 [03:37<00:12,  3.61it/s] 93%|█████████▎| 571/615 [03:37<00:12,  3.61it/s] 93%|█████████▎| 572/615 [03:37<00:11,  3.61it/s] 93%|█████████▎| 573/615 [03:38<00:11,  3.60it/s] 93%|█████████▎| 574/615 [03:38<00:11,  3.60it/s] 93%|█████████▎| 575/615 [03:38<00:11,  3.60it/s] 94%|█████████▎| 576/615 [03:39<00:10,  3.60it/s] 94%|█████████▍| 577/615 [03:39<00:10,  3.60it/s] 94%|█████████▍| 578/615 [03:39<00:10,  3.60it/s] 94%|█████████▍| 579/615 [03:39<00:09,  3.61it/s] 94%|█████████▍| 580/615 [03:40<00:09,  3.61it/s] 94%|█████████▍| 581/615 [03:40<00:09,  3.61it/s] 95%|█████████▍| 582/615 [03:40<00:09,  3.61it/s] 95%|█████████▍| 583/615 [03:40<00:08,  3.61it/s] 95%|█████████▍| 584/615 [03:41<00:08,  3.59it/s] 95%|█████████▌| 585/615 [03:41<00:08,  3.60it/s] 95%|█████████▌| 586/615 [03:41<00:08,  3.60it/s] 95%|█████████▌| 587/615 [03:42<00:07,  3.60it/s] 96%|█████████▌| 588/615 [03:42<00:07,  3.61it/s] 96%|█████████▌| 589/615 [03:42<00:07,  3.61it/s] 96%|█████████▌| 590/615 [03:42<00:06,  3.61it/s] 96%|█████████▌| 591/615 [03:43<00:06,  3.61it/s] 96%|█████████▋| 592/615 [03:43<00:06,  3.61it/s] 96%|█████████▋| 593/615 [03:43<00:06,  3.61it/s] 97%|█████████▋| 594/615 [03:44<00:05,  3.61it/s] 97%|█████████▋| 595/615 [03:44<00:05,  3.59it/s] 97%|█████████▋| 596/615 [03:44<00:05,  3.60it/s] 97%|█████████▋| 597/615 [03:44<00:04,  3.61it/s] 97%|█████████▋| 598/615 [03:45<00:04,  3.61it/s] 97%|█████████▋| 599/615 [03:45<00:04,  3.61it/s] 98%|█████████▊| 600/615 [03:45<00:04,  3.61it/s] 98%|█████████▊| 601/615 [03:45<00:03,  3.61it/s] 98%|█████████▊| 602/615 [03:46<00:03,  3.61it/s] 98%|█████████▊| 603/615 [03:46<00:03,  3.61it/s] 98%|█████████▊| 604/615 [03:46<00:03,  3.61it/s] 98%|█████████▊| 605/615 [03:47<00:02,  3.61it/s] 99%|█████████▊| 606/615 [03:47<00:02,  3.59it/s] 99%|█████████▊| 607/615 [03:47<00:02,  3.60it/s] 99%|█████████▉| 608/615 [03:47<00:01,  3.60it/s] 99%|█████████▉| 609/615 [03:48<00:01,  3.60it/s] 99%|█████████▉| 610/615 [03:48<00:01,  3.60it/s] 99%|█████████▉| 611/615 [03:48<00:01,  3.61it/s]100%|█████████▉| 612/615 [03:49<00:00,  3.61it/s]100%|█████████▉| 613/615 [03:49<00:00,  3.61it/s]100%|█████████▉| 614/615 [03:49<00:00,  3.60it/s]100%|██████████| 615/615 [03:49<00:00,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 09:53:37,273 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:53:37,273 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 09:53:37,273 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3179, 'eval_samples_per_second': 352.496, 'eval_steps_per_second': 44.082, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.41it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.62it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.56it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.44it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.96it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.37it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.20it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.00it/s][A
  9%|▊         | 47/543 [00:01<00:11, 43.97it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.02it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.24it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.35it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.33it/s][A
 13%|█▎        | 72/543 [00:01<00:11, 41.29it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.49it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.53it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.69it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.78it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.85it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.15it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.29it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.90it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.90it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.92it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.03it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.06it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.08it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.18it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.25it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.07it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.01it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.96it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.92it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 43.98it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.01it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.08it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.21it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.19it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.11it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.96it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.02it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.93it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.07it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.04it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.14it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.14it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.14it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.96it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.95it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.00it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.07it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.06it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.10it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.14it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.09it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.08it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.95it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.91it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.00it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.03it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.09it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.09it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.10it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.06it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.93it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.96it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.97it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.08it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.12it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.05it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.05it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.11it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.04it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.94it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.95it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.96it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.07it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.01it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.11it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.10it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.07it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.93it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.95it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.97it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.98it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.04it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.11it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.10it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.11it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.05it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.01it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.95it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.94it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.03it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.06it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.09it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.04it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.99it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.97it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.06it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.07it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.07it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.10it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.12it/s][A                                                 
                                                 [A100%|██████████| 615/615 [04:02<00:00,  3.60it/s]
100%|██████████| 543/543 [00:12<00:00, 44.12it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:53:49,622 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-28 09:53:49,646 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:53:51,624 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:53:51,638 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:53:51,648 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 09:53:51,900 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 09:53:51,901 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123 (score: 1.0794728994369507).
                                                 100%|██████████| 615/615 [04:06<00:00,  3.60it/s]100%|██████████| 615/615 [04:06<00:00,  2.50it/s]
[INFO|trainer.py:1894] 2023-08-28 09:53:53,633 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 09:53:53,650 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:53:55,541 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:53:55,553 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:53:55,567 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:53:55,776 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:53:55,777 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:53:55,777 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:53:55,777 >>   train_runtime            = 0:04:06.20
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:53:55,777 >>   train_samples            =       7899
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:53:55,777 >>   train_samples_per_second =    160.417
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:53:55,777 >>   train_steps_per_second   =      2.498
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3347, 'eval_samples_per_second': 352.015, 'eval_steps_per_second': 44.022, 'epoch': 5.0}
{'train_runtime': 246.2017, 'train_samples_per_second': 160.417, 'train_steps_per_second': 2.498, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 09:53:55 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 09:53:55,819 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:53:55,819 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 09:53:55,819 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.40it/s]  2%|▏         | 12/543 [00:00<00:10, 48.49it/s]  3%|▎         | 17/543 [00:00<00:11, 46.71it/s]  4%|▍         | 22/543 [00:00<00:11, 46.01it/s]  5%|▍         | 27/543 [00:00<00:11, 45.60it/s]  6%|▌         | 32/543 [00:00<00:11, 45.29it/s]  7%|▋         | 37/543 [00:00<00:11, 44.99it/s]  8%|▊         | 42/543 [00:00<00:11, 44.56it/s]  9%|▊         | 47/543 [00:01<00:11, 43.94it/s] 10%|▉         | 52/543 [00:01<00:11, 43.64it/s] 10%|█         | 57/543 [00:01<00:11, 43.77it/s] 11%|█▏        | 62/543 [00:01<00:10, 43.90it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.17it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.33it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.43it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.35it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.02it/s] 17%|█▋        | 92/543 [00:02<00:10, 43.74it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.57it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.69it/s] 20%|█▉        | 107/543 [00:02<00:09, 43.91it/s] 21%|██        | 112/543 [00:02<00:09, 44.19it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.31it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.24it/s] 23%|██▎       | 127/543 [00:02<00:09, 44.09it/s] 24%|██▍       | 132/543 [00:02<00:09, 43.87it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.69it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.67it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.71it/s] 28%|██▊       | 152/543 [00:03<00:08, 43.90it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.18it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.31it/s] 31%|███       | 167/543 [00:03<00:08, 44.31it/s] 32%|███▏      | 172/543 [00:03<00:08, 44.06it/s] 33%|███▎      | 177/543 [00:03<00:08, 43.82it/s] 34%|███▎      | 182/543 [00:04<00:08, 43.73it/s] 34%|███▍      | 187/543 [00:04<00:08, 43.65it/s] 35%|███▌      | 192/543 [00:04<00:08, 43.66it/s] 36%|███▋      | 197/543 [00:04<00:07, 43.87it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.17it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.24it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.24it/s] 40%|███▉      | 217/543 [00:04<00:07, 43.99it/s] 41%|████      | 222/543 [00:05<00:07, 43.96it/s] 42%|████▏     | 227/543 [00:05<00:07, 43.81it/s] 43%|████▎     | 232/543 [00:05<00:07, 43.75it/s] 44%|████▎     | 237/543 [00:05<00:06, 43.77it/s] 45%|████▍     | 242/543 [00:05<00:06, 43.91it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.17it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.28it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.07it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.07it/s] 49%|████▉     | 267/543 [00:06<00:06, 43.99it/s] 50%|█████     | 272/543 [00:06<00:06, 43.86it/s] 51%|█████     | 277/543 [00:06<00:06, 43.85it/s] 52%|█████▏    | 282/543 [00:06<00:05, 43.83it/s] 53%|█████▎    | 287/543 [00:06<00:05, 43.98it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.14it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.10it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.11it/s] 57%|█████▋    | 307/543 [00:06<00:05, 43.99it/s] 57%|█████▋    | 312/543 [00:07<00:05, 43.83it/s] 58%|█████▊    | 317/543 [00:07<00:05, 43.83it/s] 59%|█████▉    | 322/543 [00:07<00:05, 43.85it/s] 60%|██████    | 327/543 [00:07<00:04, 43.72it/s] 61%|██████    | 332/543 [00:07<00:04, 44.00it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.08it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.14it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.13it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.02it/s] 66%|██████▌   | 357/543 [00:08<00:04, 43.90it/s] 67%|██████▋   | 362/543 [00:08<00:04, 43.89it/s] 68%|██████▊   | 367/543 [00:08<00:04, 43.90it/s] 69%|██████▊   | 372/543 [00:08<00:03, 43.80it/s] 69%|██████▉   | 377/543 [00:08<00:03, 43.99it/s] 70%|███████   | 382/543 [00:08<00:03, 44.04it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.08it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.11it/s] 73%|███████▎  | 397/543 [00:08<00:03, 43.98it/s] 74%|███████▍  | 402/543 [00:09<00:03, 43.93it/s] 75%|███████▍  | 407/543 [00:09<00:03, 43.91it/s] 76%|███████▌  | 412/543 [00:09<00:02, 43.88it/s] 77%|███████▋  | 417/543 [00:09<00:02, 43.91it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.03it/s] 79%|███████▊  | 427/543 [00:09<00:02, 43.99it/s] 80%|███████▉  | 432/543 [00:09<00:02, 43.92it/s] 80%|████████  | 437/543 [00:09<00:02, 44.00it/s] 81%|████████▏ | 442/543 [00:10<00:02, 43.93it/s] 82%|████████▏ | 447/543 [00:10<00:02, 43.94it/s] 83%|████████▎ | 452/543 [00:10<00:02, 43.88it/s] 84%|████████▍ | 457/543 [00:10<00:01, 43.87it/s] 85%|████████▌ | 462/543 [00:10<00:01, 43.91it/s] 86%|████████▌ | 467/543 [00:10<00:01, 43.96it/s] 87%|████████▋ | 472/543 [00:10<00:01, 43.90it/s] 88%|████████▊ | 477/543 [00:10<00:01, 43.97it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.00it/s] 90%|████████▉ | 487/543 [00:11<00:01, 44.00it/s] 91%|█████████ | 492/543 [00:11<00:01, 43.97it/s] 92%|█████████▏| 497/543 [00:11<00:01, 43.93it/s] 92%|█████████▏| 502/543 [00:11<00:00, 43.94it/s] 93%|█████████▎| 507/543 [00:11<00:00, 43.95it/s] 94%|█████████▍| 512/543 [00:11<00:00, 43.99it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.09it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.10it/s] 97%|█████████▋| 527/543 [00:11<00:00, 43.95it/s] 98%|█████████▊| 532/543 [00:12<00:00, 43.98it/s] 99%|█████████▉| 537/543 [00:12<00:00, 43.93it/s]100%|█████████▉| 542/543 [00:12<00:00, 43.92it/s]100%|██████████| 543/543 [00:12<00:00, 44.06it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:54:08,162 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:54:08,162 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:54:08,162 >>   eval_loss               =     1.0795
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:54:08,162 >>   eval_runtime            = 0:00:12.34
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:54:08,162 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:54:08,162 >>   eval_samples_per_second =    351.772
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:54:08,162 >>   eval_steps_per_second   =     43.992
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:54:08,162 >>   perplexity              =     2.9431
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:13,237 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:13,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:13,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:13,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:13,244 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:54:13,949 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:54:13,950 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:54:14,207 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:54:15,248 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:54:15,249 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:16,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:16,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:16,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:16,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:16,678 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:54:17,000 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:54:17,001 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:54:17,272 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:54:17,438 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:54:17,438 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.80it/s]Extractor Predicting: 2it [00:01,  1.80it/s]Extractor Predicting: 3it [00:01,  1.82it/s]Extractor Predicting: 4it [00:02,  1.91it/s]Extractor Predicting: 5it [00:02,  1.88it/s]Extractor Predicting: 6it [00:03,  1.83it/s]Extractor Predicting: 7it [00:03,  1.85it/s]Extractor Predicting: 8it [00:04,  1.88it/s]Extractor Predicting: 9it [00:04,  1.94it/s]Extractor Predicting: 10it [00:05,  1.96it/s]Extractor Predicting: 11it [00:05,  1.98it/s]Extractor Predicting: 12it [00:06,  1.98it/s]Extractor Predicting: 13it [00:06,  1.88it/s]Extractor Predicting: 14it [00:07,  1.82it/s]Extractor Predicting: 15it [00:08,  1.77it/s]Extractor Predicting: 16it [00:08,  1.77it/s]Extractor Predicting: 17it [00:09,  1.76it/s]Extractor Predicting: 18it [00:09,  1.73it/s]Extractor Predicting: 19it [00:10,  1.68it/s]Extractor Predicting: 20it [00:11,  1.65it/s]Extractor Predicting: 21it [00:11,  1.67it/s]Extractor Predicting: 22it [00:12,  1.71it/s]Extractor Predicting: 23it [00:12,  1.72it/s]Extractor Predicting: 24it [00:13,  1.72it/s]Extractor Predicting: 25it [00:13,  1.72it/s]Extractor Predicting: 26it [00:14,  1.61it/s]Extractor Predicting: 27it [00:15,  1.63it/s]Extractor Predicting: 28it [00:15,  1.66it/s]Extractor Predicting: 29it [00:16,  1.65it/s]Extractor Predicting: 30it [00:17,  1.66it/s]Extractor Predicting: 31it [00:17,  1.69it/s]Extractor Predicting: 32it [00:18,  1.69it/s]Extractor Predicting: 33it [00:18,  1.71it/s]Extractor Predicting: 34it [00:19,  1.69it/s]Extractor Predicting: 35it [00:19,  1.67it/s]Extractor Predicting: 36it [00:20,  1.64it/s]Extractor Predicting: 37it [00:21,  1.64it/s]Extractor Predicting: 38it [00:21,  1.64it/s]Extractor Predicting: 39it [00:22,  1.68it/s]Extractor Predicting: 40it [00:22,  1.69it/s]Extractor Predicting: 41it [00:23,  1.67it/s]Extractor Predicting: 42it [00:24,  1.65it/s]Extractor Predicting: 43it [00:24,  1.69it/s]Extractor Predicting: 44it [00:25,  1.71it/s]Extractor Predicting: 45it [00:25,  1.72it/s]Extractor Predicting: 46it [00:26,  1.73it/s]Extractor Predicting: 47it [00:27,  1.73it/s]Extractor Predicting: 48it [00:27,  1.71it/s]Extractor Predicting: 49it [00:28,  1.70it/s]Extractor Predicting: 50it [00:28,  1.72it/s]Extractor Predicting: 51it [00:29,  1.71it/s]Extractor Predicting: 52it [00:30,  1.70it/s]Extractor Predicting: 53it [00:30,  1.67it/s]Extractor Predicting: 54it [00:31,  1.71it/s]Extractor Predicting: 55it [00:31,  1.75it/s]Extractor Predicting: 56it [00:32,  1.71it/s]Extractor Predicting: 57it [00:32,  1.69it/s]Extractor Predicting: 58it [00:33,  1.73it/s]Extractor Predicting: 59it [00:34,  1.69it/s]Extractor Predicting: 60it [00:34,  1.70it/s]Extractor Predicting: 61it [00:35,  1.73it/s]Extractor Predicting: 62it [00:35,  1.74it/s]Extractor Predicting: 63it [00:36,  1.74it/s]Extractor Predicting: 64it [00:36,  1.74it/s]Extractor Predicting: 65it [00:37,  1.80it/s]Extractor Predicting: 66it [00:38,  1.79it/s]Extractor Predicting: 67it [00:38,  1.79it/s]Extractor Predicting: 68it [00:39,  1.75it/s]Extractor Predicting: 69it [00:39,  1.77it/s]Extractor Predicting: 70it [00:40,  1.75it/s]Extractor Predicting: 71it [00:40,  1.75it/s]Extractor Predicting: 72it [00:41,  1.74it/s]Extractor Predicting: 73it [00:42,  1.72it/s]Extractor Predicting: 74it [00:42,  1.69it/s]Extractor Predicting: 75it [00:43,  1.66it/s]Extractor Predicting: 76it [00:43,  1.68it/s]Extractor Predicting: 77it [00:44,  1.72it/s]Extractor Predicting: 78it [00:45,  1.70it/s]Extractor Predicting: 79it [00:45,  1.68it/s]Extractor Predicting: 80it [00:46,  1.72it/s]Extractor Predicting: 81it [00:46,  1.72it/s]Extractor Predicting: 82it [00:47,  1.73it/s]Extractor Predicting: 83it [00:47,  1.73it/s]Extractor Predicting: 84it [00:48,  1.74it/s]Extractor Predicting: 85it [00:49,  1.77it/s]Extractor Predicting: 86it [00:49,  1.58it/s]Extractor Predicting: 87it [00:50,  1.62it/s]Extractor Predicting: 88it [00:51,  1.67it/s]Extractor Predicting: 89it [00:51,  1.70it/s]Extractor Predicting: 90it [00:52,  1.69it/s]Extractor Predicting: 91it [00:52,  1.67it/s]Extractor Predicting: 92it [00:53,  1.64it/s]Extractor Predicting: 93it [00:53,  1.71it/s]Extractor Predicting: 94it [00:54,  1.71it/s]Extractor Predicting: 95it [00:55,  1.71it/s]Extractor Predicting: 96it [00:55,  1.73it/s]Extractor Predicting: 97it [00:56,  1.75it/s]Extractor Predicting: 98it [00:56,  1.74it/s]Extractor Predicting: 99it [00:57,  1.72it/s]Extractor Predicting: 100it [00:57,  1.72it/s]Extractor Predicting: 101it [00:58,  1.73it/s]Extractor Predicting: 102it [00:59,  1.72it/s]Extractor Predicting: 103it [00:59,  1.73it/s]Extractor Predicting: 104it [01:00,  1.74it/s]Extractor Predicting: 105it [01:00,  1.71it/s]Extractor Predicting: 106it [01:01,  1.74it/s]Extractor Predicting: 107it [01:02,  1.73it/s]Extractor Predicting: 108it [01:02,  1.72it/s]Extractor Predicting: 109it [01:03,  1.72it/s]Extractor Predicting: 110it [01:03,  1.71it/s]Extractor Predicting: 111it [01:04,  1.73it/s]Extractor Predicting: 112it [01:04,  1.76it/s]Extractor Predicting: 113it [01:05,  1.75it/s]Extractor Predicting: 114it [01:06,  1.74it/s]Extractor Predicting: 115it [01:06,  1.73it/s]Extractor Predicting: 116it [01:07,  1.73it/s]Extractor Predicting: 117it [01:07,  1.74it/s]Extractor Predicting: 118it [01:08,  1.69it/s]Extractor Predicting: 119it [01:09,  1.70it/s]Extractor Predicting: 120it [01:09,  1.69it/s]Extractor Predicting: 121it [01:10,  1.68it/s]Extractor Predicting: 122it [01:10,  1.67it/s]Extractor Predicting: 123it [01:11,  1.73it/s]Extractor Predicting: 124it [01:11,  1.74it/s]Extractor Predicting: 125it [01:12,  1.76it/s]Extractor Predicting: 126it [01:13,  1.70it/s]Extractor Predicting: 127it [01:13,  1.70it/s]Extractor Predicting: 128it [01:14,  1.73it/s]Extractor Predicting: 129it [01:14,  1.75it/s]Extractor Predicting: 130it [01:15,  1.80it/s]Extractor Predicting: 131it [01:15,  1.75it/s]Extractor Predicting: 132it [01:16,  1.75it/s]Extractor Predicting: 133it [01:17,  1.75it/s]Extractor Predicting: 134it [01:17,  1.76it/s]Extractor Predicting: 135it [01:18,  1.77it/s]Extractor Predicting: 136it [01:18,  1.76it/s]Extractor Predicting: 137it [01:19,  1.78it/s]Extractor Predicting: 138it [01:19,  1.75it/s]Extractor Predicting: 139it [01:20,  1.73it/s]Extractor Predicting: 140it [01:21,  1.77it/s]Extractor Predicting: 141it [01:21,  1.77it/s]Extractor Predicting: 142it [01:22,  1.81it/s]Extractor Predicting: 143it [01:22,  1.81it/s]Extractor Predicting: 144it [01:23,  1.81it/s]Extractor Predicting: 145it [01:23,  1.77it/s]Extractor Predicting: 146it [01:24,  1.79it/s]Extractor Predicting: 147it [01:24,  1.79it/s]Extractor Predicting: 148it [01:25,  1.79it/s]Extractor Predicting: 149it [01:26,  1.81it/s]Extractor Predicting: 150it [01:26,  1.75it/s]Extractor Predicting: 151it [01:27,  1.76it/s]Extractor Predicting: 152it [01:27,  1.75it/s]Extractor Predicting: 153it [01:28,  1.73it/s]Extractor Predicting: 154it [01:28,  1.74it/s]Extractor Predicting: 155it [01:29,  1.71it/s]Extractor Predicting: 156it [01:30,  1.52it/s]Extractor Predicting: 157it [01:30,  1.57it/s]Extractor Predicting: 158it [01:31,  1.61it/s]Extractor Predicting: 159it [01:32,  1.66it/s]Extractor Predicting: 160it [01:32,  1.70it/s]Extractor Predicting: 161it [01:33,  1.72it/s]Extractor Predicting: 162it [01:33,  1.73it/s]Extractor Predicting: 163it [01:34,  1.62it/s]Extractor Predicting: 163it [01:34,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:00,099 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:00,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:00,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:00,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:00,104 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:56:00,741 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:56:00,742 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:56:01,305 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:56:02,360 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:56:02,360 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:05,468 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:05,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:05,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:05,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:56:05,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:56:06,115 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:56:06,116 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:56:06,696 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:56:06,860 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:56:06,860 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.77it/s]Extractor Predicting: 4it [00:02,  1.80it/s]Extractor Predicting: 5it [00:02,  1.78it/s]Extractor Predicting: 6it [00:03,  1.79it/s]Extractor Predicting: 7it [00:03,  1.82it/s]Extractor Predicting: 8it [00:04,  1.81it/s]Extractor Predicting: 9it [00:04,  1.84it/s]Extractor Predicting: 10it [00:05,  1.85it/s]Extractor Predicting: 11it [00:06,  1.86it/s]Extractor Predicting: 12it [00:06,  1.87it/s]Extractor Predicting: 13it [00:07,  1.85it/s]Extractor Predicting: 14it [00:07,  1.82it/s]Extractor Predicting: 15it [00:08,  1.82it/s]Extractor Predicting: 16it [00:08,  1.80it/s]Extractor Predicting: 17it [00:09,  1.79it/s]Extractor Predicting: 18it [00:09,  1.79it/s]Extractor Predicting: 19it [00:10,  1.84it/s]Extractor Predicting: 20it [00:11,  1.81it/s]Extractor Predicting: 21it [00:11,  1.82it/s]Extractor Predicting: 22it [00:12,  1.80it/s]Extractor Predicting: 23it [00:12,  1.79it/s]Extractor Predicting: 24it [00:13,  1.76it/s]Extractor Predicting: 25it [00:13,  1.77it/s]Extractor Predicting: 26it [00:14,  1.79it/s]Extractor Predicting: 27it [00:14,  1.83it/s]Extractor Predicting: 28it [00:15,  1.82it/s]Extractor Predicting: 29it [00:16,  1.81it/s]Extractor Predicting: 30it [00:16,  1.83it/s]Extractor Predicting: 31it [00:17,  1.82it/s]Extractor Predicting: 32it [00:17,  1.83it/s]Extractor Predicting: 33it [00:18,  1.81it/s]Extractor Predicting: 34it [00:18,  1.80it/s]Extractor Predicting: 35it [00:19,  1.83it/s]Extractor Predicting: 36it [00:19,  1.81it/s]Extractor Predicting: 37it [00:20,  1.80it/s]Extractor Predicting: 38it [00:21,  1.78it/s]Extractor Predicting: 39it [00:21,  1.83it/s]Extractor Predicting: 40it [00:22,  1.82it/s]Extractor Predicting: 41it [00:22,  1.72it/s]Extractor Predicting: 42it [00:23,  1.73it/s]Extractor Predicting: 43it [00:23,  1.72it/s]Extractor Predicting: 44it [00:24,  1.72it/s]Extractor Predicting: 45it [00:25,  1.70it/s]Extractor Predicting: 46it [00:25,  1.68it/s]Extractor Predicting: 47it [00:26,  1.51it/s]Extractor Predicting: 48it [00:27,  1.58it/s]Extractor Predicting: 49it [00:27,  1.60it/s]Extractor Predicting: 50it [00:28,  1.63it/s]Extractor Predicting: 51it [00:28,  1.65it/s]Extractor Predicting: 52it [00:29,  1.65it/s]Extractor Predicting: 53it [00:30,  1.65it/s]Extractor Predicting: 54it [00:30,  1.59it/s]Extractor Predicting: 55it [00:31,  1.64it/s]Extractor Predicting: 56it [00:31,  1.70it/s]Extractor Predicting: 57it [00:32,  1.69it/s]Extractor Predicting: 58it [00:33,  1.68it/s]Extractor Predicting: 59it [00:33,  1.65it/s]Extractor Predicting: 60it [00:34,  1.64it/s]Extractor Predicting: 61it [00:34,  1.69it/s]Extractor Predicting: 62it [00:35,  1.68it/s]Extractor Predicting: 63it [00:36,  1.62it/s]Extractor Predicting: 64it [00:36,  1.64it/s]Extractor Predicting: 65it [00:37,  1.65it/s]Extractor Predicting: 66it [00:37,  1.63it/s]Extractor Predicting: 67it [00:38,  1.64it/s]Extractor Predicting: 68it [00:39,  1.64it/s]Extractor Predicting: 69it [00:39,  1.64it/s]Extractor Predicting: 70it [00:40,  1.65it/s]Extractor Predicting: 71it [00:40,  1.67it/s]Extractor Predicting: 72it [00:41,  1.67it/s]Extractor Predicting: 73it [00:42,  1.66it/s]Extractor Predicting: 74it [00:42,  1.69it/s]Extractor Predicting: 75it [00:43,  1.68it/s]Extractor Predicting: 76it [00:43,  1.70it/s]Extractor Predicting: 77it [00:44,  1.73it/s]Extractor Predicting: 78it [00:45,  1.72it/s]Extractor Predicting: 79it [00:45,  1.67it/s]Extractor Predicting: 80it [00:46,  1.72it/s]Extractor Predicting: 81it [00:47,  1.57it/s]Extractor Predicting: 82it [00:47,  1.62it/s]Extractor Predicting: 83it [00:48,  1.65it/s]Extractor Predicting: 84it [00:48,  1.62it/s]Extractor Predicting: 85it [00:49,  1.64it/s]Extractor Predicting: 86it [00:49,  1.67it/s]Extractor Predicting: 87it [00:50,  1.70it/s]Extractor Predicting: 88it [00:51,  1.67it/s]Extractor Predicting: 89it [00:51,  1.72it/s]Extractor Predicting: 90it [00:52,  1.74it/s]Extractor Predicting: 91it [00:52,  1.77it/s]Extractor Predicting: 92it [00:53,  1.76it/s]Extractor Predicting: 93it [00:53,  1.75it/s]Extractor Predicting: 94it [00:54,  1.73it/s]Extractor Predicting: 95it [00:55,  1.75it/s]Extractor Predicting: 96it [00:55,  1.78it/s]Extractor Predicting: 97it [00:56,  1.78it/s]Extractor Predicting: 98it [00:56,  1.71it/s]Extractor Predicting: 99it [00:57,  1.71it/s]Extractor Predicting: 100it [00:58,  1.71it/s]Extractor Predicting: 101it [00:58,  1.69it/s]Extractor Predicting: 102it [00:59,  1.72it/s]Extractor Predicting: 103it [00:59,  1.69it/s]Extractor Predicting: 104it [01:00,  1.72it/s]Extractor Predicting: 105it [01:00,  1.71it/s]Extractor Predicting: 106it [01:01,  1.66it/s]Extractor Predicting: 107it [01:02,  1.64it/s]Extractor Predicting: 108it [01:02,  1.68it/s]Extractor Predicting: 109it [01:03,  1.62it/s]Extractor Predicting: 110it [01:04,  1.66it/s]Extractor Predicting: 111it [01:04,  1.65it/s]Extractor Predicting: 112it [01:05,  1.68it/s]Extractor Predicting: 113it [01:05,  1.67it/s]Extractor Predicting: 114it [01:06,  1.68it/s]Extractor Predicting: 115it [01:06,  1.68it/s]Extractor Predicting: 116it [01:07,  1.65it/s]Extractor Predicting: 117it [01:08,  1.66it/s]Extractor Predicting: 118it [01:08,  1.72it/s]Extractor Predicting: 119it [01:09,  1.72it/s]Extractor Predicting: 120it [01:09,  1.76it/s]Extractor Predicting: 121it [01:10,  1.75it/s]Extractor Predicting: 122it [01:10,  1.77it/s]Extractor Predicting: 123it [01:11,  1.79it/s]Extractor Predicting: 124it [01:12,  1.82it/s]Extractor Predicting: 125it [01:12,  1.80it/s]Extractor Predicting: 126it [01:13,  1.76it/s]Extractor Predicting: 127it [01:13,  1.76it/s]Extractor Predicting: 128it [01:14,  1.74it/s]Extractor Predicting: 129it [01:14,  1.73it/s]Extractor Predicting: 130it [01:15,  1.73it/s]Extractor Predicting: 131it [01:16,  1.80it/s]Extractor Predicting: 132it [01:16,  1.77it/s]Extractor Predicting: 133it [01:17,  1.78it/s]Extractor Predicting: 134it [01:17,  1.80it/s]Extractor Predicting: 135it [01:18,  1.79it/s]Extractor Predicting: 136it [01:18,  1.79it/s]Extractor Predicting: 137it [01:19,  1.84it/s]Extractor Predicting: 138it [01:19,  1.82it/s]Extractor Predicting: 139it [01:20,  1.82it/s]Extractor Predicting: 140it [01:21,  1.79it/s]Extractor Predicting: 141it [01:21,  1.80it/s]Extractor Predicting: 142it [01:22,  1.78it/s]Extractor Predicting: 143it [01:22,  1.75it/s]Extractor Predicting: 144it [01:23,  1.76it/s]Extractor Predicting: 145it [01:23,  1.70it/s]Extractor Predicting: 146it [01:24,  1.64it/s]Extractor Predicting: 147it [01:25,  1.68it/s]Extractor Predicting: 148it [01:25,  1.68it/s]Extractor Predicting: 149it [01:26,  1.71it/s]Extractor Predicting: 150it [01:26,  1.72it/s]Extractor Predicting: 151it [01:27,  1.52it/s]Extractor Predicting: 152it [01:28,  1.57it/s]Extractor Predicting: 153it [01:28,  1.63it/s]Extractor Predicting: 154it [01:29,  1.68it/s]Extractor Predicting: 155it [01:29,  1.73it/s]Extractor Predicting: 156it [01:30,  1.68it/s]Extractor Predicting: 157it [01:31,  1.71it/s]Extractor Predicting: 158it [01:31,  1.70it/s]Extractor Predicting: 159it [01:32,  1.71it/s]Extractor Predicting: 160it [01:32,  1.75it/s]Extractor Predicting: 161it [01:33,  1.66it/s]Extractor Predicting: 162it [01:34,  1.72it/s]Extractor Predicting: 163it [01:34,  1.75it/s]Extractor Predicting: 164it [01:35,  1.72it/s]Extractor Predicting: 165it [01:35,  1.79it/s]Extractor Predicting: 166it [01:36,  1.80it/s]Extractor Predicting: 167it [01:36,  1.79it/s]Extractor Predicting: 168it [01:37,  1.85it/s]Extractor Predicting: 169it [01:37,  1.87it/s]Extractor Predicting: 170it [01:38,  1.81it/s]Extractor Predicting: 171it [01:39,  1.79it/s]Extractor Predicting: 172it [01:39,  1.80it/s]Extractor Predicting: 173it [01:40,  1.76it/s]Extractor Predicting: 174it [01:40,  1.80it/s]Extractor Predicting: 175it [01:41,  1.79it/s]Extractor Predicting: 176it [01:41,  1.77it/s]Extractor Predicting: 177it [01:42,  1.75it/s]Extractor Predicting: 178it [01:43,  1.74it/s]Extractor Predicting: 179it [01:43,  1.83it/s]Extractor Predicting: 180it [01:44,  1.80it/s]Extractor Predicting: 181it [01:44,  1.80it/s]Extractor Predicting: 182it [01:45,  1.77it/s]Extractor Predicting: 183it [01:45,  1.76it/s]Extractor Predicting: 184it [01:46,  1.76it/s]Extractor Predicting: 185it [01:46,  1.80it/s]Extractor Predicting: 186it [01:47,  1.76it/s]Extractor Predicting: 187it [01:48,  1.75it/s]Extractor Predicting: 188it [01:48,  1.73it/s]Extractor Predicting: 189it [01:49,  1.72it/s]Extractor Predicting: 190it [01:49,  1.69it/s]Extractor Predicting: 191it [01:50,  1.69it/s]Extractor Predicting: 192it [01:51,  1.73it/s]Extractor Predicting: 193it [01:51,  1.77it/s]Extractor Predicting: 194it [01:52,  1.78it/s]Extractor Predicting: 195it [01:52,  1.75it/s]Extractor Predicting: 196it [01:53,  1.76it/s]Extractor Predicting: 197it [01:53,  1.77it/s]Extractor Predicting: 198it [01:54,  1.78it/s]Extractor Predicting: 199it [01:54,  1.77it/s]Extractor Predicting: 200it [01:55,  1.76it/s]Extractor Predicting: 201it [01:56,  1.76it/s]Extractor Predicting: 202it [01:56,  1.76it/s]Extractor Predicting: 203it [01:57,  1.76it/s]Extractor Predicting: 204it [01:57,  1.77it/s]Extractor Predicting: 205it [01:58,  1.76it/s]Extractor Predicting: 206it [01:58,  1.75it/s]Extractor Predicting: 207it [01:59,  1.77it/s]Extractor Predicting: 208it [02:00,  1.77it/s]Extractor Predicting: 209it [02:00,  1.77it/s]Extractor Predicting: 210it [02:01,  1.76it/s]Extractor Predicting: 211it [02:01,  1.75it/s]Extractor Predicting: 212it [02:02,  1.77it/s]Extractor Predicting: 213it [02:02,  1.77it/s]Extractor Predicting: 214it [02:03,  1.78it/s]Extractor Predicting: 215it [02:04,  1.73it/s]Extractor Predicting: 216it [02:04,  1.75it/s]Extractor Predicting: 217it [02:05,  1.78it/s]Extractor Predicting: 218it [02:05,  1.79it/s]Extractor Predicting: 219it [02:06,  1.75it/s]Extractor Predicting: 220it [02:06,  1.76it/s]Extractor Predicting: 221it [02:07,  1.74it/s]Extractor Predicting: 222it [02:08,  1.76it/s]Extractor Predicting: 223it [02:08,  1.67it/s]Extractor Predicting: 224it [02:09,  1.69it/s]Extractor Predicting: 225it [02:09,  1.66it/s]Extractor Predicting: 226it [02:10,  1.68it/s]Extractor Predicting: 227it [02:11,  1.64it/s]Extractor Predicting: 228it [02:11,  1.58it/s]Extractor Predicting: 229it [02:12,  1.61it/s]Extractor Predicting: 230it [02:13,  1.62it/s]Extractor Predicting: 231it [02:13,  1.63it/s]Extractor Predicting: 232it [02:14,  1.64it/s]Extractor Predicting: 233it [02:14,  1.65it/s]Extractor Predicting: 234it [02:15,  1.67it/s]Extractor Predicting: 235it [02:15,  1.69it/s]Extractor Predicting: 236it [02:16,  1.70it/s]Extractor Predicting: 237it [02:17,  1.70it/s]Extractor Predicting: 238it [02:17,  1.77it/s]Extractor Predicting: 239it [02:18,  1.80it/s]Extractor Predicting: 240it [02:18,  1.87it/s]Extractor Predicting: 241it [02:19,  1.85it/s]Extractor Predicting: 242it [02:19,  1.83it/s]Extractor Predicting: 243it [02:20,  1.79it/s]Extractor Predicting: 244it [02:21,  1.57it/s]Extractor Predicting: 245it [02:21,  1.66it/s]Extractor Predicting: 246it [02:22,  1.68it/s]Extractor Predicting: 247it [02:22,  1.71it/s]Extractor Predicting: 248it [02:23,  1.75it/s]Extractor Predicting: 249it [02:23,  1.82it/s]Extractor Predicting: 250it [02:24,  1.83it/s]Extractor Predicting: 251it [02:24,  1.91it/s]Extractor Predicting: 252it [02:25,  1.93it/s]Extractor Predicting: 253it [02:25,  1.89it/s]Extractor Predicting: 254it [02:26,  1.91it/s]Extractor Predicting: 255it [02:27,  1.85it/s]Extractor Predicting: 256it [02:27,  1.91it/s]Extractor Predicting: 257it [02:28,  1.86it/s]Extractor Predicting: 258it [02:28,  1.80it/s]Extractor Predicting: 259it [02:29,  1.74it/s]Extractor Predicting: 260it [02:29,  1.81it/s]Extractor Predicting: 261it [02:30,  1.77it/s]Extractor Predicting: 262it [02:30,  1.78it/s]Extractor Predicting: 263it [02:31,  1.84it/s]Extractor Predicting: 264it [02:32,  1.85it/s]Extractor Predicting: 265it [02:32,  1.84it/s]Extractor Predicting: 266it [02:33,  1.87it/s]Extractor Predicting: 267it [02:33,  1.90it/s]Extractor Predicting: 268it [02:34,  1.89it/s]Extractor Predicting: 269it [02:34,  1.84it/s]Extractor Predicting: 270it [02:35,  1.86it/s]Extractor Predicting: 271it [02:35,  1.84it/s]Extractor Predicting: 272it [02:36,  1.81it/s]Extractor Predicting: 273it [02:36,  1.88it/s]Extractor Predicting: 274it [02:37,  1.89it/s]Extractor Predicting: 275it [02:37,  1.86it/s]Extractor Predicting: 276it [02:38,  1.84it/s]Extractor Predicting: 277it [02:39,  1.83it/s]Extractor Predicting: 278it [02:39,  1.87it/s]Extractor Predicting: 279it [02:40,  1.80it/s]Extractor Predicting: 280it [02:40,  1.88it/s]Extractor Predicting: 281it [02:41,  1.89it/s]Extractor Predicting: 282it [02:41,  1.90it/s]Extractor Predicting: 283it [02:42,  1.91it/s]Extractor Predicting: 284it [02:42,  1.91it/s]Extractor Predicting: 285it [02:43,  1.90it/s]Extractor Predicting: 286it [02:43,  1.90it/s]Extractor Predicting: 287it [02:44,  1.90it/s]Extractor Predicting: 288it [02:44,  1.85it/s]Extractor Predicting: 289it [02:45,  1.86it/s]Extractor Predicting: 290it [02:46,  1.76it/s]Extractor Predicting: 291it [02:46,  1.79it/s]Extractor Predicting: 292it [02:47,  1.84it/s]Extractor Predicting: 293it [02:47,  1.86it/s]Extractor Predicting: 294it [02:48,  1.87it/s]Extractor Predicting: 295it [02:48,  1.88it/s]Extractor Predicting: 296it [02:49,  1.77it/s]Extractor Predicting: 297it [02:49,  1.76it/s]Extractor Predicting: 298it [02:50,  1.70it/s]Extractor Predicting: 299it [02:51,  1.70it/s]Extractor Predicting: 300it [02:51,  1.72it/s]Extractor Predicting: 301it [02:52,  1.74it/s]Extractor Predicting: 302it [02:52,  1.74it/s]Extractor Predicting: 303it [02:53,  1.71it/s]Extractor Predicting: 304it [02:53,  1.70it/s]Extractor Predicting: 305it [02:54,  1.75it/s]Extractor Predicting: 306it [02:55,  1.70it/s]Extractor Predicting: 307it [02:55,  1.67it/s]Extractor Predicting: 308it [02:56,  1.67it/s]Extractor Predicting: 309it [02:56,  1.71it/s]Extractor Predicting: 310it [02:57,  1.74it/s]Extractor Predicting: 311it [02:58,  1.76it/s]Extractor Predicting: 312it [02:58,  1.74it/s]Extractor Predicting: 313it [02:59,  1.70it/s]Extractor Predicting: 314it [02:59,  1.68it/s]Extractor Predicting: 315it [03:00,  1.68it/s]Extractor Predicting: 316it [03:01,  1.68it/s]Extractor Predicting: 317it [03:01,  1.68it/s]Extractor Predicting: 318it [03:02,  1.70it/s]Extractor Predicting: 319it [03:02,  1.71it/s]Extractor Predicting: 320it [03:03,  1.72it/s]Extractor Predicting: 321it [03:03,  1.78it/s]Extractor Predicting: 322it [03:04,  1.77it/s]Extractor Predicting: 323it [03:05,  1.75it/s]Extractor Predicting: 324it [03:05,  1.73it/s]Extractor Predicting: 325it [03:06,  1.73it/s]Extractor Predicting: 326it [03:06,  1.71it/s]Extractor Predicting: 327it [03:07,  1.67it/s]Extractor Predicting: 328it [03:08,  1.68it/s]Extractor Predicting: 329it [03:08,  1.74it/s]Extractor Predicting: 330it [03:09,  1.77it/s]Extractor Predicting: 331it [03:09,  1.96it/s]Extractor Predicting: 331it [03:09,  1.75it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:24,447 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:24,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:24,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:24,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:24,451 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:59:24,777 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:59:24,778 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:59:25,042 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:59:26,102 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:59:26,102 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:29,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:29,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:29,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:29,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:29,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:59:29,699 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:59:29,700 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:59:30,263 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:59:30,431 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:59:30,431 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:08,  1.65it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.66it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.63it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:19,  1.66it/s]Extractor Predicting: 33it [00:20,  1.68it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:21,  1.68it/s]Extractor Predicting: 36it [00:22,  1.68it/s]Extractor Predicting: 37it [00:22,  1.69it/s]Extractor Predicting: 38it [00:23,  1.69it/s]Extractor Predicting: 39it [00:23,  1.63it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.67it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:27,  1.68it/s]Extractor Predicting: 46it [00:28,  1.69it/s]Extractor Predicting: 47it [00:28,  1.68it/s]Extractor Predicting: 48it [00:29,  1.71it/s]Extractor Predicting: 49it [00:29,  1.74it/s]Extractor Predicting: 50it [00:30,  1.77it/s]Extractor Predicting: 51it [00:30,  1.76it/s]Extractor Predicting: 52it [00:31,  1.76it/s]Extractor Predicting: 53it [00:32,  1.66it/s]Extractor Predicting: 54it [00:32,  1.71it/s]Extractor Predicting: 55it [00:33,  1.72it/s]Extractor Predicting: 56it [00:33,  1.80it/s]Extractor Predicting: 57it [00:34,  1.86it/s]Extractor Predicting: 58it [00:34,  1.92it/s]Extractor Predicting: 59it [00:35,  2.02it/s]Extractor Predicting: 60it [00:35,  2.08it/s]Extractor Predicting: 61it [00:36,  2.14it/s]Extractor Predicting: 62it [00:36,  2.13it/s]Extractor Predicting: 63it [00:36,  2.15it/s]Extractor Predicting: 64it [00:37,  2.13it/s]Extractor Predicting: 65it [00:37,  2.11it/s]Extractor Predicting: 66it [00:38,  2.11it/s]Extractor Predicting: 67it [00:38,  2.08it/s]Extractor Predicting: 68it [00:39,  2.11it/s]Extractor Predicting: 69it [00:39,  2.16it/s]Extractor Predicting: 70it [00:40,  2.10it/s]Extractor Predicting: 71it [00:40,  2.10it/s]Extractor Predicting: 72it [00:41,  2.12it/s]Extractor Predicting: 73it [00:41,  2.17it/s]Extractor Predicting: 74it [00:42,  2.19it/s]Extractor Predicting: 75it [00:42,  2.18it/s]Extractor Predicting: 76it [00:43,  2.15it/s]Extractor Predicting: 77it [00:43,  2.22it/s]Extractor Predicting: 78it [00:43,  2.14it/s]Extractor Predicting: 79it [00:44,  2.14it/s]Extractor Predicting: 80it [00:44,  2.12it/s]Extractor Predicting: 81it [00:45,  2.10it/s]Extractor Predicting: 82it [00:45,  2.13it/s]Extractor Predicting: 83it [00:46,  2.15it/s]Extractor Predicting: 84it [00:46,  2.15it/s]Extractor Predicting: 85it [00:47,  2.17it/s]Extractor Predicting: 86it [00:47,  2.00it/s]Extractor Predicting: 87it [00:48,  1.93it/s]Extractor Predicting: 88it [00:48,  1.89it/s]Extractor Predicting: 89it [00:49,  1.85it/s]Extractor Predicting: 90it [00:50,  1.87it/s]Extractor Predicting: 91it [00:50,  1.83it/s]Extractor Predicting: 92it [00:51,  1.82it/s]Extractor Predicting: 93it [00:51,  1.63it/s]Extractor Predicting: 94it [00:52,  1.66it/s]Extractor Predicting: 95it [00:53,  1.73it/s]Extractor Predicting: 96it [00:53,  1.76it/s]Extractor Predicting: 97it [00:54,  1.79it/s]Extractor Predicting: 98it [00:54,  1.80it/s]Extractor Predicting: 99it [00:55,  1.79it/s]Extractor Predicting: 100it [00:55,  1.73it/s]Extractor Predicting: 101it [00:56,  1.77it/s]Extractor Predicting: 102it [00:56,  1.76it/s]Extractor Predicting: 103it [00:57,  1.71it/s]Extractor Predicting: 104it [00:58,  1.70it/s]Extractor Predicting: 105it [00:58,  1.68it/s]Extractor Predicting: 106it [00:59,  1.67it/s]Extractor Predicting: 107it [01:00,  1.64it/s]Extractor Predicting: 108it [01:00,  1.58it/s]Extractor Predicting: 108it [01:00,  1.78it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:00:32,592 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:00:32,596 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:00:32,601 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:00:32,601 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:00:32,602 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:00:35,732 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:00:35,738 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:00:35,749 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:00:35,750 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:00:35,753 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:00:35,759 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:00:35,759 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:00:35,759 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:00:35,759 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:00:35,759 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:00:35,759 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:00:36,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:36,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:37,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:37,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:38,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:39,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:40,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:40,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:41,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:42,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:43,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:43,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:44,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:45,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:45,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:46,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:47,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:47,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:48,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:48,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:49,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:50,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:50,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:15<03:38, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-28 10:00:51,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:52,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:52,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:53,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:53,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:54,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:55,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:55,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:56,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:57,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:57,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:58,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:58,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:00:59,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:00,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:01,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:01,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:02,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:03,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:03,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:04,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:04,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:05,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:06,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:06,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:31<03:24, 15.71s/it][WARNING|generation_utils.py:914] 2023-08-28 10:01:07,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:08,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:08,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:09,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:10,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:10,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:11,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:11,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:12,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:13,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:13,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:14,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:14,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:15,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:16,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:16,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:17,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:18,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:18,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:19,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:19,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:20,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:21,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:21,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:22,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:47<03:08, 15.72s/it][WARNING|generation_utils.py:914] 2023-08-28 10:01:23,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:23,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:24,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:25,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:26,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:26,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:27,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:28,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:28,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:29,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:30,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:30,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:31,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:32,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:33,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:33,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:34,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:34,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:35,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:36,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:36,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:37,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:38,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:38,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:39,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:04<03:00, 16.38s/it][WARNING|generation_utils.py:914] 2023-08-28 10:01:40,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:41,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:41,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:42,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:42,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:43,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:44,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:44,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:45,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:45,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:46,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:47,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:48,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:48,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:49,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:49,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:50,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:51,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:51,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:52,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:53,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:54,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:54,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:55,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:56,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:20<02:43, 16.33s/it][WARNING|generation_utils.py:914] 2023-08-28 10:01:56,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:57,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:58,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:58,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:59,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:01:59,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:00,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:01,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:01,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:02,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:02,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:03,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:04,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:04,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:05,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:06,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:06,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:07,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:07,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:08,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:09,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:09,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:10,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:11,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:35<02:22, 15.84s/it][WARNING|generation_utils.py:914] 2023-08-28 10:02:11,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:12,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:12,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:13,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:14,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:14,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:15,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:16,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:16,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:17,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:18,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:18,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:19,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:20,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:21,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:21,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:22,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:22,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:23,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:24,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:25,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:25,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:26,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:26,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:27,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:28,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:52<02:10, 16.31s/it][WARNING|generation_utils.py:914] 2023-08-28 10:02:28,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:29,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:30,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:30,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:31,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:31,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:32,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:33,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:33,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:34,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:34,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:35,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:35,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:36,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:37,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:37,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:38,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:38,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:39,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:40,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:40,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:41,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:41,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:42,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:42,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:43,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:43,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:44,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:45,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:45,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:10<01:56, 16.65s/it][WARNING|generation_utils.py:914] 2023-08-28 10:02:46,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:47,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:47,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:48,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:48,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:49,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:50,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:50,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:51,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:51,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:52,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:53,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:53,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:54,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:55,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:55,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:56,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:57,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:57,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:58,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:58,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:59,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:02:59,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:00,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:25<01:36, 16.11s/it][WARNING|generation_utils.py:914] 2023-08-28 10:03:01,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:01,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:02,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:02,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:03,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:04,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:04,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:05,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:05,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:06,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:07,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:07,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:08,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:08,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:09,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:09,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:10,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:10,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:11,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:12,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:12,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:13,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:13,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:14,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:15,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:39<01:18, 15.63s/it][WARNING|generation_utils.py:914] 2023-08-28 10:03:15,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:16,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:17,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:17,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:18,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:18,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:19,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:19,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:20,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:21,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:22,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:22,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:23,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:23,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:24,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:24,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:25,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:26,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:26,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:27,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:27,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:28,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:29,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:29,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:54<01:01, 15.35s/it][WARNING|generation_utils.py:914] 2023-08-28 10:03:30,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:31,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:31,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:32,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:33,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:33,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:34,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:35,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:36,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:36,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:37,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:38,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:38,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:39,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:40,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:40,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:41,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:41,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:42,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:43,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:43,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:44,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:45,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:46,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:46,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:47,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:47,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:48,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:13<00:49, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-28 10:03:49,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:49,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:50,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:51,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:51,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:52,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:53,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:53,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:54,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:54,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:55,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:56,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:56,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:57,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:58,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:58,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:03:59,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:00,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:00,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:01,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:02,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:02,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:03,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:04,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:04,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:05,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:30<00:33, 16.53s/it][WARNING|generation_utils.py:914] 2023-08-28 10:04:06,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:06,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:07,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:08,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:08,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:09,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:09,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:10,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:10,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:11,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:12,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:12,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:13,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:13,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:14,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:15,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:15,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:16,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:17,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:17,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:18,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:18,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:19,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:20,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:20,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:21,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:45<00:16, 16.30s/it][WARNING|generation_utils.py:914] 2023-08-28 10:04:21,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:22,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:23,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:23,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:24,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:25,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:26,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:26,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:27,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:28,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:28,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:29,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:29,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:30,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:31,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:31,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:32,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:33,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:33,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:34,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:35,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:35,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:36,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:37,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:37,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:04:38,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:03<00:00, 16.58s/it]Generating: 100%|██████████| 15/15 [04:03<00:00, 16.21s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:45,736 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:45,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:45,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:45,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:45,741 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:04:46,381 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:04:46,381 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:04:46,946 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:04:48,012 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:04:48,012 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:51,011 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:51,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:51,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:51,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:04:51,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:04:51,684 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:04:51,685 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:04:52,241 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:04:52,417 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:04:52,417 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in Life , he played the title character , a young princess of the family at the end of the third season of HBO s Game of Thrones . Head Entity : Game of Thrones , Tail Entity : Princess of the family .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.75375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.7525, 'errors': {'', "('pharaoh Amenhotep II', 'made from material', '', 'He is best remembered for the painting of the pharaoh Amenhotep II , composed by Alfred Wohl in 1827 for a French conservatory .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : cast member .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Billboard Pop Hits', 'cast member', '', 'On October 25 , 1978 , the band released the track The Lads , the debut single by the British band The Riveters ( now defunct ) , on Columbia Records , along with several appearances on the Billboard Pop Hits chart for June .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : follows .', 'success_rate': 0.7283653846153846, 'errors': {'', "('Nigeria', 'follows', '', 'In 2013 , a second poll showed Zaire had a 7 % approval rating , while in 2014 , Nigeria had a 24 % approval rating .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 281, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 366, 'raw': 576}
{'target': 600, 'success': 391, 'raw': 608}
{'target': 600, 'success': 410, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 452, 'raw': 704}
{'target': 600, 'success': 470, 'raw': 736}
{'target': 600, 'success': 490, 'raw': 768}
{'target': 600, 'success': 511, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 590, 'raw': 928}
{'target': 600, 'success': 608, 'raw': 960}
{'prompt': 'Relation : league .', 'success_rate': 0.6333333333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.765, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in Life , he studied with a group of Latin philologists and other thinkers at the Renaissance in Italy , in Naples from 1635 to 1645 , and from 1654 back to 1701 . Head Entity : Naples , Tail Entity : Rome .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 262, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 305, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 341, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 412, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 501, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 584, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : mother .', 'success_rate': 0.6729910714285714, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : residence .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1453 the head of the province of Ghent and of neighboring districts at Neuromö , in Burgundesland , were also under the Ottoman Empire . Head Entity : Burgundesland , Tail Entity : Ottomans .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7283653846153846, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 15273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.31it/s]Extractor Estimating: 2it [00:01,  1.40it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.49it/s]Extractor Estimating: 5it [00:03,  1.45it/s]Extractor Estimating: 6it [00:04,  1.41it/s]Extractor Estimating: 7it [00:04,  1.47it/s]Extractor Estimating: 8it [00:05,  1.45it/s]Extractor Estimating: 9it [00:06,  1.50it/s]Extractor Estimating: 10it [00:06,  1.49it/s]Extractor Estimating: 11it [00:07,  1.42it/s]Extractor Estimating: 12it [00:08,  1.47it/s]Extractor Estimating: 13it [00:08,  1.49it/s]Extractor Estimating: 14it [00:09,  1.44it/s]Extractor Estimating: 15it [00:10,  1.47it/s]Extractor Estimating: 16it [00:10,  1.49it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.54it/s]Extractor Estimating: 19it [00:12,  1.51it/s]Extractor Estimating: 20it [00:13,  1.55it/s]Extractor Estimating: 21it [00:14,  1.55it/s]Extractor Estimating: 22it [00:14,  1.62it/s]Extractor Estimating: 23it [00:15,  1.56it/s]Extractor Estimating: 24it [00:16,  1.47it/s]Extractor Estimating: 25it [00:16,  1.49it/s]Extractor Estimating: 26it [00:17,  1.54it/s]Extractor Estimating: 27it [00:18,  1.54it/s]Extractor Estimating: 28it [00:18,  1.60it/s]Extractor Estimating: 29it [00:19,  1.58it/s]Extractor Estimating: 30it [00:19,  1.60it/s]Extractor Estimating: 31it [00:20,  1.55it/s]Extractor Estimating: 32it [00:21,  1.58it/s]Extractor Estimating: 33it [00:21,  1.51it/s]Extractor Estimating: 34it [00:22,  1.49it/s]Extractor Estimating: 35it [00:23,  1.51it/s]Extractor Estimating: 36it [00:23,  1.50it/s]Extractor Estimating: 37it [00:24,  1.52it/s]Extractor Estimating: 38it [00:25,  1.54it/s]Extractor Estimating: 39it [00:25,  1.51it/s]Extractor Estimating: 40it [00:26,  1.50it/s]Extractor Estimating: 41it [00:27,  1.53it/s]Extractor Estimating: 42it [00:27,  1.54it/s]Extractor Estimating: 43it [00:28,  1.44it/s]Extractor Estimating: 44it [00:29,  1.46it/s]Extractor Estimating: 45it [00:29,  1.47it/s]Extractor Estimating: 46it [00:30,  1.51it/s]Extractor Estimating: 47it [00:31,  1.52it/s]Extractor Estimating: 48it [00:31,  1.54it/s]Extractor Estimating: 49it [00:32,  1.52it/s]Extractor Estimating: 50it [00:33,  1.48it/s]Extractor Estimating: 51it [00:33,  1.48it/s]Extractor Estimating: 52it [00:34,  1.57it/s]Extractor Estimating: 53it [00:35,  1.59it/s]Extractor Estimating: 54it [00:35,  1.57it/s]Extractor Estimating: 55it [00:36,  1.55it/s]Extractor Estimating: 56it [00:37,  1.58it/s]Extractor Estimating: 57it [00:37,  1.55it/s]Extractor Estimating: 58it [00:38,  1.57it/s]Extractor Estimating: 59it [00:38,  1.57it/s]Extractor Estimating: 60it [00:39,  1.58it/s]Extractor Estimating: 61it [00:40,  1.58it/s]Extractor Estimating: 62it [00:40,  1.61it/s]Extractor Estimating: 63it [00:41,  1.63it/s]Extractor Estimating: 64it [00:41,  1.64it/s]Extractor Estimating: 65it [00:42,  1.65it/s]Extractor Estimating: 66it [00:43,  1.61it/s]Extractor Estimating: 67it [00:43,  1.62it/s]Extractor Estimating: 68it [00:44,  1.63it/s]Extractor Estimating: 69it [00:45,  1.64it/s]Extractor Estimating: 70it [00:45,  1.63it/s]Extractor Estimating: 71it [00:46,  1.56it/s]Extractor Estimating: 72it [00:46,  1.58it/s]Extractor Estimating: 73it [00:47,  1.62it/s]Extractor Estimating: 74it [00:48,  1.60it/s]Extractor Estimating: 75it [00:48,  1.58it/s]Extractor Estimating: 76it [00:49,  1.55it/s]Extractor Estimating: 77it [00:50,  1.57it/s]Extractor Estimating: 78it [00:50,  1.44it/s]Extractor Estimating: 79it [00:51,  1.42it/s]Extractor Estimating: 80it [00:52,  1.38it/s]Extractor Estimating: 81it [00:53,  1.44it/s]Extractor Estimating: 82it [00:53,  1.42it/s]Extractor Estimating: 83it [00:54,  1.50it/s]Extractor Estimating: 84it [00:55,  1.50it/s]Extractor Estimating: 85it [00:55,  1.53it/s]Extractor Estimating: 86it [00:56,  1.57it/s]Extractor Estimating: 87it [00:56,  1.58it/s]Extractor Estimating: 88it [00:57,  1.61it/s]Extractor Estimating: 89it [00:58,  1.58it/s]Extractor Estimating: 90it [00:58,  1.55it/s]Extractor Estimating: 91it [00:59,  1.57it/s]Extractor Estimating: 92it [01:00,  1.54it/s]Extractor Estimating: 93it [01:00,  1.54it/s]Extractor Estimating: 94it [01:01,  1.53it/s]Extractor Estimating: 95it [01:02,  1.52it/s]Extractor Estimating: 96it [01:02,  1.54it/s]Extractor Estimating: 97it [01:03,  1.54it/s]Extractor Estimating: 98it [01:04,  1.52it/s]Extractor Estimating: 99it [01:04,  1.45it/s]Extractor Estimating: 100it [01:05,  1.49it/s]Extractor Estimating: 101it [01:06,  1.46it/s]Extractor Estimating: 102it [01:06,  1.49it/s]Extractor Estimating: 103it [01:07,  1.53it/s]Extractor Estimating: 104it [01:08,  1.55it/s]Extractor Estimating: 105it [01:08,  1.55it/s]Extractor Estimating: 106it [01:09,  1.56it/s]Extractor Estimating: 107it [01:09,  1.57it/s]Extractor Estimating: 108it [01:10,  1.61it/s]Extractor Estimating: 109it [01:11,  1.62it/s]Extractor Estimating: 110it [01:11,  1.56it/s]Extractor Estimating: 111it [01:12,  1.60it/s]Extractor Estimating: 112it [01:13,  1.58it/s]Extractor Estimating: 113it [01:13,  1.61it/s]Extractor Estimating: 114it [01:14,  1.62it/s]Extractor Estimating: 115it [01:14,  1.62it/s]Extractor Estimating: 116it [01:15,  1.56it/s]Extractor Estimating: 117it [01:16,  1.60it/s]Extractor Estimating: 118it [01:16,  1.55it/s]Extractor Estimating: 119it [01:17,  1.58it/s]Extractor Estimating: 120it [01:18,  1.52it/s]Extractor Estimating: 121it [01:18,  1.50it/s]Extractor Estimating: 122it [01:19,  1.51it/s]Extractor Estimating: 123it [01:20,  1.56it/s]Extractor Estimating: 124it [01:20,  1.55it/s]Extractor Estimating: 125it [01:21,  1.45it/s]Extractor Estimating: 126it [01:22,  1.47it/s]Extractor Estimating: 127it [01:22,  1.48it/s]Extractor Estimating: 128it [01:23,  1.56it/s]Extractor Estimating: 129it [01:24,  1.62it/s]Extractor Estimating: 130it [01:24,  1.57it/s]Extractor Estimating: 131it [01:25,  1.64it/s]Extractor Estimating: 132it [01:25,  1.60it/s]Extractor Estimating: 133it [01:26,  1.60it/s]Extractor Estimating: 134it [01:27,  1.58it/s]Extractor Estimating: 135it [01:27,  1.54it/s]Extractor Estimating: 136it [01:28,  1.49it/s]Extractor Estimating: 137it [01:29,  1.47it/s]Extractor Estimating: 138it [01:29,  1.52it/s]Extractor Estimating: 139it [01:30,  1.46it/s]Extractor Estimating: 140it [01:31,  1.51it/s]Extractor Estimating: 141it [01:31,  1.59it/s]Extractor Estimating: 142it [01:32,  1.56it/s]Extractor Estimating: 143it [01:33,  1.60it/s]Extractor Estimating: 144it [01:33,  1.64it/s]Extractor Estimating: 145it [01:34,  1.56it/s]Extractor Estimating: 146it [01:35,  1.54it/s]Extractor Estimating: 147it [01:35,  1.58it/s]Extractor Estimating: 148it [01:36,  1.48it/s]Extractor Estimating: 149it [01:37,  1.50it/s]Extractor Estimating: 150it [01:37,  1.50it/s]Extractor Estimating: 151it [01:38,  1.52it/s]Extractor Estimating: 152it [01:39,  1.50it/s]Extractor Estimating: 153it [01:39,  1.50it/s]Extractor Estimating: 154it [01:40,  1.52it/s]Extractor Estimating: 155it [01:41,  1.54it/s]Extractor Estimating: 156it [01:41,  1.56it/s]Extractor Estimating: 157it [01:42,  1.50it/s]Extractor Estimating: 158it [01:43,  1.51it/s]Extractor Estimating: 159it [01:43,  1.50it/s]Extractor Estimating: 160it [01:44,  1.50it/s]Extractor Estimating: 161it [01:45,  1.51it/s]Extractor Estimating: 162it [01:45,  1.54it/s]Extractor Estimating: 163it [01:46,  1.51it/s]Extractor Estimating: 164it [01:46,  1.52it/s]Extractor Estimating: 165it [01:47,  1.55it/s]Extractor Estimating: 166it [01:48,  1.54it/s]Extractor Estimating: 167it [01:48,  1.54it/s]Extractor Estimating: 168it [01:49,  1.40it/s]Extractor Estimating: 169it [01:50,  1.40it/s]Extractor Estimating: 170it [01:51,  1.45it/s]Extractor Estimating: 171it [01:51,  1.47it/s]Extractor Estimating: 172it [01:52,  1.50it/s]Extractor Estimating: 173it [01:53,  1.51it/s]Extractor Estimating: 174it [01:53,  1.52it/s]Extractor Estimating: 175it [01:54,  1.49it/s]Extractor Estimating: 176it [01:54,  1.54it/s]Extractor Estimating: 177it [01:55,  1.49it/s]Extractor Estimating: 178it [01:56,  1.53it/s]Extractor Estimating: 179it [01:56,  1.54it/s]Extractor Estimating: 180it [01:57,  1.55it/s]Extractor Estimating: 181it [01:58,  1.56it/s]Extractor Estimating: 182it [01:58,  1.59it/s]Extractor Estimating: 183it [01:59,  1.60it/s]Extractor Estimating: 184it [02:00,  1.59it/s]Extractor Estimating: 185it [02:00,  1.59it/s]Extractor Estimating: 186it [02:01,  1.50it/s]Extractor Estimating: 187it [02:02,  1.56it/s]Extractor Estimating: 188it [02:02,  1.60it/s]Extractor Estimating: 189it [02:03,  1.60it/s]Extractor Estimating: 190it [02:03,  1.57it/s]Extractor Estimating: 191it [02:04,  1.59it/s]Extractor Estimating: 192it [02:05,  1.55it/s]Extractor Estimating: 193it [02:05,  1.63it/s]Extractor Estimating: 194it [02:06,  1.60it/s]Extractor Estimating: 195it [02:07,  1.59it/s]Extractor Estimating: 196it [02:07,  1.58it/s]Extractor Estimating: 197it [02:08,  1.63it/s]Extractor Estimating: 198it [02:08,  1.61it/s]Extractor Estimating: 199it [02:09,  1.61it/s]Extractor Estimating: 200it [02:10,  1.62it/s]Extractor Estimating: 201it [02:10,  1.60it/s]Extractor Estimating: 202it [02:11,  1.65it/s]Extractor Estimating: 203it [02:11,  1.65it/s]Extractor Estimating: 204it [02:12,  1.67it/s]Extractor Estimating: 205it [02:13,  1.69it/s]Extractor Estimating: 206it [02:13,  1.68it/s]Extractor Estimating: 207it [02:14,  1.67it/s]Extractor Estimating: 208it [02:14,  1.71it/s]Extractor Estimating: 209it [02:15,  1.68it/s]Extractor Estimating: 210it [02:16,  1.63it/s]Extractor Estimating: 211it [02:16,  1.58it/s]Extractor Estimating: 212it [02:17,  1.63it/s]Extractor Estimating: 213it [02:18,  1.60it/s]Extractor Estimating: 214it [02:18,  1.66it/s]Extractor Estimating: 215it [02:19,  1.70it/s]Extractor Estimating: 216it [02:19,  1.68it/s]Extractor Estimating: 217it [02:20,  1.73it/s]Extractor Estimating: 218it [02:20,  1.69it/s]Extractor Estimating: 219it [02:21,  1.70it/s]Extractor Estimating: 220it [02:22,  1.65it/s]Extractor Estimating: 221it [02:22,  1.68it/s]Extractor Estimating: 222it [02:23,  1.70it/s]Extractor Estimating: 223it [02:23,  1.68it/s]Extractor Estimating: 224it [02:24,  1.68it/s]Extractor Estimating: 225it [02:25,  1.72it/s]Extractor Estimating: 226it [02:25,  1.75it/s]Extractor Estimating: 227it [02:26,  1.77it/s]Extractor Estimating: 228it [02:26,  1.61it/s]Extractor Estimating: 229it [02:27,  1.69it/s]Extractor Estimating: 230it [02:27,  1.74it/s]Extractor Estimating: 231it [02:28,  1.74it/s]Extractor Estimating: 232it [02:29,  1.69it/s]Extractor Estimating: 233it [02:29,  1.70it/s]Extractor Estimating: 234it [02:30,  1.76it/s]Extractor Estimating: 235it [02:30,  1.72it/s]Extractor Estimating: 236it [02:31,  1.71it/s]Extractor Estimating: 237it [02:32,  1.72it/s]Extractor Estimating: 238it [02:32,  1.71it/s]Extractor Estimating: 239it [02:33,  1.67it/s]Extractor Estimating: 240it [02:33,  1.69it/s]Extractor Estimating: 241it [02:34,  1.74it/s]Extractor Estimating: 242it [02:34,  1.73it/s]Extractor Estimating: 243it [02:35,  1.72it/s]Extractor Estimating: 244it [02:36,  1.68it/s]Extractor Estimating: 245it [02:36,  1.69it/s]Extractor Estimating: 246it [02:37,  1.70it/s]Extractor Estimating: 247it [02:37,  1.65it/s]Extractor Estimating: 248it [02:38,  1.65it/s]Extractor Estimating: 249it [02:39,  1.68it/s]Extractor Estimating: 250it [02:39,  1.69it/s]Extractor Estimating: 251it [02:40,  1.66it/s]Extractor Estimating: 252it [02:40,  1.64it/s]Extractor Estimating: 253it [02:41,  1.60it/s]Extractor Estimating: 254it [02:42,  1.60it/s]Extractor Estimating: 255it [02:42,  1.57it/s]Extractor Estimating: 256it [02:43,  1.57it/s]Extractor Estimating: 257it [02:44,  1.52it/s]Extractor Estimating: 258it [02:44,  1.54it/s]Extractor Estimating: 259it [02:45,  1.58it/s]Extractor Estimating: 260it [02:46,  1.57it/s]Extractor Estimating: 261it [02:46,  1.56it/s]Extractor Estimating: 262it [02:47,  1.57it/s]Extractor Estimating: 263it [02:48,  1.57it/s]Extractor Estimating: 264it [02:48,  1.59it/s]Extractor Estimating: 265it [02:49,  1.61it/s]Extractor Estimating: 266it [02:49,  1.61it/s]Extractor Estimating: 267it [02:50,  1.60it/s]Extractor Estimating: 268it [02:51,  1.57it/s]Extractor Estimating: 269it [02:51,  1.60it/s]Extractor Estimating: 270it [02:52,  1.64it/s]Extractor Estimating: 271it [02:52,  1.65it/s]Extractor Estimating: 272it [02:53,  1.65it/s]Extractor Estimating: 273it [02:54,  1.59it/s]Extractor Estimating: 274it [02:54,  1.59it/s]Extractor Estimating: 275it [02:55,  1.60it/s]Extractor Estimating: 276it [02:56,  1.56it/s]Extractor Estimating: 277it [02:56,  1.50it/s]Extractor Estimating: 278it [02:57,  1.48it/s]Extractor Estimating: 279it [02:58,  1.48it/s]Extractor Estimating: 280it [02:58,  1.54it/s]Extractor Estimating: 281it [02:59,  1.50it/s]Extractor Estimating: 282it [03:00,  1.51it/s]Extractor Estimating: 283it [03:00,  1.51it/s]Extractor Estimating: 284it [03:01,  1.46it/s]Extractor Estimating: 285it [03:02,  1.52it/s]Extractor Estimating: 286it [03:02,  1.55it/s]Extractor Estimating: 287it [03:03,  1.49it/s]Extractor Estimating: 288it [03:04,  1.52it/s]Extractor Estimating: 289it [03:04,  1.49it/s]Extractor Estimating: 290it [03:05,  1.50it/s]Extractor Estimating: 291it [03:06,  1.52it/s]Extractor Estimating: 292it [03:06,  1.51it/s]Extractor Estimating: 293it [03:07,  1.57it/s]Extractor Estimating: 294it [03:08,  1.56it/s]Extractor Estimating: 295it [03:08,  1.52it/s]Extractor Estimating: 296it [03:09,  1.54it/s]Extractor Estimating: 297it [03:10,  1.54it/s]Extractor Estimating: 298it [03:10,  1.52it/s]Extractor Estimating: 299it [03:11,  1.50it/s]Extractor Estimating: 300it [03:12,  1.52it/s]Extractor Estimating: 301it [03:12,  1.51it/s]Extractor Estimating: 302it [03:13,  1.55it/s]Extractor Estimating: 303it [03:14,  1.49it/s]Extractor Estimating: 304it [03:14,  1.51it/s]Extractor Estimating: 305it [03:15,  1.58it/s]Extractor Estimating: 306it [03:15,  1.61it/s]Extractor Estimating: 307it [03:16,  1.44it/s]Extractor Estimating: 308it [03:17,  1.47it/s]Extractor Estimating: 309it [03:18,  1.50it/s]Extractor Estimating: 310it [03:18,  1.53it/s]Extractor Estimating: 311it [03:19,  1.47it/s]Extractor Estimating: 312it [03:20,  1.48it/s]Extractor Estimating: 313it [03:20,  1.50it/s]Extractor Estimating: 314it [03:21,  1.54it/s]Extractor Estimating: 315it [03:21,  1.57it/s]Extractor Estimating: 316it [03:22,  1.48it/s]Extractor Estimating: 317it [03:23,  1.49it/s]Extractor Estimating: 318it [03:23,  1.50it/s]Extractor Estimating: 319it [03:24,  1.48it/s]Extractor Estimating: 320it [03:25,  1.51it/s]Extractor Estimating: 321it [03:26,  1.49it/s]Extractor Estimating: 322it [03:26,  1.42it/s]Extractor Estimating: 323it [03:27,  1.48it/s]Extractor Estimating: 324it [03:28,  1.50it/s]Extractor Estimating: 325it [03:28,  1.55it/s]Extractor Estimating: 326it [03:29,  1.58it/s]Extractor Estimating: 327it [03:29,  1.61it/s]Extractor Estimating: 328it [03:30,  1.67it/s]Extractor Estimating: 329it [03:30,  1.76it/s]Extractor Estimating: 330it [03:31,  1.73it/s]Extractor Estimating: 331it [03:32,  1.71it/s]Extractor Estimating: 332it [03:32,  1.75it/s]Extractor Estimating: 333it [03:33,  1.70it/s]Extractor Estimating: 334it [03:33,  1.72it/s]Extractor Estimating: 335it [03:34,  1.69it/s]Extractor Estimating: 336it [03:35,  1.71it/s]Extractor Estimating: 337it [03:35,  1.66it/s]Extractor Estimating: 338it [03:36,  1.63it/s]Extractor Estimating: 339it [03:36,  1.59it/s]Extractor Estimating: 340it [03:37,  1.55it/s]Extractor Estimating: 341it [03:38,  1.57it/s]Extractor Estimating: 342it [03:38,  1.61it/s]Extractor Estimating: 343it [03:39,  1.60it/s]Extractor Estimating: 344it [03:40,  1.62it/s]Extractor Estimating: 345it [03:40,  1.71it/s]Extractor Estimating: 346it [03:41,  1.69it/s]Extractor Estimating: 347it [03:41,  1.71it/s]Extractor Estimating: 348it [03:42,  1.70it/s]Extractor Estimating: 349it [03:43,  1.66it/s]Extractor Estimating: 350it [03:43,  1.68it/s]Extractor Estimating: 351it [03:44,  1.68it/s]Extractor Estimating: 352it [03:44,  1.67it/s]Extractor Estimating: 353it [03:45,  1.68it/s]Extractor Estimating: 354it [03:45,  1.66it/s]Extractor Estimating: 355it [03:46,  1.61it/s]Extractor Estimating: 356it [03:47,  1.58it/s]Extractor Estimating: 357it [03:48,  1.52it/s]Extractor Estimating: 358it [03:48,  1.53it/s]Extractor Estimating: 359it [03:49,  1.59it/s]Extractor Estimating: 360it [03:49,  1.56it/s]Extractor Estimating: 361it [03:50,  1.61it/s]Extractor Estimating: 362it [03:51,  1.61it/s]Extractor Estimating: 363it [03:51,  1.62it/s]Extractor Estimating: 364it [03:52,  1.66it/s]Extractor Estimating: 365it [03:52,  1.68it/s]Extractor Estimating: 366it [03:53,  1.66it/s]Extractor Estimating: 367it [03:54,  1.61it/s]Extractor Estimating: 368it [03:54,  1.57it/s]Extractor Estimating: 369it [03:55,  1.60it/s]Extractor Estimating: 370it [03:55,  1.63it/s]Extractor Estimating: 371it [03:56,  1.63it/s]Extractor Estimating: 372it [03:57,  1.59it/s]Extractor Estimating: 373it [03:58,  1.52it/s]Extractor Estimating: 374it [03:58,  1.51it/s]Extractor Estimating: 375it [03:59,  1.69it/s]Extractor Estimating: 375it [03:59,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:08,834 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:08,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:08,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:08,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:08,839 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:09:09,541 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:09:09,542 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:09:10,139 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:09:11,191 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:09:11,191 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:14,085 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:14,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:14,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:14,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:09:14,111 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:09:14,808 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:09:14,809 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:09:15,378 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:09:15,544 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:09:15,544 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 12:32:05,726 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 12:32:05,731 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7688 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 21517
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21617, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21617, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.106, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.107, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.111, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 79, avg_time 1.124, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 179, avg_time 1.129, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 279, avg_time 2.269, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 58, avg_time 1.104, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 158, avg_time 1.101, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 258, avg_time 1.098, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 37, avg_time 1.126, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 137, avg_time 2.302, loss:nan
g_step 1200, step 237, avg_time 1.109, loss:nan
g_step 1300, step 16, avg_time 1.107, loss:nan
g_step 1400, step 116, avg_time 1.106, loss:nan
g_step 1500, step 216, avg_time 1.111, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 316, avg_time 2.280, loss:nan
g_step 1700, step 95, avg_time 1.108, loss:nan
g_step 1800, step 195, avg_time 1.097, loss:nan
g_step 1900, step 295, avg_time 1.133, loss:nan
g_step 2000, step 74, avg_time 1.106, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 174, avg_time 2.285, loss:nan
g_step 2200, step 274, avg_time 1.126, loss:nan
g_step 2300, step 53, avg_time 1.118, loss:nan
g_step 2400, step 153, avg_time 1.118, loss:nan
g_step 2500, step 253, avg_time 1.111, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.279, loss:nan
g_step 2700, step 132, avg_time 1.121, loss:nan
g_step 2800, step 232, avg_time 1.112, loss:nan
g_step 2900, step 11, avg_time 1.097, loss:nan
g_step 3000, step 111, avg_time 1.126, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 211, avg_time 2.279, loss:nan
g_step 3200, step 311, avg_time 1.127, loss:nan
g_step 3300, step 90, avg_time 1.113, loss:nan
g_step 3400, step 190, avg_time 1.126, loss:nan
g_step 3500, step 290, avg_time 1.107, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 69, avg_time 2.301, loss:nan
g_step 3700, step 169, avg_time 1.094, loss:nan
g_step 3800, step 269, avg_time 1.130, loss:nan
g_step 3900, step 48, avg_time 1.104, loss:nan
g_step 4000, step 148, avg_time 1.101, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 248, avg_time 2.301, loss:nan
g_step 4200, step 27, avg_time 1.131, loss:nan
g_step 4300, step 127, avg_time 1.109, loss:nan
g_step 4400, step 227, avg_time 1.139, loss:nan
g_step 4500, step 6, avg_time 1.096, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 106, avg_time 2.299, loss:nan
g_step 4700, step 206, avg_time 1.123, loss:nan
g_step 4800, step 306, avg_time 1.105, loss:nan
g_step 4900, step 85, avg_time 1.116, loss:nan
g_step 5000, step 185, avg_time 1.101, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 285, avg_time 2.327, loss:nan
g_step 5200, step 64, avg_time 1.116, loss:nan
g_step 5300, step 164, avg_time 1.125, loss:nan
g_step 5400, step 264, avg_time 1.111, loss:nan
g_step 5500, step 43, avg_time 1.117, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 143, avg_time 2.290, loss:nan
g_step 5700, step 243, avg_time 1.120, loss:nan
g_step 5800, step 22, avg_time 1.106, loss:nan
g_step 5900, step 122, avg_time 1.141, loss:nan
g_step 6000, step 222, avg_time 1.107, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 1, avg_time 2.308, loss:nan
g_step 6200, step 101, avg_time 1.115, loss:nan
g_step 6300, step 201, avg_time 1.108, loss:nan
g_step 6400, step 301, avg_time 1.128, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 12:32:05 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 12:32:05 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_12-32-05_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 12:32:06 - WARNING - datasets.builder -   Using custom data configuration default-0f6123c1a8813a15
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-0f6123c1a8813a15/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 12:32:06,935 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:32:06,937 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:32:06,937 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:32:06,938 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:32:06,947 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:32:06,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:32:06,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:32:06,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:32:06,952 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:32:06,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:32:06,952 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 12:32:07,107 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:32:10,191 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 12:32:10,197 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-0f6123c1a8813a15/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.09ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.87ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.22ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.34ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.42ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.47ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.50ba/s]100%|██████████| 8/8 [00:01<00:00,  4.93ba/s]100%|██████████| 8/8 [00:01<00:00,  4.48ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.24ba/s] 40%|████      | 2/5 [00:00<00:00,  3.26ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.70ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.96ba/s]100%|██████████| 5/5 [00:01<00:00,  4.45ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.19ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.61ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.80ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.79ba/s]100%|██████████| 8/8 [00:00<00:00, 11.12ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.84ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.57ba/s]100%|██████████| 5/5 [00:00<00:00, 12.94ba/s]100%|██████████| 5/5 [00:00<00:00, 12.26ba/s]
[INFO|trainer.py:414] 2023-08-28 12:32:14,631 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 12:32:14,645 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 12:32:14,645 >>   Num examples = 7700
[INFO|trainer.py:1149] 2023-08-28 12:32:14,645 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 12:32:14,645 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 12:32:14,645 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 12:32:14,646 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 12:32:14,646 >>   Total optimization steps = 600
  0%|          | 0/600 [00:00<?, ?it/s]  0%|          | 1/600 [00:00<02:49,  3.54it/s]  0%|          | 2/600 [00:00<02:46,  3.58it/s]  0%|          | 3/600 [00:00<02:45,  3.60it/s]  1%|          | 4/600 [00:01<02:45,  3.61it/s]  1%|          | 5/600 [00:01<02:44,  3.61it/s]  1%|          | 6/600 [00:01<02:44,  3.60it/s]  1%|          | 7/600 [00:01<02:46,  3.57it/s]  1%|▏         | 8/600 [00:02<02:46,  3.56it/s]  2%|▏         | 9/600 [00:02<02:45,  3.56it/s]  2%|▏         | 10/600 [00:02<02:45,  3.56it/s]  2%|▏         | 11/600 [00:03<02:45,  3.56it/s]  2%|▏         | 12/600 [00:03<02:44,  3.57it/s]  2%|▏         | 13/600 [00:03<02:44,  3.57it/s]  2%|▏         | 14/600 [00:03<02:44,  3.57it/s]  2%|▎         | 15/600 [00:04<02:44,  3.57it/s]  3%|▎         | 16/600 [00:04<02:43,  3.57it/s]  3%|▎         | 17/600 [00:04<02:43,  3.57it/s]  3%|▎         | 18/600 [00:05<02:43,  3.57it/s]  3%|▎         | 19/600 [00:05<02:42,  3.57it/s]  3%|▎         | 20/600 [00:05<02:42,  3.56it/s]  4%|▎         | 21/600 [00:05<02:42,  3.56it/s]  4%|▎         | 22/600 [00:06<02:42,  3.56it/s]  4%|▍         | 23/600 [00:06<02:42,  3.56it/s]  4%|▍         | 24/600 [00:06<02:41,  3.56it/s]  4%|▍         | 25/600 [00:07<02:42,  3.54it/s]  4%|▍         | 26/600 [00:07<02:41,  3.54it/s]  4%|▍         | 27/600 [00:07<02:41,  3.55it/s]  5%|▍         | 28/600 [00:07<02:40,  3.55it/s]  5%|▍         | 29/600 [00:08<02:40,  3.56it/s]  5%|▌         | 30/600 [00:08<02:40,  3.56it/s]  5%|▌         | 31/600 [00:08<02:39,  3.56it/s]  5%|▌         | 32/600 [00:08<02:39,  3.56it/s]  6%|▌         | 33/600 [00:09<02:39,  3.56it/s]  6%|▌         | 34/600 [00:09<02:39,  3.56it/s]  6%|▌         | 35/600 [00:09<02:38,  3.56it/s]  6%|▌         | 36/600 [00:10<02:38,  3.56it/s]  6%|▌         | 37/600 [00:10<02:38,  3.56it/s]  6%|▋         | 38/600 [00:10<02:38,  3.56it/s]  6%|▋         | 39/600 [00:10<02:37,  3.55it/s]  7%|▋         | 40/600 [00:11<02:37,  3.55it/s]  7%|▋         | 41/600 [00:11<02:39,  3.50it/s]  7%|▋         | 42/600 [00:11<02:38,  3.51it/s]  7%|▋         | 43/600 [00:12<02:38,  3.52it/s]  7%|▋         | 44/600 [00:12<02:37,  3.53it/s]  8%|▊         | 45/600 [00:12<02:36,  3.54it/s]  8%|▊         | 46/600 [00:12<02:36,  3.54it/s]  8%|▊         | 47/600 [00:13<02:35,  3.55it/s]  8%|▊         | 48/600 [00:13<02:35,  3.55it/s]  8%|▊         | 49/600 [00:13<02:34,  3.56it/s]  8%|▊         | 50/600 [00:14<02:35,  3.53it/s]  8%|▊         | 51/600 [00:14<02:35,  3.54it/s]  9%|▊         | 52/600 [00:14<02:34,  3.54it/s]  9%|▉         | 53/600 [00:14<02:34,  3.54it/s]  9%|▉         | 54/600 [00:15<02:33,  3.55it/s]  9%|▉         | 55/600 [00:15<02:33,  3.55it/s]  9%|▉         | 56/600 [00:15<02:33,  3.55it/s] 10%|▉         | 57/600 [00:16<02:32,  3.55it/s] 10%|▉         | 58/600 [00:16<02:32,  3.55it/s] 10%|▉         | 59/600 [00:16<02:32,  3.55it/s] 10%|█         | 60/600 [00:16<02:31,  3.56it/s] 10%|█         | 61/600 [00:17<02:31,  3.56it/s] 10%|█         | 62/600 [00:17<02:31,  3.55it/s] 10%|█         | 63/600 [00:17<02:31,  3.55it/s] 11%|█         | 64/600 [00:18<02:30,  3.55it/s] 11%|█         | 65/600 [00:18<02:30,  3.55it/s] 11%|█         | 66/600 [00:18<02:30,  3.55it/s] 11%|█         | 67/600 [00:18<02:30,  3.55it/s] 11%|█▏        | 68/600 [00:19<02:29,  3.55it/s] 12%|█▏        | 69/600 [00:19<02:29,  3.55it/s] 12%|█▏        | 70/600 [00:19<02:28,  3.56it/s] 12%|█▏        | 71/600 [00:19<02:28,  3.56it/s] 12%|█▏        | 72/600 [00:20<02:28,  3.55it/s] 12%|█▏        | 73/600 [00:20<02:28,  3.55it/s] 12%|█▏        | 74/600 [00:20<02:28,  3.55it/s] 12%|█▎        | 75/600 [00:21<02:29,  3.52it/s] 13%|█▎        | 76/600 [00:21<02:28,  3.53it/s] 13%|█▎        | 77/600 [00:21<02:27,  3.53it/s] 13%|█▎        | 78/600 [00:21<02:27,  3.54it/s] 13%|█▎        | 79/600 [00:22<02:26,  3.55it/s] 13%|█▎        | 80/600 [00:22<02:26,  3.55it/s] 14%|█▎        | 81/600 [00:22<02:26,  3.55it/s] 14%|█▎        | 82/600 [00:23<02:26,  3.55it/s] 14%|█▍        | 83/600 [00:23<02:25,  3.55it/s] 14%|█▍        | 84/600 [00:23<02:25,  3.55it/s] 14%|█▍        | 85/600 [00:23<02:25,  3.55it/s] 14%|█▍        | 86/600 [00:24<02:24,  3.55it/s] 14%|█▍        | 87/600 [00:24<02:24,  3.55it/s] 15%|█▍        | 88/600 [00:24<02:24,  3.55it/s] 15%|█▍        | 89/600 [00:25<02:23,  3.55it/s] 15%|█▌        | 90/600 [00:25<02:23,  3.55it/s] 15%|█▌        | 91/600 [00:25<02:23,  3.55it/s] 15%|█▌        | 92/600 [00:25<02:22,  3.55it/s] 16%|█▌        | 93/600 [00:26<02:22,  3.55it/s] 16%|█▌        | 94/600 [00:26<02:22,  3.55it/s] 16%|█▌        | 95/600 [00:26<02:22,  3.55it/s] 16%|█▌        | 96/600 [00:27<02:21,  3.57it/s] 16%|█▌        | 97/600 [00:27<02:20,  3.58it/s] 16%|█▋        | 98/600 [00:27<02:19,  3.59it/s] 16%|█▋        | 99/600 [00:27<02:19,  3.59it/s] 17%|█▋        | 100/600 [00:28<02:18,  3.60it/s] 17%|█▋        | 101/600 [00:28<02:18,  3.60it/s] 17%|█▋        | 102/600 [00:28<02:18,  3.60it/s] 17%|█▋        | 103/600 [00:28<02:17,  3.60it/s] 17%|█▋        | 104/600 [00:29<02:17,  3.60it/s] 18%|█▊        | 105/600 [00:29<02:17,  3.60it/s] 18%|█▊        | 106/600 [00:29<02:17,  3.60it/s] 18%|█▊        | 107/600 [00:30<02:16,  3.61it/s] 18%|█▊        | 108/600 [00:30<02:16,  3.61it/s] 18%|█▊        | 109/600 [00:30<02:16,  3.61it/s] 18%|█▊        | 110/600 [00:30<02:15,  3.61it/s] 18%|█▊        | 111/600 [00:31<02:15,  3.60it/s] 19%|█▊        | 112/600 [00:31<02:15,  3.60it/s] 19%|█▉        | 113/600 [00:31<02:15,  3.60it/s] 19%|█▉        | 114/600 [00:32<02:14,  3.60it/s] 19%|█▉        | 115/600 [00:32<02:14,  3.61it/s] 19%|█▉        | 116/600 [00:32<02:14,  3.60it/s] 20%|█▉        | 117/600 [00:32<02:13,  3.60it/s] 20%|█▉        | 118/600 [00:33<02:13,  3.60it/s] 20%|█▉        | 119/600 [00:33<02:13,  3.60it/s] 20%|██        | 120/600 [00:33<02:13,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 12:32:48,364 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:32:48,365 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 12:32:48,365 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.08it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.37it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.31it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.27it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.59it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.34it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.19it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.05it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.21it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.25it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.27it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.41it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.20it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.98it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.86it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.90it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.94it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.99it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.21it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.18it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.28it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.10it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.95it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.86it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.99it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.97it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.10it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.05it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.17it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.15it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.05it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.95it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.00it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.02it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.97it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.14it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.27it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.17it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.00it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.10it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.00it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.98it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.99it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.09it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.19it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.09it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.20it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.09it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.12it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.03it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.00it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.99it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.05it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.09it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.07it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.11it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.06it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.97it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.96it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.93it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.07it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.09it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.18it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.09it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.92it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.05it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.08it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.00it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.11it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.06it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.04it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.11it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.96it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.01it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.02it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.01it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.92it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.08it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.13it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.06it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.09it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.08it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.04it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.12it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.00it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.03it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.18it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.07it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.10it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.15it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.07it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.88it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.08it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.08it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.93it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.07it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 42.33it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 42.90it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.25it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.47it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.60it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.61it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.78it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.90it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 43.90it/s][A 20%|██        | 120/600 [00:46<02:13,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:33:00,772 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 12:33:00,789 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:33:02,615 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:33:02,631 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:33:02,641 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120/special_tokens_map.json
 20%|██        | 121/600 [00:48<37:14,  4.66s/it] 20%|██        | 122/600 [00:48<26:41,  3.35s/it] 20%|██        | 123/600 [00:49<19:18,  2.43s/it] 21%|██        | 124/600 [00:49<14:09,  1.78s/it] 21%|██        | 125/600 [00:49<10:33,  1.33s/it] 21%|██        | 126/600 [00:49<08:02,  1.02s/it] 21%|██        | 127/600 [00:50<06:17,  1.25it/s] 21%|██▏       | 128/600 [00:50<05:03,  1.55it/s] 22%|██▏       | 129/600 [00:50<04:11,  1.87it/s] 22%|██▏       | 130/600 [00:51<03:35,  2.18it/s] 22%|██▏       | 131/600 [00:51<03:09,  2.47it/s] 22%|██▏       | 132/600 [00:51<02:51,  2.73it/s] 22%|██▏       | 133/600 [00:51<02:39,  2.93it/s] 22%|██▏       | 134/600 [00:52<02:30,  3.10it/s] 22%|██▎       | 135/600 [00:52<02:23,  3.24it/s] 23%|██▎       | 136/600 [00:52<02:18,  3.34it/s] 23%|██▎       | 137/600 [00:53<02:15,  3.42it/s] 23%|██▎       | 138/600 [00:53<02:13,  3.47it/s] 23%|██▎       | 139/600 [00:53<02:11,  3.51it/s] 23%|██▎       | 140/600 [00:53<02:10,  3.52it/s] 24%|██▎       | 141/600 [00:54<02:10,  3.53it/s] 24%|██▎       | 142/600 [00:54<02:09,  3.54it/s] 24%|██▍       | 143/600 [00:54<02:08,  3.55it/s] 24%|██▍       | 144/600 [00:55<02:08,  3.55it/s] 24%|██▍       | 145/600 [00:55<02:08,  3.54it/s] 24%|██▍       | 146/600 [00:55<02:08,  3.54it/s] 24%|██▍       | 147/600 [00:55<02:07,  3.54it/s] 25%|██▍       | 148/600 [00:56<02:07,  3.54it/s] 25%|██▍       | 149/600 [00:56<02:07,  3.54it/s] 25%|██▌       | 150/600 [00:56<02:06,  3.55it/s] 25%|██▌       | 151/600 [00:57<02:06,  3.55it/s] 25%|██▌       | 152/600 [00:57<02:06,  3.55it/s] 26%|██▌       | 153/600 [00:57<02:05,  3.55it/s] 26%|██▌       | 154/600 [00:57<02:05,  3.55it/s] 26%|██▌       | 155/600 [00:58<02:05,  3.55it/s] 26%|██▌       | 156/600 [00:58<02:05,  3.53it/s] 26%|██▌       | 157/600 [00:58<02:05,  3.54it/s] 26%|██▋       | 158/600 [00:58<02:04,  3.54it/s] 26%|██▋       | 159/600 [00:59<02:04,  3.54it/s] 27%|██▋       | 160/600 [00:59<02:03,  3.55it/s] 27%|██▋       | 161/600 [00:59<02:03,  3.55it/s] 27%|██▋       | 162/600 [01:00<02:03,  3.56it/s] 27%|██▋       | 163/600 [01:00<02:03,  3.55it/s] 27%|██▋       | 164/600 [01:00<02:02,  3.55it/s] 28%|██▊       | 165/600 [01:00<02:02,  3.55it/s] 28%|██▊       | 166/600 [01:01<02:02,  3.55it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|██▊       | 167/600 [01:01<02:02,  3.54it/s] 28%|██▊       | 168/600 [01:01<02:02,  3.54it/s] 28%|██▊       | 169/600 [01:02<02:01,  3.54it/s] 28%|██▊       | 170/600 [01:02<02:01,  3.55it/s] 28%|██▊       | 171/600 [01:02<02:00,  3.55it/s] 29%|██▊       | 172/600 [01:02<02:00,  3.55it/s] 29%|██▉       | 173/600 [01:03<02:00,  3.55it/s] 29%|██▉       | 174/600 [01:03<02:00,  3.55it/s] 29%|██▉       | 175/600 [01:03<01:59,  3.54it/s] 29%|██▉       | 176/600 [01:04<01:59,  3.54it/s] 30%|██▉       | 177/600 [01:04<01:59,  3.54it/s] 30%|██▉       | 178/600 [01:04<01:59,  3.54it/s] 30%|██▉       | 179/600 [01:04<01:58,  3.54it/s] 30%|███       | 180/600 [01:05<01:58,  3.55it/s] 30%|███       | 181/600 [01:05<01:58,  3.55it/s] 30%|███       | 182/600 [01:05<01:57,  3.55it/s] 30%|███       | 183/600 [01:06<01:57,  3.54it/s] 31%|███       | 184/600 [01:06<01:57,  3.55it/s] 31%|███       | 185/600 [01:06<01:56,  3.56it/s] 31%|███       | 186/600 [01:06<01:55,  3.57it/s] 31%|███       | 187/600 [01:07<01:55,  3.57it/s] 31%|███▏      | 188/600 [01:07<01:55,  3.57it/s] 32%|███▏      | 189/600 [01:07<01:55,  3.56it/s] 32%|███▏      | 190/600 [01:07<01:54,  3.57it/s] 32%|███▏      | 191/600 [01:08<01:54,  3.58it/s] 32%|███▏      | 192/600 [01:08<01:53,  3.59it/s] 32%|███▏      | 193/600 [01:08<01:53,  3.59it/s] 32%|███▏      | 194/600 [01:09<01:52,  3.59it/s] 32%|███▎      | 195/600 [01:09<01:52,  3.60it/s] 33%|███▎      | 196/600 [01:09<01:52,  3.60it/s] 33%|███▎      | 197/600 [01:09<01:52,  3.59it/s] 33%|███▎      | 198/600 [01:10<01:51,  3.59it/s] 33%|███▎      | 199/600 [01:10<01:51,  3.59it/s] 33%|███▎      | 200/600 [01:10<01:51,  3.57it/s] 34%|███▎      | 201/600 [01:11<01:51,  3.58it/s] 34%|███▎      | 202/600 [01:11<01:51,  3.59it/s] 34%|███▍      | 203/600 [01:11<01:53,  3.51it/s] 34%|███▍      | 204/600 [01:11<01:52,  3.53it/s] 34%|███▍      | 205/600 [01:12<01:51,  3.55it/s] 34%|███▍      | 206/600 [01:12<01:50,  3.56it/s] 34%|███▍      | 207/600 [01:12<01:49,  3.57it/s] 35%|███▍      | 208/600 [01:13<01:49,  3.58it/s] 35%|███▍      | 209/600 [01:13<01:48,  3.59it/s] 35%|███▌      | 210/600 [01:13<01:48,  3.59it/s] 35%|███▌      | 211/600 [01:13<01:48,  3.57it/s] 35%|███▌      | 212/600 [01:14<01:48,  3.58it/s] 36%|███▌      | 213/600 [01:14<01:47,  3.59it/s] 36%|███▌      | 214/600 [01:14<01:47,  3.59it/s] 36%|███▌      | 215/600 [01:14<01:47,  3.59it/s] 36%|███▌      | 216/600 [01:15<01:46,  3.59it/s] 36%|███▌      | 217/600 [01:15<01:46,  3.59it/s] 36%|███▋      | 218/600 [01:15<01:46,  3.60it/s] 36%|███▋      | 219/600 [01:16<01:45,  3.59it/s] 37%|███▋      | 220/600 [01:16<01:45,  3.60it/s] 37%|███▋      | 221/600 [01:16<01:45,  3.60it/s] 37%|███▋      | 222/600 [01:16<01:45,  3.58it/s] 37%|███▋      | 223/600 [01:17<01:45,  3.59it/s] 37%|███▋      | 224/600 [01:17<01:44,  3.59it/s] 38%|███▊      | 225/600 [01:17<01:44,  3.59it/s] 38%|███▊      | 226/600 [01:18<01:44,  3.59it/s] 38%|███▊      | 227/600 [01:18<01:43,  3.60it/s] 38%|███▊      | 228/600 [01:18<01:43,  3.60it/s] 38%|███▊      | 229/600 [01:18<01:43,  3.60it/s] 38%|███▊      | 230/600 [01:19<01:42,  3.60it/s] 38%|███▊      | 231/600 [01:19<01:42,  3.60it/s] 39%|███▊      | 232/600 [01:19<01:42,  3.59it/s] 39%|███▉      | 233/600 [01:19<01:42,  3.58it/s] 39%|███▉      | 234/600 [01:20<01:42,  3.58it/s] 39%|███▉      | 235/600 [01:20<01:41,  3.59it/s] 39%|███▉      | 236/600 [01:20<01:41,  3.59it/s] 40%|███▉      | 237/600 [01:21<01:40,  3.59it/s] 40%|███▉      | 238/600 [01:21<01:41,  3.58it/s] 40%|███▉      | 239/600 [01:21<01:40,  3.58it/s] 40%|████      | 240/600 [01:21<01:40,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 12:33:36,620 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:33:36,620 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 12:33:36,620 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3834, 'eval_samples_per_second': 350.63, 'eval_steps_per_second': 43.849, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.32it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.45it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.86it/s][A
  4%|▍         | 22/543 [00:00<00:11, 44.96it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.69it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.41it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.15it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.12it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.10it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.18it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.21it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.10it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 43.96it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.90it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.89it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.85it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.89it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.02it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.14it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.09it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.11it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.04it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.90it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.89it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.93it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.91it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.09it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.12it/s][A
 27%|██▋       | 147/543 [00:03<00:09, 43.96it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.99it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.85it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.85it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.95it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.93it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 43.98it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.06it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.14it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 43.93it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.75it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.87it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.01it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.84it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.96it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.02it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.10it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.06it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.96it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.96it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.93it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.92it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.93it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.98it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.14it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.10it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.05it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.96it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.99it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.98it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.84it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.91it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.02it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.14it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.09it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.99it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.01it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.93it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.81it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.88it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.01it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.09it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.04it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.08it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.94it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.00it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.96it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.80it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.96it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 43.97it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.08it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.13it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.09it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.04it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.88it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.91it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.93it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.94it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.97it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.10it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.16it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.10it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.97it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.92it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.96it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.92it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.98it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.03it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.09it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.04it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.89it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.98it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.96it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.06it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.97it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.03it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.08it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.08it/s][A 40%|████      | 240/600 [01:34<01:40,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:33:49,032 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 12:33:49,055 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:33:51,250 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:33:51,265 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:33:51,273 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240/special_tokens_map.json
 40%|████      | 241/600 [01:37<28:33,  4.77s/it] 40%|████      | 242/600 [01:37<20:25,  3.42s/it] 40%|████      | 243/600 [01:37<14:45,  2.48s/it] 41%|████      | 244/600 [01:38<10:48,  1.82s/it] 41%|████      | 245/600 [01:38<08:02,  1.36s/it] 41%|████      | 246/600 [01:38<06:06,  1.04s/it] 41%|████      | 247/600 [01:38<04:45,  1.24it/s] 41%|████▏     | 248/600 [01:39<03:49,  1.54it/s] 42%|████▏     | 249/600 [01:39<03:09,  1.85it/s] 42%|████▏     | 250/600 [01:39<02:42,  2.16it/s] 42%|████▏     | 251/600 [01:40<02:22,  2.44it/s] 42%|████▏     | 252/600 [01:40<02:09,  2.69it/s] 42%|████▏     | 253/600 [01:40<01:59,  2.90it/s] 42%|████▏     | 254/600 [01:40<01:52,  3.08it/s] 42%|████▎     | 255/600 [01:41<01:47,  3.22it/s] 43%|████▎     | 256/600 [01:41<01:43,  3.33it/s] 43%|████▎     | 257/600 [01:41<01:40,  3.40it/s] 43%|████▎     | 258/600 [01:41<01:38,  3.46it/s] 43%|████▎     | 259/600 [01:42<01:37,  3.50it/s] 43%|████▎     | 260/600 [01:42<01:36,  3.53it/s] 44%|████▎     | 261/600 [01:42<01:35,  3.55it/s] 44%|████▎     | 262/600 [01:43<01:35,  3.54it/s] 44%|████▍     | 263/600 [01:43<01:34,  3.56it/s] 44%|████▍     | 264/600 [01:43<01:34,  3.57it/s] 44%|████▍     | 265/600 [01:43<01:33,  3.58it/s] 44%|████▍     | 266/600 [01:44<01:33,  3.59it/s] 44%|████▍     | 267/600 [01:44<01:32,  3.59it/s] 45%|████▍     | 268/600 [01:44<01:32,  3.59it/s] 45%|████▍     | 269/600 [01:45<01:32,  3.59it/s] 45%|████▌     | 270/600 [01:45<01:31,  3.59it/s] 45%|████▌     | 271/600 [01:45<01:31,  3.59it/s] 45%|████▌     | 272/600 [01:45<01:31,  3.59it/s] 46%|████▌     | 273/600 [01:46<01:31,  3.58it/s] 46%|████▌     | 274/600 [01:46<01:30,  3.58it/s] 46%|████▌     | 275/600 [01:46<01:30,  3.58it/s] 46%|████▌     | 276/600 [01:46<01:30,  3.59it/s] 46%|████▌     | 277/600 [01:47<01:29,  3.59it/s] 46%|████▋     | 278/600 [01:47<01:29,  3.59it/s] 46%|████▋     | 279/600 [01:47<01:29,  3.60it/s] 47%|████▋     | 280/600 [01:48<01:28,  3.60it/s] 47%|████▋     | 281/600 [01:48<01:28,  3.60it/s] 47%|████▋     | 282/600 [01:48<01:28,  3.60it/s] 47%|████▋     | 283/600 [01:48<01:28,  3.60it/s] 47%|████▋     | 284/600 [01:49<01:28,  3.58it/s] 48%|████▊     | 285/600 [01:49<01:27,  3.59it/s] 48%|████▊     | 286/600 [01:49<01:27,  3.59it/s] 48%|████▊     | 287/600 [01:50<01:27,  3.59it/s] 48%|████▊     | 288/600 [01:50<01:26,  3.60it/s] 48%|████▊     | 289/600 [01:50<01:26,  3.60it/s] 48%|████▊     | 290/600 [01:50<01:26,  3.60it/s] 48%|████▊     | 291/600 [01:51<01:26,  3.58it/s] 49%|████▊     | 292/600 [01:51<01:25,  3.59it/s] 49%|████▉     | 293/600 [01:51<01:25,  3.59it/s] 49%|████▉     | 294/600 [01:51<01:25,  3.59it/s] 49%|████▉     | 295/600 [01:52<01:25,  3.58it/s] 49%|████▉     | 296/600 [01:52<01:24,  3.58it/s] 50%|████▉     | 297/600 [01:52<01:24,  3.59it/s] 50%|████▉     | 298/600 [01:53<01:24,  3.59it/s] 50%|████▉     | 299/600 [01:53<01:23,  3.59it/s] 50%|█████     | 300/600 [01:53<01:23,  3.58it/s] 50%|█████     | 301/600 [01:53<01:23,  3.58it/s] 50%|█████     | 302/600 [01:54<01:23,  3.58it/s] 50%|█████     | 303/600 [01:54<01:22,  3.59it/s] 51%|█████     | 304/600 [01:54<01:22,  3.59it/s] 51%|█████     | 305/600 [01:55<01:22,  3.59it/s] 51%|█████     | 306/600 [01:55<01:21,  3.59it/s] 51%|█████     | 307/600 [01:55<01:21,  3.60it/s] 51%|█████▏    | 308/600 [01:55<01:21,  3.60it/s] 52%|█████▏    | 309/600 [01:56<01:20,  3.60it/s] 52%|█████▏    | 310/600 [01:56<01:20,  3.60it/s] 52%|█████▏    | 311/600 [01:56<01:20,  3.59it/s] 52%|█████▏    | 312/600 [01:56<01:20,  3.59it/s] 52%|█████▏    | 313/600 [01:57<01:19,  3.59it/s] 52%|█████▏    | 314/600 [01:57<01:19,  3.59it/s] 52%|█████▎    | 315/600 [01:57<01:19,  3.59it/s] 53%|█████▎    | 316/600 [01:58<01:19,  3.59it/s] 53%|█████▎    | 317/600 [01:58<01:18,  3.60it/s] 53%|█████▎    | 318/600 [01:58<01:18,  3.60it/s] 53%|█████▎    | 319/600 [01:58<01:18,  3.60it/s] 53%|█████▎    | 320/600 [01:59<01:17,  3.60it/s] 54%|█████▎    | 321/600 [01:59<01:17,  3.60it/s] 54%|█████▎    | 322/600 [01:59<01:17,  3.58it/s] 54%|█████▍    | 323/600 [02:00<01:17,  3.59it/s] 54%|█████▍    | 324/600 [02:00<01:16,  3.59it/s] 54%|█████▍    | 325/600 [02:00<01:16,  3.59it/s] 54%|█████▍    | 326/600 [02:00<01:16,  3.60it/s] 55%|█████▍    | 327/600 [02:01<01:15,  3.60it/s] 55%|█████▍    | 328/600 [02:01<01:15,  3.59it/s] 55%|█████▍    | 329/600 [02:01<01:15,  3.60it/s] 55%|█████▌    | 330/600 [02:01<01:15,  3.60it/s] 55%|█████▌    | 331/600 [02:02<01:14,  3.60it/s] 55%|█████▌    | 332/600 [02:02<01:14,  3.60it/s] 56%|█████▌    | 333/600 [02:02<01:14,  3.58it/s] 56%|█████▌    | 334/600 [02:03<01:14,  3.59it/s] 56%|█████▌    | 335/600 [02:03<01:13,  3.59it/s] 56%|█████▌    | 336/600 [02:03<01:13,  3.59it/s] 56%|█████▌    | 337/600 [02:03<01:13,  3.59it/s] 56%|█████▋    | 338/600 [02:04<01:12,  3.59it/s] 56%|█████▋    | 339/600 [02:04<01:12,  3.60it/s] 57%|█████▋    | 340/600 [02:04<01:12,  3.60it/s] 57%|█████▋    | 341/600 [02:05<01:11,  3.60it/s] 57%|█████▋    | 342/600 [02:05<01:11,  3.60it/s] 57%|█████▋    | 343/600 [02:05<01:11,  3.60it/s] 57%|█████▋    | 344/600 [02:05<01:11,  3.58it/s] 57%|█████▊    | 345/600 [02:06<01:11,  3.58it/s] 58%|█████▊    | 346/600 [02:06<01:10,  3.59it/s] 58%|█████▊    | 347/600 [02:06<01:10,  3.59it/s] 58%|█████▊    | 348/600 [02:07<01:10,  3.59it/s] 58%|█████▊    | 349/600 [02:07<01:10,  3.58it/s] 58%|█████▊    | 350/600 [02:07<01:09,  3.59it/s] 58%|█████▊    | 351/600 [02:07<01:09,  3.59it/s] 59%|█████▊    | 352/600 [02:08<01:09,  3.59it/s] 59%|█████▉    | 353/600 [02:08<01:08,  3.59it/s] 59%|█████▉    | 354/600 [02:08<01:08,  3.60it/s] 59%|█████▉    | 355/600 [02:08<01:08,  3.58it/s] 59%|█████▉    | 356/600 [02:09<01:07,  3.59it/s] 60%|█████▉    | 357/600 [02:09<01:07,  3.60it/s] 60%|█████▉    | 358/600 [02:09<01:07,  3.60it/s] 60%|█████▉    | 359/600 [02:10<01:07,  3.59it/s] 60%|██████    | 360/600 [02:10<01:06,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 12:34:25,039 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:34:25,039 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 12:34:25,039 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3858, 'eval_samples_per_second': 350.562, 'eval_steps_per_second': 43.84, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.09it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.21it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.15it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.25it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.64it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.16it/s][A
  7%|▋         | 37/543 [00:00<00:11, 43.91it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.96it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.07it/s][A
 10%|▉         | 52/543 [00:01<00:11, 42.16it/s][A
 10%|█         | 57/543 [00:01<00:11, 44.09it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.29it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.20it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.11it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.88it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.74it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.83it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.96it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.97it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.10it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.22it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.22it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.02it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.92it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.85it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.81it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.90it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.05it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.13it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.22it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.15it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.09it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.91it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.88it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 43.86it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.89it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 43.96it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.07it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.11it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.19it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.96it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.83it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.80it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.87it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.01it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.09it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.12it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.22it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.07it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.95it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.72it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.59it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.77it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.95it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.02it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.23it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.24it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.16it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.02it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.84it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.83it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.85it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.92it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.06it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.21it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.31it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.03it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.92it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.92it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.90it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.97it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.01it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.02it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.16it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.16it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.09it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.01it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.86it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 43.83it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.88it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.02it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.14it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.08it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.16it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.98it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.95it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.84it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.91it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.97it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.10it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.07it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.11it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.09it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.98it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.96it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.72it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.92it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.04it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.06it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.08it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.14it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.89it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.88it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.83it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.90it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.94it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.01it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.10it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.10it/s][A 60%|██████    | 360/600 [02:22<01:06,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:34:37,446 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360
[INFO|configuration_utils.py:351] 2023-08-28 12:34:37,459 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:34:39,563 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:34:39,584 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:34:39,596 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360/special_tokens_map.json
 60%|██████    | 361/600 [02:25<18:54,  4.75s/it] 60%|██████    | 362/600 [02:25<13:31,  3.41s/it] 60%|██████    | 363/600 [02:26<09:45,  2.47s/it] 61%|██████    | 364/600 [02:26<07:07,  1.81s/it] 61%|██████    | 365/600 [02:26<05:18,  1.35s/it] 61%|██████    | 366/600 [02:26<04:01,  1.03s/it] 61%|██████    | 367/600 [02:27<03:07,  1.24it/s] 61%|██████▏   | 368/600 [02:27<02:30,  1.54it/s] 62%|██████▏   | 369/600 [02:27<02:04,  1.86it/s] 62%|██████▏   | 370/600 [02:28<01:45,  2.18it/s] 62%|██████▏   | 371/600 [02:28<01:32,  2.47it/s] 62%|██████▏   | 372/600 [02:28<01:23,  2.73it/s] 62%|██████▏   | 373/600 [02:28<01:17,  2.94it/s] 62%|██████▏   | 374/600 [02:29<01:12,  3.11it/s] 62%|██████▎   | 375/600 [02:29<01:09,  3.24it/s] 63%|██████▎   | 376/600 [02:29<01:07,  3.34it/s] 63%|██████▎   | 377/600 [02:29<01:05,  3.41it/s] 63%|██████▎   | 378/600 [02:30<01:03,  3.47it/s] 63%|██████▎   | 379/600 [02:30<01:03,  3.49it/s] 63%|██████▎   | 380/600 [02:30<01:02,  3.52it/s] 64%|██████▎   | 381/600 [02:31<01:01,  3.55it/s] 64%|██████▎   | 382/600 [02:31<01:01,  3.56it/s] 64%|██████▍   | 383/600 [02:31<01:00,  3.57it/s] 64%|██████▍   | 384/600 [02:31<01:00,  3.58it/s] 64%|██████▍   | 385/600 [02:32<00:59,  3.59it/s] 64%|██████▍   | 386/600 [02:32<00:59,  3.59it/s] 64%|██████▍   | 387/600 [02:32<00:59,  3.59it/s] 65%|██████▍   | 388/600 [02:33<00:58,  3.60it/s] 65%|██████▍   | 389/600 [02:33<00:58,  3.60it/s] 65%|██████▌   | 390/600 [02:33<00:58,  3.58it/s] 65%|██████▌   | 391/600 [02:33<00:58,  3.58it/s] 65%|██████▌   | 392/600 [02:34<00:57,  3.59it/s] 66%|██████▌   | 393/600 [02:34<00:57,  3.59it/s] 66%|██████▌   | 394/600 [02:34<00:57,  3.60it/s] 66%|██████▌   | 395/600 [02:35<00:56,  3.60it/s] 66%|██████▌   | 396/600 [02:35<00:56,  3.60it/s] 66%|██████▌   | 397/600 [02:35<00:56,  3.60it/s] 66%|██████▋   | 398/600 [02:35<00:56,  3.60it/s] 66%|██████▋   | 399/600 [02:36<00:55,  3.60it/s] 67%|██████▋   | 400/600 [02:36<00:55,  3.60it/s] 67%|██████▋   | 401/600 [02:36<00:55,  3.57it/s] 67%|██████▋   | 402/600 [02:36<00:55,  3.58it/s] 67%|██████▋   | 403/600 [02:37<00:54,  3.58it/s] 67%|██████▋   | 404/600 [02:37<00:54,  3.59it/s] 68%|██████▊   | 405/600 [02:37<00:54,  3.60it/s] 68%|██████▊   | 406/600 [02:38<00:53,  3.60it/s] 68%|██████▊   | 407/600 [02:38<00:53,  3.60it/s] 68%|██████▊   | 408/600 [02:38<00:53,  3.60it/s] 68%|██████▊   | 409/600 [02:38<00:53,  3.59it/s] 68%|██████▊   | 410/600 [02:39<00:52,  3.59it/s] 68%|██████▊   | 411/600 [02:39<00:52,  3.60it/s] 69%|██████▊   | 412/600 [02:39<00:52,  3.58it/s] 69%|██████▉   | 413/600 [02:40<00:52,  3.59it/s] 69%|██████▉   | 414/600 [02:40<00:51,  3.59it/s] 69%|██████▉   | 415/600 [02:40<00:51,  3.59it/s] 69%|██████▉   | 416/600 [02:40<00:51,  3.60it/s] 70%|██████▉   | 417/600 [02:41<00:50,  3.60it/s] 70%|██████▉   | 418/600 [02:41<00:50,  3.60it/s] 70%|██████▉   | 419/600 [02:41<00:50,  3.60it/s] 70%|███████   | 420/600 [02:41<00:50,  3.60it/s] 70%|███████   | 421/600 [02:42<00:49,  3.60it/s] 70%|███████   | 422/600 [02:42<00:49,  3.60it/s] 70%|███████   | 423/600 [02:42<00:49,  3.57it/s] 71%|███████   | 424/600 [02:43<00:49,  3.58it/s] 71%|███████   | 425/600 [02:43<00:48,  3.58it/s] 71%|███████   | 426/600 [02:43<00:48,  3.59it/s] 71%|███████   | 427/600 [02:43<00:48,  3.59it/s] 71%|███████▏  | 428/600 [02:44<00:47,  3.59it/s] 72%|███████▏  | 429/600 [02:44<00:47,  3.60it/s] 72%|███████▏  | 430/600 [02:44<00:47,  3.60it/s] 72%|███████▏  | 431/600 [02:45<00:46,  3.60it/s] 72%|███████▏  | 432/600 [02:45<00:46,  3.60it/s] 72%|███████▏  | 433/600 [02:45<00:46,  3.60it/s] 72%|███████▏  | 434/600 [02:45<00:46,  3.57it/s] 72%|███████▎  | 435/600 [02:46<00:46,  3.58it/s] 73%|███████▎  | 436/600 [02:46<00:45,  3.58it/s] 73%|███████▎  | 437/600 [02:46<00:45,  3.59it/s] 73%|███████▎  | 438/600 [02:46<00:45,  3.59it/s] 73%|███████▎  | 439/600 [02:47<00:44,  3.59it/s] 73%|███████▎  | 440/600 [02:47<00:44,  3.59it/s] 74%|███████▎  | 441/600 [02:47<00:44,  3.59it/s] 74%|███████▎  | 442/600 [02:48<00:43,  3.60it/s] 74%|███████▍  | 443/600 [02:48<00:43,  3.60it/s] 74%|███████▍  | 444/600 [02:48<00:43,  3.60it/s] 74%|███████▍  | 445/600 [02:48<00:43,  3.58it/s] 74%|███████▍  | 446/600 [02:49<00:42,  3.59it/s] 74%|███████▍  | 447/600 [02:49<00:42,  3.59it/s] 75%|███████▍  | 448/600 [02:49<00:42,  3.60it/s] 75%|███████▍  | 449/600 [02:50<00:41,  3.60it/s] 75%|███████▌  | 450/600 [02:50<00:41,  3.60it/s] 75%|███████▌  | 451/600 [02:50<00:41,  3.58it/s] 75%|███████▌  | 452/600 [02:50<00:41,  3.57it/s] 76%|███████▌  | 453/600 [02:51<00:41,  3.55it/s] 76%|███████▌  | 454/600 [02:51<00:41,  3.55it/s] 76%|███████▌  | 455/600 [02:51<00:40,  3.55it/s] 76%|███████▌  | 456/600 [02:52<00:40,  3.54it/s] 76%|███████▌  | 457/600 [02:52<00:40,  3.55it/s] 76%|███████▋  | 458/600 [02:52<00:39,  3.57it/s] 76%|███████▋  | 459/600 [02:52<00:39,  3.58it/s] 77%|███████▋  | 460/600 [02:53<00:39,  3.59it/s] 77%|███████▋  | 461/600 [02:53<00:38,  3.59it/s] 77%|███████▋  | 462/600 [02:53<00:38,  3.59it/s] 77%|███████▋  | 463/600 [02:53<00:38,  3.59it/s] 77%|███████▋  | 464/600 [02:54<00:37,  3.59it/s] 78%|███████▊  | 465/600 [02:54<00:37,  3.59it/s] 78%|███████▊  | 466/600 [02:54<00:37,  3.59it/s] 78%|███████▊  | 467/600 [02:55<00:36,  3.59it/s] 78%|███████▊  | 468/600 [02:55<00:36,  3.60it/s] 78%|███████▊  | 469/600 [02:55<00:36,  3.60it/s] 78%|███████▊  | 470/600 [02:55<00:36,  3.60it/s] 78%|███████▊  | 471/600 [02:56<00:35,  3.60it/s] 79%|███████▊  | 472/600 [02:56<00:35,  3.60it/s] 79%|███████▉  | 473/600 [02:56<00:35,  3.60it/s] 79%|███████▉  | 474/600 [02:57<00:34,  3.60it/s] 79%|███████▉  | 475/600 [02:57<00:34,  3.60it/s] 79%|███████▉  | 476/600 [02:57<00:34,  3.60it/s] 80%|███████▉  | 477/600 [02:57<00:34,  3.60it/s] 80%|███████▉  | 478/600 [02:58<00:34,  3.58it/s] 80%|███████▉  | 479/600 [02:58<00:33,  3.58it/s] 80%|████████  | 480/600 [02:58<00:33,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 12:35:13,379 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:35:13,379 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 12:35:13,379 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3904, 'eval_samples_per_second': 350.434, 'eval_steps_per_second': 43.824, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.34it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.17it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.17it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.32it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.22it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.17it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.15it/s][A
  8%|▊         | 42/543 [00:00<00:11, 43.85it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.07it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.21it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.23it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.34it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.07it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.98it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.97it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.78it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.80it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.89it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.17it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.26it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.09it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.89it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.89it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.88it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.82it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.88it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.98it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.11it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.24it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.09it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.97it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.95it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.81it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.71it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 43.89it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.08it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.11it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.23it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.15it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.10it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.97it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.83it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.76it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.95it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.99it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.11it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.12it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.99it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.92it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.78it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.94it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.91it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.10it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.09it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.05it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.10it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.03it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.83it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.84it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.95it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.95it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.04it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.11it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.09it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.10it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.99it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.84it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.78it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.00it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.01it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.98it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.15it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.10it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.05it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.98it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.98it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.87it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 44.08it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.02it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.00it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.17it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.18it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.03it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.98it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.97it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.97it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.90it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.03it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.05it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.18it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.13it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.99it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.98it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.81it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.92it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.94it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.15it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.13it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.99it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.92it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.99it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.08it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.01it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.92it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.03it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.15it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.15it/s][A 80%|████████  | 480/600 [03:11<00:33,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:35:25,781 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480
[INFO|configuration_utils.py:351] 2023-08-28 12:35:25,799 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:35:27,833 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:35:27,843 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:35:27,852 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480/special_tokens_map.json
 80%|████████  | 481/600 [03:13<09:22,  4.73s/it] 80%|████████  | 482/600 [03:14<06:40,  3.39s/it] 80%|████████  | 483/600 [03:14<04:47,  2.46s/it] 81%|████████  | 484/600 [03:14<03:29,  1.81s/it] 81%|████████  | 485/600 [03:14<02:35,  1.35s/it] 81%|████████  | 486/600 [03:15<01:57,  1.03s/it] 81%|████████  | 487/600 [03:15<01:30,  1.24it/s] 81%|████████▏ | 488/600 [03:15<01:12,  1.54it/s] 82%|████████▏ | 489/600 [03:16<00:59,  1.86it/s] 82%|████████▏ | 490/600 [03:16<00:50,  2.17it/s] 82%|████████▏ | 491/600 [03:16<00:44,  2.45it/s] 82%|████████▏ | 492/600 [03:16<00:39,  2.70it/s] 82%|████████▏ | 493/600 [03:17<00:36,  2.91it/s] 82%|████████▏ | 494/600 [03:17<00:34,  3.07it/s] 82%|████████▎ | 495/600 [03:17<00:32,  3.20it/s] 83%|████████▎ | 496/600 [03:18<00:31,  3.30it/s] 83%|████████▎ | 497/600 [03:18<00:30,  3.38it/s] 83%|████████▎ | 498/600 [03:18<00:29,  3.43it/s] 83%|████████▎ | 499/600 [03:18<00:29,  3.47it/s] 83%|████████▎ | 500/600 [03:19<00:28,  3.49it/s]                                                  83%|████████▎ | 500/600 [03:19<00:28,  3.49it/s] 84%|████████▎ | 501/600 [03:19<00:28,  3.51it/s] 84%|████████▎ | 502/600 [03:19<00:27,  3.51it/s] 84%|████████▍ | 503/600 [03:19<00:27,  3.52it/s] 84%|████████▍ | 504/600 [03:20<00:27,  3.53it/s] 84%|████████▍ | 505/600 [03:20<00:26,  3.53it/s] 84%|████████▍ | 506/600 [03:20<00:26,  3.54it/s] 84%|████████▍ | 507/600 [03:21<00:26,  3.54it/s] 85%|████████▍ | 508/600 [03:21<00:26,  3.52it/s] 85%|████████▍ | 509/600 [03:21<00:25,  3.53it/s] 85%|████████▌ | 510/600 [03:21<00:25,  3.53it/s] 85%|████████▌ | 511/600 [03:22<00:25,  3.54it/s] 85%|████████▌ | 512/600 [03:22<00:24,  3.54it/s] 86%|████████▌ | 513/600 [03:22<00:24,  3.53it/s] 86%|████████▌ | 514/600 [03:23<00:24,  3.54it/s] 86%|████████▌ | 515/600 [03:23<00:23,  3.54it/s] 86%|████████▌ | 516/600 [03:23<00:23,  3.55it/s] 86%|████████▌ | 517/600 [03:23<00:23,  3.54it/s] 86%|████████▋ | 518/600 [03:24<00:23,  3.55it/s] 86%|████████▋ | 519/600 [03:24<00:22,  3.54it/s] 87%|████████▋ | 520/600 [03:24<00:22,  3.54it/s] 87%|████████▋ | 521/600 [03:25<00:22,  3.55it/s] 87%|████████▋ | 522/600 [03:25<00:21,  3.55it/s] 87%|████████▋ | 523/600 [03:25<00:21,  3.55it/s] 87%|████████▋ | 524/600 [03:25<00:21,  3.55it/s] 88%|████████▊ | 525/600 [03:26<00:21,  3.55it/s] 88%|████████▊ | 526/600 [03:26<00:20,  3.55it/s] 88%|████████▊ | 527/600 [03:26<00:20,  3.54it/s] 88%|████████▊ | 528/600 [03:27<00:20,  3.55it/s] 88%|████████▊ | 529/600 [03:27<00:19,  3.57it/s] 88%|████████▊ | 530/600 [03:27<00:19,  3.57it/s] 88%|████████▊ | 531/600 [03:27<00:19,  3.58it/s] 89%|████████▊ | 532/600 [03:28<00:18,  3.59it/s] 89%|████████▉ | 533/600 [03:28<00:18,  3.59it/s] 89%|████████▉ | 534/600 [03:28<00:18,  3.59it/s] 89%|████████▉ | 535/600 [03:28<00:18,  3.58it/s] 89%|████████▉ | 536/600 [03:29<00:17,  3.59it/s] 90%|████████▉ | 537/600 [03:29<00:17,  3.59it/s] 90%|████████▉ | 538/600 [03:29<00:17,  3.59it/s] 90%|████████▉ | 539/600 [03:30<00:16,  3.59it/s] 90%|█████████ | 540/600 [03:30<00:16,  3.59it/s] 90%|█████████ | 541/600 [03:30<00:16,  3.59it/s] 90%|█████████ | 542/600 [03:30<00:16,  3.59it/s] 90%|█████████ | 543/600 [03:31<00:15,  3.60it/s] 91%|█████████ | 544/600 [03:31<00:15,  3.58it/s] 91%|█████████ | 545/600 [03:31<00:15,  3.59it/s] 91%|█████████ | 546/600 [03:32<00:15,  3.57it/s] 91%|█████████ | 547/600 [03:32<00:14,  3.58it/s] 91%|█████████▏| 548/600 [03:32<00:14,  3.59it/s] 92%|█████████▏| 549/600 [03:32<00:14,  3.59it/s] 92%|█████████▏| 550/600 [03:33<00:13,  3.59it/s] 92%|█████████▏| 551/600 [03:33<00:13,  3.59it/s] 92%|█████████▏| 552/600 [03:33<00:13,  3.60it/s] 92%|█████████▏| 553/600 [03:34<00:13,  3.60it/s] 92%|█████████▏| 554/600 [03:34<00:12,  3.59it/s] 92%|█████████▎| 555/600 [03:34<00:12,  3.60it/s] 93%|█████████▎| 556/600 [03:34<00:12,  3.59it/s] 93%|█████████▎| 557/600 [03:35<00:12,  3.57it/s] 93%|█████████▎| 558/600 [03:35<00:11,  3.58it/s] 93%|█████████▎| 559/600 [03:35<00:11,  3.58it/s] 93%|█████████▎| 560/600 [03:35<00:11,  3.59it/s] 94%|█████████▎| 561/600 [03:36<00:10,  3.59it/s] 94%|█████████▎| 562/600 [03:36<00:10,  3.60it/s] 94%|█████████▍| 563/600 [03:36<00:10,  3.60it/s] 94%|█████████▍| 564/600 [03:37<00:10,  3.60it/s] 94%|█████████▍| 565/600 [03:37<00:09,  3.60it/s] 94%|█████████▍| 566/600 [03:37<00:09,  3.60it/s] 94%|█████████▍| 567/600 [03:37<00:09,  3.60it/s] 95%|█████████▍| 568/600 [03:38<00:08,  3.58it/s] 95%|█████████▍| 569/600 [03:38<00:08,  3.59it/s] 95%|█████████▌| 570/600 [03:38<00:08,  3.59it/s] 95%|█████████▌| 571/600 [03:39<00:08,  3.59it/s] 95%|█████████▌| 572/600 [03:39<00:07,  3.59it/s] 96%|█████████▌| 573/600 [03:39<00:07,  3.60it/s] 96%|█████████▌| 574/600 [03:39<00:07,  3.60it/s] 96%|█████████▌| 575/600 [03:40<00:06,  3.60it/s] 96%|█████████▌| 576/600 [03:40<00:06,  3.60it/s] 96%|█████████▌| 577/600 [03:40<00:06,  3.60it/s] 96%|█████████▋| 578/600 [03:40<00:06,  3.60it/s] 96%|█████████▋| 579/600 [03:41<00:05,  3.57it/s] 97%|█████████▋| 580/600 [03:41<00:05,  3.58it/s] 97%|█████████▋| 581/600 [03:41<00:05,  3.58it/s] 97%|█████████▋| 582/600 [03:42<00:05,  3.59it/s] 97%|█████████▋| 583/600 [03:42<00:04,  3.59it/s] 97%|█████████▋| 584/600 [03:42<00:04,  3.59it/s] 98%|█████████▊| 585/600 [03:42<00:04,  3.59it/s] 98%|█████████▊| 586/600 [03:43<00:03,  3.60it/s] 98%|█████████▊| 587/600 [03:43<00:03,  3.60it/s] 98%|█████████▊| 588/600 [03:43<00:03,  3.60it/s] 98%|█████████▊| 589/600 [03:44<00:03,  3.60it/s] 98%|█████████▊| 590/600 [03:44<00:02,  3.58it/s] 98%|█████████▊| 591/600 [03:44<00:02,  3.59it/s] 99%|█████████▊| 592/600 [03:44<00:02,  3.59it/s] 99%|█████████▉| 593/600 [03:45<00:01,  3.59it/s] 99%|█████████▉| 594/600 [03:45<00:01,  3.59it/s] 99%|█████████▉| 595/600 [03:45<00:01,  3.59it/s] 99%|█████████▉| 596/600 [03:45<00:01,  3.59it/s]100%|█████████▉| 597/600 [03:46<00:00,  3.59it/s]100%|█████████▉| 598/600 [03:46<00:00,  3.59it/s]100%|█████████▉| 599/600 [03:46<00:00,  3.60it/s]100%|██████████| 600/600 [03:47<00:00,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 12:36:01,749 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:36:01,749 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 12:36:01,749 >>   Batch size = 8
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3793, 'eval_samples_per_second': 350.747, 'eval_steps_per_second': 43.864, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6625e-05, 'epoch': 4.17}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 56.17it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.57it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.58it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.43it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.90it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.51it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.29it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.01it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.03it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.24it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.29it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.23it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.07it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.07it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.93it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.93it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.86it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.95it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.15it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.15it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.15it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.11it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.03it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.88it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.77it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.86it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.96it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.16it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.18it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.03it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.13it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.00it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.81it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.80it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.81it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.00it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.12it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.06it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.10it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.02it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.98it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.84it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.81it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.86it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.09it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.07it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.14it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.14it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.04it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.95it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.00it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.87it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.96it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.99it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.06it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 43.99it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.06it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.02it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.93it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.91it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.94it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.98it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.00it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.05it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.97it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.00it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.09it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.94it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.89it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.98it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.99it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.06it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.06it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.06it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.01it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.06it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.92it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.96it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.91it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.00it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.02it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.06it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.09it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.02it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.05it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.00it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.92it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.99it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.02it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.99it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.91it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.07it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.08it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.00it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.86it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.01it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.93it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.08it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.07it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.99it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.05it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.03it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.99it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.96it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.91it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.88it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.03it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.00it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.00it/s][A100%|██████████| 600/600 [03:59<00:00,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:36:14,097 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600
[INFO|configuration_utils.py:351] 2023-08-28 12:36:14,112 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:36:16,229 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:36:16,251 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:36:16,260 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 12:36:16,526 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 12:36:16,527 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120 (score: 1.0794728994369507).
                                                 100%|██████████| 600/600 [04:03<00:00,  3.60it/s]100%|██████████| 600/600 [04:03<00:00,  2.46it/s]
[INFO|trainer.py:1894] 2023-08-28 12:36:18,238 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 12:36:18,253 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:36:19,990 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:36:20,000 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:36:20,008 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:36:20,210 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:20,210 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:20,210 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:20,210 >>   train_runtime            = 0:04:03.58
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:20,210 >>   train_samples            =       7700
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:20,210 >>   train_samples_per_second =    158.056
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:20,210 >>   train_steps_per_second   =      2.463
{'eval_loss': 1.0794728994369507, 'eval_runtime': 12.3363, 'eval_samples_per_second': 351.97, 'eval_steps_per_second': 44.017, 'epoch': 5.0}
{'train_runtime': 243.5839, 'train_samples_per_second': 158.056, 'train_steps_per_second': 2.463, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 12:36:20 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 12:36:20,247 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:36:20,247 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 12:36:20,247 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.70it/s]  2%|▏         | 12/543 [00:00<00:10, 48.73it/s]  3%|▎         | 17/543 [00:00<00:11, 47.00it/s]  4%|▍         | 22/543 [00:00<00:11, 46.06it/s]  5%|▍         | 27/543 [00:00<00:11, 45.43it/s]  6%|▌         | 32/543 [00:00<00:11, 45.30it/s]  7%|▋         | 37/543 [00:00<00:11, 45.08it/s]  8%|▊         | 42/543 [00:00<00:11, 44.40it/s]  9%|▊         | 47/543 [00:01<00:11, 43.87it/s] 10%|▉         | 52/543 [00:01<00:11, 43.62it/s] 10%|█         | 57/543 [00:01<00:11, 43.47it/s] 11%|█▏        | 62/543 [00:01<00:10, 43.97it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.14it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.19it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.46it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.28it/s] 16%|█▌        | 87/543 [00:01<00:10, 43.94it/s] 17%|█▋        | 92/543 [00:02<00:10, 43.69it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.61it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.74it/s] 20%|█▉        | 107/543 [00:02<00:09, 43.94it/s] 21%|██        | 112/543 [00:02<00:09, 44.09it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.14it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.22it/s] 23%|██▎       | 127/543 [00:02<00:09, 43.96it/s] 24%|██▍       | 132/543 [00:02<00:09, 43.87it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.78it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.74it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.79it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.03it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.01it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.22it/s] 31%|███       | 167/543 [00:03<00:08, 44.04it/s] 32%|███▏      | 172/543 [00:03<00:08, 43.89it/s] 33%|███▎      | 177/543 [00:03<00:08, 43.85it/s] 34%|███▎      | 182/543 [00:04<00:08, 43.87it/s] 34%|███▍      | 187/543 [00:04<00:08, 43.84it/s] 35%|███▌      | 192/543 [00:04<00:07, 43.88it/s] 36%|███▋      | 197/543 [00:04<00:07, 43.93it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.14it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.23it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.13it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.09it/s] 41%|████      | 222/543 [00:05<00:07, 44.03it/s] 42%|████▏     | 227/543 [00:05<00:07, 44.04it/s] 43%|████▎     | 232/543 [00:05<00:07, 43.92it/s] 44%|████▎     | 237/543 [00:05<00:06, 44.02it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.26it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.35it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.31it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.28it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.22it/s] 49%|████▉     | 267/543 [00:06<00:06, 44.15it/s] 50%|█████     | 272/543 [00:06<00:06, 44.07it/s] 51%|█████     | 277/543 [00:06<00:06, 43.91it/s] 52%|█████▏    | 282/543 [00:06<00:05, 44.05it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.18it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.31it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.21it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.24it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.19it/s] 57%|█████▋    | 312/543 [00:07<00:05, 44.14it/s] 58%|█████▊    | 317/543 [00:07<00:05, 43.97it/s] 59%|█████▉    | 322/543 [00:07<00:05, 43.97it/s] 60%|██████    | 327/543 [00:07<00:04, 44.02it/s] 61%|██████    | 332/543 [00:07<00:04, 44.14it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.25it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.22it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.02it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.18it/s] 66%|██████▌   | 357/543 [00:08<00:04, 44.16it/s] 67%|██████▋   | 362/543 [00:08<00:04, 44.08it/s] 68%|██████▊   | 367/543 [00:08<00:03, 44.05it/s] 69%|██████▊   | 372/543 [00:08<00:03, 43.96it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.22it/s] 70%|███████   | 382/543 [00:08<00:03, 44.18it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.11it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.17it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.15it/s] 74%|███████▍  | 402/543 [00:09<00:03, 44.09it/s] 75%|███████▍  | 407/543 [00:09<00:03, 43.93it/s] 76%|███████▌  | 412/543 [00:09<00:02, 43.86it/s] 77%|███████▋  | 417/543 [00:09<00:02, 43.91it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.06it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.17it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.15it/s] 80%|████████  | 437/543 [00:09<00:02, 44.15it/s] 81%|████████▏ | 442/543 [00:10<00:02, 44.15it/s] 82%|████████▏ | 447/543 [00:10<00:02, 44.09it/s] 83%|████████▎ | 452/543 [00:10<00:02, 43.95it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.04it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.17it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.14it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.20it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.19it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.19it/s] 90%|████████▉ | 487/543 [00:11<00:01, 44.17it/s] 91%|█████████ | 492/543 [00:11<00:01, 44.01it/s] 92%|█████████▏| 497/543 [00:11<00:01, 43.99it/s] 92%|█████████▏| 502/543 [00:11<00:00, 44.10it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.10it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.26it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.24it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.14it/s] 97%|█████████▋| 527/543 [00:11<00:00, 44.19it/s] 98%|█████████▊| 532/543 [00:12<00:00, 44.15it/s] 99%|█████████▉| 537/543 [00:12<00:00, 44.12it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.08it/s]100%|██████████| 543/543 [00:12<00:00, 44.15it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:36:32,565 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:32,565 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:32,565 >>   eval_loss               =     1.0795
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:32,565 >>   eval_runtime            = 0:00:12.31
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:32,565 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:32,565 >>   eval_samples_per_second =    352.513
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:32,565 >>   eval_steps_per_second   =     44.084
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:36:32,565 >>   perplexity              =     2.9431
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:38,890 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:38,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:38,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:38,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:38,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:36:39,492 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:36:39,493 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:36:40,066 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:36:41,097 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:36:41,097 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:43,921 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:43,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:43,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:43,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:36:43,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:36:44,551 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:36:44,556 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:36:45,114 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:36:45,281 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:36:45,281 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.78it/s]Extractor Predicting: 3it [00:01,  1.82it/s]Extractor Predicting: 4it [00:02,  1.90it/s]Extractor Predicting: 5it [00:02,  1.86it/s]Extractor Predicting: 6it [00:03,  1.81it/s]Extractor Predicting: 7it [00:03,  1.84it/s]Extractor Predicting: 8it [00:04,  1.87it/s]Extractor Predicting: 9it [00:04,  1.94it/s]Extractor Predicting: 10it [00:05,  1.94it/s]Extractor Predicting: 11it [00:05,  1.95it/s]Extractor Predicting: 12it [00:06,  1.97it/s]Extractor Predicting: 13it [00:06,  1.87it/s]Extractor Predicting: 14it [00:07,  1.81it/s]Extractor Predicting: 15it [00:08,  1.76it/s]Extractor Predicting: 16it [00:08,  1.76it/s]Extractor Predicting: 17it [00:09,  1.74it/s]Extractor Predicting: 18it [00:09,  1.71it/s]Extractor Predicting: 19it [00:10,  1.56it/s]Extractor Predicting: 20it [00:11,  1.57it/s]Extractor Predicting: 21it [00:11,  1.61it/s]Extractor Predicting: 22it [00:12,  1.67it/s]Extractor Predicting: 23it [00:12,  1.69it/s]Extractor Predicting: 24it [00:13,  1.70it/s]Extractor Predicting: 25it [00:14,  1.70it/s]Extractor Predicting: 26it [00:14,  1.68it/s]Extractor Predicting: 27it [00:15,  1.67it/s]Extractor Predicting: 28it [00:15,  1.69it/s]Extractor Predicting: 29it [00:16,  1.67it/s]Extractor Predicting: 30it [00:17,  1.67it/s]Extractor Predicting: 31it [00:17,  1.69it/s]Extractor Predicting: 32it [00:18,  1.68it/s]Extractor Predicting: 33it [00:18,  1.69it/s]Extractor Predicting: 34it [00:19,  1.67it/s]Extractor Predicting: 35it [00:20,  1.65it/s]Extractor Predicting: 36it [00:20,  1.63it/s]Extractor Predicting: 37it [00:21,  1.63it/s]Extractor Predicting: 38it [00:22,  1.61it/s]Extractor Predicting: 39it [00:22,  1.65it/s]Extractor Predicting: 40it [00:23,  1.67it/s]Extractor Predicting: 41it [00:23,  1.66it/s]Extractor Predicting: 42it [00:24,  1.64it/s]Extractor Predicting: 43it [00:25,  1.68it/s]Extractor Predicting: 44it [00:25,  1.70it/s]Extractor Predicting: 45it [00:26,  1.70it/s]Extractor Predicting: 46it [00:26,  1.72it/s]Extractor Predicting: 47it [00:27,  1.72it/s]Extractor Predicting: 48it [00:27,  1.70it/s]Extractor Predicting: 49it [00:28,  1.69it/s]Extractor Predicting: 50it [00:29,  1.71it/s]Extractor Predicting: 51it [00:29,  1.70it/s]Extractor Predicting: 52it [00:30,  1.68it/s]Extractor Predicting: 53it [00:30,  1.65it/s]Extractor Predicting: 54it [00:31,  1.69it/s]Extractor Predicting: 55it [00:32,  1.73it/s]Extractor Predicting: 56it [00:32,  1.69it/s]Extractor Predicting: 57it [00:33,  1.66it/s]Extractor Predicting: 58it [00:33,  1.72it/s]Extractor Predicting: 59it [00:34,  1.67it/s]Extractor Predicting: 60it [00:35,  1.68it/s]Extractor Predicting: 61it [00:35,  1.70it/s]Extractor Predicting: 62it [00:36,  1.71it/s]Extractor Predicting: 63it [00:36,  1.71it/s]Extractor Predicting: 64it [00:37,  1.72it/s]Extractor Predicting: 65it [00:37,  1.77it/s]Extractor Predicting: 66it [00:38,  1.77it/s]Extractor Predicting: 67it [00:38,  1.77it/s]Extractor Predicting: 68it [00:39,  1.73it/s]Extractor Predicting: 69it [00:40,  1.73it/s]Extractor Predicting: 70it [00:40,  1.72it/s]Extractor Predicting: 71it [00:41,  1.72it/s]Extractor Predicting: 72it [00:41,  1.72it/s]Extractor Predicting: 73it [00:42,  1.71it/s]Extractor Predicting: 74it [00:43,  1.68it/s]Extractor Predicting: 75it [00:43,  1.64it/s]Extractor Predicting: 76it [00:44,  1.66it/s]Extractor Predicting: 77it [00:44,  1.70it/s]Extractor Predicting: 78it [00:45,  1.69it/s]Extractor Predicting: 79it [00:46,  1.68it/s]Extractor Predicting: 80it [00:46,  1.71it/s]Extractor Predicting: 81it [00:47,  1.71it/s]Extractor Predicting: 82it [00:47,  1.72it/s]Extractor Predicting: 83it [00:48,  1.73it/s]Extractor Predicting: 84it [00:48,  1.74it/s]Extractor Predicting: 85it [00:49,  1.75it/s]Extractor Predicting: 86it [00:50,  1.72it/s]Extractor Predicting: 87it [00:50,  1.71it/s]Extractor Predicting: 88it [00:51,  1.74it/s]Extractor Predicting: 89it [00:51,  1.76it/s]Extractor Predicting: 90it [00:52,  1.73it/s]Extractor Predicting: 91it [00:53,  1.54it/s]Extractor Predicting: 92it [00:53,  1.56it/s]Extractor Predicting: 93it [00:54,  1.64it/s]Extractor Predicting: 94it [00:54,  1.68it/s]Extractor Predicting: 95it [00:55,  1.69it/s]Extractor Predicting: 96it [00:56,  1.72it/s]Extractor Predicting: 97it [00:56,  1.75it/s]Extractor Predicting: 98it [00:57,  1.73it/s]Extractor Predicting: 99it [00:57,  1.71it/s]Extractor Predicting: 100it [00:58,  1.72it/s]Extractor Predicting: 101it [00:59,  1.72it/s]Extractor Predicting: 102it [00:59,  1.71it/s]Extractor Predicting: 103it [01:00,  1.71it/s]Extractor Predicting: 104it [01:00,  1.72it/s]Extractor Predicting: 105it [01:01,  1.69it/s]Extractor Predicting: 106it [01:01,  1.72it/s]Extractor Predicting: 107it [01:02,  1.72it/s]Extractor Predicting: 108it [01:03,  1.71it/s]Extractor Predicting: 109it [01:03,  1.70it/s]Extractor Predicting: 110it [01:04,  1.69it/s]Extractor Predicting: 111it [01:04,  1.70it/s]Extractor Predicting: 112it [01:05,  1.74it/s]Extractor Predicting: 113it [01:06,  1.74it/s]Extractor Predicting: 114it [01:06,  1.73it/s]Extractor Predicting: 115it [01:07,  1.71it/s]Extractor Predicting: 116it [01:07,  1.71it/s]Extractor Predicting: 117it [01:08,  1.72it/s]Extractor Predicting: 118it [01:08,  1.68it/s]Extractor Predicting: 119it [01:09,  1.70it/s]Extractor Predicting: 120it [01:10,  1.68it/s]Extractor Predicting: 121it [01:10,  1.66it/s]Extractor Predicting: 122it [01:11,  1.66it/s]Extractor Predicting: 123it [01:11,  1.71it/s]Extractor Predicting: 124it [01:12,  1.73it/s]Extractor Predicting: 125it [01:13,  1.75it/s]Extractor Predicting: 126it [01:13,  1.70it/s]Extractor Predicting: 127it [01:14,  1.68it/s]Extractor Predicting: 128it [01:14,  1.71it/s]Extractor Predicting: 129it [01:15,  1.74it/s]Extractor Predicting: 130it [01:15,  1.79it/s]Extractor Predicting: 131it [01:16,  1.74it/s]Extractor Predicting: 132it [01:17,  1.73it/s]Extractor Predicting: 133it [01:17,  1.73it/s]Extractor Predicting: 134it [01:18,  1.74it/s]Extractor Predicting: 135it [01:18,  1.76it/s]Extractor Predicting: 136it [01:19,  1.75it/s]Extractor Predicting: 137it [01:19,  1.77it/s]Extractor Predicting: 138it [01:20,  1.73it/s]Extractor Predicting: 139it [01:21,  1.71it/s]Extractor Predicting: 140it [01:21,  1.75it/s]Extractor Predicting: 141it [01:22,  1.76it/s]Extractor Predicting: 142it [01:22,  1.79it/s]Extractor Predicting: 143it [01:23,  1.80it/s]Extractor Predicting: 144it [01:23,  1.80it/s]Extractor Predicting: 145it [01:24,  1.76it/s]Extractor Predicting: 146it [01:25,  1.78it/s]Extractor Predicting: 147it [01:25,  1.78it/s]Extractor Predicting: 148it [01:26,  1.79it/s]Extractor Predicting: 149it [01:26,  1.80it/s]Extractor Predicting: 150it [01:27,  1.74it/s]Extractor Predicting: 151it [01:27,  1.74it/s]Extractor Predicting: 152it [01:28,  1.74it/s]Extractor Predicting: 153it [01:29,  1.72it/s]Extractor Predicting: 154it [01:29,  1.73it/s]Extractor Predicting: 155it [01:30,  1.70it/s]Extractor Predicting: 156it [01:30,  1.68it/s]Extractor Predicting: 157it [01:31,  1.69it/s]Extractor Predicting: 158it [01:32,  1.71it/s]Extractor Predicting: 159it [01:32,  1.74it/s]Extractor Predicting: 160it [01:33,  1.77it/s]Extractor Predicting: 161it [01:33,  1.79it/s]Extractor Predicting: 162it [01:34,  1.79it/s]Extractor Predicting: 163it [01:34,  1.64it/s]Extractor Predicting: 163it [01:34,  1.72it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:29,338 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:29,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:29,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:29,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:29,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:38:29,960 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:38:29,960 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:38:30,662 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:38:31,715 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:38:31,715 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:34,558 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:34,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:34,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:34,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:34,563 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:38:35,764 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:38:35,764 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:38:36,394 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:38:36,568 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:38:36,568 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.77it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.77it/s]Extractor Predicting: 6it [00:03,  1.78it/s]Extractor Predicting: 7it [00:03,  1.80it/s]Extractor Predicting: 8it [00:04,  1.80it/s]Extractor Predicting: 9it [00:05,  1.83it/s]Extractor Predicting: 10it [00:05,  1.85it/s]Extractor Predicting: 11it [00:06,  1.86it/s]Extractor Predicting: 12it [00:06,  1.87it/s]Extractor Predicting: 13it [00:07,  1.85it/s]Extractor Predicting: 14it [00:07,  1.82it/s]Extractor Predicting: 15it [00:08,  1.82it/s]Extractor Predicting: 16it [00:08,  1.80it/s]Extractor Predicting: 17it [00:09,  1.78it/s]Extractor Predicting: 18it [00:09,  1.78it/s]Extractor Predicting: 19it [00:10,  1.83it/s]Extractor Predicting: 20it [00:11,  1.79it/s]Extractor Predicting: 21it [00:11,  1.80it/s]Extractor Predicting: 22it [00:12,  1.78it/s]Extractor Predicting: 23it [00:12,  1.77it/s]Extractor Predicting: 24it [00:13,  1.74it/s]Extractor Predicting: 25it [00:13,  1.75it/s]Extractor Predicting: 26it [00:14,  1.76it/s]Extractor Predicting: 27it [00:15,  1.81it/s]Extractor Predicting: 28it [00:15,  1.80it/s]Extractor Predicting: 29it [00:16,  1.79it/s]Extractor Predicting: 30it [00:16,  1.81it/s]Extractor Predicting: 31it [00:17,  1.80it/s]Extractor Predicting: 32it [00:17,  1.81it/s]Extractor Predicting: 33it [00:18,  1.79it/s]Extractor Predicting: 34it [00:18,  1.78it/s]Extractor Predicting: 35it [00:19,  1.81it/s]Extractor Predicting: 36it [00:20,  1.78it/s]Extractor Predicting: 37it [00:20,  1.78it/s]Extractor Predicting: 38it [00:21,  1.76it/s]Extractor Predicting: 39it [00:21,  1.82it/s]Extractor Predicting: 40it [00:22,  1.80it/s]Extractor Predicting: 41it [00:22,  1.71it/s]Extractor Predicting: 42it [00:23,  1.72it/s]Extractor Predicting: 43it [00:24,  1.71it/s]Extractor Predicting: 44it [00:24,  1.72it/s]Extractor Predicting: 45it [00:25,  1.69it/s]Extractor Predicting: 46it [00:25,  1.67it/s]Extractor Predicting: 47it [00:26,  1.65it/s]Extractor Predicting: 48it [00:27,  1.68it/s]Extractor Predicting: 49it [00:27,  1.51it/s]Extractor Predicting: 50it [00:28,  1.56it/s]Extractor Predicting: 51it [00:29,  1.60it/s]Extractor Predicting: 52it [00:29,  1.62it/s]Extractor Predicting: 53it [00:30,  1.61it/s]Extractor Predicting: 54it [00:30,  1.58it/s]Extractor Predicting: 55it [00:31,  1.63it/s]Extractor Predicting: 56it [00:32,  1.69it/s]Extractor Predicting: 57it [00:32,  1.67it/s]Extractor Predicting: 58it [00:33,  1.66it/s]Extractor Predicting: 59it [00:33,  1.63it/s]Extractor Predicting: 60it [00:34,  1.63it/s]Extractor Predicting: 61it [00:35,  1.66it/s]Extractor Predicting: 62it [00:35,  1.66it/s]Extractor Predicting: 63it [00:36,  1.60it/s]Extractor Predicting: 64it [00:37,  1.61it/s]Extractor Predicting: 65it [00:37,  1.62it/s]Extractor Predicting: 66it [00:38,  1.60it/s]Extractor Predicting: 67it [00:38,  1.62it/s]Extractor Predicting: 68it [00:39,  1.61it/s]Extractor Predicting: 69it [00:40,  1.61it/s]Extractor Predicting: 70it [00:40,  1.62it/s]Extractor Predicting: 71it [00:41,  1.65it/s]Extractor Predicting: 72it [00:41,  1.64it/s]Extractor Predicting: 73it [00:42,  1.63it/s]Extractor Predicting: 74it [00:43,  1.67it/s]Extractor Predicting: 75it [00:43,  1.67it/s]Extractor Predicting: 76it [00:44,  1.68it/s]Extractor Predicting: 77it [00:44,  1.71it/s]Extractor Predicting: 78it [00:45,  1.70it/s]Extractor Predicting: 79it [00:46,  1.65it/s]Extractor Predicting: 80it [00:46,  1.70it/s]Extractor Predicting: 81it [00:47,  1.55it/s]Extractor Predicting: 82it [00:48,  1.60it/s]Extractor Predicting: 83it [00:48,  1.63it/s]Extractor Predicting: 84it [00:49,  1.59it/s]Extractor Predicting: 85it [00:49,  1.62it/s]Extractor Predicting: 86it [00:50,  1.65it/s]Extractor Predicting: 87it [00:50,  1.69it/s]Extractor Predicting: 88it [00:51,  1.67it/s]Extractor Predicting: 89it [00:52,  1.72it/s]Extractor Predicting: 90it [00:52,  1.73it/s]Extractor Predicting: 91it [00:53,  1.76it/s]Extractor Predicting: 92it [00:53,  1.75it/s]Extractor Predicting: 93it [00:54,  1.75it/s]Extractor Predicting: 94it [00:54,  1.73it/s]Extractor Predicting: 95it [00:55,  1.75it/s]Extractor Predicting: 96it [00:56,  1.78it/s]Extractor Predicting: 97it [00:56,  1.78it/s]Extractor Predicting: 98it [00:57,  1.70it/s]Extractor Predicting: 99it [00:57,  1.71it/s]Extractor Predicting: 100it [00:58,  1.70it/s]Extractor Predicting: 101it [00:59,  1.69it/s]Extractor Predicting: 102it [00:59,  1.72it/s]Extractor Predicting: 103it [01:00,  1.69it/s]Extractor Predicting: 104it [01:00,  1.72it/s]Extractor Predicting: 105it [01:01,  1.71it/s]Extractor Predicting: 106it [01:02,  1.66it/s]Extractor Predicting: 107it [01:02,  1.64it/s]Extractor Predicting: 108it [01:03,  1.68it/s]Extractor Predicting: 109it [01:03,  1.62it/s]Extractor Predicting: 110it [01:04,  1.65it/s]Extractor Predicting: 111it [01:05,  1.63it/s]Extractor Predicting: 112it [01:05,  1.67it/s]Extractor Predicting: 113it [01:06,  1.66it/s]Extractor Predicting: 114it [01:06,  1.68it/s]Extractor Predicting: 115it [01:07,  1.67it/s]Extractor Predicting: 116it [01:08,  1.64it/s]Extractor Predicting: 117it [01:08,  1.66it/s]Extractor Predicting: 118it [01:09,  1.71it/s]Extractor Predicting: 119it [01:09,  1.71it/s]Extractor Predicting: 120it [01:10,  1.76it/s]Extractor Predicting: 121it [01:10,  1.74it/s]Extractor Predicting: 122it [01:11,  1.76it/s]Extractor Predicting: 123it [01:12,  1.79it/s]Extractor Predicting: 124it [01:12,  1.82it/s]Extractor Predicting: 125it [01:13,  1.79it/s]Extractor Predicting: 126it [01:13,  1.75it/s]Extractor Predicting: 127it [01:14,  1.74it/s]Extractor Predicting: 128it [01:15,  1.52it/s]Extractor Predicting: 129it [01:15,  1.57it/s]Extractor Predicting: 130it [01:16,  1.61it/s]Extractor Predicting: 131it [01:16,  1.70it/s]Extractor Predicting: 132it [01:17,  1.70it/s]Extractor Predicting: 133it [01:18,  1.71it/s]Extractor Predicting: 134it [01:18,  1.75it/s]Extractor Predicting: 135it [01:19,  1.75it/s]Extractor Predicting: 136it [01:19,  1.76it/s]Extractor Predicting: 137it [01:20,  1.81it/s]Extractor Predicting: 138it [01:20,  1.79it/s]Extractor Predicting: 139it [01:21,  1.78it/s]Extractor Predicting: 140it [01:21,  1.76it/s]Extractor Predicting: 141it [01:22,  1.77it/s]Extractor Predicting: 142it [01:23,  1.75it/s]Extractor Predicting: 143it [01:23,  1.72it/s]Extractor Predicting: 144it [01:24,  1.73it/s]Extractor Predicting: 145it [01:24,  1.68it/s]Extractor Predicting: 146it [01:25,  1.61it/s]Extractor Predicting: 147it [01:26,  1.65it/s]Extractor Predicting: 148it [01:26,  1.65it/s]Extractor Predicting: 149it [01:27,  1.68it/s]Extractor Predicting: 150it [01:27,  1.66it/s]Extractor Predicting: 151it [01:28,  1.66it/s]Extractor Predicting: 152it [01:29,  1.66it/s]Extractor Predicting: 153it [01:29,  1.70it/s]Extractor Predicting: 154it [01:30,  1.72it/s]Extractor Predicting: 155it [01:30,  1.76it/s]Extractor Predicting: 156it [01:31,  1.68it/s]Extractor Predicting: 157it [01:32,  1.70it/s]Extractor Predicting: 158it [01:32,  1.70it/s]Extractor Predicting: 159it [01:33,  1.70it/s]Extractor Predicting: 160it [01:33,  1.71it/s]Extractor Predicting: 161it [01:34,  1.62it/s]Extractor Predicting: 162it [01:35,  1.68it/s]Extractor Predicting: 163it [01:35,  1.71it/s]Extractor Predicting: 164it [01:36,  1.69it/s]Extractor Predicting: 165it [01:36,  1.76it/s]Extractor Predicting: 166it [01:37,  1.78it/s]Extractor Predicting: 167it [01:37,  1.76it/s]Extractor Predicting: 168it [01:38,  1.82it/s]Extractor Predicting: 169it [01:38,  1.83it/s]Extractor Predicting: 170it [01:39,  1.79it/s]Extractor Predicting: 171it [01:40,  1.78it/s]Extractor Predicting: 172it [01:40,  1.78it/s]Extractor Predicting: 173it [01:41,  1.74it/s]Extractor Predicting: 174it [01:41,  1.78it/s]Extractor Predicting: 175it [01:42,  1.76it/s]Extractor Predicting: 176it [01:42,  1.75it/s]Extractor Predicting: 177it [01:43,  1.72it/s]Extractor Predicting: 178it [01:44,  1.72it/s]Extractor Predicting: 179it [01:44,  1.81it/s]Extractor Predicting: 180it [01:45,  1.78it/s]Extractor Predicting: 181it [01:45,  1.77it/s]Extractor Predicting: 182it [01:46,  1.75it/s]Extractor Predicting: 183it [01:46,  1.75it/s]Extractor Predicting: 184it [01:47,  1.74it/s]Extractor Predicting: 185it [01:48,  1.79it/s]Extractor Predicting: 186it [01:48,  1.75it/s]Extractor Predicting: 187it [01:49,  1.74it/s]Extractor Predicting: 188it [01:49,  1.72it/s]Extractor Predicting: 189it [01:50,  1.71it/s]Extractor Predicting: 190it [01:50,  1.68it/s]Extractor Predicting: 191it [01:51,  1.68it/s]Extractor Predicting: 192it [01:52,  1.71it/s]Extractor Predicting: 193it [01:52,  1.75it/s]Extractor Predicting: 194it [01:53,  1.76it/s]Extractor Predicting: 195it [01:53,  1.73it/s]Extractor Predicting: 196it [01:54,  1.75it/s]Extractor Predicting: 197it [01:54,  1.75it/s]Extractor Predicting: 198it [01:55,  1.76it/s]Extractor Predicting: 199it [01:56,  1.75it/s]Extractor Predicting: 200it [01:56,  1.74it/s]Extractor Predicting: 201it [01:57,  1.75it/s]Extractor Predicting: 202it [01:57,  1.75it/s]Extractor Predicting: 203it [01:58,  1.75it/s]Extractor Predicting: 204it [01:58,  1.76it/s]Extractor Predicting: 205it [01:59,  1.74it/s]Extractor Predicting: 206it [02:00,  1.73it/s]Extractor Predicting: 207it [02:00,  1.76it/s]Extractor Predicting: 208it [02:01,  1.76it/s]Extractor Predicting: 209it [02:01,  1.77it/s]Extractor Predicting: 210it [02:02,  1.76it/s]Extractor Predicting: 211it [02:02,  1.74it/s]Extractor Predicting: 212it [02:03,  1.76it/s]Extractor Predicting: 213it [02:04,  1.75it/s]Extractor Predicting: 214it [02:04,  1.75it/s]Extractor Predicting: 215it [02:05,  1.70it/s]Extractor Predicting: 216it [02:05,  1.73it/s]Extractor Predicting: 217it [02:06,  1.76it/s]Extractor Predicting: 218it [02:06,  1.77it/s]Extractor Predicting: 219it [02:07,  1.74it/s]Extractor Predicting: 220it [02:08,  1.54it/s]Extractor Predicting: 221it [02:08,  1.57it/s]Extractor Predicting: 222it [02:09,  1.62it/s]Extractor Predicting: 223it [02:10,  1.58it/s]Extractor Predicting: 224it [02:10,  1.61it/s]Extractor Predicting: 225it [02:11,  1.60it/s]Extractor Predicting: 226it [02:12,  1.62it/s]Extractor Predicting: 227it [02:12,  1.59it/s]Extractor Predicting: 228it [02:13,  1.53it/s]Extractor Predicting: 229it [02:14,  1.56it/s]Extractor Predicting: 230it [02:14,  1.58it/s]Extractor Predicting: 231it [02:15,  1.59it/s]Extractor Predicting: 232it [02:15,  1.59it/s]Extractor Predicting: 233it [02:16,  1.60it/s]Extractor Predicting: 234it [02:17,  1.62it/s]Extractor Predicting: 235it [02:17,  1.64it/s]Extractor Predicting: 236it [02:18,  1.65it/s]Extractor Predicting: 237it [02:18,  1.65it/s]Extractor Predicting: 238it [02:19,  1.72it/s]Extractor Predicting: 239it [02:19,  1.75it/s]Extractor Predicting: 240it [02:20,  1.81it/s]Extractor Predicting: 241it [02:21,  1.80it/s]Extractor Predicting: 242it [02:21,  1.79it/s]Extractor Predicting: 243it [02:22,  1.74it/s]Extractor Predicting: 244it [02:22,  1.75it/s]Extractor Predicting: 245it [02:23,  1.78it/s]Extractor Predicting: 246it [02:23,  1.75it/s]Extractor Predicting: 247it [02:24,  1.77it/s]Extractor Predicting: 248it [02:25,  1.78it/s]Extractor Predicting: 249it [02:25,  1.84it/s]Extractor Predicting: 250it [02:26,  1.83it/s]Extractor Predicting: 251it [02:26,  1.90it/s]Extractor Predicting: 252it [02:27,  1.92it/s]Extractor Predicting: 253it [02:27,  1.88it/s]Extractor Predicting: 254it [02:28,  1.89it/s]Extractor Predicting: 255it [02:28,  1.82it/s]Extractor Predicting: 256it [02:29,  1.86it/s]Extractor Predicting: 257it [02:29,  1.82it/s]Extractor Predicting: 258it [02:30,  1.78it/s]Extractor Predicting: 259it [02:31,  1.72it/s]Extractor Predicting: 260it [02:31,  1.79it/s]Extractor Predicting: 261it [02:32,  1.76it/s]Extractor Predicting: 262it [02:32,  1.78it/s]Extractor Predicting: 263it [02:33,  1.83it/s]Extractor Predicting: 264it [02:33,  1.85it/s]Extractor Predicting: 265it [02:34,  1.84it/s]Extractor Predicting: 266it [02:34,  1.87it/s]Extractor Predicting: 267it [02:35,  1.90it/s]Extractor Predicting: 268it [02:35,  1.89it/s]Extractor Predicting: 269it [02:36,  1.84it/s]Extractor Predicting: 270it [02:36,  1.86it/s]Extractor Predicting: 271it [02:37,  1.85it/s]Extractor Predicting: 272it [02:38,  1.82it/s]Extractor Predicting: 273it [02:38,  1.89it/s]Extractor Predicting: 274it [02:39,  1.90it/s]Extractor Predicting: 275it [02:39,  1.87it/s]Extractor Predicting: 276it [02:40,  1.85it/s]Extractor Predicting: 277it [02:40,  1.85it/s]Extractor Predicting: 278it [02:41,  1.88it/s]Extractor Predicting: 279it [02:41,  1.82it/s]Extractor Predicting: 280it [02:42,  1.89it/s]Extractor Predicting: 281it [02:42,  1.91it/s]Extractor Predicting: 282it [02:43,  1.92it/s]Extractor Predicting: 283it [02:43,  1.92it/s]Extractor Predicting: 284it [02:44,  1.92it/s]Extractor Predicting: 285it [02:44,  1.92it/s]Extractor Predicting: 286it [02:45,  1.91it/s]Extractor Predicting: 287it [02:45,  1.91it/s]Extractor Predicting: 288it [02:46,  1.86it/s]Extractor Predicting: 289it [02:47,  1.86it/s]Extractor Predicting: 290it [02:47,  1.77it/s]Extractor Predicting: 291it [02:48,  1.80it/s]Extractor Predicting: 292it [02:48,  1.85it/s]Extractor Predicting: 293it [02:49,  1.86it/s]Extractor Predicting: 294it [02:49,  1.88it/s]Extractor Predicting: 295it [02:50,  1.89it/s]Extractor Predicting: 296it [02:50,  1.78it/s]Extractor Predicting: 297it [02:51,  1.77it/s]Extractor Predicting: 298it [02:52,  1.70it/s]Extractor Predicting: 299it [02:52,  1.70it/s]Extractor Predicting: 300it [02:53,  1.73it/s]Extractor Predicting: 301it [02:53,  1.73it/s]Extractor Predicting: 302it [02:54,  1.74it/s]Extractor Predicting: 303it [02:55,  1.70it/s]Extractor Predicting: 304it [02:55,  1.71it/s]Extractor Predicting: 305it [02:56,  1.75it/s]Extractor Predicting: 306it [02:56,  1.70it/s]Extractor Predicting: 307it [02:57,  1.66it/s]Extractor Predicting: 308it [02:58,  1.66it/s]Extractor Predicting: 309it [02:58,  1.70it/s]Extractor Predicting: 310it [02:59,  1.74it/s]Extractor Predicting: 311it [02:59,  1.75it/s]Extractor Predicting: 312it [03:00,  1.73it/s]Extractor Predicting: 313it [03:00,  1.68it/s]Extractor Predicting: 314it [03:01,  1.66it/s]Extractor Predicting: 315it [03:02,  1.66it/s]Extractor Predicting: 316it [03:02,  1.66it/s]Extractor Predicting: 317it [03:03,  1.67it/s]Extractor Predicting: 318it [03:03,  1.68it/s]Extractor Predicting: 319it [03:04,  1.69it/s]Extractor Predicting: 320it [03:05,  1.70it/s]Extractor Predicting: 321it [03:05,  1.77it/s]Extractor Predicting: 322it [03:06,  1.54it/s]Extractor Predicting: 323it [03:07,  1.57it/s]Extractor Predicting: 324it [03:07,  1.59it/s]Extractor Predicting: 325it [03:08,  1.62it/s]Extractor Predicting: 326it [03:08,  1.62it/s]Extractor Predicting: 327it [03:09,  1.61it/s]Extractor Predicting: 328it [03:10,  1.62it/s]Extractor Predicting: 329it [03:10,  1.68it/s]Extractor Predicting: 330it [03:11,  1.72it/s]Extractor Predicting: 331it [03:11,  1.91it/s]Extractor Predicting: 331it [03:11,  1.73it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:41:56,661 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:41:56,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:41:56,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:41:56,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:41:56,669 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:41:57,302 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:41:57,302 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:41:57,869 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:41:58,915 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:41:58,915 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:01,799 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:01,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:01,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:01,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:42:01,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:42:02,444 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:42:02,445 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:42:03,005 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:42:03,180 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:42:03,180 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:05,  1.64it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.64it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:08,  1.65it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:11,  1.67it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:12,  1.60it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.57it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.64it/s]Extractor Predicting: 33it [00:20,  1.66it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:22,  1.67it/s]Extractor Predicting: 38it [00:23,  1.68it/s]Extractor Predicting: 39it [00:24,  1.62it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.65it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.67it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:28,  1.69it/s]Extractor Predicting: 47it [00:28,  1.66it/s]Extractor Predicting: 48it [00:29,  1.71it/s]Extractor Predicting: 49it [00:29,  1.72it/s]Extractor Predicting: 50it [00:30,  1.75it/s]Extractor Predicting: 51it [00:31,  1.75it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:32,  1.64it/s]Extractor Predicting: 54it [00:32,  1.69it/s]Extractor Predicting: 55it [00:33,  1.71it/s]Extractor Predicting: 56it [00:33,  1.79it/s]Extractor Predicting: 57it [00:34,  1.84it/s]Extractor Predicting: 58it [00:34,  1.90it/s]Extractor Predicting: 59it [00:35,  2.00it/s]Extractor Predicting: 60it [00:35,  2.09it/s]Extractor Predicting: 61it [00:36,  2.14it/s]Extractor Predicting: 62it [00:36,  2.13it/s]Extractor Predicting: 63it [00:37,  2.15it/s]Extractor Predicting: 64it [00:37,  2.13it/s]Extractor Predicting: 65it [00:38,  2.11it/s]Extractor Predicting: 66it [00:38,  2.10it/s]Extractor Predicting: 67it [00:39,  2.09it/s]Extractor Predicting: 68it [00:39,  2.11it/s]Extractor Predicting: 69it [00:39,  2.15it/s]Extractor Predicting: 70it [00:40,  2.10it/s]Extractor Predicting: 71it [00:40,  2.11it/s]Extractor Predicting: 72it [00:41,  2.14it/s]Extractor Predicting: 73it [00:41,  2.18it/s]Extractor Predicting: 74it [00:42,  2.20it/s]Extractor Predicting: 75it [00:42,  2.18it/s]Extractor Predicting: 76it [00:43,  2.15it/s]Extractor Predicting: 77it [00:43,  2.22it/s]Extractor Predicting: 78it [00:44,  2.14it/s]Extractor Predicting: 79it [00:44,  2.13it/s]Extractor Predicting: 80it [00:45,  2.08it/s]Extractor Predicting: 81it [00:45,  2.08it/s]Extractor Predicting: 82it [00:46,  2.11it/s]Extractor Predicting: 83it [00:46,  2.12it/s]Extractor Predicting: 84it [00:47,  2.13it/s]Extractor Predicting: 85it [00:47,  2.15it/s]Extractor Predicting: 86it [00:48,  1.97it/s]Extractor Predicting: 87it [00:48,  1.92it/s]Extractor Predicting: 88it [00:49,  1.88it/s]Extractor Predicting: 89it [00:49,  1.85it/s]Extractor Predicting: 90it [00:50,  1.87it/s]Extractor Predicting: 91it [00:50,  1.83it/s]Extractor Predicting: 92it [00:51,  1.82it/s]Extractor Predicting: 93it [00:52,  1.63it/s]Extractor Predicting: 94it [00:52,  1.66it/s]Extractor Predicting: 95it [00:53,  1.73it/s]Extractor Predicting: 96it [00:53,  1.76it/s]Extractor Predicting: 97it [00:54,  1.78it/s]Extractor Predicting: 98it [00:54,  1.80it/s]Extractor Predicting: 99it [00:55,  1.79it/s]Extractor Predicting: 100it [00:56,  1.73it/s]Extractor Predicting: 101it [00:56,  1.77it/s]Extractor Predicting: 102it [00:57,  1.75it/s]Extractor Predicting: 103it [00:57,  1.70it/s]Extractor Predicting: 104it [00:58,  1.69it/s]Extractor Predicting: 105it [00:59,  1.68it/s]Extractor Predicting: 106it [00:59,  1.67it/s]Extractor Predicting: 107it [01:00,  1.64it/s]Extractor Predicting: 108it [01:00,  1.57it/s]Extractor Predicting: 108it [01:00,  1.77it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_10_seed_0', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_10_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:56, 16.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:34, 16.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:49<03:15, 16.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:07<03:07, 17.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:24<02:49, 16.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:39<02:27, 16.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:57<02:15, 16.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:15<02:00, 17.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:30<01:39, 16.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:45<01:20, 16.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:00<01:03, 15.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:20<00:50, 16.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:37<00:34, 17.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:53<00:16, 16.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:11<00:00, 17.05s/it]Generating: 100%|██████████| 15/15 [04:11<00:00, 16.76s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : characters . Context : Later in Life , he played the title character , a young princess of the family at the end of the third season of HBO s Game of Thrones . Head Entity : Game of Thrones , Tail Entity : Princess of the family .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 578, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : characters .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 441, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 578, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 230, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 305, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 401, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 477, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 532, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 603, 'raw': 800}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.75375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 313, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 484, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 528, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 579, 'raw': 768}
{'target': 600, 'success': 602, 'raw': 800}
{'prompt': 'Relation : made from material .', 'success_rate': 0.7525, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('pharaoh Amenhotep II', 'made from material', '', 'He is best remembered for the painting of the pharaoh Amenhotep II , composed by Alfred Wohl in 1827 for a French conservatory .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 624, 'raw': 800}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.78, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 351, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : cast member .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Billboard Pop Hits', 'cast member', '', 'On October 25 , 1978 , the band released the track The Lads , the debut single by the British band The Riveters ( now defunct ) , on Columbia Records , along with several appearances on the Billboard Pop Hits chart for June .')"}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 91, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 256, 'raw': 352}
{'target': 600, 'success': 278, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 390, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 433, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 506, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 553, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : follows .', 'success_rate': 0.7283653846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Nigeria', 'follows', '', 'In 2013 , a second poll showed Zaire had a 7 % approval rating , while in 2014 , Nigeria had a 24 % approval rating .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 150, 'raw': 224}
{'target': 600, 'success': 171, 'raw': 256}
{'target': 600, 'success': 189, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 246, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 281, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 347, 'raw': 544}
{'target': 600, 'success': 366, 'raw': 576}
{'target': 600, 'success': 391, 'raw': 608}
{'target': 600, 'success': 410, 'raw': 640}
{'target': 600, 'success': 431, 'raw': 672}
{'target': 600, 'success': 452, 'raw': 704}
{'target': 600, 'success': 470, 'raw': 736}
{'target': 600, 'success': 490, 'raw': 768}
{'target': 600, 'success': 511, 'raw': 800}
{'target': 600, 'success': 531, 'raw': 832}
{'target': 600, 'success': 553, 'raw': 864}
{'target': 600, 'success': 572, 'raw': 896}
{'target': 600, 'success': 590, 'raw': 928}
{'target': 600, 'success': 608, 'raw': 960}
{'prompt': 'Relation : league .', 'success_rate': 0.6333333333333333, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 309, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 361, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 612, 'raw': 800}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.765, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 555, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 608, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7916666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : mother . Context : Later in Life , he studied with a group of Latin philologists and other thinkers at the Renaissance in Italy , in Naples from 1635 to 1645 , and from 1654 back to 1701 . Head Entity : Naples , Tail Entity : Rome .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 86, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 153, 'raw': 224}
{'target': 600, 'success': 175, 'raw': 256}
{'target': 600, 'success': 197, 'raw': 288}
{'target': 600, 'success': 218, 'raw': 320}
{'target': 600, 'success': 242, 'raw': 352}
{'target': 600, 'success': 262, 'raw': 384}
{'target': 600, 'success': 284, 'raw': 416}
{'target': 600, 'success': 305, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 341, 'raw': 512}
{'target': 600, 'success': 368, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 412, 'raw': 608}
{'target': 600, 'success': 437, 'raw': 640}
{'target': 600, 'success': 463, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 501, 'raw': 736}
{'target': 600, 'success': 524, 'raw': 768}
{'target': 600, 'success': 547, 'raw': 800}
{'target': 600, 'success': 564, 'raw': 832}
{'target': 600, 'success': 584, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : mother .', 'success_rate': 0.6729910714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 351, 'raw': 480}
{'target': 600, 'success': 375, 'raw': 512}
{'target': 600, 'success': 400, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 445, 'raw': 608}
{'target': 600, 'success': 471, 'raw': 640}
{'target': 600, 'success': 488, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 536, 'raw': 736}
{'target': 600, 'success': 563, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : residence .', 'success_rate': 0.7283653846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 344, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 390, 'raw': 512}
{'target': 600, 'success': 417, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 464, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 533, 'raw': 704}
{'target': 600, 'success': 553, 'raw': 736}
{'target': 600, 'success': 573, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.7439903846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1453 the head of the province of Ghent and of neighboring districts at Neuromö , in Burgundesland , were also under the Ottoman Empire . Head Entity : Burgundesland , Tail Entity : Ottomans .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 434, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 479, 'raw': 640}
{'target': 600, 'success': 502, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 586, 'raw': 800}
{'target': 600, 'success': 606, 'raw': 832}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7283653846153846, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 15273
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15373, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_10_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.67s/it]Extractor Estimating: 2it [00:18,  8.00s/it]Extractor Estimating: 3it [00:19,  4.63s/it]Extractor Estimating: 4it [00:20,  3.05s/it]Extractor Estimating: 5it [00:20,  2.17s/it]Extractor Estimating: 6it [00:22,  2.04s/it]Extractor Estimating: 7it [00:23,  1.57s/it]Extractor Estimating: 8it [00:24,  1.45s/it]Extractor Estimating: 9it [00:24,  1.19s/it]Extractor Estimating: 10it [00:25,  1.03s/it]Extractor Estimating: 11it [00:26,  1.06it/s]Extractor Estimating: 12it [00:26,  1.19it/s]Extractor Estimating: 13it [00:27,  1.28it/s]Extractor Estimating: 14it [00:28,  1.25it/s]Extractor Estimating: 15it [00:29,  1.33it/s]Extractor Estimating: 16it [00:29,  1.39it/s]Extractor Estimating: 17it [00:30,  1.44it/s]Extractor Estimating: 18it [00:31,  1.50it/s]Extractor Estimating: 19it [00:31,  1.49it/s]Extractor Estimating: 20it [00:32,  1.55it/s]Extractor Estimating: 21it [00:32,  1.56it/s]Extractor Estimating: 22it [00:33,  1.64it/s]Extractor Estimating: 23it [00:34,  1.58it/s]Extractor Estimating: 24it [00:34,  1.50it/s]Extractor Estimating: 25it [00:35,  1.52it/s]Extractor Estimating: 26it [00:36,  1.56it/s]Extractor Estimating: 27it [00:36,  1.57it/s]Extractor Estimating: 28it [00:37,  1.62it/s]Extractor Estimating: 29it [00:37,  1.61it/s]Extractor Estimating: 30it [00:38,  1.64it/s]Extractor Estimating: 31it [00:39,  1.60it/s]Extractor Estimating: 32it [00:39,  1.62it/s]Extractor Estimating: 33it [00:40,  1.54it/s]Extractor Estimating: 34it [00:41,  1.52it/s]Extractor Estimating: 35it [00:41,  1.53it/s]Extractor Estimating: 36it [00:42,  1.52it/s]Extractor Estimating: 37it [00:43,  1.54it/s]Extractor Estimating: 38it [00:43,  1.55it/s]Extractor Estimating: 39it [00:44,  1.52it/s]Extractor Estimating: 40it [00:45,  1.52it/s]Extractor Estimating: 41it [00:45,  1.55it/s]Extractor Estimating: 42it [00:46,  1.55it/s]Extractor Estimating: 43it [00:48,  1.01s/it]Extractor Estimating: 44it [00:48,  1.11it/s]Extractor Estimating: 45it [00:49,  1.20it/s]Extractor Estimating: 46it [00:50,  1.31it/s]Extractor Estimating: 47it [00:50,  1.38it/s]Extractor Estimating: 48it [00:51,  1.43it/s]Extractor Estimating: 49it [00:52,  1.45it/s]Extractor Estimating: 50it [00:52,  1.43it/s]Extractor Estimating: 51it [00:53,  1.45it/s]Extractor Estimating: 52it [00:54,  1.54it/s]Extractor Estimating: 53it [00:54,  1.57it/s]Extractor Estimating: 54it [00:55,  1.56it/s]Extractor Estimating: 55it [00:55,  1.54it/s]Extractor Estimating: 56it [00:56,  1.57it/s]Extractor Estimating: 57it [00:57,  1.55it/s]Extractor Estimating: 58it [00:57,  1.57it/s]Extractor Estimating: 59it [00:58,  1.57it/s]Extractor Estimating: 60it [00:59,  1.58it/s]Extractor Estimating: 61it [00:59,  1.58it/s]Extractor Estimating: 62it [01:00,  1.61it/s]Extractor Estimating: 63it [01:00,  1.62it/s]Extractor Estimating: 64it [01:01,  1.64it/s]Extractor Estimating: 65it [01:02,  1.64it/s]Extractor Estimating: 66it [01:02,  1.61it/s]Extractor Estimating: 67it [01:03,  1.62it/s]Extractor Estimating: 68it [01:04,  1.64it/s]Extractor Estimating: 69it [01:04,  1.64it/s]Extractor Estimating: 70it [01:05,  1.63it/s]Extractor Estimating: 71it [01:05,  1.56it/s]Extractor Estimating: 72it [01:06,  1.59it/s]Extractor Estimating: 73it [01:07,  1.62it/s]Extractor Estimating: 74it [01:07,  1.60it/s]Extractor Estimating: 75it [01:08,  1.58it/s]Extractor Estimating: 76it [01:09,  1.55it/s]Extractor Estimating: 77it [01:09,  1.57it/s]Extractor Estimating: 78it [01:10,  1.58it/s]Extractor Estimating: 79it [01:11,  1.51it/s]Extractor Estimating: 80it [01:11,  1.45it/s]Extractor Estimating: 81it [01:12,  1.49it/s]Extractor Estimating: 82it [01:13,  1.46it/s]Extractor Estimating: 83it [01:13,  1.53it/s]Extractor Estimating: 84it [01:14,  1.53it/s]Extractor Estimating: 85it [01:15,  1.56it/s]Extractor Estimating: 86it [01:15,  1.47it/s]Extractor Estimating: 87it [01:16,  1.51it/s]Extractor Estimating: 88it [01:17,  1.56it/s]Extractor Estimating: 89it [01:17,  1.55it/s]Extractor Estimating: 90it [01:18,  1.54it/s]Extractor Estimating: 91it [01:18,  1.57it/s]Extractor Estimating: 92it [01:19,  1.56it/s]Extractor Estimating: 93it [01:20,  1.55it/s]Extractor Estimating: 94it [01:20,  1.55it/s]Extractor Estimating: 95it [01:21,  1.54it/s]Extractor Estimating: 96it [01:22,  1.56it/s]Extractor Estimating: 97it [01:22,  1.57it/s]Extractor Estimating: 98it [01:23,  1.55it/s]Extractor Estimating: 99it [01:24,  1.47it/s]Extractor Estimating: 100it [01:24,  1.52it/s]Extractor Estimating: 101it [01:25,  1.49it/s]Extractor Estimating: 102it [01:26,  1.52it/s]Extractor Estimating: 103it [01:26,  1.56it/s]Extractor Estimating: 104it [01:27,  1.57it/s]Extractor Estimating: 105it [01:28,  1.57it/s]Extractor Estimating: 106it [01:28,  1.58it/s]Extractor Estimating: 107it [01:29,  1.61it/s]Extractor Estimating: 108it [01:29,  1.63it/s]Extractor Estimating: 109it [01:30,  1.64it/s]Extractor Estimating: 110it [01:31,  1.58it/s]Extractor Estimating: 111it [01:31,  1.62it/s]Extractor Estimating: 112it [01:32,  1.60it/s]Extractor Estimating: 113it [01:32,  1.62it/s]Extractor Estimating: 114it [01:33,  1.62it/s]Extractor Estimating: 115it [01:34,  1.63it/s]Extractor Estimating: 116it [01:34,  1.57it/s]Extractor Estimating: 117it [01:35,  1.61it/s]Extractor Estimating: 118it [01:36,  1.56it/s]Extractor Estimating: 119it [01:36,  1.59it/s]Extractor Estimating: 120it [01:37,  1.53it/s]Extractor Estimating: 121it [01:38,  1.51it/s]Extractor Estimating: 122it [01:38,  1.52it/s]Extractor Estimating: 123it [01:39,  1.56it/s]Extractor Estimating: 124it [01:40,  1.55it/s]Extractor Estimating: 125it [01:41,  1.18it/s]Extractor Estimating: 126it [01:41,  1.26it/s]Extractor Estimating: 127it [01:42,  1.32it/s]Extractor Estimating: 128it [01:43,  1.43it/s]Extractor Estimating: 129it [01:43,  1.52it/s]Extractor Estimating: 130it [01:44,  1.51it/s]Extractor Estimating: 131it [01:45,  1.59it/s]Extractor Estimating: 132it [01:45,  1.57it/s]Extractor Estimating: 133it [01:46,  1.58it/s]Extractor Estimating: 134it [01:46,  1.57it/s]Extractor Estimating: 135it [01:47,  1.54it/s]Extractor Estimating: 136it [01:48,  1.49it/s]Extractor Estimating: 137it [01:49,  1.47it/s]Extractor Estimating: 138it [01:49,  1.52it/s]Extractor Estimating: 139it [01:50,  1.46it/s]Extractor Estimating: 140it [01:51,  1.51it/s]Extractor Estimating: 141it [01:51,  1.58it/s]Extractor Estimating: 142it [01:52,  1.56it/s]Extractor Estimating: 143it [01:52,  1.60it/s]Extractor Estimating: 144it [01:53,  1.63it/s]Extractor Estimating: 145it [01:54,  1.55it/s]Extractor Estimating: 146it [01:54,  1.54it/s]Extractor Estimating: 147it [01:55,  1.58it/s]Extractor Estimating: 148it [01:55,  1.62it/s]Extractor Estimating: 149it [01:56,  1.60it/s]Extractor Estimating: 150it [01:57,  1.57it/s]Extractor Estimating: 151it [01:57,  1.57it/s]Extractor Estimating: 152it [01:58,  1.55it/s]Extractor Estimating: 153it [01:59,  1.54it/s]Extractor Estimating: 154it [02:00,  1.44it/s]Extractor Estimating: 155it [02:00,  1.48it/s]Extractor Estimating: 156it [02:01,  1.53it/s]Extractor Estimating: 157it [02:01,  1.50it/s]Extractor Estimating: 158it [02:02,  1.52it/s]Extractor Estimating: 159it [02:03,  1.52it/s]Extractor Estimating: 160it [02:03,  1.51it/s]Extractor Estimating: 161it [02:04,  1.47it/s]Extractor Estimating: 162it [02:05,  1.52it/s]Extractor Estimating: 163it [02:05,  1.50it/s]Extractor Estimating: 164it [02:06,  1.52it/s]Extractor Estimating: 165it [02:07,  1.56it/s]Extractor Estimating: 166it [02:07,  1.55it/s]Extractor Estimating: 167it [02:08,  1.56it/s]Extractor Estimating: 168it [02:09,  1.44it/s]Extractor Estimating: 169it [02:10,  1.44it/s]Extractor Estimating: 170it [02:10,  1.48it/s]Extractor Estimating: 171it [02:11,  1.50it/s]Extractor Estimating: 172it [02:11,  1.54it/s]Extractor Estimating: 173it [02:12,  1.55it/s]Extractor Estimating: 174it [02:13,  1.55it/s]Extractor Estimating: 175it [02:13,  1.53it/s]Extractor Estimating: 176it [02:14,  1.57it/s]Extractor Estimating: 177it [02:15,  1.52it/s]Extractor Estimating: 178it [02:15,  1.57it/s]Extractor Estimating: 179it [02:16,  1.57it/s]Extractor Estimating: 180it [02:17,  1.58it/s]Extractor Estimating: 181it [02:17,  1.58it/s]Extractor Estimating: 182it [02:18,  1.61it/s]Extractor Estimating: 183it [02:18,  1.62it/s]Extractor Estimating: 184it [02:19,  1.61it/s]Extractor Estimating: 185it [02:20,  1.60it/s]Extractor Estimating: 186it [02:20,  1.52it/s]Extractor Estimating: 187it [02:21,  1.58it/s]Extractor Estimating: 188it [02:21,  1.62it/s]Extractor Estimating: 189it [02:22,  1.61it/s]Extractor Estimating: 190it [02:23,  1.58it/s]Extractor Estimating: 191it [02:23,  1.61it/s]Extractor Estimating: 192it [02:24,  1.57it/s]Extractor Estimating: 193it [02:25,  1.64it/s]Extractor Estimating: 194it [02:25,  1.61it/s]Extractor Estimating: 195it [02:26,  1.60it/s]Extractor Estimating: 196it [02:27,  1.59it/s]Extractor Estimating: 197it [02:27,  1.63it/s]Extractor Estimating: 198it [02:28,  1.61it/s]Extractor Estimating: 199it [02:28,  1.61it/s]Extractor Estimating: 200it [02:29,  1.62it/s]Extractor Estimating: 201it [02:30,  1.61it/s]Extractor Estimating: 202it [02:30,  1.66it/s]Extractor Estimating: 203it [02:31,  1.66it/s]Extractor Estimating: 204it [02:31,  1.69it/s]Extractor Estimating: 205it [02:32,  1.72it/s]Extractor Estimating: 206it [02:32,  1.70it/s]Extractor Estimating: 207it [02:33,  1.69it/s]Extractor Estimating: 208it [02:34,  1.72it/s]Extractor Estimating: 209it [02:34,  1.67it/s]Extractor Estimating: 210it [02:35,  1.63it/s]Extractor Estimating: 211it [02:36,  1.59it/s]Extractor Estimating: 212it [02:36,  1.63it/s]Extractor Estimating: 213it [02:37,  1.60it/s]Extractor Estimating: 214it [02:37,  1.66it/s]Extractor Estimating: 215it [02:38,  1.70it/s]Extractor Estimating: 216it [02:39,  1.68it/s]Extractor Estimating: 217it [02:39,  1.73it/s]Extractor Estimating: 218it [02:40,  1.70it/s]Extractor Estimating: 219it [02:40,  1.70it/s]Extractor Estimating: 220it [02:41,  1.66it/s]Extractor Estimating: 221it [02:41,  1.70it/s]Extractor Estimating: 222it [02:42,  1.71it/s]Extractor Estimating: 223it [02:43,  1.67it/s]Extractor Estimating: 224it [02:43,  1.67it/s]Extractor Estimating: 225it [02:44,  1.71it/s]Extractor Estimating: 226it [02:44,  1.73it/s]Extractor Estimating: 227it [02:45,  1.76it/s]Extractor Estimating: 228it [02:45,  1.78it/s]Extractor Estimating: 229it [02:46,  1.82it/s]Extractor Estimating: 230it [02:47,  1.84it/s]Extractor Estimating: 231it [02:47,  1.81it/s]Extractor Estimating: 232it [02:48,  1.75it/s]Extractor Estimating: 233it [02:48,  1.59it/s]Extractor Estimating: 234it [02:49,  1.68it/s]Extractor Estimating: 235it [02:50,  1.67it/s]Extractor Estimating: 236it [02:50,  1.69it/s]Extractor Estimating: 237it [02:51,  1.71it/s]Extractor Estimating: 238it [02:51,  1.69it/s]Extractor Estimating: 239it [02:52,  1.67it/s]Extractor Estimating: 240it [02:53,  1.70it/s]Extractor Estimating: 241it [02:53,  1.76it/s]Extractor Estimating: 242it [02:54,  1.75it/s]Extractor Estimating: 243it [02:54,  1.74it/s]Extractor Estimating: 244it [02:55,  1.71it/s]Extractor Estimating: 245it [02:55,  1.72it/s]Extractor Estimating: 246it [02:56,  1.73it/s]Extractor Estimating: 247it [02:57,  1.67it/s]Extractor Estimating: 248it [02:57,  1.68it/s]Extractor Estimating: 249it [02:58,  1.71it/s]Extractor Estimating: 250it [02:58,  1.72it/s]Extractor Estimating: 251it [02:59,  1.70it/s]Extractor Estimating: 252it [03:00,  1.67it/s]Extractor Estimating: 253it [03:00,  1.64it/s]Extractor Estimating: 254it [03:01,  1.64it/s]Extractor Estimating: 255it [03:01,  1.61it/s]Extractor Estimating: 256it [03:02,  1.61it/s]Extractor Estimating: 257it [03:03,  1.57it/s]Extractor Estimating: 258it [03:03,  1.58it/s]Extractor Estimating: 259it [03:04,  1.61it/s]Extractor Estimating: 260it [03:05,  1.60it/s]Extractor Estimating: 261it [03:05,  1.59it/s]Extractor Estimating: 262it [03:06,  1.60it/s]Extractor Estimating: 263it [03:06,  1.60it/s]Extractor Estimating: 264it [03:07,  1.62it/s]Extractor Estimating: 265it [03:08,  1.64it/s]Extractor Estimating: 266it [03:08,  1.64it/s]Extractor Estimating: 267it [03:09,  1.62it/s]Extractor Estimating: 268it [03:10,  1.59it/s]Extractor Estimating: 269it [03:10,  1.63it/s]Extractor Estimating: 270it [03:11,  1.66it/s]Extractor Estimating: 271it [03:11,  1.67it/s]Extractor Estimating: 272it [03:12,  1.67it/s]Extractor Estimating: 273it [03:13,  1.62it/s]Extractor Estimating: 274it [03:13,  1.61it/s]Extractor Estimating: 275it [03:14,  1.62it/s]Extractor Estimating: 276it [03:14,  1.58it/s]Extractor Estimating: 277it [03:15,  1.51it/s]Extractor Estimating: 278it [03:16,  1.49it/s]Extractor Estimating: 279it [03:17,  1.49it/s]Extractor Estimating: 280it [03:17,  1.54it/s]Extractor Estimating: 281it [03:18,  1.51it/s]Extractor Estimating: 282it [03:19,  1.52it/s]Extractor Estimating: 283it [03:19,  1.51it/s]Extractor Estimating: 284it [03:20,  1.46it/s]Extractor Estimating: 285it [03:21,  1.52it/s]Extractor Estimating: 286it [03:21,  1.55it/s]Extractor Estimating: 287it [03:22,  1.50it/s]Extractor Estimating: 288it [03:22,  1.52it/s]Extractor Estimating: 289it [03:23,  1.49it/s]Extractor Estimating: 290it [03:24,  1.50it/s]Extractor Estimating: 291it [03:24,  1.52it/s]Extractor Estimating: 292it [03:25,  1.51it/s]Extractor Estimating: 293it [03:26,  1.57it/s]Extractor Estimating: 294it [03:26,  1.55it/s]Extractor Estimating: 295it [03:27,  1.52it/s]Extractor Estimating: 296it [03:28,  1.54it/s]Extractor Estimating: 297it [03:28,  1.53it/s]Extractor Estimating: 298it [03:29,  1.51it/s]Extractor Estimating: 299it [03:30,  1.51it/s]Extractor Estimating: 300it [03:30,  1.53it/s]Extractor Estimating: 301it [03:31,  1.52it/s]Extractor Estimating: 302it [03:32,  1.56it/s]Extractor Estimating: 303it [03:32,  1.50it/s]Extractor Estimating: 304it [03:33,  1.52it/s]Extractor Estimating: 305it [03:34,  1.58it/s]Extractor Estimating: 306it [03:34,  1.62it/s]Extractor Estimating: 307it [03:35,  1.60it/s]Extractor Estimating: 308it [03:35,  1.58it/s]Extractor Estimating: 309it [03:36,  1.60it/s]Extractor Estimating: 310it [03:37,  1.61it/s]Extractor Estimating: 311it [03:37,  1.53it/s]Extractor Estimating: 312it [03:38,  1.39it/s]Extractor Estimating: 313it [03:39,  1.44it/s]Extractor Estimating: 314it [03:40,  1.50it/s]Extractor Estimating: 315it [03:40,  1.55it/s]Extractor Estimating: 316it [03:41,  1.48it/s]Extractor Estimating: 317it [03:42,  1.49it/s]Extractor Estimating: 318it [03:42,  1.51it/s]Extractor Estimating: 319it [03:43,  1.48it/s]Extractor Estimating: 320it [03:43,  1.52it/s]Extractor Estimating: 321it [03:44,  1.50it/s]Extractor Estimating: 322it [03:45,  1.44it/s]Extractor Estimating: 323it [03:46,  1.50it/s]Extractor Estimating: 324it [03:46,  1.53it/s]Extractor Estimating: 325it [03:47,  1.58it/s]Extractor Estimating: 326it [03:47,  1.61it/s]Extractor Estimating: 327it [03:48,  1.65it/s]Extractor Estimating: 328it [03:48,  1.70it/s]Extractor Estimating: 329it [03:49,  1.80it/s]Extractor Estimating: 330it [03:50,  1.77it/s]Extractor Estimating: 331it [03:50,  1.75it/s]Extractor Estimating: 332it [03:51,  1.78it/s]Extractor Estimating: 333it [03:51,  1.73it/s]Extractor Estimating: 334it [03:52,  1.75it/s]Extractor Estimating: 335it [03:52,  1.72it/s]Extractor Estimating: 336it [03:53,  1.74it/s]Extractor Estimating: 337it [03:54,  1.69it/s]Extractor Estimating: 338it [03:54,  1.65it/s]Extractor Estimating: 339it [03:55,  1.61it/s]Extractor Estimating: 340it [03:56,  1.56it/s]Extractor Estimating: 341it [03:56,  1.58it/s]Extractor Estimating: 342it [03:57,  1.62it/s]Extractor Estimating: 343it [03:57,  1.61it/s]Extractor Estimating: 344it [03:58,  1.63it/s]Extractor Estimating: 345it [03:59,  1.72it/s]Extractor Estimating: 346it [03:59,  1.70it/s]Extractor Estimating: 347it [04:00,  1.72it/s]Extractor Estimating: 348it [04:00,  1.70it/s]Extractor Estimating: 349it [04:01,  1.67it/s]Extractor Estimating: 350it [04:01,  1.69it/s]Extractor Estimating: 351it [04:02,  1.70it/s]Extractor Estimating: 352it [04:03,  1.68it/s]Extractor Estimating: 353it [04:03,  1.69it/s]Extractor Estimating: 354it [04:04,  1.66it/s]Extractor Estimating: 355it [04:05,  1.63it/s]Extractor Estimating: 356it [04:05,  1.59it/s]Extractor Estimating: 357it [04:06,  1.53it/s]Extractor Estimating: 358it [04:07,  1.54it/s]Extractor Estimating: 359it [04:07,  1.60it/s]Extractor Estimating: 360it [04:08,  1.56it/s]Extractor Estimating: 361it [04:08,  1.62it/s]Extractor Estimating: 362it [04:09,  1.61it/s]Extractor Estimating: 363it [04:10,  1.62it/s]Extractor Estimating: 364it [04:10,  1.66it/s]Extractor Estimating: 365it [04:11,  1.69it/s]Extractor Estimating: 366it [04:11,  1.66it/s]Extractor Estimating: 367it [04:12,  1.61it/s]Extractor Estimating: 368it [04:13,  1.57it/s]Extractor Estimating: 369it [04:13,  1.60it/s]Extractor Estimating: 370it [04:14,  1.63it/s]Extractor Estimating: 371it [04:14,  1.63it/s]Extractor Estimating: 372it [04:15,  1.59it/s]Extractor Estimating: 373it [04:16,  1.53it/s]Extractor Estimating: 374it [04:17,  1.52it/s]Extractor Estimating: 375it [04:17,  1.49it/s]Extractor Estimating: 375it [04:17,  1.45it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7397 mean pseudo reward: 0.8789345971585228
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 27565
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27665, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_filtered_large/unseen_10_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27665, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.241, loss:1381.3458
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.989, loss:1325.8324
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.967, loss:1370.2015
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 91, avg_time 0.968, loss:1238.5548
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 191, avg_time 0.982, loss:1287.9653
>> valid entity prec:0.5622, rec:0.2439, f1:0.3403
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 291, avg_time 2.334, loss:1258.6748
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 82, avg_time 0.971, loss:1197.8278
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 182, avg_time 0.964, loss:1212.7830
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 282, avg_time 0.975, loss:1231.4266
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 73, avg_time 0.973, loss:1142.9000
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5697, rec:0.3701, f1:0.4487
>> valid relation prec:0.3636, rec:0.0009, f1:0.0018
>> valid relation with NER prec:0.3636, rec:0.0009, f1:0.0018
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 173, avg_time 2.359, loss:1168.8966
g_step 1200, step 273, avg_time 0.978, loss:1155.1989
g_step 1300, step 64, avg_time 0.970, loss:1158.2522
g_step 1400, step 164, avg_time 0.975, loss:1089.8850
g_step 1500, step 264, avg_time 0.977, loss:1130.3919
>> valid entity prec:0.5149, rec:0.5769, f1:0.5441
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 55, avg_time 2.381, loss:1072.7785
g_step 1700, step 155, avg_time 0.981, loss:1076.9616
g_step 1800, step 255, avg_time 0.970, loss:1062.2242
g_step 1900, step 46, avg_time 0.966, loss:1034.6060
g_step 2000, step 146, avg_time 0.977, loss:1031.1778
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5406, rec:0.3392, f1:0.4169
>> valid relation prec:0.1707, rec:0.0016, f1:0.0032
>> valid relation with NER prec:0.1707, rec:0.0016, f1:0.0032
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 246, avg_time 2.357, loss:1015.2523
g_step 2200, step 37, avg_time 0.965, loss:996.1708
g_step 2300, step 137, avg_time 0.977, loss:961.7079
g_step 2400, step 237, avg_time 0.979, loss:972.7891
g_step 2500, step 28, avg_time 0.972, loss:978.7391
>> valid entity prec:0.4875, rec:0.5031, f1:0.4952
>> valid relation prec:0.0606, rec:0.0014, f1:0.0027
>> valid relation with NER prec:0.0606, rec:0.0014, f1:0.0027
g_step 2600, step 128, avg_time 2.381, loss:958.9303
g_step 2700, step 228, avg_time 0.971, loss:975.8730
g_step 2800, step 19, avg_time 0.970, loss:930.6581
g_step 2900, step 119, avg_time 0.991, loss:909.8845
g_step 3000, step 219, avg_time 0.978, loss:914.0907
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4944, rec:0.3795, f1:0.4294
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 10, avg_time 2.349, loss:904.9450
g_step 3200, step 110, avg_time 0.965, loss:881.8988
g_step 3300, step 210, avg_time 0.982, loss:905.7234
g_step 3400, step 1, avg_time 0.964, loss:869.4929
g_step 3500, step 101, avg_time 0.975, loss:843.7669
>> valid entity prec:0.4929, rec:0.3224, f1:0.3898
>> valid relation prec:0.0447, rec:0.0018, f1:0.0035
>> valid relation with NER prec:0.0447, rec:0.0018, f1:0.0035
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 201, avg_time 2.370, loss:879.0620
g_step 3700, step 301, avg_time 0.962, loss:863.0167
g_step 3800, step 92, avg_time 0.962, loss:812.4333
g_step 3900, step 192, avg_time 0.966, loss:841.1301
g_step 4000, step 292, avg_time 0.983, loss:856.6774
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5052, rec:0.4137, f1:0.4549
>> valid relation prec:0.0700, rec:0.0032, f1:0.0062
>> valid relation with NER prec:0.0700, rec:0.0032, f1:0.0062
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 83, avg_time 2.377, loss:809.5741
g_step 4200, step 183, avg_time 0.968, loss:794.8236
g_step 4300, step 283, avg_time 0.971, loss:796.7441
g_step 4400, step 74, avg_time 0.968, loss:775.1800
g_step 4500, step 174, avg_time 0.975, loss:800.8980
>> valid entity prec:0.5303, rec:0.3637, f1:0.4315
>> valid relation prec:0.0634, rec:0.0030, f1:0.0057
>> valid relation with NER prec:0.0634, rec:0.0030, f1:0.0057
g_step 4600, step 274, avg_time 2.362, loss:781.7887
g_step 4700, step 65, avg_time 0.968, loss:751.3139
g_step 4800, step 165, avg_time 0.963, loss:764.0454
g_step 4900, step 265, avg_time 0.970, loss:772.9411
g_step 5000, step 56, avg_time 0.980, loss:714.8323
learning rate was adjusted to 0.0008
>> valid entity prec:0.4755, rec:0.4134, f1:0.4423
>> valid relation prec:0.0482, rec:0.0055, f1:0.0099
>> valid relation with NER prec:0.0482, rec:0.0055, f1:0.0099
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 156, avg_time 2.378, loss:724.3042
g_step 5200, step 256, avg_time 0.976, loss:739.0603
g_step 5300, step 47, avg_time 0.968, loss:719.7934
g_step 5400, step 147, avg_time 0.978, loss:689.1394
g_step 5500, step 247, avg_time 0.966, loss:733.5437
>> valid entity prec:0.4965, rec:0.4517, f1:0.4731
>> valid relation prec:0.0548, rec:0.0062, f1:0.0112
>> valid relation with NER prec:0.0548, rec:0.0062, f1:0.0112
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 38, avg_time 2.361, loss:671.1282
g_step 5700, step 138, avg_time 0.971, loss:683.5099
g_step 5800, step 238, avg_time 0.967, loss:672.5902
g_step 5900, step 29, avg_time 0.970, loss:696.7128
g_step 6000, step 129, avg_time 0.968, loss:661.7989
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4634, rec:0.4437, f1:0.4533
>> valid relation prec:0.0592, rec:0.0060, f1:0.0109
>> valid relation with NER prec:0.0592, rec:0.0060, f1:0.0109
g_step 6100, step 229, avg_time 2.378, loss:676.1222
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 15:01:19 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 15:01:19 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_15-01-19_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 15:01:20 - WARNING - datasets.builder -   Using custom data configuration default-ce480a4647dd3bee
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ce480a4647dd3bee/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 15:01:20,834 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:01:20,836 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:01:20,836 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:01:20,837 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:01:20,844 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:20,848 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:20,848 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:20,848 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:20,848 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:20,848 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:01:20,848 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 15:01:20,963 >> loading weights file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:01:23,982 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 15:01:23,998 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_10_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ce480a4647dd3bee/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 15:01:24 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x153e5cb7b0e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.97ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.93ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.52ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.87ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.10ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.26ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.35ba/s]100%|██████████| 8/8 [00:01<00:00,  5.17ba/s]100%|██████████| 8/8 [00:01<00:00,  4.24ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.11ba/s] 40%|████      | 2/5 [00:00<00:00,  4.30ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.38ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.40ba/s]100%|██████████| 5/5 [00:00<00:00,  5.01ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.68ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.35ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.67ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.83ba/s]100%|██████████| 8/8 [00:00<00:00, 11.30ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.60ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.60ba/s]100%|██████████| 5/5 [00:00<00:00, 12.92ba/s]100%|██████████| 5/5 [00:00<00:00, 12.23ba/s]
[INFO|trainer.py:414] 2023-08-28 15:01:28,460 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 15:01:28,474 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 15:01:28,474 >>   Num examples = 7515
[INFO|trainer.py:1149] 2023-08-28 15:01:28,474 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 15:01:28,474 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 15:01:28,474 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 15:01:28,475 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 15:01:28,475 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:53,  3.36it/s]  0%|          | 2/585 [00:00<02:51,  3.41it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:49,  3.43it/s]  1%|          | 6/585 [00:01<02:48,  3.43it/s]  1%|          | 7/585 [00:02<02:49,  3.42it/s]  1%|▏         | 8/585 [00:02<02:48,  3.43it/s]  2%|▏         | 9/585 [00:02<02:47,  3.43it/s]  2%|▏         | 10/585 [00:02<02:47,  3.43it/s]  2%|▏         | 11/585 [00:03<02:47,  3.44it/s]  2%|▏         | 12/585 [00:03<02:46,  3.44it/s]  2%|▏         | 13/585 [00:03<02:46,  3.44it/s]  2%|▏         | 14/585 [00:04<02:46,  3.44it/s]  3%|▎         | 15/585 [00:04<02:45,  3.44it/s]  3%|▎         | 16/585 [00:04<02:45,  3.44it/s]  3%|▎         | 17/585 [00:04<02:45,  3.44it/s]  3%|▎         | 18/585 [00:05<02:45,  3.43it/s]  3%|▎         | 19/585 [00:05<02:44,  3.43it/s]  3%|▎         | 20/585 [00:05<02:44,  3.43it/s]  4%|▎         | 21/585 [00:06<02:44,  3.44it/s]  4%|▍         | 22/585 [00:06<02:43,  3.43it/s]  4%|▍         | 23/585 [00:06<02:43,  3.44it/s]  4%|▍         | 24/585 [00:06<02:43,  3.44it/s]  4%|▍         | 25/585 [00:07<02:42,  3.44it/s]  4%|▍         | 26/585 [00:07<02:42,  3.44it/s]  5%|▍         | 27/585 [00:07<02:42,  3.44it/s]  5%|▍         | 28/585 [00:08<02:41,  3.44it/s]  5%|▍         | 29/585 [00:08<02:42,  3.42it/s]  5%|▌         | 30/585 [00:08<02:42,  3.42it/s]  5%|▌         | 31/585 [00:09<02:41,  3.43it/s]  5%|▌         | 32/585 [00:09<02:41,  3.43it/s]  6%|▌         | 33/585 [00:09<02:40,  3.43it/s]  6%|▌         | 34/585 [00:09<02:40,  3.43it/s]  6%|▌         | 35/585 [00:10<02:40,  3.44it/s]  6%|▌         | 36/585 [00:10<02:39,  3.44it/s]  6%|▋         | 37/585 [00:10<02:39,  3.44it/s]  6%|▋         | 38/585 [00:11<02:39,  3.44it/s]  7%|▋         | 39/585 [00:11<02:38,  3.44it/s]  7%|▋         | 40/585 [00:11<02:38,  3.43it/s]  7%|▋         | 41/585 [00:11<02:38,  3.43it/s]  7%|▋         | 42/585 [00:12<02:38,  3.43it/s]  7%|▋         | 43/585 [00:12<02:37,  3.43it/s]  8%|▊         | 44/585 [00:12<02:37,  3.43it/s]  8%|▊         | 45/585 [00:13<02:37,  3.43it/s]  8%|▊         | 46/585 [00:13<02:36,  3.43it/s]  8%|▊         | 47/585 [00:13<02:36,  3.43it/s]  8%|▊         | 48/585 [00:13<02:36,  3.44it/s]  8%|▊         | 49/585 [00:14<02:35,  3.44it/s]  9%|▊         | 50/585 [00:14<02:35,  3.44it/s]  9%|▊         | 51/585 [00:14<02:35,  3.43it/s]  9%|▉         | 52/585 [00:15<02:35,  3.43it/s]  9%|▉         | 53/585 [00:15<02:34,  3.43it/s]  9%|▉         | 54/585 [00:15<02:34,  3.43it/s]  9%|▉         | 55/585 [00:16<02:34,  3.43it/s] 10%|▉         | 56/585 [00:16<02:33,  3.44it/s] 10%|▉         | 57/585 [00:16<02:33,  3.44it/s] 10%|▉         | 58/585 [00:16<02:33,  3.44it/s] 10%|█         | 59/585 [00:17<02:33,  3.44it/s] 10%|█         | 60/585 [00:17<02:32,  3.44it/s] 10%|█         | 61/585 [00:17<02:32,  3.44it/s] 11%|█         | 62/585 [00:18<02:32,  3.43it/s] 11%|█         | 63/585 [00:18<02:32,  3.43it/s] 11%|█         | 64/585 [00:18<02:31,  3.43it/s] 11%|█         | 65/585 [00:18<02:31,  3.43it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.43it/s] 12%|█▏        | 72/585 [00:20<02:29,  3.43it/s] 12%|█▏        | 73/585 [00:21<02:29,  3.43it/s] 13%|█▎        | 74/585 [00:21<02:29,  3.43it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.43it/s] 13%|█▎        | 76/585 [00:22<02:28,  3.43it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.43it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.43it/s] 14%|█▎        | 79/585 [00:23<02:27,  3.43it/s] 14%|█▎        | 80/585 [00:23<02:27,  3.43it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.43it/s] 14%|█▍        | 82/585 [00:23<02:26,  3.43it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 84/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.42it/s] 15%|█▍        | 87/585 [00:25<02:25,  3.42it/s] 15%|█▌        | 88/585 [00:25<02:25,  3.42it/s] 15%|█▌        | 89/585 [00:25<02:24,  3.43it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.42it/s] 16%|█▌        | 91/585 [00:26<02:24,  3.42it/s] 16%|█▌        | 92/585 [00:26<02:24,  3.42it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 94/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.43it/s] 16%|█▋        | 96/585 [00:27<02:22,  3.43it/s] 17%|█▋        | 97/585 [00:28<02:22,  3.42it/s] 17%|█▋        | 98/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 101/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.43it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 106/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.42it/s] 19%|█▊        | 109/585 [00:31<02:19,  3.42it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.42it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 113/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.43it/s] 20%|█▉        | 115/585 [00:33<02:17,  3.43it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.43it/s] 20%|██        | 117/585 [00:34<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 15:02:02,697 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:02:02,701 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 15:02:02,701 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.63it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.86it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.92it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.29it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.71it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.46it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.31it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.22it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.32it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.35it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.26it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.20it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.11it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.01it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.97it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.95it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.98it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.09it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.16it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.15it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.14it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.11it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.01it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.03it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.99it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.04it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.16it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.19it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.17it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.09it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.04it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.05it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.03it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.02it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.97it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.00it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.21it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.17it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.14it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.09it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.95it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.06it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.98it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.02it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.14it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.19it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.02it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.06it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.01it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.98it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.02it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.05it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.06it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.04it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.12it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.05it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.97it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.03it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.98it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.95it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.05it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.03it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.10it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.06it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.00it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.12it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.03it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.93it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.13it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.17it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.08it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.08it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.99it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.03it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.11it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.99it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.96it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.97it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.15it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.11it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 43.95it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.03it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.07it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.11it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.00it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.87it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.86it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.11it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.10it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.97it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.08it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.21it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.00it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.00it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.04it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.06it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.99it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.12it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.12it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.06it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.08it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.06it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.09it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.09it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.02it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:46<02:16,  3.43it/s]
100%|██████████| 543/543 [00:12<00:00, 44.02it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:02:15,047 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 15:02:15,067 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:02:17,150 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:02:17,165 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:02:17,179 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:52<45:23,  5.83s/it] 20%|██        | 119/585 [00:53<32:23,  4.17s/it] 21%|██        | 120/585 [00:53<23:18,  3.01s/it] 21%|██        | 121/585 [00:53<16:57,  2.19s/it] 21%|██        | 122/585 [00:54<12:31,  1.62s/it] 21%|██        | 123/585 [00:54<09:24,  1.22s/it] 21%|██        | 124/585 [00:54<07:14,  1.06it/s] 21%|██▏       | 125/585 [00:54<05:43,  1.34it/s] 22%|██▏       | 126/585 [00:55<04:40,  1.64it/s] 22%|██▏       | 127/585 [00:55<03:55,  1.94it/s] 22%|██▏       | 128/585 [00:55<03:25,  2.23it/s] 22%|██▏       | 129/585 [00:56<03:03,  2.49it/s] 22%|██▏       | 130/585 [00:56<02:47,  2.72it/s] 22%|██▏       | 131/585 [00:56<02:37,  2.89it/s] 23%|██▎       | 132/585 [00:56<02:29,  3.02it/s] 23%|██▎       | 133/585 [00:57<02:24,  3.12it/s] 23%|██▎       | 134/585 [00:57<02:21,  3.20it/s] 23%|██▎       | 135/585 [00:57<02:18,  3.25it/s] 23%|██▎       | 136/585 [00:58<02:16,  3.29it/s] 23%|██▎       | 137/585 [00:58<02:14,  3.32it/s] 24%|██▎       | 138/585 [00:58<02:13,  3.34it/s] 24%|██▍       | 139/585 [00:59<02:13,  3.34it/s] 24%|██▍       | 140/585 [00:59<02:12,  3.36it/s] 24%|██▍       | 141/585 [00:59<02:11,  3.37it/s] 24%|██▍       | 142/585 [00:59<02:11,  3.37it/s] 24%|██▍       | 143/585 [01:00<02:14,  3.29it/s] 25%|██▍       | 144/585 [01:00<02:12,  3.32it/s] 25%|██▍       | 145/585 [01:00<02:11,  3.34it/s] 25%|██▍       | 146/585 [01:01<02:10,  3.35it/s] 25%|██▌       | 147/585 [01:01<02:10,  3.36it/s] 25%|██▌       | 148/585 [01:01<02:09,  3.37it/s] 25%|██▌       | 149/585 [01:02<02:09,  3.38it/s] 26%|██▌       | 150/585 [01:02<02:09,  3.37it/s] 26%|██▌       | 151/585 [01:02<02:08,  3.37it/s] 26%|██▌       | 152/585 [01:02<02:08,  3.38it/s] 26%|██▌       | 153/585 [01:03<02:07,  3.38it/s] 26%|██▋       | 154/585 [01:03<02:07,  3.38it/s] 26%|██▋       | 155/585 [01:03<02:07,  3.38it/s] 27%|██▋       | 156/585 [01:04<02:06,  3.39it/s] 27%|██▋       | 157/585 [01:04<02:06,  3.39it/s] 27%|██▋       | 158/585 [01:04<02:06,  3.39it/s] 27%|██▋       | 159/585 [01:04<02:05,  3.38it/s] 27%|██▋       | 160/585 [01:05<02:05,  3.39it/s] 28%|██▊       | 161/585 [01:05<02:05,  3.37it/s] 28%|██▊       | 162/585 [01:05<02:05,  3.38it/s] 28%|██▊       | 163/585 [01:06<02:04,  3.38it/s] 28%|██▊       | 164/585 [01:06<02:04,  3.38it/s] 28%|██▊       | 165/585 [01:06<02:04,  3.38it/s] 28%|██▊       | 166/585 [01:07<02:03,  3.38it/s] 29%|██▊       | 167/585 [01:07<02:03,  3.39it/s] 29%|██▊       | 168/585 [01:07<02:03,  3.38it/s] 29%|██▉       | 169/585 [01:07<02:02,  3.38it/s] 29%|██▉       | 170/585 [01:08<02:02,  3.38it/s] 29%|██▉       | 171/585 [01:08<02:02,  3.38it/s] 29%|██▉       | 172/585 [01:08<02:02,  3.37it/s] 30%|██▉       | 173/585 [01:09<02:01,  3.38it/s] 30%|██▉       | 174/585 [01:09<02:01,  3.38it/s] 30%|██▉       | 175/585 [01:09<02:01,  3.38it/s] 30%|███       | 176/585 [01:09<02:00,  3.39it/s] 30%|███       | 177/585 [01:10<02:00,  3.39it/s] 30%|███       | 178/585 [01:10<02:00,  3.38it/s] 31%|███       | 179/585 [01:10<01:59,  3.39it/s] 31%|███       | 180/585 [01:11<01:59,  3.39it/s] 31%|███       | 181/585 [01:11<01:59,  3.38it/s] 31%|███       | 182/585 [01:11<01:59,  3.38it/s] 31%|███▏      | 183/585 [01:12<01:59,  3.37it/s] 31%|███▏      | 184/585 [01:12<01:58,  3.37it/s] 32%|███▏      | 185/585 [01:12<01:58,  3.38it/s] 32%|███▏      | 186/585 [01:12<01:57,  3.38it/s] 32%|███▏      | 187/585 [01:13<01:57,  3.38it/s] 32%|███▏      | 188/585 [01:13<01:57,  3.38it/s] 32%|███▏      | 189/585 [01:13<01:57,  3.38it/s] 32%|███▏      | 190/585 [01:14<01:56,  3.38it/s] 33%|███▎      | 191/585 [01:14<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:14<01:56,  3.39it/s] 33%|███▎      | 193/585 [01:15<01:55,  3.38it/s] 33%|███▎      | 194/585 [01:15<01:56,  3.36it/s] 33%|███▎      | 195/585 [01:15<01:55,  3.37it/s] 34%|███▎      | 196/585 [01:15<01:55,  3.37it/s] 34%|███▎      | 197/585 [01:16<01:54,  3.38it/s] 34%|███▍      | 198/585 [01:16<01:54,  3.38it/s] 34%|███▍      | 199/585 [01:16<01:54,  3.38it/s] 34%|███▍      | 200/585 [01:17<01:53,  3.38it/s] 34%|███▍      | 201/585 [01:17<01:53,  3.37it/s] 35%|███▍      | 202/585 [01:17<01:53,  3.38it/s] 35%|███▍      | 203/585 [01:17<01:53,  3.38it/s] 35%|███▍      | 204/585 [01:18<01:52,  3.38it/s] 35%|███▌      | 205/585 [01:18<01:52,  3.37it/s] 35%|███▌      | 206/585 [01:18<01:52,  3.38it/s] 35%|███▌      | 207/585 [01:19<01:51,  3.38it/s] 36%|███▌      | 208/585 [01:19<01:51,  3.38it/s] 36%|███▌      | 209/585 [01:19<01:51,  3.37it/s] 36%|███▌      | 210/585 [01:20<01:50,  3.39it/s] 36%|███▌      | 211/585 [01:20<01:49,  3.40it/s] 36%|███▌      | 212/585 [01:20<01:49,  3.41it/s] 36%|███▋      | 213/585 [01:20<01:48,  3.42it/s] 37%|███▋      | 214/585 [01:21<01:48,  3.42it/s] 37%|███▋      | 215/585 [01:21<01:48,  3.42it/s] 37%|███▋      | 216/585 [01:21<01:48,  3.41it/s] 37%|███▋      | 217/585 [01:22<01:47,  3.42it/s] 37%|███▋      | 218/585 [01:22<01:47,  3.42it/s] 37%|███▋      | 219/585 [01:22<01:46,  3.42it/s] 38%|███▊      | 220/585 [01:22<01:46,  3.43it/s] 38%|███▊      | 221/585 [01:23<01:46,  3.43it/s] 38%|███▊      | 222/585 [01:23<01:46,  3.42it/s] 38%|███▊      | 223/585 [01:23<01:45,  3.43it/s] 38%|███▊      | 224/585 [01:24<01:45,  3.43it/s] 38%|███▊      | 225/585 [01:24<01:45,  3.43it/s] 39%|███▊      | 226/585 [01:24<01:44,  3.43it/s] 39%|███▉      | 227/585 [01:25<01:44,  3.43it/s] 39%|███▉      | 228/585 [01:25<01:44,  3.43it/s] 39%|███▉      | 229/585 [01:25<01:43,  3.43it/s] 39%|███▉      | 230/585 [01:25<01:43,  3.43it/s] 39%|███▉      | 231/585 [01:26<01:43,  3.43it/s] 40%|███▉      | 232/585 [01:26<01:43,  3.42it/s] 40%|███▉      | 233/585 [01:26<01:42,  3.42it/s] 40%|████      | 234/585 [01:27<01:42,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 15:02:55,643 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:02:55,643 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 15:02:55,643 >>   Batch size = 8
{'eval_loss': 0.9715092182159424, 'eval_runtime': 12.3282, 'eval_samples_per_second': 352.2, 'eval_steps_per_second': 44.045, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.28it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.89it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.06it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.26it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.81it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.55it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.24it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.16it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.19it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.38it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.20it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.16it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.13it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.96it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.07it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.90it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.97it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.04it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.13it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.17it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.16it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.01it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.01it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.93it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.90it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.98it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.04it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.09it/s][A
 27%|██▋       | 147/543 [00:03<00:09, 43.91it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.12it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.07it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.01it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.97it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.95it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.99it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.96it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.14it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.09it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.07it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.12it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.97it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.98it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.01it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.92it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.96it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.14it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.11it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.05it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.06it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.04it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.99it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.04it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.02it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.95it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.15it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 43.94it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.04it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.05it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.08it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.04it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.09it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.01it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.07it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.00it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.03it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.00it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.96it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.00it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.16it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.11it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.08it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.12it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.00it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.95it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.97it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.86it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.98it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.03it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 43.99it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.03it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.06it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.00it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.94it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.96it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.03it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.05it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.06it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.14it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.06it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.07it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.07it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.96it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.91it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.92it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.04it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.07it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.06it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.96it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.96it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.92it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.09it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.04it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.97it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.13it/s][A                                                 
                                                 [A 40%|████      | 234/585 [01:39<01:42,  3.42it/s]
100%|██████████| 543/543 [00:12<00:00, 44.13it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:03:07,996 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 15:03:08,027 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:03:09,940 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:03:09,953 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:03:09,960 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:46<35:07,  6.02s/it] 40%|████      | 236/585 [01:46<25:01,  4.30s/it] 41%|████      | 237/585 [01:47<17:58,  3.10s/it] 41%|████      | 238/585 [01:47<13:03,  2.26s/it] 41%|████      | 239/585 [01:47<09:36,  1.67s/it] 41%|████      | 240/585 [01:47<07:12,  1.25s/it] 41%|████      | 241/585 [01:48<05:32,  1.04it/s] 41%|████▏     | 242/585 [01:48<04:21,  1.31it/s] 42%|████▏     | 243/585 [01:48<03:32,  1.61it/s] 42%|████▏     | 244/585 [01:49<02:58,  1.91it/s] 42%|████▏     | 245/585 [01:49<02:33,  2.21it/s] 42%|████▏     | 246/585 [01:49<02:17,  2.47it/s] 42%|████▏     | 247/585 [01:49<02:05,  2.69it/s] 42%|████▏     | 248/585 [01:50<01:57,  2.88it/s] 43%|████▎     | 249/585 [01:50<01:51,  3.03it/s] 43%|████▎     | 250/585 [01:50<01:46,  3.14it/s] 43%|████▎     | 251/585 [01:51<01:43,  3.22it/s] 43%|████▎     | 252/585 [01:51<01:41,  3.28it/s] 43%|████▎     | 253/585 [01:51<01:39,  3.33it/s] 43%|████▎     | 254/585 [01:51<01:38,  3.36it/s] 44%|████▎     | 255/585 [01:52<01:37,  3.38it/s] 44%|████▍     | 256/585 [01:52<01:39,  3.32it/s] 44%|████▍     | 257/585 [01:52<01:37,  3.35it/s] 44%|████▍     | 258/585 [01:53<01:37,  3.35it/s] 44%|████▍     | 259/585 [01:53<01:36,  3.38it/s] 44%|████▍     | 260/585 [01:53<01:35,  3.39it/s] 45%|████▍     | 261/585 [01:54<01:35,  3.41it/s] 45%|████▍     | 262/585 [01:54<01:34,  3.41it/s] 45%|████▍     | 263/585 [01:54<01:34,  3.42it/s] 45%|████▌     | 264/585 [01:54<01:33,  3.42it/s] 45%|████▌     | 265/585 [01:55<01:33,  3.43it/s] 45%|████▌     | 266/585 [01:55<01:32,  3.43it/s] 46%|████▌     | 267/585 [01:55<01:32,  3.43it/s] 46%|████▌     | 268/585 [01:56<01:32,  3.43it/s] 46%|████▌     | 269/585 [01:56<01:31,  3.44it/s] 46%|████▌     | 270/585 [01:56<01:31,  3.43it/s] 46%|████▋     | 271/585 [01:56<01:31,  3.43it/s] 46%|████▋     | 272/585 [01:57<01:31,  3.41it/s] 47%|████▋     | 273/585 [01:57<01:31,  3.42it/s] 47%|████▋     | 274/585 [01:57<01:30,  3.43it/s] 47%|████▋     | 275/585 [01:58<01:30,  3.43it/s] 47%|████▋     | 276/585 [01:58<01:30,  3.43it/s] 47%|████▋     | 277/585 [01:58<01:29,  3.43it/s] 48%|████▊     | 278/585 [01:59<01:29,  3.43it/s] 48%|████▊     | 279/585 [01:59<01:29,  3.43it/s] 48%|████▊     | 280/585 [01:59<01:28,  3.44it/s] 48%|████▊     | 281/585 [01:59<01:28,  3.44it/s] 48%|████▊     | 282/585 [02:00<01:28,  3.43it/s] 48%|████▊     | 283/585 [02:00<01:30,  3.34it/s] 49%|████▊     | 284/585 [02:00<01:29,  3.36it/s] 49%|████▊     | 285/585 [02:01<01:28,  3.39it/s] 49%|████▉     | 286/585 [02:01<01:27,  3.40it/s] 49%|████▉     | 287/585 [02:01<01:27,  3.41it/s] 49%|████▉     | 288/585 [02:01<01:26,  3.42it/s] 49%|████▉     | 289/585 [02:02<01:26,  3.42it/s] 50%|████▉     | 290/585 [02:02<01:26,  3.43it/s] 50%|████▉     | 291/585 [02:02<01:25,  3.43it/s] 50%|████▉     | 292/585 [02:03<01:25,  3.43it/s] 50%|█████     | 293/585 [02:03<01:25,  3.43it/s] 50%|█████     | 294/585 [02:03<01:25,  3.41it/s] 50%|█████     | 295/585 [02:03<01:24,  3.42it/s] 51%|█████     | 296/585 [02:04<01:24,  3.43it/s] 51%|█████     | 297/585 [02:04<01:24,  3.43it/s] 51%|█████     | 298/585 [02:04<01:23,  3.43it/s] 51%|█████     | 299/585 [02:05<01:23,  3.43it/s] 51%|█████▏    | 300/585 [02:05<01:23,  3.43it/s] 51%|█████▏    | 301/585 [02:05<01:22,  3.43it/s] 52%|█████▏    | 302/585 [02:06<01:22,  3.43it/s] 52%|█████▏    | 303/585 [02:06<01:22,  3.43it/s] 52%|█████▏    | 304/585 [02:06<01:21,  3.43it/s] 52%|█████▏    | 305/585 [02:06<01:26,  3.24it/s] 52%|█████▏    | 306/585 [02:07<01:24,  3.30it/s] 52%|█████▏    | 307/585 [02:07<01:23,  3.34it/s] 53%|█████▎    | 308/585 [02:07<01:22,  3.36it/s] 53%|█████▎    | 309/585 [02:08<01:21,  3.39it/s] 53%|█████▎    | 310/585 [02:08<01:20,  3.40it/s] 53%|█████▎    | 311/585 [02:08<01:20,  3.41it/s] 53%|█████▎    | 312/585 [02:08<01:20,  3.41it/s] 54%|█████▎    | 313/585 [02:09<01:19,  3.42it/s] 54%|█████▎    | 314/585 [02:09<01:19,  3.42it/s] 54%|█████▍    | 315/585 [02:09<01:18,  3.42it/s] 54%|█████▍    | 316/585 [02:10<01:19,  3.40it/s] 54%|█████▍    | 317/585 [02:10<01:18,  3.41it/s] 54%|█████▍    | 318/585 [02:10<01:18,  3.42it/s] 55%|█████▍    | 319/585 [02:11<01:17,  3.42it/s] 55%|█████▍    | 320/585 [02:11<01:17,  3.42it/s] 55%|█████▍    | 321/585 [02:11<01:16,  3.43it/s] 55%|█████▌    | 322/585 [02:11<01:16,  3.43it/s] 55%|█████▌    | 323/585 [02:12<01:16,  3.43it/s] 55%|█████▌    | 324/585 [02:12<01:16,  3.43it/s] 56%|█████▌    | 325/585 [02:12<01:15,  3.43it/s] 56%|█████▌    | 326/585 [02:13<01:15,  3.43it/s] 56%|█████▌    | 327/585 [02:13<01:16,  3.39it/s] 56%|█████▌    | 328/585 [02:13<01:15,  3.40it/s] 56%|█████▌    | 329/585 [02:13<01:15,  3.41it/s] 56%|█████▋    | 330/585 [02:14<01:14,  3.42it/s] 57%|█████▋    | 331/585 [02:14<01:14,  3.42it/s] 57%|█████▋    | 332/585 [02:14<01:13,  3.43it/s] 57%|█████▋    | 333/585 [02:15<01:13,  3.42it/s] 57%|█████▋    | 334/585 [02:15<01:13,  3.43it/s] 57%|█████▋    | 335/585 [02:15<01:13,  3.42it/s] 57%|█████▋    | 336/585 [02:16<01:12,  3.43it/s] 58%|█████▊    | 337/585 [02:16<01:12,  3.43it/s] 58%|█████▊    | 338/585 [02:16<01:12,  3.42it/s] 58%|█████▊    | 339/585 [02:16<01:11,  3.42it/s] 58%|█████▊    | 340/585 [02:17<01:11,  3.43it/s] 58%|█████▊    | 341/585 [02:17<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:17<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:18<01:10,  3.43it/s] 59%|█████▉    | 344/585 [02:18<01:10,  3.43it/s] 59%|█████▉    | 345/585 [02:18<01:09,  3.43it/s] 59%|█████▉    | 346/585 [02:18<01:09,  3.43it/s] 59%|█████▉    | 347/585 [02:19<01:09,  3.42it/s] 59%|█████▉    | 348/585 [02:19<01:09,  3.42it/s] 60%|█████▉    | 349/585 [02:19<01:09,  3.42it/s] 60%|█████▉    | 350/585 [02:20<01:08,  3.42it/s] 60%|██████    | 351/585 [02:20<01:08,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 15:03:48,982 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:03:48,983 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 15:03:48,983 >>   Batch size = 8
{'eval_loss': 0.9565653204917908, 'eval_runtime': 12.3288, 'eval_samples_per_second': 352.183, 'eval_steps_per_second': 44.043, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.39it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.55it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.84it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.17it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.78it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.47it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.11it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.21it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.22it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.25it/s][A
 10%|█         | 57/543 [00:01<00:11, 44.17it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.16it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.11it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.04it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.99it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.96it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.04it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.98it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.99it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.12it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.12it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.05it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.99it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.95it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.78it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.10it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.08it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 43.97it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.10it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.14it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.11it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.13it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.90it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.99it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.09it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.02it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.07it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.16it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.26it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.05it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.98it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.00it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.99it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.11it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.19it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.15it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.13it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.18it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.10it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.94it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.95it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.05it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.14it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.14it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.10it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.14it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.15it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.93it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.85it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.98it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.10it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.11it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.05it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.06it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.11it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.15it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.00it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.92it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.00it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.14it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.04it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.02it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.08it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.11it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.97it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.99it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.00it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.03it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.08it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.12it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.10it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.17it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.02it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.98it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.87it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.95it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.87it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.06it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.20it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.09it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.06it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.15it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.92it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.94it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.90it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.62it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.24it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.21it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.06it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.08it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.12it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.08it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.93it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.99it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.06it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.12it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.13it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.07it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [02:32<01:08,  3.42it/s]
100%|██████████| 543/543 [00:12<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:04:01,324 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 15:04:01,357 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:04:03,348 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:04:03,371 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:04:03,384 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:39<23:02,  5.93s/it] 60%|██████    | 353/585 [02:39<16:23,  4.24s/it] 61%|██████    | 354/585 [02:40<11:46,  3.06s/it] 61%|██████    | 355/585 [02:40<08:32,  2.23s/it] 61%|██████    | 356/585 [02:40<06:17,  1.65s/it] 61%|██████    | 357/585 [02:40<04:43,  1.24s/it] 61%|██████    | 358/585 [02:41<03:37,  1.04it/s] 61%|██████▏   | 359/585 [02:41<02:51,  1.32it/s] 62%|██████▏   | 360/585 [02:41<02:19,  1.61it/s] 62%|██████▏   | 361/585 [02:42<01:57,  1.91it/s] 62%|██████▏   | 362/585 [02:42<01:41,  2.20it/s] 62%|██████▏   | 363/585 [02:42<01:30,  2.46it/s] 62%|██████▏   | 364/585 [02:43<01:22,  2.67it/s] 62%|██████▏   | 365/585 [02:43<01:17,  2.85it/s] 63%|██████▎   | 366/585 [02:43<01:13,  3.00it/s] 63%|██████▎   | 367/585 [02:43<01:10,  3.10it/s] 63%|██████▎   | 368/585 [02:44<01:08,  3.18it/s] 63%|██████▎   | 369/585 [02:44<01:06,  3.24it/s] 63%|██████▎   | 370/585 [02:44<01:05,  3.29it/s] 63%|██████▎   | 371/585 [02:45<01:04,  3.32it/s] 64%|██████▎   | 372/585 [02:45<01:03,  3.34it/s] 64%|██████▍   | 373/585 [02:45<01:03,  3.36it/s] 64%|██████▍   | 374/585 [02:45<01:02,  3.38it/s] 64%|██████▍   | 375/585 [02:46<01:02,  3.39it/s] 64%|██████▍   | 376/585 [02:46<01:01,  3.40it/s] 64%|██████▍   | 377/585 [02:46<01:00,  3.41it/s] 65%|██████▍   | 378/585 [02:47<01:00,  3.42it/s] 65%|██████▍   | 379/585 [02:47<01:00,  3.42it/s] 65%|██████▍   | 380/585 [02:47<00:59,  3.43it/s] 65%|██████▌   | 381/585 [02:48<00:59,  3.43it/s] 65%|██████▌   | 382/585 [02:48<00:59,  3.43it/s] 65%|██████▌   | 383/585 [02:48<00:58,  3.43it/s] 66%|██████▌   | 384/585 [02:48<00:58,  3.43it/s] 66%|██████▌   | 385/585 [02:49<00:58,  3.43it/s] 66%|██████▌   | 386/585 [02:49<00:58,  3.41it/s] 66%|██████▌   | 387/585 [02:49<00:57,  3.42it/s] 66%|██████▋   | 388/585 [02:50<00:57,  3.43it/s] 66%|██████▋   | 389/585 [02:50<00:57,  3.43it/s] 67%|██████▋   | 390/585 [02:50<00:56,  3.43it/s] 67%|██████▋   | 391/585 [02:50<00:56,  3.43it/s] 67%|██████▋   | 392/585 [02:51<00:56,  3.43it/s] 67%|██████▋   | 393/585 [02:51<00:55,  3.43it/s] 67%|██████▋   | 394/585 [02:51<00:55,  3.43it/s] 68%|██████▊   | 395/585 [02:52<00:55,  3.43it/s] 68%|██████▊   | 396/585 [02:52<00:55,  3.43it/s] 68%|██████▊   | 397/585 [02:52<00:56,  3.35it/s] 68%|██████▊   | 398/585 [02:52<00:55,  3.38it/s] 68%|██████▊   | 399/585 [02:53<00:54,  3.39it/s] 68%|██████▊   | 400/585 [02:53<00:54,  3.41it/s] 69%|██████▊   | 401/585 [02:53<00:53,  3.41it/s] 69%|██████▊   | 402/585 [02:54<00:53,  3.42it/s] 69%|██████▉   | 403/585 [02:54<00:53,  3.42it/s] 69%|██████▉   | 404/585 [02:54<00:52,  3.43it/s] 69%|██████▉   | 405/585 [02:55<00:52,  3.43it/s] 69%|██████▉   | 406/585 [02:55<00:52,  3.43it/s] 70%|██████▉   | 407/585 [02:55<00:51,  3.43it/s] 70%|██████▉   | 408/585 [02:55<00:51,  3.43it/s] 70%|██████▉   | 409/585 [02:56<00:51,  3.44it/s] 70%|███████   | 410/585 [02:56<00:50,  3.43it/s] 70%|███████   | 411/585 [02:56<00:50,  3.43it/s] 70%|███████   | 412/585 [02:57<00:50,  3.43it/s] 71%|███████   | 413/585 [02:57<00:50,  3.44it/s] 71%|███████   | 414/585 [02:57<00:49,  3.43it/s] 71%|███████   | 415/585 [02:57<00:49,  3.43it/s] 71%|███████   | 416/585 [02:58<00:49,  3.43it/s] 71%|███████▏  | 417/585 [02:58<00:48,  3.43it/s] 71%|███████▏  | 418/585 [02:58<00:48,  3.42it/s] 72%|███████▏  | 419/585 [02:59<00:48,  3.43it/s] 72%|███████▏  | 420/585 [02:59<00:48,  3.43it/s] 72%|███████▏  | 421/585 [02:59<00:47,  3.43it/s] 72%|███████▏  | 422/585 [02:59<00:47,  3.43it/s] 72%|███████▏  | 423/585 [03:00<00:47,  3.42it/s] 72%|███████▏  | 424/585 [03:00<00:47,  3.40it/s] 73%|███████▎  | 425/585 [03:00<00:46,  3.41it/s] 73%|███████▎  | 426/585 [03:01<00:46,  3.42it/s] 73%|███████▎  | 427/585 [03:01<00:46,  3.42it/s] 73%|███████▎  | 428/585 [03:01<00:45,  3.42it/s] 73%|███████▎  | 429/585 [03:02<00:45,  3.42it/s] 74%|███████▎  | 430/585 [03:02<00:45,  3.43it/s] 74%|███████▎  | 431/585 [03:02<00:44,  3.43it/s] 74%|███████▍  | 432/585 [03:02<00:44,  3.43it/s] 74%|███████▍  | 433/585 [03:03<00:44,  3.43it/s] 74%|███████▍  | 434/585 [03:03<00:44,  3.43it/s] 74%|███████▍  | 435/585 [03:03<00:43,  3.43it/s] 75%|███████▍  | 436/585 [03:04<00:43,  3.43it/s] 75%|███████▍  | 437/585 [03:04<00:43,  3.43it/s] 75%|███████▍  | 438/585 [03:04<00:42,  3.43it/s] 75%|███████▌  | 439/585 [03:04<00:42,  3.43it/s] 75%|███████▌  | 440/585 [03:05<00:42,  3.42it/s] 75%|███████▌  | 441/585 [03:05<00:42,  3.42it/s] 76%|███████▌  | 442/585 [03:05<00:41,  3.43it/s] 76%|███████▌  | 443/585 [03:06<00:41,  3.43it/s] 76%|███████▌  | 444/585 [03:06<00:41,  3.43it/s] 76%|███████▌  | 445/585 [03:06<00:40,  3.43it/s] 76%|███████▌  | 446/585 [03:06<00:40,  3.43it/s] 76%|███████▋  | 447/585 [03:07<00:40,  3.43it/s] 77%|███████▋  | 448/585 [03:07<00:39,  3.43it/s] 77%|███████▋  | 449/585 [03:07<00:39,  3.43it/s] 77%|███████▋  | 450/585 [03:08<00:39,  3.43it/s] 77%|███████▋  | 451/585 [03:08<00:39,  3.42it/s] 77%|███████▋  | 452/585 [03:08<00:38,  3.42it/s] 77%|███████▋  | 453/585 [03:09<00:38,  3.43it/s] 78%|███████▊  | 454/585 [03:09<00:38,  3.43it/s] 78%|███████▊  | 455/585 [03:09<00:37,  3.43it/s] 78%|███████▊  | 456/585 [03:09<00:37,  3.43it/s] 78%|███████▊  | 457/585 [03:10<00:37,  3.43it/s] 78%|███████▊  | 458/585 [03:10<00:37,  3.43it/s] 78%|███████▊  | 459/585 [03:10<00:36,  3.43it/s] 79%|███████▊  | 460/585 [03:11<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:11<00:36,  3.43it/s] 79%|███████▉  | 462/585 [03:11<00:36,  3.42it/s] 79%|███████▉  | 463/585 [03:11<00:35,  3.42it/s] 79%|███████▉  | 464/585 [03:12<00:35,  3.43it/s] 79%|███████▉  | 465/585 [03:12<00:35,  3.43it/s] 80%|███████▉  | 466/585 [03:12<00:34,  3.43it/s] 80%|███████▉  | 467/585 [03:13<00:34,  3.43it/s] 80%|████████  | 468/585 [03:13<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 15:04:42,000 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:04:42,000 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 15:04:42,000 >>   Batch size = 8
{'eval_loss': 0.9599989652633667, 'eval_runtime': 12.3245, 'eval_samples_per_second': 352.306, 'eval_steps_per_second': 44.059, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.82it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.73it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.93it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.09it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.71it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.49it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.32it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.29it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.32it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.36it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.26it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.07it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.07it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.00it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.99it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.05it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.12it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.22it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.15it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.00it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.06it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.95it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.98it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.01it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.02it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.14it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.27it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.11it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.98it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.08it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.00it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.96it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.99it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.13it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.19it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.22it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.20it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.95it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.98it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.99it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.76it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.00it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.12it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.16it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.19it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.95it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.99it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.95it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.95it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.07it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.17it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.23it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.10it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.16it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.00it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.84it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.93it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.08it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.93it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.10it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.19it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.13it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.08it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.93it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.94it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.02it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.10it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.05it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.08it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.14it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.97it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.05it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.08it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.96it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.97it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.18it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.19it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.06it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.12it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.10it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.03it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.02it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.96it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.01it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.09it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.03it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.18it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.93it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.19it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.17it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.10it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.98it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.00it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.13it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.05it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.99it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.16it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.10it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.02it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.08it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.99it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.06it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.01it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.03it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.15it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [03:25<00:34,  3.43it/s]
100%|██████████| 543/543 [00:12<00:00, 44.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:04:54,344 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 15:04:54,367 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:04:56,071 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:04:56,084 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:04:56,092 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:32<11:12,  5.80s/it] 80%|████████  | 470/585 [03:32<07:57,  4.15s/it] 81%|████████  | 471/585 [03:32<05:41,  2.99s/it] 81%|████████  | 472/585 [03:32<04:06,  2.18s/it] 81%|████████  | 473/585 [03:33<03:01,  1.62s/it] 81%|████████  | 474/585 [03:33<02:15,  1.22s/it] 81%|████████  | 475/585 [03:33<01:43,  1.06it/s] 81%|████████▏ | 476/585 [03:34<01:21,  1.34it/s] 82%|████████▏ | 477/585 [03:34<01:06,  1.63it/s] 82%|████████▏ | 478/585 [03:34<00:55,  1.93it/s] 82%|████████▏ | 479/585 [03:35<00:47,  2.22it/s] 82%|████████▏ | 480/585 [03:35<00:42,  2.48it/s] 82%|████████▏ | 481/585 [03:35<00:38,  2.69it/s] 82%|████████▏ | 482/585 [03:35<00:35,  2.87it/s] 83%|████████▎ | 483/585 [03:36<00:33,  3.01it/s] 83%|████████▎ | 484/585 [03:36<00:32,  3.11it/s] 83%|████████▎ | 485/585 [03:36<00:31,  3.19it/s] 83%|████████▎ | 486/585 [03:37<00:30,  3.25it/s] 83%|████████▎ | 487/585 [03:37<00:29,  3.29it/s] 83%|████████▎ | 488/585 [03:37<00:29,  3.32it/s] 84%|████████▎ | 489/585 [03:37<00:28,  3.34it/s] 84%|████████▍ | 490/585 [03:38<00:28,  3.36it/s] 84%|████████▍ | 491/585 [03:38<00:27,  3.37it/s] 84%|████████▍ | 492/585 [03:38<00:27,  3.37it/s] 84%|████████▍ | 493/585 [03:39<00:27,  3.37it/s] 84%|████████▍ | 494/585 [03:39<00:26,  3.37it/s] 85%|████████▍ | 495/585 [03:39<00:26,  3.38it/s] 85%|████████▍ | 496/585 [03:40<00:26,  3.38it/s] 85%|████████▍ | 497/585 [03:40<00:26,  3.38it/s] 85%|████████▌ | 498/585 [03:40<00:25,  3.38it/s] 85%|████████▌ | 499/585 [03:40<00:25,  3.39it/s] 85%|████████▌ | 500/585 [03:41<00:25,  3.39it/s]                                                  85%|████████▌ | 500/585 [03:41<00:25,  3.39it/s] 86%|████████▌ | 501/585 [03:41<00:24,  3.39it/s] 86%|████████▌ | 502/585 [03:41<00:24,  3.39it/s] 86%|████████▌ | 503/585 [03:42<00:24,  3.38it/s] 86%|████████▌ | 504/585 [03:42<00:23,  3.38it/s] 86%|████████▋ | 505/585 [03:42<00:23,  3.39it/s] 86%|████████▋ | 506/585 [03:42<00:23,  3.39it/s] 87%|████████▋ | 507/585 [03:43<00:23,  3.39it/s] 87%|████████▋ | 508/585 [03:43<00:22,  3.38it/s] 87%|████████▋ | 509/585 [03:43<00:22,  3.40it/s] 87%|████████▋ | 510/585 [03:44<00:22,  3.41it/s] 87%|████████▋ | 511/585 [03:44<00:21,  3.42it/s] 88%|████████▊ | 512/585 [03:44<00:21,  3.42it/s] 88%|████████▊ | 513/585 [03:45<00:21,  3.42it/s] 88%|████████▊ | 514/585 [03:45<00:20,  3.42it/s] 88%|████████▊ | 515/585 [03:45<00:20,  3.42it/s] 88%|████████▊ | 516/585 [03:45<00:20,  3.43it/s] 88%|████████▊ | 517/585 [03:46<00:19,  3.43it/s] 89%|████████▊ | 518/585 [03:46<00:19,  3.43it/s] 89%|████████▊ | 519/585 [03:46<00:19,  3.43it/s] 89%|████████▉ | 520/585 [03:47<00:18,  3.43it/s] 89%|████████▉ | 521/585 [03:47<00:18,  3.44it/s] 89%|████████▉ | 522/585 [03:47<00:18,  3.44it/s] 89%|████████▉ | 523/585 [03:47<00:18,  3.44it/s] 90%|████████▉ | 524/585 [03:48<00:17,  3.44it/s] 90%|████████▉ | 525/585 [03:48<00:17,  3.43it/s] 90%|████████▉ | 526/585 [03:48<00:17,  3.43it/s] 90%|█████████ | 527/585 [03:49<00:16,  3.43it/s] 90%|█████████ | 528/585 [03:49<00:16,  3.43it/s] 90%|█████████ | 529/585 [03:49<00:16,  3.43it/s] 91%|█████████ | 530/585 [03:49<00:16,  3.43it/s] 91%|█████████ | 531/585 [03:50<00:15,  3.43it/s] 91%|█████████ | 532/585 [03:50<00:15,  3.43it/s] 91%|█████████ | 533/585 [03:50<00:15,  3.43it/s] 91%|█████████▏| 534/585 [03:51<00:14,  3.43it/s] 91%|█████████▏| 535/585 [03:51<00:14,  3.44it/s] 92%|█████████▏| 536/585 [03:51<00:14,  3.42it/s] 92%|█████████▏| 537/585 [03:52<00:13,  3.43it/s] 92%|█████████▏| 538/585 [03:52<00:13,  3.43it/s] 92%|█████████▏| 539/585 [03:52<00:13,  3.43it/s] 92%|█████████▏| 540/585 [03:52<00:13,  3.34it/s] 92%|█████████▏| 541/585 [03:53<00:13,  3.37it/s] 93%|█████████▎| 542/585 [03:53<00:12,  3.39it/s] 93%|█████████▎| 543/585 [03:53<00:12,  3.40it/s] 93%|█████████▎| 544/585 [03:54<00:12,  3.41it/s] 93%|█████████▎| 545/585 [03:54<00:11,  3.42it/s] 93%|█████████▎| 546/585 [03:54<00:11,  3.42it/s] 94%|█████████▎| 547/585 [03:54<00:11,  3.41it/s] 94%|█████████▎| 548/585 [03:55<00:10,  3.42it/s] 94%|█████████▍| 549/585 [03:55<00:10,  3.42it/s] 94%|█████████▍| 550/585 [03:55<00:10,  3.43it/s] 94%|█████████▍| 551/585 [03:56<00:09,  3.43it/s] 94%|█████████▍| 552/585 [03:56<00:09,  3.43it/s] 95%|█████████▍| 553/585 [03:56<00:09,  3.43it/s] 95%|█████████▍| 554/585 [03:57<00:09,  3.43it/s] 95%|█████████▍| 555/585 [03:57<00:08,  3.43it/s] 95%|█████████▌| 556/585 [03:57<00:08,  3.43it/s] 95%|█████████▌| 557/585 [03:57<00:08,  3.43it/s] 95%|█████████▌| 558/585 [03:58<00:07,  3.43it/s] 96%|█████████▌| 559/585 [03:58<00:07,  3.43it/s] 96%|█████████▌| 560/585 [03:58<00:07,  3.43it/s] 96%|█████████▌| 561/585 [03:59<00:06,  3.43it/s] 96%|█████████▌| 562/585 [03:59<00:06,  3.43it/s] 96%|█████████▌| 563/585 [03:59<00:06,  3.43it/s] 96%|█████████▋| 564/585 [03:59<00:06,  3.43it/s] 97%|█████████▋| 565/585 [04:00<00:05,  3.41it/s] 97%|█████████▋| 566/585 [04:00<00:05,  3.40it/s] 97%|█████████▋| 567/585 [04:00<00:05,  3.39it/s] 97%|█████████▋| 568/585 [04:01<00:04,  3.40it/s] 97%|█████████▋| 569/585 [04:01<00:04,  3.41it/s] 97%|█████████▋| 570/585 [04:01<00:04,  3.42it/s] 98%|█████████▊| 571/585 [04:01<00:04,  3.42it/s] 98%|█████████▊| 572/585 [04:02<00:03,  3.43it/s] 98%|█████████▊| 573/585 [04:02<00:03,  3.43it/s] 98%|█████████▊| 574/585 [04:02<00:03,  3.43it/s] 98%|█████████▊| 575/585 [04:03<00:02,  3.43it/s] 98%|█████████▊| 576/585 [04:03<00:02,  3.42it/s] 99%|█████████▊| 577/585 [04:03<00:02,  3.42it/s] 99%|█████████▉| 578/585 [04:04<00:02,  3.43it/s] 99%|█████████▉| 579/585 [04:04<00:01,  3.43it/s] 99%|█████████▉| 580/585 [04:04<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:04<00:01,  3.43it/s] 99%|█████████▉| 582/585 [04:05<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:05<00:00,  3.43it/s]100%|█████████▉| 584/585 [04:05<00:00,  3.43it/s]100%|██████████| 585/585 [04:06<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 15:05:34,542 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:05:34,542 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 15:05:34,542 >>   Batch size = 8
{'eval_loss': 0.9640052914619446, 'eval_runtime': 12.3209, 'eval_samples_per_second': 352.41, 'eval_steps_per_second': 44.072, 'epoch': 4.0}
{'loss': 0.8291, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.84it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.72it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.08it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.22it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.73it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.57it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.44it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.23it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.33it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.30it/s][A
 10%|█         | 57/543 [00:01<00:11, 44.04it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 43.90it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.04it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.85it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.03it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.05it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.07it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.03it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.13it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.08it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.03it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.12it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.11it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.03it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.02it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.14it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.13it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.16it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.08it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.00it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.94it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.07it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.97it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.08it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.18it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.19it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.16it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.11it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.91it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.06it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.00it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.07it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.25it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.13it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.16it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.10it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.10it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.05it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.97it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.06it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.06it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.14it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.10it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.06it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.12it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.11it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.97it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.13it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.04it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.06it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.11it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.07it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.06it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.99it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.02it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.99it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.18it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.19it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.06it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.16it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.07it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.04it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.08it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.00it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.13it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.13it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.10it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.16it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.14it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.14it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.04it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.70it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.02it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.11it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.18it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.15it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.11it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.07it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.11it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.99it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.93it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.98it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.03it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.12it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.15it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.20it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.19it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.10it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.06it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.86it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.07it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.08it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.12it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.15it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.03it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.20it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.06it/s][A                                                 
                                                 [A100%|██████████| 585/585 [04:18<00:00,  3.43it/s]
100%|██████████| 543/543 [00:12<00:00, 44.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 15:05:46,892 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 15:05:46,924 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:05:49,145 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:05:49,165 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:05:49,175 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 15:05:53,851 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 15:05:53,855 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234 (score: 0.9565653204917908).
                                                 100%|██████████| 585/585 [04:27<00:00,  3.43it/s]100%|██████████| 585/585 [04:27<00:00,  2.19it/s]
[INFO|trainer.py:1894] 2023-08-28 15:05:55,879 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 15:05:55,894 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 15:05:57,959 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 15:05:57,979 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 15:05:57,990 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:05:58,190 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:05:58,190 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:05:58,190 >>   train_loss               =     0.8237
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:05:58,190 >>   train_runtime            = 0:04:27.39
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:05:58,190 >>   train_samples            =       7515
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:05:58,190 >>   train_samples_per_second =    140.523
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:05:58,190 >>   train_steps_per_second   =      2.188
{'eval_loss': 0.9656103849411011, 'eval_runtime': 12.3172, 'eval_samples_per_second': 352.516, 'eval_steps_per_second': 44.085, 'epoch': 5.0}
{'train_runtime': 267.3943, 'train_samples_per_second': 140.523, 'train_steps_per_second': 2.188, 'train_loss': 0.8236608260717148, 'epoch': 5.0}
08/28/2023 15:05:58 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 15:05:58,227 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 15:05:58,227 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 15:05:58,227 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.49it/s]  2%|▏         | 12/543 [00:00<00:10, 48.64it/s]  3%|▎         | 17/543 [00:00<00:11, 46.81it/s]  4%|▍         | 22/543 [00:00<00:11, 46.12it/s]  5%|▍         | 27/543 [00:00<00:11, 45.72it/s]  6%|▌         | 32/543 [00:00<00:11, 45.49it/s]  7%|▋         | 37/543 [00:00<00:11, 45.20it/s]  8%|▊         | 42/543 [00:00<00:11, 44.70it/s]  9%|▊         | 47/543 [00:01<00:11, 44.14it/s] 10%|▉         | 52/543 [00:01<00:11, 43.98it/s] 10%|█         | 57/543 [00:01<00:11, 44.06it/s] 11%|█▏        | 62/543 [00:01<00:10, 44.13it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.38it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.46it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.57it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.49it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.18it/s] 17%|█▋        | 92/543 [00:02<00:10, 43.90it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.74it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.87it/s] 20%|█▉        | 107/543 [00:02<00:09, 44.06it/s] 21%|██        | 112/543 [00:02<00:09, 44.31it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.41it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.49it/s] 23%|██▎       | 127/543 [00:02<00:09, 44.37it/s] 24%|██▍       | 132/543 [00:02<00:09, 44.14it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.86it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.80it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.92it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.07it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.32it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.39it/s] 31%|███       | 167/543 [00:03<00:08, 44.35it/s] 32%|███▏      | 172/543 [00:03<00:08, 44.29it/s] 33%|███▎      | 177/543 [00:03<00:08, 44.03it/s] 34%|███▎      | 182/543 [00:04<00:08, 43.75it/s] 34%|███▍      | 187/543 [00:04<00:08, 43.73it/s] 35%|███▌      | 192/543 [00:04<00:07, 43.98it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.18it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.23it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.36it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.33it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.08it/s] 41%|████      | 222/543 [00:04<00:07, 43.99it/s] 42%|████▏     | 227/543 [00:05<00:07, 43.87it/s] 43%|████▎     | 232/543 [00:05<00:07, 43.80it/s] 44%|████▎     | 237/543 [00:05<00:06, 44.07it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.19it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.33it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.48it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.25it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.02it/s] 49%|████▉     | 267/543 [00:06<00:06, 43.93it/s] 50%|█████     | 272/543 [00:06<00:06, 43.83it/s] 51%|█████     | 277/543 [00:06<00:06, 43.84it/s] 52%|█████▏    | 282/543 [00:06<00:05, 43.95it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.14it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.41it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.38it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.31it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.03it/s] 57%|█████▋    | 312/543 [00:07<00:05, 43.60it/s] 58%|█████▊    | 317/543 [00:07<00:05, 43.83it/s] 59%|█████▉    | 322/543 [00:07<00:05, 43.95it/s] 60%|██████    | 327/543 [00:07<00:04, 44.14it/s] 61%|██████    | 332/543 [00:07<00:04, 44.18it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.41it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.37it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.15it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.02it/s] 66%|██████▌   | 357/543 [00:08<00:04, 43.96it/s] 67%|██████▋   | 362/543 [00:08<00:04, 43.79it/s] 68%|██████▊   | 367/543 [00:08<00:04, 43.65it/s] 69%|██████▊   | 372/543 [00:08<00:03, 44.10it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.18it/s] 70%|███████   | 382/543 [00:08<00:03, 44.34it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.31it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.15it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.02it/s] 74%|███████▍  | 402/543 [00:09<00:03, 43.89it/s] 75%|███████▍  | 407/543 [00:09<00:03, 43.84it/s] 76%|███████▌  | 412/543 [00:09<00:02, 43.96it/s] 77%|███████▋  | 417/543 [00:09<00:02, 43.96it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.17it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.27it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.29it/s] 80%|████████  | 437/543 [00:09<00:02, 44.20it/s] 81%|████████▏ | 442/543 [00:09<00:02, 43.97it/s] 82%|████████▏ | 447/543 [00:10<00:02, 43.92it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.00it/s] 84%|████████▍ | 457/543 [00:10<00:01, 43.96it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.13it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.12it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.27it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.26it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.10it/s] 90%|████████▉ | 487/543 [00:11<00:01, 43.88it/s] 91%|█████████ | 492/543 [00:11<00:01, 43.95it/s] 92%|█████████▏| 497/543 [00:11<00:01, 43.98it/s] 92%|█████████▏| 502/543 [00:11<00:00, 43.99it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.02it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.17it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.12it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.18it/s] 97%|█████████▋| 527/543 [00:11<00:00, 44.09it/s] 98%|█████████▊| 532/543 [00:12<00:00, 43.95it/s] 99%|█████████▉| 537/543 [00:12<00:00, 43.95it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.01it/s]100%|██████████| 543/543 [00:12<00:00, 44.19it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 15:06:10,532 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:10,532 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:10,532 >>   eval_loss               =     0.9566
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:10,532 >>   eval_runtime            = 0:00:12.30
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:10,532 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:10,532 >>   eval_samples_per_second =    352.879
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:10,532 >>   eval_steps_per_second   =      44.13
[INFO|trainer_pt_utils.py:913] 2023-08-28 15:06:10,532 >>   perplexity              =     2.6027
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:17,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:17,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:17,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:17,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:17,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:06:18,108 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:06:18,109 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:06:18,685 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:06:19,697 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:06:19,698 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:22,745 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:22,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:22,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:22,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:06:22,751 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:06:23,375 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:06:23,376 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:06:23,955 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:06:24,135 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:06:24,135 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.84it/s]Extractor Predicting: 2it [00:01,  1.80it/s]Extractor Predicting: 3it [00:01,  1.76it/s]Extractor Predicting: 4it [00:02,  1.81it/s]Extractor Predicting: 5it [00:02,  1.76it/s]Extractor Predicting: 6it [00:03,  1.71it/s]Extractor Predicting: 7it [00:03,  1.73it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.79it/s]Extractor Predicting: 10it [00:05,  1.79it/s]Extractor Predicting: 11it [00:06,  1.81it/s]Extractor Predicting: 12it [00:06,  1.82it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:07,  1.69it/s]Extractor Predicting: 15it [00:08,  1.63it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:09,  1.62it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.56it/s]Extractor Predicting: 20it [00:11,  1.54it/s]Extractor Predicting: 21it [00:12,  1.57it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:13,  1.61it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.51it/s]Extractor Predicting: 26it [00:15,  1.53it/s]Extractor Predicting: 27it [00:16,  1.54it/s]Extractor Predicting: 28it [00:16,  1.57it/s]Extractor Predicting: 29it [00:17,  1.56it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:18,  1.58it/s]Extractor Predicting: 32it [00:19,  1.59it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:20,  1.59it/s]Extractor Predicting: 35it [00:21,  1.58it/s]Extractor Predicting: 36it [00:22,  1.55it/s]Extractor Predicting: 37it [00:22,  1.54it/s]Extractor Predicting: 38it [00:23,  1.54it/s]Extractor Predicting: 39it [00:23,  1.57it/s]Extractor Predicting: 40it [00:24,  1.57it/s]Extractor Predicting: 41it [00:25,  1.55it/s]Extractor Predicting: 42it [00:25,  1.54it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:27,  1.58it/s]Extractor Predicting: 46it [00:28,  1.60it/s]Extractor Predicting: 47it [00:29,  1.60it/s]Extractor Predicting: 48it [00:29,  1.58it/s]Extractor Predicting: 49it [00:30,  1.57it/s]Extractor Predicting: 50it [00:30,  1.59it/s]Extractor Predicting: 51it [00:31,  1.59it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:32,  1.55it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.62it/s]Extractor Predicting: 56it [00:34,  1.59it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:35,  1.60it/s]Extractor Predicting: 59it [00:36,  1.55it/s]Extractor Predicting: 60it [00:37,  1.57it/s]Extractor Predicting: 61it [00:37,  1.59it/s]Extractor Predicting: 62it [00:38,  1.59it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:40,  1.63it/s]Extractor Predicting: 66it [00:40,  1.63it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.58it/s]Extractor Predicting: 69it [00:42,  1.59it/s]Extractor Predicting: 70it [00:43,  1.59it/s]Extractor Predicting: 71it [00:44,  1.58it/s]Extractor Predicting: 72it [00:44,  1.59it/s]Extractor Predicting: 73it [00:45,  1.57it/s]Extractor Predicting: 74it [00:46,  1.55it/s]Extractor Predicting: 75it [00:46,  1.52it/s]Extractor Predicting: 76it [00:47,  1.54it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:48,  1.56it/s]Extractor Predicting: 79it [00:49,  1.55it/s]Extractor Predicting: 80it [00:49,  1.57it/s]Extractor Predicting: 81it [00:50,  1.57it/s]Extractor Predicting: 82it [00:51,  1.58it/s]Extractor Predicting: 83it [00:51,  1.58it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:53,  1.58it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:54,  1.60it/s]Extractor Predicting: 89it [00:55,  1.62it/s]Extractor Predicting: 90it [00:56,  1.59it/s]Extractor Predicting: 91it [00:56,  1.56it/s]Extractor Predicting: 92it [00:57,  1.54it/s]Extractor Predicting: 93it [00:58,  1.59it/s]Extractor Predicting: 94it [00:58,  1.60it/s]Extractor Predicting: 95it [00:59,  1.59it/s]Extractor Predicting: 96it [00:59,  1.61it/s]Extractor Predicting: 97it [01:00,  1.63it/s]Extractor Predicting: 98it [01:01,  1.60it/s]Extractor Predicting: 99it [01:02,  1.45it/s]Extractor Predicting: 100it [01:02,  1.49it/s]Extractor Predicting: 101it [01:03,  1.53it/s]Extractor Predicting: 102it [01:03,  1.54it/s]Extractor Predicting: 103it [01:04,  1.56it/s]Extractor Predicting: 104it [01:05,  1.57it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:06,  1.58it/s]Extractor Predicting: 107it [01:07,  1.59it/s]Extractor Predicting: 108it [01:07,  1.58it/s]Extractor Predicting: 109it [01:08,  1.59it/s]Extractor Predicting: 110it [01:08,  1.57it/s]Extractor Predicting: 111it [01:09,  1.58it/s]Extractor Predicting: 112it [01:10,  1.61it/s]Extractor Predicting: 113it [01:10,  1.62it/s]Extractor Predicting: 114it [01:11,  1.59it/s]Extractor Predicting: 115it [01:12,  1.58it/s]Extractor Predicting: 116it [01:12,  1.58it/s]Extractor Predicting: 117it [01:13,  1.59it/s]Extractor Predicting: 118it [01:14,  1.55it/s]Extractor Predicting: 119it [01:14,  1.56it/s]Extractor Predicting: 120it [01:15,  1.55it/s]Extractor Predicting: 121it [01:15,  1.53it/s]Extractor Predicting: 122it [01:16,  1.53it/s]Extractor Predicting: 123it [01:17,  1.57it/s]Extractor Predicting: 124it [01:17,  1.59it/s]Extractor Predicting: 125it [01:18,  1.61it/s]Extractor Predicting: 126it [01:19,  1.55it/s]Extractor Predicting: 127it [01:19,  1.55it/s]Extractor Predicting: 128it [01:20,  1.57it/s]Extractor Predicting: 129it [01:21,  1.59it/s]Extractor Predicting: 130it [01:21,  1.64it/s]Extractor Predicting: 131it [01:22,  1.58it/s]Extractor Predicting: 132it [01:22,  1.58it/s]Extractor Predicting: 133it [01:23,  1.59it/s]Extractor Predicting: 134it [01:24,  1.59it/s]Extractor Predicting: 135it [01:24,  1.60it/s]Extractor Predicting: 136it [01:25,  1.60it/s]Extractor Predicting: 137it [01:26,  1.61it/s]Extractor Predicting: 138it [01:26,  1.58it/s]Extractor Predicting: 139it [01:27,  1.56it/s]Extractor Predicting: 140it [01:27,  1.60it/s]Extractor Predicting: 141it [01:28,  1.61it/s]Extractor Predicting: 142it [01:29,  1.63it/s]Extractor Predicting: 143it [01:29,  1.63it/s]Extractor Predicting: 144it [01:30,  1.64it/s]Extractor Predicting: 145it [01:30,  1.61it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:32,  1.62it/s]Extractor Predicting: 148it [01:32,  1.63it/s]Extractor Predicting: 149it [01:33,  1.63it/s]Extractor Predicting: 150it [01:34,  1.59it/s]Extractor Predicting: 151it [01:34,  1.59it/s]Extractor Predicting: 152it [01:35,  1.59it/s]Extractor Predicting: 153it [01:36,  1.57it/s]Extractor Predicting: 154it [01:36,  1.58it/s]Extractor Predicting: 155it [01:37,  1.56it/s]Extractor Predicting: 156it [01:37,  1.55it/s]Extractor Predicting: 157it [01:38,  1.56it/s]Extractor Predicting: 158it [01:39,  1.57it/s]Extractor Predicting: 159it [01:39,  1.59it/s]Extractor Predicting: 160it [01:40,  1.62it/s]Extractor Predicting: 161it [01:41,  1.62it/s]Extractor Predicting: 162it [01:41,  1.62it/s]Extractor Predicting: 163it [01:42,  1.54it/s]Extractor Predicting: 163it [01:42,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:14,043 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:14,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:14,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:14,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:14,047 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:08:14,773 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:08:14,774 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:08:15,031 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:08:16,069 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:08:16,069 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:17,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:17,652 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:17,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:17,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:08:17,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:08:17,994 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:08:17,995 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:08:18,276 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:08:18,449 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:08:18,449 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6511627906976745,
  "recall": 0.006448641179180101,
  "score": 0.012770809578107183,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.66it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:04,  1.58it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.64it/s]Extractor Predicting: 24it [00:14,  1.62it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.64it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:16,  1.65it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:18,  1.66it/s]Extractor Predicting: 32it [00:19,  1.66it/s]Extractor Predicting: 33it [00:19,  1.65it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:21,  1.65it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:24,  1.58it/s]Extractor Predicting: 42it [00:25,  1.59it/s]Extractor Predicting: 43it [00:26,  1.57it/s]Extractor Predicting: 44it [00:26,  1.58it/s]Extractor Predicting: 45it [00:27,  1.56it/s]Extractor Predicting: 46it [00:28,  1.54it/s]Extractor Predicting: 47it [00:28,  1.52it/s]Extractor Predicting: 48it [00:29,  1.55it/s]Extractor Predicting: 49it [00:30,  1.54it/s]Extractor Predicting: 50it [00:30,  1.55it/s]Extractor Predicting: 51it [00:31,  1.55it/s]Extractor Predicting: 52it [00:32,  1.54it/s]Extractor Predicting: 53it [00:32,  1.53it/s]Extractor Predicting: 54it [00:33,  1.49it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:34,  1.58it/s]Extractor Predicting: 57it [00:35,  1.56it/s]Extractor Predicting: 58it [00:35,  1.56it/s]Extractor Predicting: 59it [00:36,  1.53it/s]Extractor Predicting: 60it [00:37,  1.53it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.56it/s]Extractor Predicting: 63it [00:39,  1.51it/s]Extractor Predicting: 64it [00:39,  1.52it/s]Extractor Predicting: 65it [00:40,  1.53it/s]Extractor Predicting: 66it [00:41,  1.51it/s]Extractor Predicting: 67it [00:41,  1.52it/s]Extractor Predicting: 68it [00:42,  1.52it/s]Extractor Predicting: 69it [00:43,  1.52it/s]Extractor Predicting: 70it [00:43,  1.53it/s]Extractor Predicting: 71it [00:44,  1.54it/s]Extractor Predicting: 72it [00:45,  1.54it/s]Extractor Predicting: 73it [00:45,  1.53it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:48,  1.59it/s]Extractor Predicting: 78it [00:48,  1.58it/s]Extractor Predicting: 79it [00:49,  1.55it/s]Extractor Predicting: 80it [00:50,  1.58it/s]Extractor Predicting: 81it [00:50,  1.47it/s]Extractor Predicting: 82it [00:51,  1.52it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:52,  1.52it/s]Extractor Predicting: 85it [00:53,  1.53it/s]Extractor Predicting: 86it [00:54,  1.55it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:55,  1.55it/s]Extractor Predicting: 89it [00:56,  1.59it/s]Extractor Predicting: 90it [00:56,  1.60it/s]Extractor Predicting: 91it [00:57,  1.62it/s]Extractor Predicting: 92it [00:57,  1.62it/s]Extractor Predicting: 93it [00:58,  1.62it/s]Extractor Predicting: 94it [00:59,  1.60it/s]Extractor Predicting: 95it [00:59,  1.62it/s]Extractor Predicting: 96it [01:00,  1.64it/s]Extractor Predicting: 97it [01:01,  1.46it/s]Extractor Predicting: 98it [01:01,  1.45it/s]Extractor Predicting: 99it [01:02,  1.49it/s]Extractor Predicting: 100it [01:03,  1.51it/s]Extractor Predicting: 101it [01:03,  1.52it/s]Extractor Predicting: 102it [01:04,  1.55it/s]Extractor Predicting: 103it [01:05,  1.54it/s]Extractor Predicting: 104it [01:05,  1.56it/s]Extractor Predicting: 105it [01:06,  1.55it/s]Extractor Predicting: 106it [01:07,  1.52it/s]Extractor Predicting: 107it [01:07,  1.51it/s]Extractor Predicting: 108it [01:08,  1.55it/s]Extractor Predicting: 109it [01:09,  1.49it/s]Extractor Predicting: 110it [01:09,  1.52it/s]Extractor Predicting: 111it [01:10,  1.51it/s]Extractor Predicting: 112it [01:10,  1.53it/s]Extractor Predicting: 113it [01:11,  1.52it/s]Extractor Predicting: 114it [01:12,  1.53it/s]Extractor Predicting: 115it [01:12,  1.53it/s]Extractor Predicting: 116it [01:13,  1.51it/s]Extractor Predicting: 117it [01:14,  1.52it/s]Extractor Predicting: 118it [01:14,  1.56it/s]Extractor Predicting: 119it [01:15,  1.56it/s]Extractor Predicting: 120it [01:16,  1.60it/s]Extractor Predicting: 121it [01:16,  1.59it/s]Extractor Predicting: 122it [01:17,  1.61it/s]Extractor Predicting: 123it [01:17,  1.62it/s]Extractor Predicting: 124it [01:18,  1.65it/s]Extractor Predicting: 125it [01:19,  1.63it/s]Extractor Predicting: 126it [01:19,  1.60it/s]Extractor Predicting: 127it [01:20,  1.59it/s]Extractor Predicting: 128it [01:21,  1.58it/s]Extractor Predicting: 129it [01:21,  1.58it/s]Extractor Predicting: 130it [01:22,  1.57it/s]Extractor Predicting: 131it [01:22,  1.63it/s]Extractor Predicting: 132it [01:23,  1.60it/s]Extractor Predicting: 133it [01:24,  1.60it/s]Extractor Predicting: 134it [01:24,  1.63it/s]Extractor Predicting: 135it [01:25,  1.63it/s]Extractor Predicting: 136it [01:26,  1.61it/s]Extractor Predicting: 137it [01:26,  1.66it/s]Extractor Predicting: 138it [01:27,  1.64it/s]Extractor Predicting: 139it [01:27,  1.64it/s]Extractor Predicting: 140it [01:28,  1.62it/s]Extractor Predicting: 141it [01:29,  1.63it/s]Extractor Predicting: 142it [01:29,  1.61it/s]Extractor Predicting: 143it [01:30,  1.58it/s]Extractor Predicting: 144it [01:30,  1.60it/s]Extractor Predicting: 145it [01:31,  1.55it/s]Extractor Predicting: 146it [01:32,  1.50it/s]Extractor Predicting: 147it [01:32,  1.53it/s]Extractor Predicting: 148it [01:33,  1.53it/s]Extractor Predicting: 149it [01:34,  1.56it/s]Extractor Predicting: 150it [01:34,  1.57it/s]Extractor Predicting: 151it [01:35,  1.58it/s]Extractor Predicting: 152it [01:36,  1.57it/s]Extractor Predicting: 153it [01:36,  1.61it/s]Extractor Predicting: 154it [01:37,  1.62it/s]Extractor Predicting: 155it [01:37,  1.65it/s]Extractor Predicting: 156it [01:38,  1.60it/s]Extractor Predicting: 157it [01:39,  1.61it/s]Extractor Predicting: 158it [01:39,  1.61it/s]Extractor Predicting: 159it [01:40,  1.61it/s]Extractor Predicting: 160it [01:41,  1.64it/s]Extractor Predicting: 161it [01:41,  1.55it/s]Extractor Predicting: 162it [01:42,  1.60it/s]Extractor Predicting: 163it [01:42,  1.63it/s]Extractor Predicting: 164it [01:43,  1.61it/s]Extractor Predicting: 165it [01:44,  1.67it/s]Extractor Predicting: 166it [01:44,  1.67it/s]Extractor Predicting: 167it [01:45,  1.66it/s]Extractor Predicting: 168it [01:45,  1.71it/s]Extractor Predicting: 169it [01:46,  1.72it/s]Extractor Predicting: 170it [01:47,  1.67it/s]Extractor Predicting: 171it [01:47,  1.66it/s]Extractor Predicting: 172it [01:48,  1.66it/s]Extractor Predicting: 173it [01:48,  1.62it/s]Extractor Predicting: 174it [01:49,  1.66it/s]Extractor Predicting: 175it [01:50,  1.65it/s]Extractor Predicting: 176it [01:50,  1.63it/s]Extractor Predicting: 177it [01:51,  1.62it/s]Extractor Predicting: 178it [01:52,  1.60it/s]Extractor Predicting: 179it [01:52,  1.67it/s]Extractor Predicting: 180it [01:53,  1.64it/s]Extractor Predicting: 181it [01:53,  1.63it/s]Extractor Predicting: 182it [01:54,  1.62it/s]Extractor Predicting: 183it [01:55,  1.61it/s]Extractor Predicting: 184it [01:55,  1.60it/s]Extractor Predicting: 185it [01:56,  1.63it/s]Extractor Predicting: 186it [01:56,  1.61it/s]Extractor Predicting: 187it [01:57,  1.43it/s]Extractor Predicting: 188it [01:58,  1.47it/s]Extractor Predicting: 189it [01:59,  1.49it/s]Extractor Predicting: 190it [01:59,  1.48it/s]Extractor Predicting: 191it [02:00,  1.50it/s]Extractor Predicting: 192it [02:01,  1.53it/s]Extractor Predicting: 193it [02:01,  1.57it/s]Extractor Predicting: 194it [02:02,  1.59it/s]Extractor Predicting: 195it [02:02,  1.56it/s]Extractor Predicting: 196it [02:03,  1.57it/s]Extractor Predicting: 197it [02:04,  1.58it/s]Extractor Predicting: 198it [02:04,  1.59it/s]Extractor Predicting: 199it [02:05,  1.58it/s]Extractor Predicting: 200it [02:06,  1.57it/s]Extractor Predicting: 201it [02:06,  1.58it/s]Extractor Predicting: 202it [02:07,  1.59it/s]Extractor Predicting: 203it [02:07,  1.59it/s]Extractor Predicting: 204it [02:08,  1.60it/s]Extractor Predicting: 205it [02:09,  1.59it/s]Extractor Predicting: 206it [02:09,  1.58it/s]Extractor Predicting: 207it [02:10,  1.60it/s]Extractor Predicting: 208it [02:11,  1.60it/s]Extractor Predicting: 209it [02:11,  1.59it/s]Extractor Predicting: 210it [02:12,  1.58it/s]Extractor Predicting: 211it [02:13,  1.58it/s]Extractor Predicting: 212it [02:13,  1.60it/s]Extractor Predicting: 213it [02:14,  1.59it/s]Extractor Predicting: 214it [02:14,  1.59it/s]Extractor Predicting: 215it [02:15,  1.55it/s]Extractor Predicting: 216it [02:16,  1.58it/s]Extractor Predicting: 217it [02:16,  1.60it/s]Extractor Predicting: 218it [02:17,  1.60it/s]Extractor Predicting: 219it [02:18,  1.58it/s]Extractor Predicting: 220it [02:18,  1.59it/s]Extractor Predicting: 221it [02:19,  1.57it/s]Extractor Predicting: 222it [02:19,  1.59it/s]Extractor Predicting: 223it [02:20,  1.53it/s]Extractor Predicting: 224it [02:21,  1.54it/s]Extractor Predicting: 225it [02:21,  1.52it/s]Extractor Predicting: 226it [02:22,  1.53it/s]Extractor Predicting: 227it [02:23,  1.50it/s]Extractor Predicting: 228it [02:24,  1.45it/s]Extractor Predicting: 229it [02:24,  1.47it/s]Extractor Predicting: 230it [02:25,  1.48it/s]Extractor Predicting: 231it [02:26,  1.49it/s]Extractor Predicting: 232it [02:26,  1.50it/s]Extractor Predicting: 233it [02:27,  1.52it/s]Extractor Predicting: 234it [02:27,  1.53it/s]Extractor Predicting: 235it [02:28,  1.54it/s]Extractor Predicting: 236it [02:29,  1.55it/s]Extractor Predicting: 237it [02:29,  1.55it/s]Extractor Predicting: 238it [02:30,  1.61it/s]Extractor Predicting: 239it [02:31,  1.63it/s]Extractor Predicting: 240it [02:31,  1.67it/s]Extractor Predicting: 241it [02:32,  1.66it/s]Extractor Predicting: 242it [02:32,  1.65it/s]Extractor Predicting: 243it [02:33,  1.62it/s]Extractor Predicting: 244it [02:34,  1.63it/s]Extractor Predicting: 245it [02:34,  1.66it/s]Extractor Predicting: 246it [02:35,  1.64it/s]Extractor Predicting: 247it [02:35,  1.65it/s]Extractor Predicting: 248it [02:36,  1.66it/s]Extractor Predicting: 249it [02:37,  1.71it/s]Extractor Predicting: 250it [02:37,  1.71it/s]Extractor Predicting: 251it [02:38,  1.77it/s]Extractor Predicting: 252it [02:38,  1.78it/s]Extractor Predicting: 253it [02:39,  1.75it/s]Extractor Predicting: 254it [02:39,  1.76it/s]Extractor Predicting: 255it [02:40,  1.71it/s]Extractor Predicting: 256it [02:41,  1.75it/s]Extractor Predicting: 257it [02:41,  1.70it/s]Extractor Predicting: 258it [02:42,  1.66it/s]Extractor Predicting: 259it [02:42,  1.61it/s]Extractor Predicting: 260it [02:43,  1.67it/s]Extractor Predicting: 261it [02:44,  1.63it/s]Extractor Predicting: 262it [02:44,  1.64it/s]Extractor Predicting: 263it [02:45,  1.69it/s]Extractor Predicting: 264it [02:45,  1.69it/s]Extractor Predicting: 265it [02:46,  1.69it/s]Extractor Predicting: 266it [02:47,  1.71it/s]Extractor Predicting: 267it [02:47,  1.74it/s]Extractor Predicting: 268it [02:48,  1.73it/s]Extractor Predicting: 269it [02:48,  1.69it/s]Extractor Predicting: 270it [02:49,  1.71it/s]Extractor Predicting: 271it [02:49,  1.69it/s]Extractor Predicting: 272it [02:50,  1.66it/s]Extractor Predicting: 273it [02:51,  1.71it/s]Extractor Predicting: 274it [02:51,  1.72it/s]Extractor Predicting: 275it [02:52,  1.70it/s]Extractor Predicting: 276it [02:52,  1.68it/s]Extractor Predicting: 277it [02:53,  1.68it/s]Extractor Predicting: 278it [02:54,  1.71it/s]Extractor Predicting: 279it [02:54,  1.65it/s]Extractor Predicting: 280it [02:55,  1.71it/s]Extractor Predicting: 281it [02:55,  1.72it/s]Extractor Predicting: 282it [02:56,  1.74it/s]Extractor Predicting: 283it [02:56,  1.75it/s]Extractor Predicting: 284it [02:57,  1.74it/s]Extractor Predicting: 285it [02:58,  1.74it/s]Extractor Predicting: 286it [02:58,  1.74it/s]Extractor Predicting: 287it [02:59,  1.53it/s]Extractor Predicting: 288it [03:00,  1.55it/s]Extractor Predicting: 289it [03:00,  1.59it/s]Extractor Predicting: 290it [03:01,  1.55it/s]Extractor Predicting: 291it [03:02,  1.59it/s]Extractor Predicting: 292it [03:02,  1.64it/s]Extractor Predicting: 293it [03:03,  1.66it/s]Extractor Predicting: 294it [03:03,  1.68it/s]Extractor Predicting: 295it [03:04,  1.70it/s]Extractor Predicting: 296it [03:05,  1.61it/s]Extractor Predicting: 297it [03:05,  1.61it/s]Extractor Predicting: 298it [03:06,  1.56it/s]Extractor Predicting: 299it [03:07,  1.56it/s]Extractor Predicting: 300it [03:07,  1.58it/s]Extractor Predicting: 301it [03:08,  1.59it/s]Extractor Predicting: 302it [03:08,  1.60it/s]Extractor Predicting: 303it [03:09,  1.56it/s]Extractor Predicting: 304it [03:10,  1.56it/s]Extractor Predicting: 305it [03:10,  1.60it/s]Extractor Predicting: 306it [03:11,  1.56it/s]Extractor Predicting: 307it [03:12,  1.53it/s]Extractor Predicting: 308it [03:12,  1.53it/s]Extractor Predicting: 309it [03:13,  1.56it/s]Extractor Predicting: 310it [03:13,  1.59it/s]Extractor Predicting: 311it [03:14,  1.60it/s]Extractor Predicting: 312it [03:15,  1.58it/s]Extractor Predicting: 313it [03:15,  1.55it/s]Extractor Predicting: 314it [03:16,  1.53it/s]Extractor Predicting: 315it [03:17,  1.52it/s]Extractor Predicting: 316it [03:17,  1.52it/s]Extractor Predicting: 317it [03:18,  1.52it/s]Extractor Predicting: 318it [03:19,  1.54it/s]Extractor Predicting: 319it [03:19,  1.55it/s]Extractor Predicting: 320it [03:20,  1.55it/s]Extractor Predicting: 321it [03:21,  1.61it/s]Extractor Predicting: 322it [03:21,  1.61it/s]Extractor Predicting: 323it [03:22,  1.59it/s]Extractor Predicting: 324it [03:22,  1.57it/s]Extractor Predicting: 325it [03:23,  1.56it/s]Extractor Predicting: 326it [03:24,  1.55it/s]Extractor Predicting: 327it [03:24,  1.52it/s]Extractor Predicting: 328it [03:25,  1.53it/s]Extractor Predicting: 329it [03:26,  1.57it/s]Extractor Predicting: 330it [03:26,  1.60it/s]Extractor Predicting: 331it [03:27,  1.80it/s]Extractor Predicting: 331it [03:27,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:53,516 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:53,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:53,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:53,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:53,522 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:11:53,827 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:11:53,827 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:11:54,087 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:11:55,273 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:11:55,273 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:56,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:56,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:56,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:56,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:11:56,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:11:56,930 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:11:56,931 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:11:57,191 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:11:57,366 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:11:57,367 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.6052631578947368,
  "recall": 0.05220022695750851,
  "score": 0.09611143354614046,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.48it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.48it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.55it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:24,  1.57it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.57it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:30,  1.57it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.58it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:34,  1.48it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:36,  1.62it/s]Extractor Predicting: 57it [00:37,  1.67it/s]Extractor Predicting: 58it [00:37,  1.73it/s]Extractor Predicting: 59it [00:38,  1.81it/s]Extractor Predicting: 60it [00:38,  1.89it/s]Extractor Predicting: 61it [00:39,  1.93it/s]Extractor Predicting: 62it [00:39,  1.92it/s]Extractor Predicting: 63it [00:40,  1.95it/s]Extractor Predicting: 64it [00:40,  1.91it/s]Extractor Predicting: 65it [00:41,  1.90it/s]Extractor Predicting: 66it [00:41,  1.90it/s]Extractor Predicting: 67it [00:42,  1.89it/s]Extractor Predicting: 68it [00:42,  1.92it/s]Extractor Predicting: 69it [00:43,  1.95it/s]Extractor Predicting: 70it [00:43,  1.88it/s]Extractor Predicting: 71it [00:44,  1.89it/s]Extractor Predicting: 72it [00:44,  1.92it/s]Extractor Predicting: 73it [00:45,  1.97it/s]Extractor Predicting: 74it [00:45,  1.99it/s]Extractor Predicting: 75it [00:46,  1.97it/s]Extractor Predicting: 76it [00:46,  1.94it/s]Extractor Predicting: 77it [00:47,  2.00it/s]Extractor Predicting: 78it [00:47,  1.94it/s]Extractor Predicting: 79it [00:48,  1.93it/s]Extractor Predicting: 80it [00:48,  1.92it/s]Extractor Predicting: 81it [00:49,  1.90it/s]Extractor Predicting: 82it [00:49,  1.93it/s]Extractor Predicting: 83it [00:50,  1.94it/s]Extractor Predicting: 84it [00:50,  1.94it/s]Extractor Predicting: 85it [00:51,  1.96it/s]Extractor Predicting: 86it [00:52,  1.81it/s]Extractor Predicting: 87it [00:52,  1.74it/s]Extractor Predicting: 88it [00:53,  1.69it/s]Extractor Predicting: 89it [00:53,  1.67it/s]Extractor Predicting: 90it [00:54,  1.68it/s]Extractor Predicting: 91it [00:55,  1.65it/s]Extractor Predicting: 92it [00:55,  1.64it/s]Extractor Predicting: 93it [00:56,  1.64it/s]Extractor Predicting: 94it [00:57,  1.63it/s]Extractor Predicting: 95it [00:57,  1.66it/s]Extractor Predicting: 96it [00:58,  1.67it/s]Extractor Predicting: 97it [00:58,  1.68it/s]Extractor Predicting: 98it [00:59,  1.68it/s]Extractor Predicting: 99it [00:59,  1.65it/s]Extractor Predicting: 100it [01:00,  1.60it/s]Extractor Predicting: 101it [01:01,  1.62it/s]Extractor Predicting: 102it [01:01,  1.62it/s]Extractor Predicting: 103it [01:02,  1.58it/s]Extractor Predicting: 104it [01:03,  1.56it/s]Extractor Predicting: 105it [01:03,  1.55it/s]Extractor Predicting: 106it [01:04,  1.54it/s]Extractor Predicting: 107it [01:05,  1.52it/s]Extractor Predicting: 108it [01:05,  1.49it/s]Extractor Predicting: 108it [01:05,  1.64it/s]
[INFO|configuration_utils.py:515] 2023-08-28 15:13:04,933 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:13:04,934 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 15:13:04,939 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:13:04,940 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 15:13:04,942 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 15:13:09,451 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 15:13:09,453 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 15:13:09,480 >> loading configuration file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 15:13:09,480 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 15:13:09,488 >> Didn't find file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:13:09,492 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:13:09,493 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:13:09,493 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:13:09,493 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:13:09,493 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 15:13:09,493 >> loading file outputs/wrapper/wiki/unseen_10_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8904109589041096,
  "recall": 0.02082332212077527,
  "score": 0.04069494443574895,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 15:13:09,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:10,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:10,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:11,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:12,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:12,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:13,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:14,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:14,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:15,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:15,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:16,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:17,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:17,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:18,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:18,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:19,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:20,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:20,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:21,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:22,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:22,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:09, 13.56s/it][WARNING|generation_utils.py:914] 2023-08-28 15:13:23,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:23,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:24,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:25,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:25,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:26,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:27,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:27,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:28,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:28,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:29,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:30,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:30,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:31,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:31,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:32,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:33,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:33,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:34,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:35,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:35,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:36,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:36,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:37,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:38,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:29<03:10, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-28 15:13:38,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:39,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:39,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:40,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:41,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:41,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:42,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:43,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:43,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:44,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:44,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:45,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:46,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:46,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:47,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:47,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:48,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:49,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:49,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:50,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:51,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:51,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:52,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:43<02:54, 14.53s/it][WARNING|generation_utils.py:914] 2023-08-28 15:13:53,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:53,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:54,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:55,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:56,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:56,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:57,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:57,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:58,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:58,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:13:59,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:00,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:01,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:02,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:02,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:03,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:04,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:04,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:05,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:05,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:06,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:07,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:07,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:58<02:43, 14.87s/it][WARNING|generation_utils.py:914] 2023-08-28 15:14:08,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:09,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:09,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:10,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:11,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:11,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:12,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:13,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:13,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:14,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:14,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:15,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:16,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:16,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:17,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:17,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:18,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:19,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:19,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:20,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:20,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:21,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:22,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:12<02:25, 14.60s/it][WARNING|generation_utils.py:914] 2023-08-28 15:14:22,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:23,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:23,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:24,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:24,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:25,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:26,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:26,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:27,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:28,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:28,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:29,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:29,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:30,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:31,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:31,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:32,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:32,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:33,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:34,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:34,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:35,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:25<02:06, 14.09s/it][WARNING|generation_utils.py:914] 2023-08-28 15:14:35,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:36,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:37,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:37,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:38,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:38,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:39,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:40,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:40,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:41,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:42,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:42,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:43,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:44,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:44,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:45,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:46,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:46,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:47,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:48,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:49,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:49,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:50,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:51,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:42<01:57, 14.73s/it][WARNING|generation_utils.py:914] 2023-08-28 15:14:51,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:52,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:52,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:53,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:54,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:54,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:55,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:55,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:56,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:56,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:57,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:58,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:58,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:59,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:14:59,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:00,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:00,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:01,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:02,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:02,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:03,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:03,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:04,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:05,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:05,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:56<01:42, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-28 15:15:06,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:06,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:07,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:08,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:08,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:09,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:09,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:10,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:10,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:11,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:12,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:12,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:13,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:14,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:14,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:15,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:15,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:16,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:17,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:17,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:18,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:18,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:19,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:10<01:26, 14.39s/it][WARNING|generation_utils.py:914] 2023-08-28 15:15:20,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:20,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:21,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:21,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:22,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:22,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:23,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:23,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:24,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:24,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:25,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:25,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:26,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:26,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:27,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:27,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:28,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:29,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:29,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:30,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:31,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:31,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:32,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:32,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:23<01:10, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-28 15:15:33,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:33,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:34,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:34,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:35,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:36,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:36,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:37,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:37,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:38,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:39,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:39,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:40,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:40,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:41,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:42,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:42,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:43,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:43,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:44,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:45,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:45,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:46,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:47,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:37<00:56, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-28 15:15:47,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:48,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:49,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:49,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:50,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:51,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:51,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:52,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:53,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:53,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:54,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:55,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:55,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:56,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:57,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:57,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:58,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:58,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:15:59,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:00,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:00,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:01,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:02,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:02,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:53<00:43, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-28 15:16:03,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:04,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:04,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:05,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:05,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:06,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:07,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:07,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:08,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:08,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:09,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:10,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:10,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:11,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:12,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:12,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:13,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:14,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:14,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:15,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:16,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:16,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:17,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:17,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:18,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:09<00:29, 14.96s/it][WARNING|generation_utils.py:914] 2023-08-28 15:16:19,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:19,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:20,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:20,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:21,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:22,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:22,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:23,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:24,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:24,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:25,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:26,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:26,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:27,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:27,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:28,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:28,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:29,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:30,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:30,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:31,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:31,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:22<00:14, 14.45s/it][WARNING|generation_utils.py:914] 2023-08-28 15:16:32,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:33,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:33,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:34,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:34,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:35,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:36,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:36,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:37,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:38,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:38,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:39,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:40,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:40,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:41,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:42,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:43,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:43,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:44,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:45,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:45,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:46,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:47,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 15:16:47,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:38<00:00, 14.87s/it]Generating: 100%|██████████| 15/15 [03:38<00:00, 14.57s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:54,801 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:54,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:54,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:54,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:54,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:16:55,425 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:16:55,426 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:16:56,014 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:16:57,114 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:16:57,114 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:59,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:59,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:59,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:59,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:16:59,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:17:00,580 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:17:00,581 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:17:01,222 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:17:01,398 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:17:01,399 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in Life , he played the title character , a young son of a Scottish novelist . Head Entity : son of a Scottish novelist , Tail Entity : Harry , a Scottish novelist .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : characters .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : from narrative universe . Context : Later in the year ( 1151 ) , Pheidippides of Tyrenthou was married into the Kingdom of Tyre ; Pheidippides , daughter of King Berenstain , married Nefertari , daughter of King Agathon . Head Entity : Nefertari , Tail Entity : Kingdom of Tyrenthou .\n']
['Relation : from narrative universe . Context : Later in the year ( 1151 ) , Pheidippides of Tyrenthou was married into the Kingdom of Tyre ; Pheidippides , daughter of King Berenstain , married Nefertari , daughter of King Agathon . Head Entity : Nefertari , Tail Entity : Kingdom of Tyrenthou .\n', 'Relation : from narrative universe . Context : After the death of The Doctor and Amy , she played the role of the daughter of the former Doctor to James Time . Head Entity : James Time , Tail Entity : the Doctor .\n']
['Relation : from narrative universe . Context : Later in the year ( 1151 ) , Pheidippides of Tyrenthou was married into the Kingdom of Tyre ; Pheidippides , daughter of King Berenstain , married Nefertari , daughter of King Agathon . Head Entity : Nefertari , Tail Entity : Kingdom of Tyrenthou .\n', 'Relation : from narrative universe . Context : After the death of The Doctor and Amy , she played the role of the daughter of the former Doctor to James Time . Head Entity : James Time , Tail Entity : the Doctor .\n', 'Relation : from narrative universe . Context : This film explores the relationships between the human mind and its creator ( William Shakespeare ) . Head Entity : William Shakespeare , Tail Entity : his imagination .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 542, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Its orbit', 'located on terrain feature', '', 'Its orbit shows a starry surface , and its core is approximately 7 km in length .')"}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 553, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : made from material .', 'success_rate': 0.8192934782608695, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : follows .', 'success_rate': 0.7981770833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : league . Context : Steve Caceres ( born July 28 , 1991 ) is an American football safety at the University of Arkansas , who last played for the Broncos in the National Football League for six seasons . Head Entity : National Football League , Tail Entity : NFL .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 623, 'raw': 800}
{'prompt': 'Relation : league .', 'success_rate': 0.77875, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.7981770833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : the Prime Minister .\n']
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : the Prime Minister .\n', 'Relation : member of political party . Context : After the death of former Prime Minister Paul Martin , Premier of Nova Scotia Martin was asked to step down to become Premier . Head Entity : Paul Martin , Tail Entity : Liberal .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 299, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 382, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 626, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8151041666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 377, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 426, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 526, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 573, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : mother .', 'success_rate': 0.7825520833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 524, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 574, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : residence .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8622159090909091, 'errors': {''}}
['Relation : twinned administrative body . Context : Later in 1453 the county became under the rule of the Roman Empire , but at different times it had been absorbed by the Roman Commonwealth , the Empire , and the Catholic church , and under the control of the former Pope . Head Entity : Rome , Tail Entity : Roman Empire .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 244, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 343, 'raw': 448}
{'target': 600, 'success': 367, 'raw': 480}
{'target': 600, 'success': 395, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 527, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.7825520833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 13730
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13830, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.61it/s]Extractor Estimating: 2it [00:01,  1.53it/s]Extractor Estimating: 3it [00:01,  1.55it/s]Extractor Estimating: 4it [00:02,  1.67it/s]Extractor Estimating: 5it [00:03,  1.65it/s]Extractor Estimating: 6it [00:03,  1.60it/s]Extractor Estimating: 7it [00:04,  1.59it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:05,  1.57it/s]Extractor Estimating: 10it [00:06,  1.64it/s]Extractor Estimating: 11it [00:06,  1.63it/s]Extractor Estimating: 12it [00:07,  1.67it/s]Extractor Estimating: 13it [00:08,  1.66it/s]Extractor Estimating: 14it [00:08,  1.68it/s]Extractor Estimating: 15it [00:09,  1.65it/s]Extractor Estimating: 16it [00:09,  1.70it/s]Extractor Estimating: 17it [00:10,  1.66it/s]Extractor Estimating: 18it [00:10,  1.70it/s]Extractor Estimating: 19it [00:11,  1.74it/s]Extractor Estimating: 20it [00:12,  1.70it/s]Extractor Estimating: 21it [00:12,  1.71it/s]Extractor Estimating: 22it [00:13,  1.68it/s]Extractor Estimating: 23it [00:13,  1.68it/s]Extractor Estimating: 24it [00:14,  1.67it/s]Extractor Estimating: 25it [00:15,  1.70it/s]Extractor Estimating: 26it [00:15,  1.69it/s]Extractor Estimating: 27it [00:16,  1.70it/s]Extractor Estimating: 28it [00:16,  1.68it/s]Extractor Estimating: 29it [00:17,  1.65it/s]Extractor Estimating: 30it [00:18,  1.66it/s]Extractor Estimating: 31it [00:18,  1.66it/s]Extractor Estimating: 32it [00:19,  1.65it/s]Extractor Estimating: 33it [00:19,  1.60it/s]Extractor Estimating: 34it [00:20,  1.61it/s]Extractor Estimating: 35it [00:21,  1.65it/s]Extractor Estimating: 36it [00:21,  1.57it/s]Extractor Estimating: 37it [00:22,  1.55it/s]Extractor Estimating: 38it [00:23,  1.63it/s]Extractor Estimating: 39it [00:23,  1.68it/s]Extractor Estimating: 40it [00:24,  1.65it/s]Extractor Estimating: 41it [00:24,  1.65it/s]Extractor Estimating: 42it [00:25,  1.56it/s]Extractor Estimating: 43it [00:26,  1.58it/s]Extractor Estimating: 44it [00:26,  1.59it/s]Extractor Estimating: 45it [00:27,  1.63it/s]Extractor Estimating: 46it [00:27,  1.66it/s]Extractor Estimating: 47it [00:28,  1.69it/s]Extractor Estimating: 48it [00:29,  1.67it/s]Extractor Estimating: 49it [00:29,  1.65it/s]Extractor Estimating: 50it [00:30,  1.64it/s]Extractor Estimating: 51it [00:30,  1.70it/s]Extractor Estimating: 52it [00:31,  1.68it/s]Extractor Estimating: 53it [00:32,  1.70it/s]Extractor Estimating: 54it [00:32,  1.70it/s]Extractor Estimating: 55it [00:33,  1.60it/s]Extractor Estimating: 56it [00:34,  1.61it/s]Extractor Estimating: 57it [00:34,  1.61it/s]Extractor Estimating: 58it [00:35,  1.67it/s]Extractor Estimating: 59it [00:35,  1.63it/s]Extractor Estimating: 60it [00:36,  1.61it/s]Extractor Estimating: 61it [00:37,  1.64it/s]Extractor Estimating: 62it [00:37,  1.64it/s]Extractor Estimating: 63it [00:38,  1.67it/s]Extractor Estimating: 64it [00:38,  1.73it/s]Extractor Estimating: 65it [00:39,  1.75it/s]Extractor Estimating: 66it [00:39,  1.73it/s]Extractor Estimating: 67it [00:40,  1.71it/s]Extractor Estimating: 68it [00:41,  1.74it/s]Extractor Estimating: 69it [00:41,  1.60it/s]Extractor Estimating: 70it [00:42,  1.64it/s]Extractor Estimating: 71it [00:42,  1.69it/s]Extractor Estimating: 72it [00:43,  1.67it/s]Extractor Estimating: 73it [00:44,  1.71it/s]Extractor Estimating: 74it [00:44,  1.72it/s]Extractor Estimating: 75it [00:45,  1.66it/s]Extractor Estimating: 76it [00:45,  1.70it/s]Extractor Estimating: 77it [00:46,  1.56it/s]Extractor Estimating: 78it [00:47,  1.59it/s]Extractor Estimating: 79it [00:47,  1.61it/s]Extractor Estimating: 80it [00:48,  1.64it/s]Extractor Estimating: 81it [00:49,  1.60it/s]Extractor Estimating: 82it [00:49,  1.63it/s]Extractor Estimating: 83it [00:50,  1.65it/s]Extractor Estimating: 84it [00:50,  1.67it/s]Extractor Estimating: 85it [00:51,  1.70it/s]Extractor Estimating: 86it [00:52,  1.57it/s]Extractor Estimating: 87it [00:52,  1.46it/s]Extractor Estimating: 88it [00:53,  1.54it/s]Extractor Estimating: 89it [00:54,  1.57it/s]Extractor Estimating: 90it [00:54,  1.56it/s]Extractor Estimating: 91it [00:55,  1.56it/s]Extractor Estimating: 92it [00:56,  1.60it/s]Extractor Estimating: 93it [00:56,  1.64it/s]Extractor Estimating: 94it [00:57,  1.69it/s]Extractor Estimating: 95it [00:57,  1.66it/s]Extractor Estimating: 96it [00:58,  1.65it/s]Extractor Estimating: 97it [00:59,  1.64it/s]Extractor Estimating: 98it [00:59,  1.63it/s]Extractor Estimating: 99it [01:00,  1.55it/s]Extractor Estimating: 100it [01:01,  1.53it/s]Extractor Estimating: 101it [01:01,  1.57it/s]Extractor Estimating: 102it [01:02,  1.55it/s]Extractor Estimating: 103it [01:02,  1.54it/s]Extractor Estimating: 104it [01:03,  1.54it/s]Extractor Estimating: 105it [01:04,  1.56it/s]Extractor Estimating: 106it [01:04,  1.60it/s]Extractor Estimating: 107it [01:05,  1.50it/s]Extractor Estimating: 108it [01:06,  1.50it/s]Extractor Estimating: 109it [01:06,  1.54it/s]Extractor Estimating: 110it [01:07,  1.55it/s]Extractor Estimating: 111it [01:08,  1.60it/s]Extractor Estimating: 112it [01:08,  1.58it/s]Extractor Estimating: 113it [01:09,  1.62it/s]Extractor Estimating: 114it [01:09,  1.60it/s]Extractor Estimating: 115it [01:10,  1.59it/s]Extractor Estimating: 116it [01:11,  1.60it/s]Extractor Estimating: 117it [01:11,  1.60it/s]Extractor Estimating: 118it [01:12,  1.57it/s]Extractor Estimating: 119it [01:13,  1.56it/s]Extractor Estimating: 120it [01:13,  1.59it/s]Extractor Estimating: 121it [01:14,  1.63it/s]Extractor Estimating: 122it [01:14,  1.62it/s]Extractor Estimating: 123it [01:15,  1.67it/s]Extractor Estimating: 124it [01:16,  1.64it/s]Extractor Estimating: 125it [01:16,  1.66it/s]Extractor Estimating: 126it [01:17,  1.66it/s]Extractor Estimating: 127it [01:17,  1.66it/s]Extractor Estimating: 128it [01:18,  1.65it/s]Extractor Estimating: 129it [01:19,  1.68it/s]Extractor Estimating: 130it [01:19,  1.64it/s]Extractor Estimating: 131it [01:20,  1.70it/s]Extractor Estimating: 132it [01:20,  1.69it/s]Extractor Estimating: 133it [01:21,  1.69it/s]Extractor Estimating: 134it [01:22,  1.68it/s]Extractor Estimating: 135it [01:22,  1.65it/s]Extractor Estimating: 136it [01:23,  1.60it/s]Extractor Estimating: 137it [01:23,  1.64it/s]Extractor Estimating: 138it [01:24,  1.64it/s]Extractor Estimating: 139it [01:25,  1.62it/s]Extractor Estimating: 140it [01:25,  1.66it/s]Extractor Estimating: 141it [01:26,  1.65it/s]Extractor Estimating: 142it [01:27,  1.62it/s]Extractor Estimating: 143it [01:27,  1.63it/s]Extractor Estimating: 144it [01:28,  1.61it/s]Extractor Estimating: 145it [01:28,  1.65it/s]Extractor Estimating: 146it [01:29,  1.62it/s]Extractor Estimating: 147it [01:30,  1.67it/s]Extractor Estimating: 148it [01:30,  1.67it/s]Extractor Estimating: 149it [01:31,  1.70it/s]Extractor Estimating: 150it [01:31,  1.65it/s]Extractor Estimating: 151it [01:32,  1.68it/s]Extractor Estimating: 152it [01:33,  1.57it/s]Extractor Estimating: 153it [01:33,  1.57it/s]Extractor Estimating: 154it [01:34,  1.59it/s]Extractor Estimating: 155it [01:35,  1.56it/s]Extractor Estimating: 156it [01:35,  1.57it/s]Extractor Estimating: 157it [01:36,  1.60it/s]Extractor Estimating: 158it [01:37,  1.56it/s]Extractor Estimating: 159it [01:37,  1.59it/s]Extractor Estimating: 160it [01:38,  1.61it/s]Extractor Estimating: 161it [01:38,  1.55it/s]Extractor Estimating: 162it [01:39,  1.54it/s]Extractor Estimating: 163it [01:40,  1.53it/s]Extractor Estimating: 164it [01:40,  1.58it/s]Extractor Estimating: 165it [01:41,  1.55it/s]Extractor Estimating: 166it [01:42,  1.55it/s]Extractor Estimating: 167it [01:42,  1.60it/s]Extractor Estimating: 168it [01:43,  1.66it/s]Extractor Estimating: 169it [01:43,  1.65it/s]Extractor Estimating: 170it [01:44,  1.64it/s]Extractor Estimating: 171it [01:45,  1.66it/s]Extractor Estimating: 172it [01:45,  1.61it/s]Extractor Estimating: 173it [01:46,  1.64it/s]Extractor Estimating: 174it [01:46,  1.62it/s]Extractor Estimating: 175it [01:47,  1.61it/s]Extractor Estimating: 176it [01:48,  1.57it/s]Extractor Estimating: 177it [01:48,  1.58it/s]Extractor Estimating: 178it [01:49,  1.58it/s]Extractor Estimating: 179it [01:50,  1.55it/s]Extractor Estimating: 180it [01:50,  1.54it/s]Extractor Estimating: 181it [01:51,  1.44it/s]Extractor Estimating: 182it [01:52,  1.52it/s]Extractor Estimating: 183it [01:52,  1.55it/s]Extractor Estimating: 184it [01:53,  1.51it/s]Extractor Estimating: 185it [01:54,  1.57it/s]Extractor Estimating: 186it [01:54,  1.60it/s]Extractor Estimating: 187it [01:55,  1.65it/s]Extractor Estimating: 188it [01:55,  1.64it/s]Extractor Estimating: 189it [01:56,  1.66it/s]Extractor Estimating: 190it [01:57,  1.59it/s]Extractor Estimating: 191it [01:57,  1.58it/s]Extractor Estimating: 192it [01:58,  1.61it/s]Extractor Estimating: 193it [01:59,  1.60it/s]Extractor Estimating: 194it [01:59,  1.58it/s]Extractor Estimating: 195it [02:00,  1.58it/s]Extractor Estimating: 196it [02:00,  1.60it/s]Extractor Estimating: 197it [02:01,  1.62it/s]Extractor Estimating: 198it [02:02,  1.52it/s]Extractor Estimating: 199it [02:02,  1.54it/s]Extractor Estimating: 200it [02:03,  1.55it/s]Extractor Estimating: 201it [02:04,  1.55it/s]Extractor Estimating: 202it [02:04,  1.65it/s]Extractor Estimating: 203it [02:05,  1.67it/s]Extractor Estimating: 204it [02:05,  1.67it/s]Extractor Estimating: 205it [02:06,  1.70it/s]Extractor Estimating: 206it [02:07,  1.72it/s]Extractor Estimating: 207it [02:07,  1.74it/s]Extractor Estimating: 208it [02:08,  1.76it/s]Extractor Estimating: 209it [02:08,  1.78it/s]Extractor Estimating: 210it [02:09,  1.81it/s]Extractor Estimating: 211it [02:09,  1.83it/s]Extractor Estimating: 212it [02:10,  1.82it/s]Extractor Estimating: 213it [02:10,  1.83it/s]Extractor Estimating: 214it [02:11,  1.81it/s]Extractor Estimating: 215it [02:11,  1.78it/s]Extractor Estimating: 216it [02:12,  1.77it/s]Extractor Estimating: 217it [02:13,  1.79it/s]Extractor Estimating: 218it [02:13,  1.86it/s]Extractor Estimating: 219it [02:14,  1.85it/s]Extractor Estimating: 220it [02:14,  1.77it/s]Extractor Estimating: 221it [02:15,  1.80it/s]Extractor Estimating: 222it [02:15,  1.76it/s]Extractor Estimating: 223it [02:16,  1.72it/s]Extractor Estimating: 224it [02:17,  1.71it/s]Extractor Estimating: 225it [02:17,  1.73it/s]Extractor Estimating: 226it [02:18,  1.82it/s]Extractor Estimating: 227it [02:18,  1.86it/s]Extractor Estimating: 228it [02:19,  1.88it/s]Extractor Estimating: 229it [02:19,  1.86it/s]Extractor Estimating: 230it [02:20,  1.85it/s]Extractor Estimating: 231it [02:20,  1.83it/s]Extractor Estimating: 232it [02:21,  1.91it/s]Extractor Estimating: 233it [02:21,  1.94it/s]Extractor Estimating: 234it [02:22,  1.98it/s]Extractor Estimating: 235it [02:22,  1.99it/s]Extractor Estimating: 236it [02:23,  1.98it/s]Extractor Estimating: 237it [02:23,  1.99it/s]Extractor Estimating: 238it [02:24,  2.01it/s]Extractor Estimating: 239it [02:24,  1.95it/s]Extractor Estimating: 240it [02:25,  1.83it/s]Extractor Estimating: 241it [02:25,  1.85it/s]Extractor Estimating: 242it [02:26,  1.86it/s]Extractor Estimating: 243it [02:27,  1.89it/s]Extractor Estimating: 244it [02:27,  1.90it/s]Extractor Estimating: 245it [02:28,  1.80it/s]Extractor Estimating: 246it [02:28,  1.73it/s]Extractor Estimating: 247it [02:29,  1.71it/s]Extractor Estimating: 248it [02:29,  1.69it/s]Extractor Estimating: 249it [02:30,  1.75it/s]Extractor Estimating: 250it [02:31,  1.77it/s]Extractor Estimating: 251it [02:31,  1.76it/s]Extractor Estimating: 252it [02:32,  1.72it/s]Extractor Estimating: 253it [02:32,  1.74it/s]Extractor Estimating: 254it [02:33,  1.67it/s]Extractor Estimating: 255it [02:34,  1.60it/s]Extractor Estimating: 256it [02:34,  1.64it/s]Extractor Estimating: 257it [02:35,  1.66it/s]Extractor Estimating: 258it [02:35,  1.69it/s]Extractor Estimating: 259it [02:36,  1.66it/s]Extractor Estimating: 260it [02:37,  1.70it/s]Extractor Estimating: 261it [02:37,  1.69it/s]Extractor Estimating: 262it [02:38,  1.68it/s]Extractor Estimating: 263it [02:38,  1.70it/s]Extractor Estimating: 264it [02:39,  1.66it/s]Extractor Estimating: 265it [02:40,  1.63it/s]Extractor Estimating: 266it [02:40,  1.59it/s]Extractor Estimating: 267it [02:41,  1.49it/s]Extractor Estimating: 268it [02:42,  1.58it/s]Extractor Estimating: 269it [02:42,  1.60it/s]Extractor Estimating: 270it [02:43,  1.59it/s]Extractor Estimating: 271it [02:43,  1.59it/s]Extractor Estimating: 272it [02:44,  1.55it/s]Extractor Estimating: 273it [02:45,  1.56it/s]Extractor Estimating: 274it [02:45,  1.64it/s]Extractor Estimating: 275it [02:46,  1.53it/s]Extractor Estimating: 276it [02:47,  1.55it/s]Extractor Estimating: 277it [02:47,  1.57it/s]Extractor Estimating: 278it [02:48,  1.52it/s]Extractor Estimating: 279it [02:49,  1.55it/s]Extractor Estimating: 280it [02:49,  1.56it/s]Extractor Estimating: 281it [02:50,  1.59it/s]Extractor Estimating: 282it [02:50,  1.63it/s]Extractor Estimating: 283it [02:51,  1.58it/s]Extractor Estimating: 284it [02:52,  1.61it/s]Extractor Estimating: 285it [02:52,  1.58it/s]Extractor Estimating: 286it [02:53,  1.57it/s]Extractor Estimating: 287it [02:54,  1.53it/s]Extractor Estimating: 288it [02:54,  1.54it/s]Extractor Estimating: 289it [02:55,  1.56it/s]Extractor Estimating: 290it [02:56,  1.56it/s]Extractor Estimating: 291it [02:56,  1.57it/s]Extractor Estimating: 292it [02:57,  1.57it/s]Extractor Estimating: 293it [02:57,  1.60it/s]Extractor Estimating: 294it [02:58,  1.56it/s]Extractor Estimating: 295it [02:59,  1.58it/s]Extractor Estimating: 296it [02:59,  1.57it/s]Extractor Estimating: 297it [03:00,  1.60it/s]Extractor Estimating: 298it [03:01,  1.57it/s]Extractor Estimating: 299it [03:01,  1.63it/s]Extractor Estimating: 300it [03:02,  1.60it/s]Extractor Estimating: 301it [03:03,  1.60it/s]Extractor Estimating: 302it [03:03,  1.59it/s]Extractor Estimating: 303it [03:04,  1.59it/s]Extractor Estimating: 304it [03:04,  1.58it/s]Extractor Estimating: 305it [03:05,  1.60it/s]Extractor Estimating: 306it [03:06,  1.63it/s]Extractor Estimating: 307it [03:06,  1.66it/s]Extractor Estimating: 308it [03:07,  1.71it/s]Extractor Estimating: 309it [03:07,  1.65it/s]Extractor Estimating: 310it [03:08,  1.68it/s]Extractor Estimating: 311it [03:09,  1.64it/s]Extractor Estimating: 312it [03:09,  1.62it/s]Extractor Estimating: 313it [03:10,  1.64it/s]Extractor Estimating: 314it [03:10,  1.63it/s]Extractor Estimating: 315it [03:11,  1.67it/s]Extractor Estimating: 316it [03:12,  1.64it/s]Extractor Estimating: 317it [03:12,  1.56it/s]Extractor Estimating: 318it [03:13,  1.64it/s]Extractor Estimating: 319it [03:14,  1.62it/s]Extractor Estimating: 320it [03:14,  1.62it/s]Extractor Estimating: 321it [03:15,  1.60it/s]Extractor Estimating: 322it [03:15,  1.63it/s]Extractor Estimating: 323it [03:16,  1.60it/s]Extractor Estimating: 324it [03:17,  1.58it/s]Extractor Estimating: 325it [03:17,  1.61it/s]Extractor Estimating: 326it [03:18,  1.63it/s]Extractor Estimating: 327it [03:18,  1.66it/s]Extractor Estimating: 328it [03:19,  1.71it/s]Extractor Estimating: 329it [03:20,  1.77it/s]Extractor Estimating: 330it [03:20,  1.78it/s]Extractor Estimating: 331it [03:21,  1.76it/s]Extractor Estimating: 332it [03:21,  1.80it/s]Extractor Estimating: 333it [03:22,  1.75it/s]Extractor Estimating: 334it [03:22,  1.71it/s]Extractor Estimating: 335it [03:23,  1.73it/s]Extractor Estimating: 336it [03:24,  1.77it/s]Extractor Estimating: 337it [03:24,  1.82it/s]Extractor Estimating: 338it [03:25,  1.82it/s]Extractor Estimating: 339it [03:25,  1.80it/s]Extractor Estimating: 340it [03:26,  1.76it/s]Extractor Estimating: 341it [03:27,  1.58it/s]Extractor Estimating: 342it [03:27,  1.70it/s]Extractor Estimating: 343it [03:27,  1.81it/s]Extractor Estimating: 344it [03:28,  1.77it/s]Extractor Estimating: 345it [03:29,  1.79it/s]Extractor Estimating: 346it [03:29,  1.80it/s]Extractor Estimating: 347it [03:30,  1.79it/s]Extractor Estimating: 348it [03:30,  1.76it/s]Extractor Estimating: 349it [03:31,  1.70it/s]Extractor Estimating: 350it [03:32,  1.69it/s]Extractor Estimating: 351it [03:32,  1.68it/s]Extractor Estimating: 352it [03:33,  1.66it/s]Extractor Estimating: 353it [03:33,  1.61it/s]Extractor Estimating: 354it [03:34,  1.66it/s]Extractor Estimating: 355it [03:35,  1.60it/s]Extractor Estimating: 356it [03:35,  1.65it/s]Extractor Estimating: 357it [03:36,  1.69it/s]Extractor Estimating: 358it [03:36,  1.65it/s]Extractor Estimating: 359it [03:37,  1.66it/s]Extractor Estimating: 360it [03:38,  1.65it/s]Extractor Estimating: 361it [03:38,  1.63it/s]Extractor Estimating: 362it [03:39,  1.64it/s]Extractor Estimating: 363it [03:39,  1.67it/s]Extractor Estimating: 364it [03:40,  1.67it/s]Extractor Estimating: 365it [03:41,  1.65it/s]Extractor Estimating: 366it [03:41,  1.63it/s]Extractor Estimating: 367it [03:42,  1.61it/s]Extractor Estimating: 368it [03:43,  1.64it/s]Extractor Estimating: 369it [03:43,  1.66it/s]Extractor Estimating: 370it [03:44,  1.65it/s]Extractor Estimating: 371it [03:44,  1.69it/s]Extractor Estimating: 372it [03:45,  1.69it/s]Extractor Estimating: 373it [03:46,  1.65it/s]Extractor Estimating: 374it [03:46,  1.68it/s]Extractor Estimating: 375it [03:46,  1.89it/s]Extractor Estimating: 375it [03:46,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:07,811 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:07,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:07,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:07,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:07,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 15:21:08,438 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 15:21:08,439 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:21:09,115 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 15:21:10,209 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:21:10,209 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:13,179 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:13,183 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:13,184 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:13,184 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 15:21:13,184 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 15:21:13,843 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 15:21:13,844 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 15:21:14,759 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 15:21:14,944 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 15:21:14,945 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:30:37,682 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:30:37,712 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7467 mean pseudo reward: 0.9057541503748805
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 25565
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25665, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25665, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.981, loss:1015.7533
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.985, loss:985.5571
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.981, loss:988.7627
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.970, loss:980.6389
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.974, loss:945.2109
>> valid entity prec:0.4999, rec:0.5715, f1:0.5334
>> valid relation prec:0.0357, rec:0.0005, f1:0.0009
>> valid relation with NER prec:0.0357, rec:0.0005, f1:0.0009
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.386, loss:968.9998
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.964, loss:963.7341
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.979, loss:949.2457
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.982, loss:953.4849
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.968, loss:944.1701
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4873, rec:0.5317, f1:0.5085
>> valid relation prec:0.1081, rec:0.0018, f1:0.0036
>> valid relation with NER prec:0.1081, rec:0.0018, f1:0.0036
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.377, loss:932.2126
g_step 1200, step 264, avg_time 0.984, loss:973.2128
g_step 1300, step 52, avg_time 0.980, loss:938.2343
g_step 1400, step 152, avg_time 0.965, loss:939.7146
g_step 1500, step 252, avg_time 0.980, loss:901.9278
>> valid entity prec:0.5502, rec:0.4498, f1:0.4949
>> valid relation prec:0.0937, rec:0.0055, f1:0.0105
>> valid relation with NER prec:0.0937, rec:0.0055, f1:0.0105
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 40, avg_time 2.371, loss:900.7585
g_step 1700, step 140, avg_time 0.971, loss:885.8691
g_step 1800, step 240, avg_time 0.989, loss:896.5120
g_step 1900, step 28, avg_time 0.968, loss:876.8957
g_step 2000, step 128, avg_time 0.984, loss:873.1985
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5040, rec:0.4234, f1:0.4602
>> valid relation prec:0.1037, rec:0.0058, f1:0.0109
>> valid relation with NER prec:0.1037, rec:0.0058, f1:0.0109
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.356, loss:853.0272
g_step 2200, step 16, avg_time 0.963, loss:868.2703
g_step 2300, step 116, avg_time 0.976, loss:834.7776
g_step 2400, step 216, avg_time 0.981, loss:802.1339
g_step 2500, step 4, avg_time 0.977, loss:848.9952
>> valid entity prec:0.5560, rec:0.3392, f1:0.4214
>> valid relation prec:0.0735, rec:0.0035, f1:0.0066
>> valid relation with NER prec:0.0735, rec:0.0035, f1:0.0066
g_step 2600, step 104, avg_time 2.362, loss:781.8107
g_step 2700, step 204, avg_time 0.971, loss:819.8066
g_step 2800, step 304, avg_time 0.984, loss:820.8645
g_step 2900, step 92, avg_time 0.963, loss:758.5098
g_step 3000, step 192, avg_time 0.986, loss:789.1348
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4992, rec:0.4235, f1:0.4582
>> valid relation prec:0.1084, rec:0.0051, f1:0.0097
>> valid relation with NER prec:0.1084, rec:0.0051, f1:0.0097
g_step 3100, step 292, avg_time 2.368, loss:782.5903
g_step 3200, step 80, avg_time 0.973, loss:745.1411
g_step 3300, step 180, avg_time 0.968, loss:769.4031
g_step 3400, step 280, avg_time 0.984, loss:751.4391
g_step 3500, step 68, avg_time 0.977, loss:724.3841
>> valid entity prec:0.4706, rec:0.4697, f1:0.4702
>> valid relation prec:0.0526, rec:0.0035, f1:0.0065
>> valid relation with NER prec:0.0526, rec:0.0035, f1:0.0065
g_step 3600, step 168, avg_time 2.380, loss:747.8247
g_step 3700, step 268, avg_time 0.975, loss:740.2808
g_step 3800, step 56, avg_time 0.959, loss:722.4714
g_step 3900, step 156, avg_time 0.988, loss:711.3394
g_step 4000, step 256, avg_time 0.977, loss:709.9159
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5206, rec:0.3657, f1:0.4296
>> valid relation prec:0.0605, rec:0.0048, f1:0.0090
>> valid relation with NER prec:0.0605, rec:0.0048, f1:0.0090
g_step 4100, step 44, avg_time 2.336, loss:692.4180
g_step 4200, step 144, avg_time 0.979, loss:664.8067
g_step 4300, step 244, avg_time 0.961, loss:697.4234
g_step 4400, step 32, avg_time 0.996, loss:694.2064
g_step 4500, step 132, avg_time 0.970, loss:640.8223
>> valid entity prec:0.5056, rec:0.4242, f1:0.4613
>> valid relation prec:0.0876, rec:0.0088, f1:0.0159
>> valid relation with NER prec:0.0876, rec:0.0088, f1:0.0159
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 232, avg_time 2.360, loss:667.5245
g_step 4700, step 20, avg_time 0.983, loss:658.7174
g_step 4800, step 120, avg_time 0.983, loss:623.0507
g_step 4900, step 220, avg_time 0.967, loss:644.9520
g_step 5000, step 8, avg_time 0.968, loss:665.6640
learning rate was adjusted to 0.0008
>> valid entity prec:0.4715, rec:0.3971, f1:0.4311
>> valid relation prec:0.0689, rec:0.0085, f1:0.0152
>> valid relation with NER prec:0.0689, rec:0.0085, f1:0.0152
g_step 5100, step 108, avg_time 2.357, loss:605.0456
g_step 5200, step 208, avg_time 0.988, loss:642.7537
g_step 5300, step 308, avg_time 0.979, loss:640.9021
g_step 5400, step 96, avg_time 0.981, loss:598.8503
g_step 5500, step 196, avg_time 0.982, loss:605.5804
>> valid entity prec:0.5137, rec:0.4255, f1:0.4655
>> valid relation prec:0.0754, rec:0.0088, f1:0.0157
>> valid relation with NER prec:0.0754, rec:0.0088, f1:0.0157
g_step 5600, step 296, avg_time 2.355, loss:615.1835
g_step 5700, step 84, avg_time 0.970, loss:576.5297
g_step 5800, step 184, avg_time 0.985, loss:605.2078
g_step 5900, step 284, avg_time 0.977, loss:595.9285
g_step 6000, step 72, avg_time 0.970, loss:568.0802
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4873, rec:0.3977, f1:0.4380
>> valid relation prec:0.0737, rec:0.0118, f1:0.0203
>> valid relation with NER prec:0.0737, rec:0.0118, f1:0.0203
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 172, avg_time 2.368, loss:564.7370
g_step 6200, step 272, avg_time 0.978, loss:575.9108
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:30:37 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:30:37 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-30-37_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:30:39 - WARNING - datasets.builder -   Using custom data configuration default-c7b6eb0838f23a9b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c7b6eb0838f23a9b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:30:40,523 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:30:40,524 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:30:40,524 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:30:40,525 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:30:40,566 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:30:40,585 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:30:40,585 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:30:40,585 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:30:40,585 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:30:40,585 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:30:40,586 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:30:40,877 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:30:43,968 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:30:43,979 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c7b6eb0838f23a9b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.15ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.95ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.35ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.77ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.04ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.24ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.35ba/s]100%|██████████| 8/8 [00:01<00:00,  5.13ba/s]100%|██████████| 8/8 [00:01<00:00,  4.30ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.08ba/s] 40%|████      | 2/5 [00:00<00:00,  4.33ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.39ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.43ba/s]100%|██████████| 5/5 [00:00<00:00,  5.03ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:01,  6.96ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.49ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  9.96ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.30ba/s]100%|██████████| 8/8 [00:00<00:00, 10.59ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.95ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.43ba/s]100%|██████████| 5/5 [00:00<00:00, 12.79ba/s]100%|██████████| 5/5 [00:00<00:00, 12.03ba/s]
[INFO|trainer.py:414] 2023-08-28 17:30:48,572 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:30:48,587 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:30:48,587 >>   Num examples = 7541
[INFO|trainer.py:1149] 2023-08-28 17:30:48,587 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:30:48,587 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:30:48,587 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:30:48,587 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:30:48,587 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<02:56,  3.34it/s]  0%|          | 2/590 [00:00<02:52,  3.41it/s]  1%|          | 3/590 [00:00<02:51,  3.43it/s]  1%|          | 4/590 [00:01<02:50,  3.44it/s]  1%|          | 5/590 [00:01<02:50,  3.44it/s]  1%|          | 6/590 [00:01<02:49,  3.45it/s]  1%|          | 7/590 [00:02<02:49,  3.45it/s]  1%|▏         | 8/590 [00:02<02:48,  3.45it/s]  2%|▏         | 9/590 [00:02<02:48,  3.45it/s]  2%|▏         | 10/590 [00:02<02:48,  3.45it/s]  2%|▏         | 11/590 [00:03<02:47,  3.45it/s]  2%|▏         | 12/590 [00:03<02:47,  3.45it/s]  2%|▏         | 13/590 [00:03<02:47,  3.45it/s]  2%|▏         | 14/590 [00:04<02:47,  3.45it/s]  3%|▎         | 15/590 [00:04<02:51,  3.34it/s]  3%|▎         | 16/590 [00:04<02:50,  3.37it/s]  3%|▎         | 17/590 [00:04<02:48,  3.40it/s]  3%|▎         | 18/590 [00:05<02:47,  3.41it/s]  3%|▎         | 19/590 [00:05<02:46,  3.42it/s]  3%|▎         | 20/590 [00:05<02:46,  3.43it/s]  4%|▎         | 21/590 [00:06<02:45,  3.44it/s]  4%|▎         | 22/590 [00:06<02:45,  3.44it/s]  4%|▍         | 23/590 [00:06<02:44,  3.44it/s]  4%|▍         | 24/590 [00:06<02:44,  3.44it/s]  4%|▍         | 25/590 [00:07<02:44,  3.44it/s]  4%|▍         | 26/590 [00:07<02:43,  3.45it/s]  5%|▍         | 27/590 [00:07<02:43,  3.45it/s]  5%|▍         | 28/590 [00:08<02:43,  3.45it/s]  5%|▍         | 29/590 [00:08<02:42,  3.45it/s]  5%|▌         | 30/590 [00:08<02:42,  3.45it/s]  5%|▌         | 31/590 [00:09<02:42,  3.45it/s]  5%|▌         | 32/590 [00:09<02:44,  3.39it/s]  6%|▌         | 33/590 [00:09<02:43,  3.41it/s]  6%|▌         | 34/590 [00:09<02:42,  3.42it/s]  6%|▌         | 35/590 [00:10<02:41,  3.43it/s]  6%|▌         | 36/590 [00:10<02:41,  3.43it/s]  6%|▋         | 37/590 [00:10<02:40,  3.44it/s]  6%|▋         | 38/590 [00:11<02:40,  3.44it/s]  7%|▋         | 39/590 [00:11<02:40,  3.44it/s]  7%|▋         | 40/590 [00:11<02:39,  3.44it/s]  7%|▋         | 41/590 [00:11<02:39,  3.44it/s]  7%|▋         | 42/590 [00:12<02:39,  3.44it/s]  7%|▋         | 43/590 [00:12<02:38,  3.44it/s]  7%|▋         | 44/590 [00:12<02:38,  3.44it/s]  8%|▊         | 45/590 [00:13<02:38,  3.44it/s]  8%|▊         | 46/590 [00:13<02:37,  3.45it/s]  8%|▊         | 47/590 [00:13<02:37,  3.45it/s]  8%|▊         | 48/590 [00:13<02:37,  3.44it/s]  8%|▊         | 49/590 [00:14<02:37,  3.44it/s]  8%|▊         | 50/590 [00:14<02:46,  3.25it/s]  9%|▊         | 51/590 [00:14<02:42,  3.31it/s]  9%|▉         | 52/590 [00:15<02:40,  3.35it/s]  9%|▉         | 53/590 [00:15<02:39,  3.37it/s]  9%|▉         | 54/590 [00:15<02:38,  3.39it/s]  9%|▉         | 55/590 [00:16<02:37,  3.41it/s]  9%|▉         | 56/590 [00:16<02:36,  3.42it/s] 10%|▉         | 57/590 [00:16<02:35,  3.42it/s] 10%|▉         | 58/590 [00:16<02:35,  3.43it/s] 10%|█         | 59/590 [00:17<02:34,  3.43it/s] 10%|█         | 60/590 [00:17<02:34,  3.43it/s] 10%|█         | 61/590 [00:17<02:35,  3.39it/s] 11%|█         | 62/590 [00:18<02:35,  3.41it/s] 11%|█         | 63/590 [00:18<02:34,  3.42it/s] 11%|█         | 64/590 [00:18<02:33,  3.42it/s] 11%|█         | 65/590 [00:18<02:33,  3.43it/s] 11%|█         | 66/590 [00:19<02:32,  3.43it/s] 11%|█▏        | 67/590 [00:19<02:32,  3.43it/s] 12%|█▏        | 68/590 [00:19<02:31,  3.44it/s] 12%|█▏        | 69/590 [00:20<02:31,  3.44it/s] 12%|█▏        | 70/590 [00:20<02:31,  3.44it/s] 12%|█▏        | 71/590 [00:20<02:30,  3.44it/s] 12%|█▏        | 72/590 [00:21<02:34,  3.35it/s] 12%|█▏        | 73/590 [00:21<02:33,  3.38it/s] 13%|█▎        | 74/590 [00:21<02:32,  3.39it/s] 13%|█▎        | 75/590 [00:21<02:31,  3.40it/s] 13%|█▎        | 76/590 [00:22<02:30,  3.41it/s] 13%|█▎        | 77/590 [00:22<02:29,  3.42it/s] 13%|█▎        | 78/590 [00:22<02:29,  3.43it/s] 13%|█▎        | 79/590 [00:23<02:29,  3.43it/s] 14%|█▎        | 80/590 [00:23<02:28,  3.43it/s] 14%|█▎        | 81/590 [00:23<02:28,  3.43it/s] 14%|█▍        | 82/590 [00:23<02:27,  3.44it/s] 14%|█▍        | 83/590 [00:24<02:28,  3.41it/s] 14%|█▍        | 84/590 [00:24<02:27,  3.42it/s] 14%|█▍        | 85/590 [00:24<02:27,  3.43it/s] 15%|█▍        | 86/590 [00:25<02:27,  3.43it/s] 15%|█▍        | 87/590 [00:25<02:26,  3.43it/s] 15%|█▍        | 88/590 [00:25<02:26,  3.43it/s] 15%|█▌        | 89/590 [00:26<02:25,  3.43it/s] 15%|█▌        | 90/590 [00:26<02:25,  3.44it/s] 15%|█▌        | 91/590 [00:26<02:25,  3.44it/s] 16%|█▌        | 92/590 [00:26<02:24,  3.44it/s] 16%|█▌        | 93/590 [00:27<02:24,  3.44it/s] 16%|█▌        | 94/590 [00:27<02:29,  3.33it/s] 16%|█▌        | 95/590 [00:27<02:27,  3.36it/s] 16%|█▋        | 96/590 [00:28<02:26,  3.38it/s] 16%|█▋        | 97/590 [00:28<02:25,  3.39it/s] 17%|█▋        | 98/590 [00:28<02:24,  3.41it/s] 17%|█▋        | 99/590 [00:28<02:23,  3.42it/s] 17%|█▋        | 100/590 [00:29<02:23,  3.43it/s] 17%|█▋        | 101/590 [00:29<02:22,  3.43it/s] 17%|█▋        | 102/590 [00:29<02:22,  3.43it/s] 17%|█▋        | 103/590 [00:30<02:21,  3.43it/s] 18%|█▊        | 104/590 [00:30<02:21,  3.44it/s] 18%|█▊        | 105/590 [00:30<02:22,  3.40it/s] 18%|█▊        | 106/590 [00:30<02:22,  3.41it/s] 18%|█▊        | 107/590 [00:31<02:21,  3.42it/s] 18%|█▊        | 108/590 [00:31<02:20,  3.42it/s] 18%|█▊        | 109/590 [00:31<02:20,  3.43it/s] 19%|█▊        | 110/590 [00:32<02:19,  3.43it/s] 19%|█▉        | 111/590 [00:32<02:19,  3.43it/s] 19%|█▉        | 112/590 [00:32<02:19,  3.44it/s] 19%|█▉        | 113/590 [00:33<02:18,  3.44it/s] 19%|█▉        | 114/590 [00:33<02:18,  3.44it/s] 19%|█▉        | 115/590 [00:33<02:18,  3.43it/s] 20%|█▉        | 116/590 [00:33<02:18,  3.41it/s] 20%|█▉        | 117/590 [00:34<02:18,  3.42it/s] 20%|██        | 118/590 [00:34<02:11,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 17:31:23,035 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:31:23,035 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 17:31:23,035 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.10it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.67it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.90it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.10it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.84it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.45it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.18it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.24it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.28it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.28it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.23it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.08it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.17it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.18it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.94it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.87it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.01it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.05it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.85it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 43.98it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.08it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.02it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.01it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.99it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.90it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.92it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.03it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.07it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.13it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.08it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.02it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.07it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.99it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.97it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.88it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.08it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.12it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.05it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.04it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.03it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.05it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.93it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.96it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.94it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.04it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.11it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.05it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.01it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.07it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.10it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.97it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.94it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.94it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.03it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.06it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.05it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.01it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.04it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.05it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.99it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.87it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.99it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.92it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.09it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.97it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.95it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.13it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.13it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.95it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.02it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.05it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.06it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 44.00it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.95it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.06it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.98it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.02it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.94it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.07it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.15it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.10it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 43.91it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.99it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.04it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.93it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.93it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.96it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.10it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.17it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.05it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.96it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.88it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.05it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.04it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.91it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.99it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.06it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.03it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.13it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.89it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.85it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.99it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.98it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.94it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.02it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.09it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.01it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.06it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.06it/s][A 20%|██        | 118/590 [00:46<02:11,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:31:35,494 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-28 17:31:35,574 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:31:40,181 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:31:40,262 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:31:40,270 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [01:00<1:02:13,  7.93s/it] 20%|██        | 120/590 [01:00<44:09,  5.64s/it]   21%|██        | 121/590 [01:00<31:32,  4.03s/it] 21%|██        | 122/590 [01:01<22:43,  2.91s/it] 21%|██        | 123/590 [01:01<16:33,  2.13s/it] 21%|██        | 124/590 [01:01<12:14,  1.58s/it] 21%|██        | 125/590 [01:01<09:14,  1.19s/it] 21%|██▏       | 126/590 [01:02<07:08,  1.08it/s] 22%|██▏       | 127/590 [01:02<05:40,  1.36it/s] 22%|██▏       | 128/590 [01:02<04:38,  1.66it/s] 22%|██▏       | 129/590 [01:03<03:55,  1.96it/s] 22%|██▏       | 130/590 [01:03<03:24,  2.24it/s] 22%|██▏       | 131/590 [01:03<03:05,  2.48it/s] 22%|██▏       | 132/590 [01:04<02:49,  2.70it/s] 23%|██▎       | 133/590 [01:04<02:38,  2.87it/s] 23%|██▎       | 134/590 [01:04<02:31,  3.01it/s] 23%|██▎       | 135/590 [01:04<02:25,  3.12it/s] 23%|██▎       | 136/590 [01:05<02:22,  3.19it/s] 23%|██▎       | 137/590 [01:05<02:19,  3.25it/s] 23%|██▎       | 138/590 [01:05<02:17,  3.30it/s] 24%|██▎       | 139/590 [01:06<02:15,  3.32it/s] 24%|██▎       | 140/590 [01:06<02:14,  3.34it/s] 24%|██▍       | 141/590 [01:06<02:13,  3.36it/s] 24%|██▍       | 142/590 [01:07<02:15,  3.30it/s] 24%|██▍       | 143/590 [01:07<02:14,  3.33it/s] 24%|██▍       | 144/590 [01:07<02:13,  3.35it/s] 25%|██▍       | 145/590 [01:07<02:12,  3.36it/s] 25%|██▍       | 146/590 [01:08<02:11,  3.37it/s] 25%|██▍       | 147/590 [01:08<02:11,  3.38it/s] 25%|██▌       | 148/590 [01:08<02:10,  3.38it/s] 25%|██▌       | 149/590 [01:09<02:10,  3.38it/s] 25%|██▌       | 150/590 [01:09<02:10,  3.38it/s] 26%|██▌       | 151/590 [01:09<02:09,  3.39it/s] 26%|██▌       | 152/590 [01:09<02:09,  3.39it/s] 26%|██▌       | 153/590 [01:10<02:12,  3.31it/s] 26%|██▌       | 154/590 [01:10<02:10,  3.33it/s] 26%|██▋       | 155/590 [01:10<02:09,  3.35it/s] 26%|██▋       | 156/590 [01:11<02:09,  3.36it/s] 27%|██▋       | 157/590 [01:11<02:08,  3.37it/s] 27%|██▋       | 158/590 [01:11<02:07,  3.38it/s] 27%|██▋       | 159/590 [01:12<02:07,  3.38it/s] 27%|██▋       | 160/590 [01:12<02:07,  3.38it/s] 27%|██▋       | 161/590 [01:12<02:06,  3.39it/s] 27%|██▋       | 162/590 [01:12<02:06,  3.39it/s] 28%|██▊       | 163/590 [01:13<02:06,  3.39it/s] 28%|██▊       | 164/590 [01:13<02:05,  3.39it/s] 28%|██▊       | 165/590 [01:13<02:05,  3.39it/s] 28%|██▊       | 166/590 [01:14<02:05,  3.39it/s] 28%|██▊       | 167/590 [01:14<02:04,  3.39it/s] 28%|██▊       | 168/590 [01:14<02:04,  3.39it/s] 29%|██▊       | 169/590 [01:15<02:04,  3.39it/s] 29%|██▉       | 170/590 [01:15<02:03,  3.39it/s] 29%|██▉       | 171/590 [01:15<02:03,  3.39it/s] 29%|██▉       | 172/590 [01:15<02:03,  3.38it/s] 29%|██▉       | 173/590 [01:16<02:03,  3.38it/s] 29%|██▉       | 174/590 [01:16<02:03,  3.38it/s] 30%|██▉       | 175/590 [01:16<02:02,  3.39it/s] 30%|██▉       | 176/590 [01:17<02:02,  3.39it/s] 30%|███       | 177/590 [01:17<02:01,  3.39it/s] 30%|███       | 178/590 [01:17<02:01,  3.39it/s] 30%|███       | 179/590 [01:17<02:01,  3.39it/s] 31%|███       | 180/590 [01:18<02:00,  3.39it/s] 31%|███       | 181/590 [01:18<02:00,  3.39it/s] 31%|███       | 182/590 [01:18<02:00,  3.39it/s] 31%|███       | 183/590 [01:19<02:00,  3.39it/s] 31%|███       | 184/590 [01:19<01:59,  3.38it/s] 31%|███▏      | 185/590 [01:19<01:59,  3.39it/s] 32%|███▏      | 186/590 [01:20<01:59,  3.39it/s] 32%|███▏      | 187/590 [01:20<01:58,  3.39it/s] 32%|███▏      | 188/590 [01:20<01:58,  3.39it/s] 32%|███▏      | 189/590 [01:20<01:58,  3.39it/s] 32%|███▏      | 190/590 [01:21<01:57,  3.39it/s] 32%|███▏      | 191/590 [01:21<01:57,  3.39it/s] 33%|███▎      | 192/590 [01:21<01:57,  3.39it/s] 33%|███▎      | 193/590 [01:22<01:57,  3.39it/s] 33%|███▎      | 194/590 [01:22<01:57,  3.37it/s] 33%|███▎      | 195/590 [01:22<01:56,  3.38it/s] 33%|███▎      | 196/590 [01:22<01:56,  3.38it/s] 33%|███▎      | 197/590 [01:23<01:56,  3.38it/s] 34%|███▎      | 198/590 [01:23<01:55,  3.38it/s] 34%|███▎      | 199/590 [01:23<01:55,  3.39it/s] 34%|███▍      | 200/590 [01:24<01:55,  3.39it/s] 34%|███▍      | 201/590 [01:24<01:54,  3.39it/s] 34%|███▍      | 202/590 [01:24<01:54,  3.39it/s] 34%|███▍      | 203/590 [01:25<01:54,  3.39it/s] 35%|███▍      | 204/590 [01:25<01:53,  3.39it/s] 35%|███▍      | 205/590 [01:25<01:55,  3.34it/s] 35%|███▍      | 206/590 [01:25<01:53,  3.37it/s] 35%|███▌      | 207/590 [01:26<01:52,  3.39it/s] 35%|███▌      | 208/590 [01:26<01:52,  3.40it/s] 35%|███▌      | 209/590 [01:26<01:51,  3.41it/s] 36%|███▌      | 210/590 [01:27<01:51,  3.42it/s] 36%|███▌      | 211/590 [01:27<01:50,  3.43it/s] 36%|███▌      | 212/590 [01:27<01:50,  3.43it/s] 36%|███▌      | 213/590 [01:27<01:49,  3.43it/s] 36%|███▋      | 214/590 [01:28<01:49,  3.43it/s] 36%|███▋      | 215/590 [01:28<01:49,  3.43it/s] 37%|███▋      | 216/590 [01:28<01:51,  3.37it/s] 37%|███▋      | 217/590 [01:29<01:50,  3.39it/s] 37%|███▋      | 218/590 [01:29<01:49,  3.40it/s] 37%|███▋      | 219/590 [01:29<01:48,  3.41it/s] 37%|███▋      | 220/590 [01:30<01:48,  3.42it/s] 37%|███▋      | 221/590 [01:30<01:47,  3.42it/s] 38%|███▊      | 222/590 [01:30<01:47,  3.43it/s] 38%|███▊      | 223/590 [01:30<01:47,  3.43it/s] 38%|███▊      | 224/590 [01:31<01:46,  3.43it/s] 38%|███▊      | 225/590 [01:31<01:46,  3.43it/s] 38%|███▊      | 226/590 [01:31<01:46,  3.43it/s] 38%|███▊      | 227/590 [01:32<01:46,  3.42it/s] 39%|███▊      | 228/590 [01:32<01:45,  3.43it/s] 39%|███▉      | 229/590 [01:32<01:45,  3.43it/s] 39%|███▉      | 230/590 [01:32<01:44,  3.43it/s] 39%|███▉      | 231/590 [01:33<01:44,  3.43it/s] 39%|███▉      | 232/590 [01:33<01:44,  3.43it/s] 39%|███▉      | 233/590 [01:33<01:44,  3.43it/s] 40%|███▉      | 234/590 [01:34<01:43,  3.43it/s] 40%|███▉      | 235/590 [01:34<01:43,  3.43it/s] 40%|████      | 236/590 [01:34<01:38,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 17:32:23,243 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:32:23,244 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 17:32:23,244 >>   Batch size = 8
{'eval_loss': 0.9634774327278137, 'eval_runtime': 12.3334, 'eval_samples_per_second': 352.051, 'eval_steps_per_second': 44.027, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.97it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.58it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.07it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.18it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.77it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.53it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.34it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.29it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.35it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.30it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.20it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.02it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.04it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.95it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.00it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.01it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.11it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.22it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.19it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.05it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.07it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.98it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.94it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.99it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.01it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.10it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.15it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.14it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.08it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.94it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.98it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.98it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.96it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.02it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.98it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.10it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.16it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 43.97it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.95it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.08it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.02it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.01it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.99it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.11it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.11it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.05it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.02it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.86it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.97it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.95it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.00it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.04it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.10it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.07it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.04it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 43.98it/s][A
 53%|█████▎    | 287/543 [00:06<00:06, 41.31it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 42.15it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 42.75it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.24it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.39it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.69it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.76it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.79it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.64it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.70it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.69it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.98it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.11it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.03it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.13it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.16it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.91it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.82it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.83it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.95it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.06it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.16it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 44.16it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.12it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.13it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 43.97it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.93it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.87it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.02it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.94it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.15it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.28it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.05it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.97it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.86it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.91it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.95it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.81it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.00it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.10it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.17it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 43.95it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.95it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.93it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 43.95it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.02it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.09it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.05it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.11it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.02it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.04it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.04it/s][A 40%|████      | 236/590 [01:47<01:38,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:32:35,615 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-28 17:32:35,636 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:32:39,605 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:32:39,668 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:32:39,686 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [02:00<46:16,  7.87s/it] 40%|████      | 238/590 [02:00<32:51,  5.60s/it] 41%|████      | 239/590 [02:00<23:27,  4.01s/it] 41%|████      | 240/590 [02:01<16:52,  2.89s/it] 41%|████      | 241/590 [02:01<12:17,  2.11s/it] 41%|████      | 242/590 [02:01<09:05,  1.57s/it] 41%|████      | 243/590 [02:02<06:51,  1.19s/it] 41%|████▏     | 244/590 [02:02<05:17,  1.09it/s] 42%|████▏     | 245/590 [02:02<04:11,  1.37it/s] 42%|████▏     | 246/590 [02:02<03:25,  1.67it/s] 42%|████▏     | 247/590 [02:03<02:53,  1.98it/s] 42%|████▏     | 248/590 [02:03<02:30,  2.27it/s] 42%|████▏     | 249/590 [02:03<02:16,  2.49it/s] 42%|████▏     | 250/590 [02:04<02:05,  2.72it/s] 43%|████▎     | 251/590 [02:04<01:56,  2.90it/s] 43%|████▎     | 252/590 [02:04<01:51,  3.04it/s] 43%|████▎     | 253/590 [02:04<01:46,  3.16it/s] 43%|████▎     | 254/590 [02:05<01:43,  3.24it/s] 43%|████▎     | 255/590 [02:05<01:41,  3.30it/s] 43%|████▎     | 256/590 [02:05<01:40,  3.34it/s] 44%|████▎     | 257/590 [02:06<01:38,  3.37it/s] 44%|████▎     | 258/590 [02:06<01:37,  3.39it/s] 44%|████▍     | 259/590 [02:06<01:37,  3.41it/s] 44%|████▍     | 260/590 [02:07<01:40,  3.28it/s] 44%|████▍     | 261/590 [02:07<01:38,  3.33it/s] 44%|████▍     | 262/590 [02:07<01:37,  3.36it/s] 45%|████▍     | 263/590 [02:07<01:36,  3.38it/s] 45%|████▍     | 264/590 [02:08<01:35,  3.40it/s] 45%|████▍     | 265/590 [02:08<01:35,  3.42it/s] 45%|████▌     | 266/590 [02:08<01:34,  3.42it/s] 45%|████▌     | 267/590 [02:09<01:34,  3.43it/s] 45%|████▌     | 268/590 [02:09<01:33,  3.43it/s] 46%|████▌     | 269/590 [02:09<01:33,  3.44it/s] 46%|████▌     | 270/590 [02:09<01:33,  3.44it/s] 46%|████▌     | 271/590 [02:10<01:35,  3.35it/s] 46%|████▌     | 272/590 [02:10<01:34,  3.38it/s] 46%|████▋     | 273/590 [02:10<01:33,  3.40it/s] 46%|████▋     | 274/590 [02:11<01:32,  3.41it/s] 47%|████▋     | 275/590 [02:11<01:32,  3.42it/s] 47%|████▋     | 276/590 [02:11<01:31,  3.43it/s] 47%|████▋     | 277/590 [02:11<01:31,  3.43it/s] 47%|████▋     | 278/590 [02:12<01:30,  3.44it/s] 47%|████▋     | 279/590 [02:12<01:30,  3.44it/s] 47%|████▋     | 280/590 [02:12<01:30,  3.44it/s] 48%|████▊     | 281/590 [02:13<01:29,  3.44it/s] 48%|████▊     | 282/590 [02:13<01:30,  3.42it/s] 48%|████▊     | 283/590 [02:13<01:29,  3.43it/s] 48%|████▊     | 284/590 [02:14<01:29,  3.43it/s] 48%|████▊     | 285/590 [02:14<01:28,  3.43it/s] 48%|████▊     | 286/590 [02:14<01:28,  3.44it/s] 49%|████▊     | 287/590 [02:14<01:28,  3.44it/s] 49%|████▉     | 288/590 [02:15<01:27,  3.44it/s] 49%|████▉     | 289/590 [02:15<01:27,  3.44it/s] 49%|████▉     | 290/590 [02:15<01:27,  3.44it/s] 49%|████▉     | 291/590 [02:16<01:26,  3.44it/s] 49%|████▉     | 292/590 [02:16<01:26,  3.44it/s] 50%|████▉     | 293/590 [02:16<01:26,  3.44it/s] 50%|████▉     | 294/590 [02:16<01:26,  3.44it/s] 50%|█████     | 295/590 [02:17<01:25,  3.44it/s] 50%|█████     | 296/590 [02:17<01:27,  3.36it/s] 50%|█████     | 297/590 [02:17<01:26,  3.38it/s] 51%|█████     | 298/590 [02:18<01:25,  3.40it/s] 51%|█████     | 299/590 [02:18<01:25,  3.41it/s] 51%|█████     | 300/590 [02:18<01:24,  3.42it/s] 51%|█████     | 301/590 [02:18<01:24,  3.42it/s] 51%|█████     | 302/590 [02:19<01:24,  3.43it/s] 51%|█████▏    | 303/590 [02:19<01:23,  3.43it/s] 52%|█████▏    | 304/590 [02:19<01:23,  3.43it/s] 52%|█████▏    | 305/590 [02:20<01:23,  3.43it/s] 52%|█████▏    | 306/590 [02:20<01:22,  3.44it/s] 52%|█████▏    | 307/590 [02:20<01:23,  3.39it/s] 52%|█████▏    | 308/590 [02:21<01:22,  3.40it/s] 52%|█████▏    | 309/590 [02:21<01:22,  3.42it/s] 53%|█████▎    | 310/590 [02:21<01:21,  3.42it/s] 53%|█████▎    | 311/590 [02:21<01:21,  3.42it/s] 53%|█████▎    | 312/590 [02:22<01:21,  3.43it/s] 53%|█████▎    | 313/590 [02:22<01:20,  3.43it/s] 53%|█████▎    | 314/590 [02:22<01:20,  3.43it/s] 53%|█████▎    | 315/590 [02:23<01:20,  3.44it/s] 54%|█████▎    | 316/590 [02:23<01:19,  3.44it/s] 54%|█████▎    | 317/590 [02:23<01:19,  3.44it/s] 54%|█████▍    | 318/590 [02:23<01:19,  3.43it/s] 54%|█████▍    | 319/590 [02:24<01:19,  3.43it/s] 54%|█████▍    | 320/590 [02:24<01:18,  3.43it/s] 54%|█████▍    | 321/590 [02:24<01:18,  3.43it/s] 55%|█████▍    | 322/590 [02:25<01:17,  3.44it/s] 55%|█████▍    | 323/590 [02:25<01:17,  3.44it/s] 55%|█████▍    | 324/590 [02:25<01:17,  3.44it/s] 55%|█████▌    | 325/590 [02:25<01:17,  3.44it/s] 55%|█████▌    | 326/590 [02:26<01:16,  3.44it/s] 55%|█████▌    | 327/590 [02:26<01:16,  3.44it/s] 56%|█████▌    | 328/590 [02:26<01:16,  3.44it/s] 56%|█████▌    | 329/590 [02:27<01:16,  3.43it/s] 56%|█████▌    | 330/590 [02:27<01:15,  3.43it/s] 56%|█████▌    | 331/590 [02:27<01:15,  3.43it/s] 56%|█████▋    | 332/590 [02:28<01:15,  3.43it/s] 56%|█████▋    | 333/590 [02:28<01:14,  3.44it/s] 57%|█████▋    | 334/590 [02:28<01:14,  3.44it/s] 57%|█████▋    | 335/590 [02:28<01:14,  3.44it/s] 57%|█████▋    | 336/590 [02:29<01:13,  3.44it/s] 57%|█████▋    | 337/590 [02:29<01:13,  3.44it/s] 57%|█████▋    | 338/590 [02:29<01:13,  3.44it/s] 57%|█████▋    | 339/590 [02:30<01:13,  3.44it/s] 58%|█████▊    | 340/590 [02:30<01:12,  3.43it/s] 58%|█████▊    | 341/590 [02:30<01:12,  3.43it/s] 58%|█████▊    | 342/590 [02:30<01:12,  3.43it/s] 58%|█████▊    | 343/590 [02:31<01:11,  3.43it/s] 58%|█████▊    | 344/590 [02:31<01:11,  3.43it/s] 58%|█████▊    | 345/590 [02:31<01:11,  3.43it/s] 59%|█████▊    | 346/590 [02:32<01:11,  3.43it/s] 59%|█████▉    | 347/590 [02:32<01:10,  3.44it/s] 59%|█████▉    | 348/590 [02:32<01:10,  3.44it/s] 59%|█████▉    | 349/590 [02:32<01:10,  3.44it/s] 59%|█████▉    | 350/590 [02:33<01:09,  3.44it/s] 59%|█████▉    | 351/590 [02:33<01:09,  3.42it/s] 60%|█████▉    | 352/590 [02:33<01:09,  3.43it/s] 60%|█████▉    | 353/590 [02:34<01:09,  3.43it/s] 60%|██████    | 354/590 [02:34<01:05,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 17:33:22,964 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:33:22,964 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 17:33:22,964 >>   Batch size = 8
{'eval_loss': 0.9753339290618896, 'eval_runtime': 12.3542, 'eval_samples_per_second': 351.46, 'eval_steps_per_second': 43.953, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.46it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.02it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.29it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.20it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.80it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.47it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.34it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.19it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.28it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.33it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.39it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.20it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.07it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.03it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.95it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.95it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.95it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.13it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.17it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.06it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.14it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.05it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.97it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.97it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.91it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.94it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.17it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.15it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.07it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.09it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.06it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.98it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.96it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.91it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.00it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.21it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.09it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.03it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.16it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.10it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.91it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.96it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.02it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.10it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.05it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.08it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.08it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.03it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.91it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.97it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.11it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.10it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.96it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.16it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.10it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.05it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.96it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.99it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.93it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.04it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.00it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.06it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.09it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.06it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.04it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.97it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.05it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.02it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.95it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.88it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.05it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.07it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.00it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.97it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.98it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.03it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.13it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.89it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.08it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.18it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.06it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.04it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 43.97it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.96it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.93it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.04it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.96it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.07it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.17it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.06it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.98it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.06it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.07it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.99it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.96it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.97it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.08it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.12it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.97it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.93it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.06it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.07it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.91it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.00it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.13it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.09it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.18it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.18it/s][A 60%|██████    | 354/590 [02:46<01:05,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:33:35,314 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-28 17:33:35,341 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:33:39,882 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:33:39,917 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:33:39,930 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [03:01<32:05,  8.19s/it] 60%|██████    | 356/590 [03:01<22:43,  5.83s/it] 61%|██████    | 357/590 [03:01<16:10,  4.17s/it] 61%|██████    | 358/590 [03:01<11:37,  3.00s/it] 61%|██████    | 359/590 [03:02<08:26,  2.19s/it] 61%|██████    | 360/590 [03:02<06:13,  1.62s/it] 61%|██████    | 361/590 [03:02<04:40,  1.22s/it] 61%|██████▏   | 362/590 [03:03<03:35,  1.06it/s] 62%|██████▏   | 363/590 [03:03<02:50,  1.33it/s] 62%|██████▏   | 364/590 [03:03<02:18,  1.63it/s] 62%|██████▏   | 365/590 [03:03<01:56,  1.93it/s] 62%|██████▏   | 366/590 [03:04<01:40,  2.22it/s] 62%|██████▏   | 367/590 [03:04<01:30,  2.47it/s] 62%|██████▏   | 368/590 [03:04<01:22,  2.69it/s] 63%|██████▎   | 369/590 [03:05<01:16,  2.87it/s] 63%|██████▎   | 370/590 [03:05<01:13,  3.01it/s] 63%|██████▎   | 371/590 [03:05<01:10,  3.12it/s] 63%|██████▎   | 372/590 [03:06<01:08,  3.20it/s] 63%|██████▎   | 373/590 [03:06<01:06,  3.25it/s] 63%|██████▎   | 374/590 [03:06<01:05,  3.30it/s] 64%|██████▎   | 375/590 [03:06<01:04,  3.33it/s] 64%|██████▎   | 376/590 [03:07<01:03,  3.35it/s] 64%|██████▍   | 377/590 [03:07<01:03,  3.36it/s] 64%|██████▍   | 378/590 [03:07<01:03,  3.36it/s] 64%|██████▍   | 379/590 [03:08<01:02,  3.37it/s] 64%|██████▍   | 380/590 [03:08<01:02,  3.38it/s] 65%|██████▍   | 381/590 [03:08<01:01,  3.38it/s] 65%|██████▍   | 382/590 [03:08<01:01,  3.39it/s] 65%|██████▍   | 383/590 [03:09<01:00,  3.40it/s] 65%|██████▌   | 384/590 [03:09<01:00,  3.41it/s] 65%|██████▌   | 385/590 [03:09<00:59,  3.42it/s] 65%|██████▌   | 386/590 [03:10<00:59,  3.43it/s] 66%|██████▌   | 387/590 [03:10<00:59,  3.44it/s] 66%|██████▌   | 388/590 [03:10<00:58,  3.44it/s] 66%|██████▌   | 389/590 [03:11<00:58,  3.43it/s] 66%|██████▌   | 390/590 [03:11<00:58,  3.43it/s] 66%|██████▋   | 391/590 [03:11<00:57,  3.44it/s] 66%|██████▋   | 392/590 [03:11<00:57,  3.44it/s] 67%|██████▋   | 393/590 [03:12<00:57,  3.44it/s] 67%|██████▋   | 394/590 [03:12<00:56,  3.44it/s] 67%|██████▋   | 395/590 [03:12<00:56,  3.44it/s] 67%|██████▋   | 396/590 [03:13<00:56,  3.44it/s] 67%|██████▋   | 397/590 [03:13<00:56,  3.44it/s] 67%|██████▋   | 398/590 [03:13<00:55,  3.44it/s] 68%|██████▊   | 399/590 [03:13<00:55,  3.44it/s] 68%|██████▊   | 400/590 [03:14<00:55,  3.43it/s] 68%|██████▊   | 401/590 [03:14<00:55,  3.44it/s] 68%|██████▊   | 402/590 [03:14<00:54,  3.44it/s] 68%|██████▊   | 403/590 [03:15<00:54,  3.44it/s] 68%|██████▊   | 404/590 [03:15<00:54,  3.44it/s] 69%|██████▊   | 405/590 [03:15<00:53,  3.44it/s] 69%|██████▉   | 406/590 [03:15<00:53,  3.44it/s] 69%|██████▉   | 407/590 [03:16<00:53,  3.44it/s] 69%|██████▉   | 408/590 [03:16<00:52,  3.44it/s] 69%|██████▉   | 409/590 [03:16<00:52,  3.44it/s] 69%|██████▉   | 410/590 [03:17<00:52,  3.44it/s] 70%|██████▉   | 411/590 [03:17<00:51,  3.45it/s] 70%|██████▉   | 412/590 [03:17<00:51,  3.45it/s] 70%|███████   | 413/590 [03:18<00:51,  3.45it/s] 70%|███████   | 414/590 [03:18<00:51,  3.45it/s] 70%|███████   | 415/590 [03:18<00:50,  3.44it/s] 71%|███████   | 416/590 [03:18<00:50,  3.42it/s] 71%|███████   | 417/590 [03:19<00:50,  3.43it/s] 71%|███████   | 418/590 [03:19<00:50,  3.43it/s] 71%|███████   | 419/590 [03:19<00:49,  3.44it/s] 71%|███████   | 420/590 [03:20<00:49,  3.44it/s] 71%|███████▏  | 421/590 [03:20<00:49,  3.44it/s] 72%|███████▏  | 422/590 [03:20<00:48,  3.44it/s] 72%|███████▏  | 423/590 [03:20<00:48,  3.44it/s] 72%|███████▏  | 424/590 [03:21<00:48,  3.44it/s] 72%|███████▏  | 425/590 [03:21<00:47,  3.44it/s] 72%|███████▏  | 426/590 [03:21<00:47,  3.44it/s] 72%|███████▏  | 427/590 [03:22<00:48,  3.37it/s] 73%|███████▎  | 428/590 [03:22<00:47,  3.39it/s] 73%|███████▎  | 429/590 [03:22<00:47,  3.40it/s] 73%|███████▎  | 430/590 [03:22<00:46,  3.42it/s] 73%|███████▎  | 431/590 [03:23<00:46,  3.42it/s] 73%|███████▎  | 432/590 [03:23<00:46,  3.43it/s] 73%|███████▎  | 433/590 [03:23<00:45,  3.43it/s] 74%|███████▎  | 434/590 [03:24<00:45,  3.43it/s] 74%|███████▎  | 435/590 [03:24<00:45,  3.43it/s] 74%|███████▍  | 436/590 [03:24<00:44,  3.44it/s] 74%|███████▍  | 437/590 [03:25<00:44,  3.44it/s] 74%|███████▍  | 438/590 [03:25<00:44,  3.40it/s] 74%|███████▍  | 439/590 [03:25<00:44,  3.42it/s] 75%|███████▍  | 440/590 [03:25<00:43,  3.43it/s] 75%|███████▍  | 441/590 [03:26<00:43,  3.43it/s] 75%|███████▍  | 442/590 [03:26<00:43,  3.43it/s] 75%|███████▌  | 443/590 [03:26<00:42,  3.44it/s] 75%|███████▌  | 444/590 [03:27<00:42,  3.44it/s] 75%|███████▌  | 445/590 [03:27<00:42,  3.43it/s] 76%|███████▌  | 446/590 [03:27<00:41,  3.44it/s] 76%|███████▌  | 447/590 [03:27<00:41,  3.44it/s] 76%|███████▌  | 448/590 [03:28<00:41,  3.44it/s] 76%|███████▌  | 449/590 [03:28<00:46,  3.04it/s] 76%|███████▋  | 450/590 [03:28<00:44,  3.15it/s] 76%|███████▋  | 451/590 [03:29<00:43,  3.23it/s] 77%|███████▋  | 452/590 [03:29<00:41,  3.29it/s] 77%|███████▋  | 453/590 [03:29<00:41,  3.33it/s] 77%|███████▋  | 454/590 [03:30<00:40,  3.37it/s] 77%|███████▋  | 455/590 [03:30<00:39,  3.39it/s] 77%|███████▋  | 456/590 [03:30<00:39,  3.40it/s] 77%|███████▋  | 457/590 [03:30<00:38,  3.41it/s] 78%|███████▊  | 458/590 [03:31<00:38,  3.42it/s] 78%|███████▊  | 459/590 [03:31<00:38,  3.40it/s] 78%|███████▊  | 460/590 [03:31<00:38,  3.41it/s] 78%|███████▊  | 461/590 [03:32<00:37,  3.42it/s] 78%|███████▊  | 462/590 [03:32<00:37,  3.43it/s] 78%|███████▊  | 463/590 [03:32<00:37,  3.43it/s] 79%|███████▊  | 464/590 [03:32<00:36,  3.43it/s] 79%|███████▉  | 465/590 [03:33<00:36,  3.43it/s] 79%|███████▉  | 466/590 [03:33<00:36,  3.44it/s] 79%|███████▉  | 467/590 [03:33<00:35,  3.44it/s] 79%|███████▉  | 468/590 [03:34<00:35,  3.44it/s] 79%|███████▉  | 469/590 [03:34<00:35,  3.44it/s] 80%|███████▉  | 470/590 [03:34<00:35,  3.38it/s] 80%|███████▉  | 471/590 [03:35<00:35,  3.38it/s] 80%|████████  | 472/590 [03:35<00:33,  3.54it/s][INFO|trainer.py:2140] 2023-08-28 17:34:23,896 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:34:23,896 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 17:34:23,896 >>   Batch size = 8
{'eval_loss': 0.9836181998252869, 'eval_runtime': 12.3252, 'eval_samples_per_second': 352.287, 'eval_steps_per_second': 44.056, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.77it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.63it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.96it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.13it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.81it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.49it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.35it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.39it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.30it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.33it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.26it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.13it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 43.96it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.07it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.13it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.05it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.04it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.24it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.11it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 41.09it/s][A
 20%|█▉        | 107/543 [00:02<00:10, 42.09it/s][A
 21%|██        | 112/543 [00:02<00:10, 42.71it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.16it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.42it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.61it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.84it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.84it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 43.65it/s][A
 27%|██▋       | 147/543 [00:03<00:09, 43.71it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.00it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.10it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.08it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.05it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.21it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 44.12it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.01it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 43.90it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 43.94it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.08it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.08it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.99it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.23it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.17it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.04it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.96it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 43.98it/s][A
 44%|████▎     | 237/543 [00:05<00:07, 40.90it/s][A
 45%|████▍     | 242/543 [00:05<00:07, 42.01it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 42.72it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.15it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.48it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.72it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.65it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.67it/s][A
 51%|█████     | 277/543 [00:06<00:06, 43.43it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 43.67it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.05it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.22it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.19it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.21it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.23it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.09it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.84it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.77it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.94it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.02it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.04it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.16it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.29it/s][A
 65%|██████▍   | 352/543 [00:08<00:04, 44.19it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.14it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.88it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.89it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 42.83it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.29it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.53it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.86it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.92it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 43.92it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.84it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 43.79it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 43.70it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.92it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.07it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.16it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.24it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.18it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.13it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.97it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.82it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.84it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.98it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.05it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.16it/s][A
 88%|████████▊ | 477/543 [00:11<00:01, 44.27it/s][A
 89%|████████▉ | 482/543 [00:11<00:01, 33.02it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 35.75it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 38.00it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 39.76it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 41.09it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 42.12it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 42.77it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.06it/s][A
 96%|█████████▌| 522/543 [00:12<00:00, 42.96it/s][A
 97%|█████████▋| 527/543 [00:12<00:00, 43.05it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.30it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.61it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.94it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 43.94it/s][A 80%|████████  | 472/590 [03:47<00:33,  3.54it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:34:36,435 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-28 17:34:36,496 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:34:42,484 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:34:42,515 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:34:42,522 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [04:03<16:37,  8.53s/it] 80%|████████  | 474/590 [04:03<11:42,  6.06s/it] 81%|████████  | 475/590 [04:03<08:17,  4.33s/it] 81%|████████  | 476/590 [04:03<05:55,  3.12s/it] 81%|████████  | 477/590 [04:04<04:16,  2.27s/it] 81%|████████  | 478/590 [04:04<03:07,  1.68s/it] 81%|████████  | 479/590 [04:04<02:20,  1.26s/it] 81%|████████▏ | 480/590 [04:05<01:46,  1.03it/s] 82%|████████▏ | 481/590 [04:05<01:23,  1.30it/s] 82%|████████▏ | 482/590 [04:05<01:07,  1.60it/s] 82%|████████▏ | 483/590 [04:06<00:56,  1.90it/s] 82%|████████▏ | 484/590 [04:06<00:48,  2.19it/s] 82%|████████▏ | 485/590 [04:06<00:43,  2.39it/s] 82%|████████▏ | 486/590 [04:06<00:39,  2.62it/s] 83%|████████▎ | 487/590 [04:07<00:36,  2.82it/s] 83%|████████▎ | 488/590 [04:07<00:34,  2.97it/s] 83%|████████▎ | 489/590 [04:07<00:32,  3.09it/s] 83%|████████▎ | 490/590 [04:08<00:31,  3.17it/s] 83%|████████▎ | 491/590 [04:08<00:30,  3.24it/s] 83%|████████▎ | 492/590 [04:08<00:29,  3.28it/s] 84%|████████▎ | 493/590 [04:08<00:29,  3.32it/s] 84%|████████▎ | 494/590 [04:09<00:28,  3.34it/s] 84%|████████▍ | 495/590 [04:09<00:28,  3.36it/s] 84%|████████▍ | 496/590 [04:09<00:28,  3.33it/s] 84%|████████▍ | 497/590 [04:10<00:27,  3.35it/s] 84%|████████▍ | 498/590 [04:10<00:27,  3.36it/s] 85%|████████▍ | 499/590 [04:10<00:26,  3.37it/s] 85%|████████▍ | 500/590 [04:11<00:26,  3.38it/s]                                                  85%|████████▍ | 500/590 [04:11<00:26,  3.38it/s] 85%|████████▍ | 501/590 [04:11<00:26,  3.38it/s] 85%|████████▌ | 502/590 [04:11<00:25,  3.39it/s] 85%|████████▌ | 503/590 [04:11<00:25,  3.39it/s] 85%|████████▌ | 504/590 [04:12<00:25,  3.40it/s] 86%|████████▌ | 505/590 [04:12<00:24,  3.41it/s] 86%|████████▌ | 506/590 [04:12<00:24,  3.42it/s] 86%|████████▌ | 507/590 [04:13<00:24,  3.41it/s] 86%|████████▌ | 508/590 [04:13<00:23,  3.43it/s] 86%|████████▋ | 509/590 [04:13<00:23,  3.43it/s] 86%|████████▋ | 510/590 [04:13<00:23,  3.44it/s] 87%|████████▋ | 511/590 [04:14<00:22,  3.44it/s] 87%|████████▋ | 512/590 [04:14<00:22,  3.44it/s] 87%|████████▋ | 513/590 [04:14<00:22,  3.44it/s] 87%|████████▋ | 514/590 [04:15<00:22,  3.44it/s] 87%|████████▋ | 515/590 [04:15<00:21,  3.44it/s] 87%|████████▋ | 516/590 [04:15<00:21,  3.44it/s] 88%|████████▊ | 517/590 [04:16<00:21,  3.44it/s] 88%|████████▊ | 518/590 [04:16<00:21,  3.29it/s] 88%|████████▊ | 519/590 [04:16<00:21,  3.34it/s] 88%|████████▊ | 520/590 [04:16<00:20,  3.37it/s] 88%|████████▊ | 521/590 [04:17<00:20,  3.39it/s] 88%|████████▊ | 522/590 [04:17<00:19,  3.40it/s] 89%|████████▊ | 523/590 [04:17<00:19,  3.42it/s] 89%|████████▉ | 524/590 [04:18<00:19,  3.42it/s] 89%|████████▉ | 525/590 [04:18<00:18,  3.43it/s] 89%|████████▉ | 526/590 [04:18<00:18,  3.43it/s] 89%|████████▉ | 527/590 [04:18<00:18,  3.44it/s] 89%|████████▉ | 528/590 [04:19<00:18,  3.44it/s] 90%|████████▉ | 529/590 [04:19<00:17,  3.44it/s] 90%|████████▉ | 530/590 [04:19<00:17,  3.44it/s] 90%|█████████ | 531/590 [04:20<00:17,  3.44it/s] 90%|█████████ | 532/590 [04:20<00:16,  3.43it/s] 90%|█████████ | 533/590 [04:20<00:16,  3.43it/s] 91%|█████████ | 534/590 [04:21<00:16,  3.44it/s] 91%|█████████ | 535/590 [04:21<00:16,  3.44it/s] 91%|█████████ | 536/590 [04:21<00:15,  3.44it/s] 91%|█████████ | 537/590 [04:21<00:15,  3.44it/s] 91%|█████████ | 538/590 [04:22<00:15,  3.44it/s] 91%|█████████▏| 539/590 [04:22<00:14,  3.44it/s] 92%|█████████▏| 540/590 [04:22<00:14,  3.44it/s] 92%|█████████▏| 541/590 [04:23<00:14,  3.44it/s] 92%|█████████▏| 542/590 [04:23<00:13,  3.44it/s] 92%|█████████▏| 543/590 [04:23<00:13,  3.38it/s] 92%|█████████▏| 544/590 [04:23<00:13,  3.40it/s] 92%|█████████▏| 545/590 [04:24<00:13,  3.41it/s] 93%|█████████▎| 546/590 [04:24<00:12,  3.42it/s] 93%|█████████▎| 547/590 [04:24<00:12,  3.42it/s] 93%|█████████▎| 548/590 [04:25<00:12,  3.43it/s] 93%|█████████▎| 549/590 [04:25<00:11,  3.43it/s] 93%|█████████▎| 550/590 [04:25<00:11,  3.43it/s] 93%|█████████▎| 551/590 [04:25<00:11,  3.43it/s] 94%|█████████▎| 552/590 [04:26<00:11,  3.43it/s] 94%|█████████▎| 553/590 [04:26<00:10,  3.44it/s] 94%|█████████▍| 554/590 [04:27<00:12,  2.92it/s] 94%|█████████▍| 555/590 [04:27<00:11,  3.06it/s] 94%|█████████▍| 556/590 [04:27<00:10,  3.17it/s] 94%|█████████▍| 557/590 [04:27<00:10,  3.24it/s] 95%|█████████▍| 558/590 [04:28<00:09,  3.30it/s] 95%|█████████▍| 559/590 [04:28<00:09,  3.34it/s] 95%|█████████▍| 560/590 [04:28<00:08,  3.37it/s] 95%|█████████▌| 561/590 [04:29<00:08,  3.39it/s] 95%|█████████▌| 562/590 [04:29<00:08,  3.41it/s] 95%|█████████▌| 563/590 [04:29<00:07,  3.42it/s] 96%|█████████▌| 564/590 [04:29<00:07,  3.42it/s] 96%|█████████▌| 565/590 [04:30<00:07,  3.42it/s] 96%|█████████▌| 566/590 [04:30<00:07,  3.43it/s] 96%|█████████▌| 567/590 [04:30<00:06,  3.43it/s] 96%|█████████▋| 568/590 [04:31<00:06,  3.43it/s] 96%|█████████▋| 569/590 [04:31<00:06,  3.43it/s] 97%|█████████▋| 570/590 [04:31<00:05,  3.44it/s] 97%|█████████▋| 571/590 [04:31<00:05,  3.44it/s] 97%|█████████▋| 572/590 [04:32<00:05,  3.44it/s] 97%|█████████▋| 573/590 [04:32<00:04,  3.44it/s] 97%|█████████▋| 574/590 [04:32<00:04,  3.44it/s] 97%|█████████▋| 575/590 [04:33<00:04,  3.42it/s] 98%|█████████▊| 576/590 [04:33<00:04,  3.42it/s] 98%|█████████▊| 577/590 [04:33<00:03,  3.43it/s] 98%|█████████▊| 578/590 [04:33<00:03,  3.43it/s] 98%|█████████▊| 579/590 [04:34<00:03,  3.44it/s] 98%|█████████▊| 580/590 [04:34<00:02,  3.44it/s] 98%|█████████▊| 581/590 [04:34<00:02,  3.44it/s] 99%|█████████▊| 582/590 [04:35<00:02,  3.44it/s] 99%|█████████▉| 583/590 [04:35<00:02,  3.44it/s] 99%|█████████▉| 584/590 [04:35<00:01,  3.44it/s] 99%|█████████▉| 585/590 [04:36<00:01,  3.44it/s] 99%|█████████▉| 586/590 [04:36<00:01,  3.43it/s] 99%|█████████▉| 587/590 [04:36<00:00,  3.43it/s]100%|█████████▉| 588/590 [04:36<00:00,  3.43it/s]100%|█████████▉| 589/590 [04:37<00:00,  3.44it/s]100%|██████████| 590/590 [04:37<00:00,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 17:35:26,033 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:35:26,034 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 17:35:26,034 >>   Batch size = 8
{'eval_loss': 0.9908245205879211, 'eval_runtime': 12.508, 'eval_samples_per_second': 347.138, 'eval_steps_per_second': 43.412, 'epoch': 4.0}
{'loss': 0.7265, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.91it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.58it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.93it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.17it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.74it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.46it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.36it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.15it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.26it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.33it/s][A
 10%|█         | 57/543 [00:01<00:11, 44.15it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.04it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.12it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.11it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.40it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.49it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.71it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.90it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.86it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.02it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.01it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.87it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.02it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.05it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.94it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.01it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.07it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.17it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.12it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.00it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.93it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.00it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.09it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.06it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 44.03it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.08it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.14it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.03it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.03it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.97it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.04it/s][A
 39%|███▉      | 212/543 [00:04<00:09, 33.39it/s][A
 40%|███▉      | 217/543 [00:05<00:09, 36.17it/s][A
 41%|████      | 222/543 [00:05<00:08, 38.33it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 39.99it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 41.30it/s][A
 44%|████▎     | 237/543 [00:05<00:07, 42.27it/s][A
 45%|████▍     | 242/543 [00:05<00:07, 42.95it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.10it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.06it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 42.99it/s][A
 48%|████▊     | 262/543 [00:06<00:06, 43.31it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.56it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.94it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.16it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.18it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.31it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.06it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.78it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.51it/s][A
 57%|█████▋    | 307/543 [00:07<00:05, 43.68it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.85it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.15it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 44.37it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.33it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.23it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.06it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.84it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.63it/s][A
 65%|██████▍   | 352/543 [00:08<00:04, 43.58it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.80it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.07it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.28it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.33it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.29it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.16it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.85it/s][A
 72%|███████▏  | 392/543 [00:09<00:03, 43.63it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 43.68it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.89it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.11it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.34it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.35it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.16it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.08it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.91it/s][A
 80%|████████  | 437/543 [00:10<00:02, 43.67it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.76it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.95it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.11it/s][A
 84%|████████▍ | 457/543 [00:10<00:02, 41.16it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 42.17it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 42.83it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.18it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.36it/s][A
 89%|████████▉ | 482/543 [00:11<00:01, 43.50it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.56it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.77it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 43.71it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.84it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.85it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.13it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.11it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.01it/s][A
 97%|█████████▋| 527/543 [00:12<00:00, 44.01it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.00it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.89it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.96it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 43.96it/s][A100%|██████████| 590/590 [04:49<00:00,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:35:38,630 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-28 17:35:38,908 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:35:42,401 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:35:42,419 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:35:42,427 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:35:49,495 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:35:49,498 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118 (score: 0.9634774327278137).
                                                 100%|██████████| 590/590 [05:07<00:00,  3.60it/s]100%|██████████| 590/590 [05:07<00:00,  1.92it/s]
[INFO|trainer.py:1894] 2023-08-28 17:35:56,299 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 17:35:56,515 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:36:01,892 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:36:01,913 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:36:01,924 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:36:02,230 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:02,230 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:02,230 >>   train_loss               =     0.7218
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:02,230 >>   train_runtime            = 0:05:07.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:02,230 >>   train_samples            =       7541
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:02,230 >>   train_samples_per_second =    122.684
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:02,230 >>   train_steps_per_second   =       1.92
{'eval_loss': 0.9954342842102051, 'eval_runtime': 12.4778, 'eval_samples_per_second': 347.977, 'eval_steps_per_second': 43.517, 'epoch': 5.0}
{'train_runtime': 307.3347, 'train_samples_per_second': 122.684, 'train_steps_per_second': 1.92, 'train_loss': 0.7218250597937632, 'epoch': 5.0}
08/28/2023 17:36:02 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:36:02,308 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:36:02,308 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 17:36:02,308 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 56.27it/s]  2%|▏         | 12/543 [00:00<00:10, 48.86it/s]  3%|▎         | 17/543 [00:00<00:11, 47.41it/s]  4%|▍         | 22/543 [00:00<00:11, 46.59it/s]  5%|▍         | 27/543 [00:00<00:11, 45.99it/s]  6%|▌         | 32/543 [00:00<00:11, 45.62it/s]  7%|▋         | 37/543 [00:00<00:11, 45.51it/s]  8%|▊         | 42/543 [00:00<00:11, 44.95it/s]  9%|▊         | 47/543 [00:01<00:11, 44.35it/s] 10%|▉         | 52/543 [00:01<00:11, 43.97it/s] 10%|█         | 57/543 [00:01<00:11, 43.96it/s] 11%|█▏        | 62/543 [00:01<00:10, 44.24it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.46it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.69it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.79it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.71it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.42it/s] 17%|█▋        | 92/543 [00:02<00:10, 44.14it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.89it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.92it/s] 20%|█▉        | 107/543 [00:02<00:09, 44.06it/s] 21%|██        | 112/543 [00:02<00:09, 44.31it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.38it/s] 22%|██▏       | 122/543 [00:02<00:09, 43.40it/s] 23%|██▎       | 127/543 [00:02<00:09, 43.87it/s] 24%|██▍       | 132/543 [00:02<00:09, 43.92it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.83it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.81it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.78it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.07it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.22it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.24it/s] 31%|███       | 167/543 [00:03<00:08, 44.40it/s] 32%|███▏      | 172/543 [00:03<00:08, 44.48it/s] 33%|███▎      | 177/543 [00:03<00:08, 44.25it/s] 34%|███▎      | 182/543 [00:04<00:08, 44.17it/s] 34%|███▍      | 187/543 [00:04<00:08, 44.07it/s] 35%|███▌      | 192/543 [00:04<00:07, 44.00it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.12it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.35it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.42it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.41it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.32it/s] 41%|████      | 222/543 [00:04<00:07, 44.21it/s] 42%|████▏     | 227/543 [00:05<00:07, 44.14it/s] 43%|████▎     | 232/543 [00:05<00:07, 44.08it/s] 44%|████▎     | 237/543 [00:05<00:06, 44.08it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.28it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.29it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.31it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.38it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.21it/s] 49%|████▉     | 267/543 [00:06<00:06, 44.08it/s] 50%|█████     | 272/543 [00:06<00:06, 44.13it/s] 51%|█████     | 277/543 [00:06<00:06, 44.04it/s] 52%|█████▏    | 282/543 [00:06<00:05, 44.20it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.26it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.32it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.28it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.29it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.14it/s] 57%|█████▋    | 312/543 [00:07<00:05, 44.07it/s] 58%|█████▊    | 317/543 [00:07<00:05, 44.04it/s] 59%|█████▉    | 322/543 [00:07<00:05, 44.09it/s] 60%|██████    | 327/543 [00:07<00:04, 44.22it/s] 61%|██████    | 332/543 [00:07<00:04, 44.28it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.15it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.24it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.25it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.14it/s] 66%|██████▌   | 357/543 [00:08<00:04, 44.08it/s] 67%|██████▋   | 362/543 [00:08<00:04, 44.00it/s] 68%|██████▊   | 367/543 [00:08<00:03, 44.18it/s] 69%|██████▊   | 372/543 [00:08<00:03, 44.28it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.12it/s] 70%|███████   | 382/543 [00:08<00:03, 44.31it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.22it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.24it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.20it/s] 74%|███████▍  | 402/543 [00:09<00:03, 44.06it/s] 75%|███████▍  | 407/543 [00:09<00:03, 44.05it/s] 76%|███████▌  | 412/543 [00:09<00:02, 44.11it/s] 77%|███████▋  | 417/543 [00:09<00:02, 44.27it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.24it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.09it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.20it/s] 80%|████████  | 437/543 [00:09<00:02, 44.18it/s] 81%|████████▏ | 442/543 [00:09<00:02, 43.98it/s] 82%|████████▏ | 447/543 [00:10<00:02, 44.06it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.10it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.14it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.19it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.12it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.21it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.24it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.16it/s] 90%|████████▉ | 487/543 [00:11<00:01, 39.61it/s] 91%|█████████ | 492/543 [00:11<00:01, 41.06it/s] 92%|█████████▏| 497/543 [00:11<00:01, 42.20it/s] 92%|█████████▏| 502/543 [00:11<00:00, 42.99it/s] 93%|█████████▎| 507/543 [00:11<00:00, 43.28it/s] 94%|█████████▍| 512/543 [00:11<00:00, 43.65it/s] 95%|█████████▌| 517/543 [00:11<00:00, 43.81it/s] 96%|█████████▌| 522/543 [00:11<00:00, 43.78it/s] 97%|█████████▋| 527/543 [00:11<00:00, 43.55it/s] 98%|█████████▊| 532/543 [00:12<00:00, 43.54it/s] 99%|█████████▉| 537/543 [00:12<00:00, 43.78it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.04it/s]100%|██████████| 543/543 [00:12<00:00, 44.14it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:36:14,630 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:14,630 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:14,630 >>   eval_loss               =     0.9635
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:14,630 >>   eval_runtime            = 0:00:12.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:14,630 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:14,630 >>   eval_samples_per_second =    352.398
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:14,630 >>   eval_steps_per_second   =      44.07
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:36:14,630 >>   perplexity              =     2.6208
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:21,738 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:21,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:21,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:21,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:21,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:36:22,525 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:36:22,526 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:36:22,785 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:36:23,849 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:36:23,850 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:25,178 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:25,187 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:25,188 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:25,188 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:36:25,188 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:36:25,515 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:36:25,516 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:36:25,772 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:36:25,920 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:36:25,920 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-236
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-354
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.85it/s]Extractor Predicting: 2it [00:01,  1.81it/s]Extractor Predicting: 3it [00:01,  1.74it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.71it/s]Extractor Predicting: 8it [00:04,  1.73it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.80it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:06,  1.82it/s]Extractor Predicting: 13it [00:07,  1.73it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:08,  1.63it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:09,  1.62it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.55it/s]Extractor Predicting: 20it [00:11,  1.54it/s]Extractor Predicting: 21it [00:12,  1.56it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:13,  1.61it/s]Extractor Predicting: 24it [00:14,  1.59it/s]Extractor Predicting: 25it [00:15,  1.60it/s]Extractor Predicting: 26it [00:15,  1.58it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:16,  1.60it/s]Extractor Predicting: 29it [00:17,  1.57it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:18,  1.59it/s]Extractor Predicting: 32it [00:19,  1.59it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:20,  1.58it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.54it/s]Extractor Predicting: 37it [00:22,  1.54it/s]Extractor Predicting: 38it [00:23,  1.53it/s]Extractor Predicting: 39it [00:24,  1.49it/s]Extractor Predicting: 40it [00:24,  1.52it/s]Extractor Predicting: 41it [00:25,  1.52it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:26,  1.56it/s]Extractor Predicting: 44it [00:27,  1.54it/s]Extractor Predicting: 45it [00:27,  1.55it/s]Extractor Predicting: 46it [00:28,  1.58it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:29,  1.57it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:31,  1.57it/s]Extractor Predicting: 51it [00:31,  1.57it/s]Extractor Predicting: 52it [00:32,  1.55it/s]Extractor Predicting: 53it [00:33,  1.53it/s]Extractor Predicting: 54it [00:33,  1.54it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:34,  1.56it/s]Extractor Predicting: 57it [00:35,  1.54it/s]Extractor Predicting: 58it [00:36,  1.58it/s]Extractor Predicting: 59it [00:37,  1.45it/s]Extractor Predicting: 60it [00:37,  1.49it/s]Extractor Predicting: 61it [00:38,  1.53it/s]Extractor Predicting: 62it [00:38,  1.54it/s]Extractor Predicting: 63it [00:39,  1.55it/s]Extractor Predicting: 64it [00:40,  1.56it/s]Extractor Predicting: 65it [00:40,  1.61it/s]Extractor Predicting: 66it [00:41,  1.61it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:42,  1.58it/s]Extractor Predicting: 69it [00:43,  1.59it/s]Extractor Predicting: 70it [00:43,  1.58it/s]Extractor Predicting: 71it [00:44,  1.58it/s]Extractor Predicting: 72it [00:45,  1.58it/s]Extractor Predicting: 73it [00:45,  1.56it/s]Extractor Predicting: 74it [00:46,  1.54it/s]Extractor Predicting: 75it [00:47,  1.51it/s]Extractor Predicting: 76it [00:47,  1.53it/s]Extractor Predicting: 77it [00:48,  1.56it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:49,  1.54it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:50,  1.57it/s]Extractor Predicting: 82it [00:51,  1.57it/s]Extractor Predicting: 83it [00:52,  1.59it/s]Extractor Predicting: 84it [00:52,  1.60it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:54,  1.58it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:55,  1.60it/s]Extractor Predicting: 89it [00:55,  1.62it/s]Extractor Predicting: 90it [00:56,  1.59it/s]Extractor Predicting: 91it [00:57,  1.56it/s]Extractor Predicting: 92it [00:57,  1.53it/s]Extractor Predicting: 93it [00:58,  1.58it/s]Extractor Predicting: 94it [00:59,  1.60it/s]Extractor Predicting: 95it [00:59,  1.59it/s]Extractor Predicting: 96it [01:00,  1.61it/s]Extractor Predicting: 97it [01:01,  1.63it/s]Extractor Predicting: 98it [01:01,  1.61it/s]Extractor Predicting: 99it [01:02,  1.59it/s]Extractor Predicting: 100it [01:02,  1.60it/s]Extractor Predicting: 101it [01:03,  1.60it/s]Extractor Predicting: 102it [01:04,  1.59it/s]Extractor Predicting: 103it [01:04,  1.60it/s]Extractor Predicting: 104it [01:05,  1.61it/s]Extractor Predicting: 105it [01:06,  1.58it/s]Extractor Predicting: 106it [01:06,  1.61it/s]Extractor Predicting: 107it [01:07,  1.60it/s]Extractor Predicting: 108it [01:07,  1.60it/s]Extractor Predicting: 109it [01:08,  1.60it/s]Extractor Predicting: 110it [01:09,  1.58it/s]Extractor Predicting: 111it [01:09,  1.59it/s]Extractor Predicting: 112it [01:10,  1.62it/s]Extractor Predicting: 113it [01:11,  1.62it/s]Extractor Predicting: 114it [01:11,  1.61it/s]Extractor Predicting: 115it [01:12,  1.60it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:13,  1.61it/s]Extractor Predicting: 118it [01:14,  1.57it/s]Extractor Predicting: 119it [01:14,  1.58it/s]Extractor Predicting: 120it [01:15,  1.57it/s]Extractor Predicting: 121it [01:16,  1.55it/s]Extractor Predicting: 122it [01:16,  1.54it/s]Extractor Predicting: 123it [01:17,  1.58it/s]Extractor Predicting: 124it [01:18,  1.59it/s]Extractor Predicting: 125it [01:18,  1.61it/s]Extractor Predicting: 126it [01:19,  1.43it/s]Extractor Predicting: 127it [01:20,  1.46it/s]Extractor Predicting: 128it [01:20,  1.50it/s]Extractor Predicting: 129it [01:21,  1.54it/s]Extractor Predicting: 130it [01:21,  1.59it/s]Extractor Predicting: 131it [01:22,  1.56it/s]Extractor Predicting: 132it [01:23,  1.57it/s]Extractor Predicting: 133it [01:23,  1.58it/s]Extractor Predicting: 134it [01:24,  1.58it/s]Extractor Predicting: 135it [01:25,  1.60it/s]Extractor Predicting: 136it [01:25,  1.59it/s]Extractor Predicting: 137it [01:26,  1.61it/s]Extractor Predicting: 138it [01:27,  1.58it/s]Extractor Predicting: 139it [01:27,  1.56it/s]Extractor Predicting: 140it [01:28,  1.59it/s]Extractor Predicting: 141it [01:28,  1.61it/s]Extractor Predicting: 142it [01:29,  1.63it/s]Extractor Predicting: 143it [01:30,  1.63it/s]Extractor Predicting: 144it [01:30,  1.62it/s]Extractor Predicting: 145it [01:31,  1.60it/s]Extractor Predicting: 146it [01:31,  1.61it/s]Extractor Predicting: 147it [01:32,  1.61it/s]Extractor Predicting: 148it [01:33,  1.63it/s]Extractor Predicting: 149it [01:33,  1.62it/s]Extractor Predicting: 150it [01:34,  1.58it/s]Extractor Predicting: 151it [01:35,  1.58it/s]Extractor Predicting: 152it [01:35,  1.59it/s]Extractor Predicting: 153it [01:36,  1.57it/s]Extractor Predicting: 154it [01:37,  1.56it/s]Extractor Predicting: 155it [01:37,  1.54it/s]Extractor Predicting: 156it [01:38,  1.53it/s]Extractor Predicting: 157it [01:39,  1.54it/s]Extractor Predicting: 158it [01:39,  1.55it/s]Extractor Predicting: 159it [01:40,  1.57it/s]Extractor Predicting: 160it [01:40,  1.59it/s]Extractor Predicting: 161it [01:41,  1.61it/s]Extractor Predicting: 162it [01:42,  1.62it/s]Extractor Predicting: 163it [01:42,  1.52it/s]Extractor Predicting: 163it [01:42,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:17,218 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:17,221 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:17,221 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:17,221 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:17,221 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:38:17,551 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:38:17,552 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:38:17,813 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:38:18,877 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:38:18,883 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:20,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:20,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:20,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:20,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:20,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:38:20,741 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:38:20,742 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:38:21,009 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:38:21,181 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:38:21,181 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2647058823529412,
  "recall": 0.0020727775218793184,
  "score": 0.004113345521023767,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.71it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:02,  1.65it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:04,  1.59it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.62it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.67it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.65it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:24,  1.59it/s]Extractor Predicting: 42it [00:25,  1.60it/s]Extractor Predicting: 43it [00:26,  1.59it/s]Extractor Predicting: 44it [00:26,  1.56it/s]Extractor Predicting: 45it [00:27,  1.55it/s]Extractor Predicting: 46it [00:28,  1.54it/s]Extractor Predicting: 47it [00:28,  1.52it/s]Extractor Predicting: 48it [00:29,  1.55it/s]Extractor Predicting: 49it [00:30,  1.54it/s]Extractor Predicting: 50it [00:30,  1.55it/s]Extractor Predicting: 51it [00:31,  1.56it/s]Extractor Predicting: 52it [00:32,  1.55it/s]Extractor Predicting: 53it [00:32,  1.54it/s]Extractor Predicting: 54it [00:33,  1.49it/s]Extractor Predicting: 55it [00:34,  1.54it/s]Extractor Predicting: 56it [00:34,  1.58it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:35,  1.56it/s]Extractor Predicting: 59it [00:36,  1.48it/s]Extractor Predicting: 60it [00:37,  1.49it/s]Extractor Predicting: 61it [00:37,  1.53it/s]Extractor Predicting: 62it [00:38,  1.54it/s]Extractor Predicting: 63it [00:39,  1.50it/s]Extractor Predicting: 64it [00:39,  1.51it/s]Extractor Predicting: 65it [00:40,  1.52it/s]Extractor Predicting: 66it [00:41,  1.50it/s]Extractor Predicting: 67it [00:41,  1.51it/s]Extractor Predicting: 68it [00:42,  1.51it/s]Extractor Predicting: 69it [00:43,  1.50it/s]Extractor Predicting: 70it [00:43,  1.51it/s]Extractor Predicting: 71it [00:44,  1.53it/s]Extractor Predicting: 72it [00:45,  1.53it/s]Extractor Predicting: 73it [00:45,  1.53it/s]Extractor Predicting: 74it [00:46,  1.54it/s]Extractor Predicting: 75it [00:47,  1.54it/s]Extractor Predicting: 76it [00:47,  1.56it/s]Extractor Predicting: 77it [00:48,  1.59it/s]Extractor Predicting: 78it [00:49,  1.58it/s]Extractor Predicting: 79it [00:49,  1.53it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.46it/s]Extractor Predicting: 82it [00:51,  1.51it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:53,  1.49it/s]Extractor Predicting: 85it [00:53,  1.51it/s]Extractor Predicting: 86it [00:54,  1.54it/s]Extractor Predicting: 87it [00:54,  1.56it/s]Extractor Predicting: 88it [00:55,  1.55it/s]Extractor Predicting: 89it [00:56,  1.59it/s]Extractor Predicting: 90it [00:56,  1.60it/s]Extractor Predicting: 91it [00:57,  1.56it/s]Extractor Predicting: 92it [00:58,  1.57it/s]Extractor Predicting: 93it [00:58,  1.58it/s]Extractor Predicting: 94it [00:59,  1.57it/s]Extractor Predicting: 95it [01:00,  1.60it/s]Extractor Predicting: 96it [01:00,  1.60it/s]Extractor Predicting: 97it [01:01,  1.60it/s]Extractor Predicting: 98it [01:01,  1.55it/s]Extractor Predicting: 99it [01:02,  1.56it/s]Extractor Predicting: 100it [01:03,  1.56it/s]Extractor Predicting: 101it [01:03,  1.50it/s]Extractor Predicting: 102it [01:04,  1.54it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:05,  1.56it/s]Extractor Predicting: 105it [01:06,  1.56it/s]Extractor Predicting: 106it [01:07,  1.36it/s]Extractor Predicting: 107it [01:08,  1.40it/s]Extractor Predicting: 108it [01:08,  1.46it/s]Extractor Predicting: 109it [01:09,  1.43it/s]Extractor Predicting: 110it [01:10,  1.47it/s]Extractor Predicting: 111it [01:10,  1.47it/s]Extractor Predicting: 112it [01:11,  1.50it/s]Extractor Predicting: 113it [01:12,  1.50it/s]Extractor Predicting: 114it [01:12,  1.51it/s]Extractor Predicting: 115it [01:13,  1.51it/s]Extractor Predicting: 116it [01:14,  1.48it/s]Extractor Predicting: 117it [01:14,  1.51it/s]Extractor Predicting: 118it [01:15,  1.55it/s]Extractor Predicting: 119it [01:15,  1.56it/s]Extractor Predicting: 120it [01:16,  1.60it/s]Extractor Predicting: 121it [01:17,  1.59it/s]Extractor Predicting: 122it [01:17,  1.61it/s]Extractor Predicting: 123it [01:18,  1.63it/s]Extractor Predicting: 124it [01:18,  1.66it/s]Extractor Predicting: 125it [01:19,  1.64it/s]Extractor Predicting: 126it [01:20,  1.60it/s]Extractor Predicting: 127it [01:20,  1.60it/s]Extractor Predicting: 128it [01:21,  1.59it/s]Extractor Predicting: 129it [01:22,  1.59it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.63it/s]Extractor Predicting: 132it [01:23,  1.61it/s]Extractor Predicting: 133it [01:24,  1.61it/s]Extractor Predicting: 134it [01:25,  1.64it/s]Extractor Predicting: 135it [01:25,  1.64it/s]Extractor Predicting: 136it [01:26,  1.64it/s]Extractor Predicting: 137it [01:26,  1.68it/s]Extractor Predicting: 138it [01:27,  1.66it/s]Extractor Predicting: 139it [01:28,  1.64it/s]Extractor Predicting: 140it [01:28,  1.63it/s]Extractor Predicting: 141it [01:29,  1.64it/s]Extractor Predicting: 142it [01:30,  1.61it/s]Extractor Predicting: 143it [01:30,  1.59it/s]Extractor Predicting: 144it [01:31,  1.55it/s]Extractor Predicting: 145it [01:32,  1.52it/s]Extractor Predicting: 146it [01:32,  1.49it/s]Extractor Predicting: 147it [01:33,  1.52it/s]Extractor Predicting: 148it [01:34,  1.52it/s]Extractor Predicting: 149it [01:34,  1.53it/s]Extractor Predicting: 150it [01:35,  1.55it/s]Extractor Predicting: 151it [01:35,  1.55it/s]Extractor Predicting: 152it [01:36,  1.56it/s]Extractor Predicting: 153it [01:37,  1.59it/s]Extractor Predicting: 154it [01:37,  1.59it/s]Extractor Predicting: 155it [01:38,  1.63it/s]Extractor Predicting: 156it [01:39,  1.58it/s]Extractor Predicting: 157it [01:39,  1.59it/s]Extractor Predicting: 158it [01:40,  1.60it/s]Extractor Predicting: 159it [01:40,  1.59it/s]Extractor Predicting: 160it [01:41,  1.62it/s]Extractor Predicting: 161it [01:42,  1.54it/s]Extractor Predicting: 162it [01:42,  1.60it/s]Extractor Predicting: 163it [01:43,  1.62it/s]Extractor Predicting: 164it [01:44,  1.59it/s]Extractor Predicting: 165it [01:44,  1.64it/s]Extractor Predicting: 166it [01:45,  1.65it/s]Extractor Predicting: 167it [01:45,  1.64it/s]Extractor Predicting: 168it [01:46,  1.70it/s]Extractor Predicting: 169it [01:47,  1.70it/s]Extractor Predicting: 170it [01:47,  1.64it/s]Extractor Predicting: 171it [01:48,  1.63it/s]Extractor Predicting: 172it [01:48,  1.63it/s]Extractor Predicting: 173it [01:49,  1.59it/s]Extractor Predicting: 174it [01:50,  1.63it/s]Extractor Predicting: 175it [01:50,  1.63it/s]Extractor Predicting: 176it [01:51,  1.61it/s]Extractor Predicting: 177it [01:52,  1.60it/s]Extractor Predicting: 178it [01:52,  1.59it/s]Extractor Predicting: 179it [01:53,  1.67it/s]Extractor Predicting: 180it [01:53,  1.63it/s]Extractor Predicting: 181it [01:54,  1.63it/s]Extractor Predicting: 182it [01:55,  1.62it/s]Extractor Predicting: 183it [01:55,  1.61it/s]Extractor Predicting: 184it [01:56,  1.60it/s]Extractor Predicting: 185it [01:56,  1.64it/s]Extractor Predicting: 186it [01:57,  1.61it/s]Extractor Predicting: 187it [01:58,  1.60it/s]Extractor Predicting: 188it [01:58,  1.58it/s]Extractor Predicting: 189it [01:59,  1.57it/s]Extractor Predicting: 190it [02:00,  1.54it/s]Extractor Predicting: 191it [02:00,  1.54it/s]Extractor Predicting: 192it [02:01,  1.57it/s]Extractor Predicting: 193it [02:02,  1.59it/s]Extractor Predicting: 194it [02:02,  1.61it/s]Extractor Predicting: 195it [02:03,  1.58it/s]Extractor Predicting: 196it [02:03,  1.60it/s]Extractor Predicting: 197it [02:04,  1.60it/s]Extractor Predicting: 198it [02:05,  1.59it/s]Extractor Predicting: 199it [02:05,  1.59it/s]Extractor Predicting: 200it [02:06,  1.58it/s]Extractor Predicting: 201it [02:07,  1.59it/s]Extractor Predicting: 202it [02:07,  1.59it/s]Extractor Predicting: 203it [02:08,  1.59it/s]Extractor Predicting: 204it [02:08,  1.60it/s]Extractor Predicting: 205it [02:09,  1.59it/s]Extractor Predicting: 206it [02:10,  1.42it/s]Extractor Predicting: 207it [02:11,  1.48it/s]Extractor Predicting: 208it [02:11,  1.49it/s]Extractor Predicting: 209it [02:12,  1.52it/s]Extractor Predicting: 210it [02:13,  1.53it/s]Extractor Predicting: 211it [02:13,  1.54it/s]Extractor Predicting: 212it [02:14,  1.57it/s]Extractor Predicting: 213it [02:14,  1.55it/s]Extractor Predicting: 214it [02:15,  1.57it/s]Extractor Predicting: 215it [02:16,  1.53it/s]Extractor Predicting: 216it [02:16,  1.56it/s]Extractor Predicting: 217it [02:17,  1.59it/s]Extractor Predicting: 218it [02:18,  1.59it/s]Extractor Predicting: 219it [02:18,  1.57it/s]Extractor Predicting: 220it [02:19,  1.59it/s]Extractor Predicting: 221it [02:20,  1.57it/s]Extractor Predicting: 222it [02:20,  1.58it/s]Extractor Predicting: 223it [02:21,  1.52it/s]Extractor Predicting: 224it [02:22,  1.53it/s]Extractor Predicting: 225it [02:22,  1.52it/s]Extractor Predicting: 226it [02:23,  1.52it/s]Extractor Predicting: 227it [02:24,  1.49it/s]Extractor Predicting: 228it [02:24,  1.44it/s]Extractor Predicting: 229it [02:25,  1.46it/s]Extractor Predicting: 230it [02:26,  1.48it/s]Extractor Predicting: 231it [02:26,  1.49it/s]Extractor Predicting: 232it [02:27,  1.50it/s]Extractor Predicting: 233it [02:28,  1.51it/s]Extractor Predicting: 234it [02:28,  1.52it/s]Extractor Predicting: 235it [02:29,  1.53it/s]Extractor Predicting: 236it [02:29,  1.55it/s]Extractor Predicting: 237it [02:30,  1.55it/s]Extractor Predicting: 238it [02:31,  1.60it/s]Extractor Predicting: 239it [02:31,  1.62it/s]Extractor Predicting: 240it [02:32,  1.67it/s]Extractor Predicting: 241it [02:33,  1.64it/s]Extractor Predicting: 242it [02:33,  1.63it/s]Extractor Predicting: 243it [02:34,  1.60it/s]Extractor Predicting: 244it [02:34,  1.61it/s]Extractor Predicting: 245it [02:35,  1.65it/s]Extractor Predicting: 246it [02:36,  1.61it/s]Extractor Predicting: 247it [02:36,  1.62it/s]Extractor Predicting: 248it [02:37,  1.64it/s]Extractor Predicting: 249it [02:37,  1.69it/s]Extractor Predicting: 250it [02:38,  1.70it/s]Extractor Predicting: 251it [02:38,  1.76it/s]Extractor Predicting: 252it [02:39,  1.75it/s]Extractor Predicting: 253it [02:40,  1.72it/s]Extractor Predicting: 254it [02:40,  1.74it/s]Extractor Predicting: 255it [02:41,  1.69it/s]Extractor Predicting: 256it [02:41,  1.74it/s]Extractor Predicting: 257it [02:42,  1.69it/s]Extractor Predicting: 258it [02:43,  1.58it/s]Extractor Predicting: 259it [02:43,  1.55it/s]Extractor Predicting: 260it [02:44,  1.63it/s]Extractor Predicting: 261it [02:45,  1.60it/s]Extractor Predicting: 262it [02:45,  1.62it/s]Extractor Predicting: 263it [02:46,  1.66it/s]Extractor Predicting: 264it [02:46,  1.68it/s]Extractor Predicting: 265it [02:47,  1.68it/s]Extractor Predicting: 266it [02:48,  1.70it/s]Extractor Predicting: 267it [02:48,  1.73it/s]Extractor Predicting: 268it [02:49,  1.72it/s]Extractor Predicting: 269it [02:49,  1.67it/s]Extractor Predicting: 270it [02:50,  1.69it/s]Extractor Predicting: 271it [02:50,  1.68it/s]Extractor Predicting: 272it [02:51,  1.65it/s]Extractor Predicting: 273it [02:52,  1.70it/s]Extractor Predicting: 274it [02:52,  1.71it/s]Extractor Predicting: 275it [02:53,  1.68it/s]Extractor Predicting: 276it [02:53,  1.67it/s]Extractor Predicting: 277it [02:54,  1.66it/s]Extractor Predicting: 278it [02:55,  1.69it/s]Extractor Predicting: 279it [02:55,  1.64it/s]Extractor Predicting: 280it [02:56,  1.70it/s]Extractor Predicting: 281it [02:56,  1.71it/s]Extractor Predicting: 282it [02:57,  1.73it/s]Extractor Predicting: 283it [02:58,  1.74it/s]Extractor Predicting: 284it [02:58,  1.73it/s]Extractor Predicting: 285it [02:59,  1.73it/s]Extractor Predicting: 286it [02:59,  1.73it/s]Extractor Predicting: 287it [03:00,  1.72it/s]Extractor Predicting: 288it [03:00,  1.68it/s]Extractor Predicting: 289it [03:01,  1.68it/s]Extractor Predicting: 290it [03:02,  1.61it/s]Extractor Predicting: 291it [03:02,  1.63it/s]Extractor Predicting: 292it [03:03,  1.64it/s]Extractor Predicting: 293it [03:04,  1.67it/s]Extractor Predicting: 294it [03:04,  1.67it/s]Extractor Predicting: 295it [03:05,  1.69it/s]Extractor Predicting: 296it [03:05,  1.61it/s]Extractor Predicting: 297it [03:06,  1.59it/s]Extractor Predicting: 298it [03:07,  1.54it/s]Extractor Predicting: 299it [03:07,  1.55it/s]Extractor Predicting: 300it [03:08,  1.58it/s]Extractor Predicting: 301it [03:09,  1.59it/s]Extractor Predicting: 302it [03:09,  1.58it/s]Extractor Predicting: 303it [03:10,  1.55it/s]Extractor Predicting: 304it [03:11,  1.56it/s]Extractor Predicting: 305it [03:11,  1.59it/s]Extractor Predicting: 306it [03:12,  1.56it/s]Extractor Predicting: 307it [03:13,  1.34it/s]Extractor Predicting: 308it [03:13,  1.39it/s]Extractor Predicting: 309it [03:14,  1.46it/s]Extractor Predicting: 310it [03:15,  1.51it/s]Extractor Predicting: 311it [03:15,  1.54it/s]Extractor Predicting: 312it [03:16,  1.53it/s]Extractor Predicting: 313it [03:17,  1.51it/s]Extractor Predicting: 314it [03:17,  1.51it/s]Extractor Predicting: 315it [03:18,  1.51it/s]Extractor Predicting: 316it [03:19,  1.51it/s]Extractor Predicting: 317it [03:19,  1.51it/s]Extractor Predicting: 318it [03:20,  1.53it/s]Extractor Predicting: 319it [03:21,  1.54it/s]Extractor Predicting: 320it [03:21,  1.55it/s]Extractor Predicting: 321it [03:22,  1.61it/s]Extractor Predicting: 322it [03:22,  1.61it/s]Extractor Predicting: 323it [03:23,  1.59it/s]Extractor Predicting: 324it [03:24,  1.57it/s]Extractor Predicting: 325it [03:24,  1.57it/s]Extractor Predicting: 326it [03:25,  1.55it/s]Extractor Predicting: 327it [03:26,  1.52it/s]Extractor Predicting: 328it [03:26,  1.54it/s]Extractor Predicting: 329it [03:27,  1.58it/s]Extractor Predicting: 330it [03:28,  1.60it/s]Extractor Predicting: 331it [03:28,  1.79it/s]Extractor Predicting: 331it [03:28,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:59,882 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:59,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:59,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:59,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:59,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:42:00,550 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:42:00,551 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:42:01,140 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:42:02,187 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:42:02,187 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:05,473 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:05,488 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:05,488 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:05,488 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:42:05,488 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:42:07,214 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:42:07,215 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:42:07,781 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:42:07,960 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:42:07,960 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.578656853725851,
  "recall": 0.07930904047408902,
  "score": 0.13949878021734308,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.50it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.45it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:23,  1.56it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.56it/s]Extractor Predicting: 49it [00:32,  1.58it/s]Extractor Predicting: 50it [00:32,  1.60it/s]Extractor Predicting: 51it [00:33,  1.59it/s]Extractor Predicting: 52it [00:34,  1.59it/s]Extractor Predicting: 53it [00:34,  1.50it/s]Extractor Predicting: 54it [00:35,  1.54it/s]Extractor Predicting: 55it [00:36,  1.56it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:37,  1.58it/s]Extractor Predicting: 58it [00:37,  1.65it/s]Extractor Predicting: 59it [00:38,  1.74it/s]Extractor Predicting: 60it [00:38,  1.83it/s]Extractor Predicting: 61it [00:39,  1.88it/s]Extractor Predicting: 62it [00:39,  1.88it/s]Extractor Predicting: 63it [00:40,  1.91it/s]Extractor Predicting: 64it [00:40,  1.89it/s]Extractor Predicting: 65it [00:41,  1.88it/s]Extractor Predicting: 66it [00:41,  1.87it/s]Extractor Predicting: 67it [00:42,  1.87it/s]Extractor Predicting: 68it [00:43,  1.89it/s]Extractor Predicting: 69it [00:43,  1.92it/s]Extractor Predicting: 70it [00:44,  1.89it/s]Extractor Predicting: 71it [00:44,  1.88it/s]Extractor Predicting: 72it [00:45,  1.91it/s]Extractor Predicting: 73it [00:45,  1.94it/s]Extractor Predicting: 74it [00:46,  1.96it/s]Extractor Predicting: 75it [00:46,  1.95it/s]Extractor Predicting: 76it [00:47,  1.93it/s]Extractor Predicting: 77it [00:47,  1.99it/s]Extractor Predicting: 78it [00:48,  1.92it/s]Extractor Predicting: 79it [00:48,  1.91it/s]Extractor Predicting: 80it [00:49,  1.89it/s]Extractor Predicting: 81it [00:49,  1.89it/s]Extractor Predicting: 82it [00:50,  1.92it/s]Extractor Predicting: 83it [00:50,  1.93it/s]Extractor Predicting: 84it [00:51,  1.93it/s]Extractor Predicting: 85it [00:51,  1.93it/s]Extractor Predicting: 86it [00:52,  1.80it/s]Extractor Predicting: 87it [00:53,  1.73it/s]Extractor Predicting: 88it [00:53,  1.69it/s]Extractor Predicting: 89it [00:54,  1.67it/s]Extractor Predicting: 90it [00:54,  1.67it/s]Extractor Predicting: 91it [00:55,  1.64it/s]Extractor Predicting: 92it [00:56,  1.63it/s]Extractor Predicting: 93it [00:56,  1.64it/s]Extractor Predicting: 94it [00:57,  1.62it/s]Extractor Predicting: 95it [00:58,  1.64it/s]Extractor Predicting: 96it [00:58,  1.64it/s]Extractor Predicting: 97it [00:59,  1.65it/s]Extractor Predicting: 98it [00:59,  1.65it/s]Extractor Predicting: 99it [01:00,  1.63it/s]Extractor Predicting: 100it [01:01,  1.57it/s]Extractor Predicting: 101it [01:01,  1.60it/s]Extractor Predicting: 102it [01:02,  1.60it/s]Extractor Predicting: 103it [01:03,  1.56it/s]Extractor Predicting: 104it [01:03,  1.55it/s]Extractor Predicting: 105it [01:04,  1.54it/s]Extractor Predicting: 106it [01:05,  1.53it/s]Extractor Predicting: 107it [01:05,  1.52it/s]Extractor Predicting: 108it [01:06,  1.48it/s]Extractor Predicting: 108it [01:06,  1.63it/s]
[INFO|configuration_utils.py:515] 2023-08-28 17:43:17,046 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:43:17,049 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:43:17,053 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:43:17,053 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 17:43:17,056 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:43:22,708 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 17:43:22,708 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 17:43:22,738 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:43:22,739 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:43:22,748 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:43:22,754 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:43:22,754 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:43:22,754 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:43:22,754 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:43:22,754 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:43:22,754 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.9272727272727272,
  "recall": 0.0735223450264296,
  "score": 0.13624220837043632,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 17:43:23,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:23,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:24,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:24,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:25,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:26,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:26,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:27,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:27,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:28,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:29,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:29,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:30,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:30,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:31,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:32,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:32,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:33,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:33,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:34,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:34,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<02:55, 12.55s/it][WARNING|generation_utils.py:914] 2023-08-28 17:43:35,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:36,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:36,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:37,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:38,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:39,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:39,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:40,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:40,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:41,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:41,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:42,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:43,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:43,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:44,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:45,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:45,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:46,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:47,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:47,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:48,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:48,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:49,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<02:58, 13.70s/it][WARNING|generation_utils.py:914] 2023-08-28 17:43:50,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:50,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:51,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:51,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:52,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:53,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:54,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:54,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:55,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:55,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:56,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:57,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:57,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:58,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:58,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:43:59,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:00,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:00,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:01,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:02,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:02,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:03,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:40<02:45, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-28 17:44:03,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:04,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:05,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:05,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:06,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:07,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:07,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:08,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:08,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:09,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:09,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:10,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:11,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:12,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:12,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:13,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:13,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:14,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:15,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:15,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:16,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:16,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:54<02:30, 13.72s/it][WARNING|generation_utils.py:914] 2023-08-28 17:44:17,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:18,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:18,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:19,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:20,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:20,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:21,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:22,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:23,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:23,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:24,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:25,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:25,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:26,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:26,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:27,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:28,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:28,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:29,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:29,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:30,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:31,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:08<02:18, 13.88s/it][WARNING|generation_utils.py:914] 2023-08-28 17:44:31,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:32,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:32,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:33,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:34,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:34,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:35,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:36,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:36,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:37,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:37,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:38,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:39,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:39,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:40,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:40,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:41,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:42,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:42,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:43,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:43,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:21<02:01, 13.52s/it][WARNING|generation_utils.py:914] 2023-08-28 17:44:44,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:45,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:45,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:46,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:47,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:47,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:48,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:48,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:49,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:50,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:50,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:51,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:52,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:52,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:53,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:54,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:54,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:55,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:55,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:56,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:56,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:57,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:58,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:35<01:50, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-28 17:44:58,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:44:59,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:00,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:00,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:01,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:01,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:02,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:02,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:03,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:04,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:04,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:05,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:05,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:06,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:06,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:07,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:08,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:08,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:09,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:09,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:10,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:10,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:11,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:11,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:49<01:36, 13.73s/it][WARNING|generation_utils.py:914] 2023-08-28 17:45:12,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:13,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:13,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:14,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:14,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:15,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:15,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:16,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:16,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:17,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:17,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:18,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:19,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:19,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:20,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:20,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:21,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:22,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:22,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:23,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:23,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:24,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:02<01:20, 13.35s/it][WARNING|generation_utils.py:914] 2023-08-28 17:45:25,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:25,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:26,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:26,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:27,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:27,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:28,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:28,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:29,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:29,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:30,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:31,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:31,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:32,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:32,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:33,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:33,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:34,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:34,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:34,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:35,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:35,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:36,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:14<01:04, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-28 17:45:37,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:37,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:38,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:38,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:39,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:39,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:40,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:40,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:41,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:42,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:43,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:43,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:44,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:44,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:45,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:45,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:46,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:46,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:47,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:48,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:48,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:49,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:49,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:27<00:52, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-28 17:45:50,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:51,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:51,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:52,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:52,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:53,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:54,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:54,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:55,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:55,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:56,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:57,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:58,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:58,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:45:59,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:00,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:00,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:01,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:02,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:02,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:03,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:04,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:04,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:05,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:05,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:43<00:42, 14.06s/it][WARNING|generation_utils.py:914] 2023-08-28 17:46:06,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:07,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:08,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:08,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:09,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:10,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:10,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:11,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:11,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:12,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:13,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:13,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:14,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:15,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:15,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:16,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:17,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:17,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:18,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:18,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:19,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:20,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:20,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:21,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:59<00:28, 14.48s/it][WARNING|generation_utils.py:914] 2023-08-28 17:46:22,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:22,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:23,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:23,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:24,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:25,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:25,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:26,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:26,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:27,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:27,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:28,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:28,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:29,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:30,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:30,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:31,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:31,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:32,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:32,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:33,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:34,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:11<00:13, 13.85s/it][WARNING|generation_utils.py:914] 2023-08-28 17:46:34,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:35,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:35,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:36,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:37,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:37,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:38,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:39,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:39,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:40,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:40,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:41,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:42,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:43,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:43,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:44,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:44,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:45,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:46,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:46,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:47,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:47,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:48,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:49,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:26<00:00, 14.27s/it]Generating: 100%|██████████| 15/15 [03:26<00:00, 13.79s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:57,160 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:57,165 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:57,165 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:57,165 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:57,165 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:46:57,767 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:46:57,768 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:46:58,381 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:46:59,492 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:46:59,492 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:47:02,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:47:02,437 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:47:02,437 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:47:02,437 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:47:02,437 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:47:03,091 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:47:03,092 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:47:03,684 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:47:03,870 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:47:03,870 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : made from material .', 'success_rate': 0.890625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8536931818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : cast member . Context : Later in the film , the character of the villainous character Charles Xavier as the villainous villainous Professor X becomes popular among a wide range of audiences , including Marvel Comics creators , writers , and artists . Head Entity : Charles Xavier , Tail Entity : John Cassaday .\n']
['Relation : cast member . Context : Later in the film , the character of the villainous character Charles Xavier as the villainous villainous Professor X becomes popular among a wide range of audiences , including Marvel Comics creators , writers , and artists . Head Entity : Charles Xavier , Tail Entity : John Cassaday .\n', 'Relation : cast member . Context : After he was cast as Henry IV of England , he played Henry VII of England , a widower who wanted to give Henry an heir . Head Entity : Henry VII of England , Tail Entity : Henry VIII of England .\n']
['Relation : cast member . Context : Later in the film , the character of the villainous character Charles Xavier as the villainous villainous Professor X becomes popular among a wide range of audiences , including Marvel Comics creators , writers , and artists . Head Entity : Charles Xavier , Tail Entity : John Cassaday .\n', 'Relation : cast member . Context : After he was cast as Henry IV of England , he played Henry VII of England , a widower who wanted to give Henry an heir . Head Entity : Henry VII of England , Tail Entity : Henry VIII of England .\n', 'Relation : cast member . Context : Her film credits include The Night Before , Night Before , Night After , Night of the Living Dead , and Night on the Town . Head Entity : Night before Night of the Living Dead , Tail Entity : Night on the Town .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9002976190476191, 'errors': {''}}
['Relation : follows . Context : Later in the year ( 1151 ) , he married Alixandra , whom at the end of the year was married to a Roman emperor named Horace , daughter of Alexander , who ruled from 1153 until 1241 . Head Entity : Horace , Tail Entity : Alexander , son of Alexander .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 551, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 540, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : league .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : located in or next to body of water . Context : The city of Marietta is a major tourist destination for both winter and summer days . Head Entity : Marietta , Tail Entity : May .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8821022727272727, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8532608695652174, 'errors': {''}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major . Head Entity : John Major , Tail Entity : the Prime Minister .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8410326086956522, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 321, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 549, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 599, 'raw': 768}
{'target': 600, 'success': 625, 'raw': 800}
{'prompt': 'Relation : mother .', 'success_rate': 0.78125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : residence . Context : Later in life he studied at the Conservatory of Fine Arts at Loyola at the end of the 1780s , where he became the first Professor of Fine Arts at Bologna . Head Entity : Charles I , Tail Entity : Loyola .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 536, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8020833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in 1478 he was appointed under the order of the Emperor of Spain to serve as a delegate from the Kingdom of Portugal . Head Entity : King of Portugal , Tail Entity : Kingdom of Portugal .\n']
['Relation : twinned administrative body . Context : Later in 1478 he was appointed under the order of the Emperor of Spain to serve as a delegate from the Kingdom of Portugal . Head Entity : King of Portugal , Tail Entity : Kingdom of Portugal .\n', 'Relation : twinned administrative body . Context : After the death of King Henry IV of England ( 9 January 1779 &ndash; 6 December 1777 ) , the family moved from the town of Stirling ( now Wigan ) where they were incorporated into the Wigan Council . Head Entity : Wigan Council , Tail Entity : Surrey .\n']
['Relation : twinned administrative body . Context : Later in 1478 he was appointed under the order of the Emperor of Spain to serve as a delegate from the Kingdom of Portugal . Head Entity : King of Portugal , Tail Entity : Kingdom of Portugal .\n', 'Relation : twinned administrative body . Context : After the death of King Henry IV of England ( 9 January 1779 &ndash; 6 December 1777 ) , the family moved from the town of Stirling ( now Wigan ) where they were incorporated into the Wigan Council . Head Entity : Wigan Council , Tail Entity : Surrey .\n', 'Relation : twinned administrative body . Context : This was the first to establish one of the most basic principles of Catholic teaching of ethics , namely that morality is a rational relation of right to wrong . Head Entity : ethics , Tail Entity : cardinal .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 457, 'raw': 576}
{'target': 600, 'success': 485, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 562, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8046875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 11596
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11696, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.62it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:01,  1.59it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.55it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.69it/s]Extractor Estimating: 13it [00:08,  1.68it/s]Extractor Estimating: 14it [00:08,  1.62it/s]Extractor Estimating: 15it [00:09,  1.63it/s]Extractor Estimating: 16it [00:09,  1.64it/s]Extractor Estimating: 17it [00:10,  1.65it/s]Extractor Estimating: 18it [00:11,  1.66it/s]Extractor Estimating: 19it [00:11,  1.71it/s]Extractor Estimating: 20it [00:12,  1.70it/s]Extractor Estimating: 21it [00:12,  1.66it/s]Extractor Estimating: 22it [00:13,  1.64it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:14,  1.58it/s]Extractor Estimating: 25it [00:15,  1.55it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:16,  1.60it/s]Extractor Estimating: 28it [00:17,  1.52it/s]Extractor Estimating: 29it [00:18,  1.55it/s]Extractor Estimating: 30it [00:18,  1.36it/s]Extractor Estimating: 31it [00:19,  1.46it/s]Extractor Estimating: 32it [00:20,  1.51it/s]Extractor Estimating: 33it [00:20,  1.55it/s]Extractor Estimating: 34it [00:21,  1.49it/s]Extractor Estimating: 35it [00:22,  1.55it/s]Extractor Estimating: 36it [00:22,  1.58it/s]Extractor Estimating: 37it [00:23,  1.62it/s]Extractor Estimating: 38it [00:23,  1.64it/s]Extractor Estimating: 39it [00:24,  1.61it/s]Extractor Estimating: 40it [00:25,  1.63it/s]Extractor Estimating: 41it [00:25,  1.64it/s]Extractor Estimating: 42it [00:26,  1.65it/s]Extractor Estimating: 43it [00:26,  1.66it/s]Extractor Estimating: 44it [00:27,  1.66it/s]Extractor Estimating: 45it [00:28,  1.65it/s]Extractor Estimating: 46it [00:28,  1.66it/s]Extractor Estimating: 47it [00:29,  1.61it/s]Extractor Estimating: 48it [00:29,  1.61it/s]Extractor Estimating: 49it [00:30,  1.65it/s]Extractor Estimating: 50it [00:31,  1.68it/s]Extractor Estimating: 51it [00:31,  1.75it/s]Extractor Estimating: 52it [00:32,  1.76it/s]Extractor Estimating: 53it [00:32,  1.76it/s]Extractor Estimating: 54it [00:33,  1.73it/s]Extractor Estimating: 55it [00:33,  1.72it/s]Extractor Estimating: 56it [00:34,  1.71it/s]Extractor Estimating: 57it [00:35,  1.76it/s]Extractor Estimating: 58it [00:35,  1.75it/s]Extractor Estimating: 59it [00:36,  1.74it/s]Extractor Estimating: 60it [00:36,  1.73it/s]Extractor Estimating: 61it [00:37,  1.76it/s]Extractor Estimating: 62it [00:37,  1.77it/s]Extractor Estimating: 63it [00:38,  1.72it/s]Extractor Estimating: 64it [00:39,  1.69it/s]Extractor Estimating: 65it [00:39,  1.69it/s]Extractor Estimating: 66it [00:40,  1.68it/s]Extractor Estimating: 67it [00:40,  1.68it/s]Extractor Estimating: 68it [00:41,  1.70it/s]Extractor Estimating: 69it [00:42,  1.72it/s]Extractor Estimating: 70it [00:42,  1.68it/s]Extractor Estimating: 71it [00:43,  1.61it/s]Extractor Estimating: 72it [00:43,  1.71it/s]Extractor Estimating: 73it [00:44,  1.75it/s]Extractor Estimating: 74it [00:45,  1.73it/s]Extractor Estimating: 75it [00:45,  1.71it/s]Extractor Estimating: 76it [00:46,  1.66it/s]Extractor Estimating: 77it [00:46,  1.64it/s]Extractor Estimating: 78it [00:47,  1.64it/s]Extractor Estimating: 79it [00:48,  1.66it/s]Extractor Estimating: 80it [00:48,  1.65it/s]Extractor Estimating: 81it [00:49,  1.65it/s]Extractor Estimating: 82it [00:49,  1.65it/s]Extractor Estimating: 83it [00:50,  1.70it/s]Extractor Estimating: 84it [00:51,  1.71it/s]Extractor Estimating: 85it [00:51,  1.70it/s]Extractor Estimating: 86it [00:52,  1.73it/s]Extractor Estimating: 87it [00:52,  1.77it/s]Extractor Estimating: 88it [00:53,  1.69it/s]Extractor Estimating: 89it [00:54,  1.65it/s]Extractor Estimating: 90it [00:54,  1.65it/s]Extractor Estimating: 91it [00:55,  1.61it/s]Extractor Estimating: 92it [00:55,  1.62it/s]Extractor Estimating: 93it [00:56,  1.61it/s]Extractor Estimating: 94it [00:57,  1.59it/s]Extractor Estimating: 95it [00:57,  1.62it/s]Extractor Estimating: 96it [00:58,  1.62it/s]Extractor Estimating: 97it [00:59,  1.60it/s]Extractor Estimating: 98it [00:59,  1.59it/s]Extractor Estimating: 99it [01:00,  1.60it/s]Extractor Estimating: 100it [01:00,  1.58it/s]Extractor Estimating: 101it [01:01,  1.55it/s]Extractor Estimating: 102it [01:02,  1.65it/s]Extractor Estimating: 103it [01:02,  1.64it/s]Extractor Estimating: 104it [01:03,  1.61it/s]Extractor Estimating: 105it [01:04,  1.59it/s]Extractor Estimating: 106it [01:04,  1.62it/s]Extractor Estimating: 107it [01:05,  1.60it/s]Extractor Estimating: 108it [01:05,  1.58it/s]Extractor Estimating: 109it [01:06,  1.62it/s]Extractor Estimating: 110it [01:07,  1.62it/s]Extractor Estimating: 111it [01:07,  1.64it/s]Extractor Estimating: 112it [01:08,  1.65it/s]Extractor Estimating: 113it [01:08,  1.61it/s]Extractor Estimating: 114it [01:09,  1.65it/s]Extractor Estimating: 115it [01:10,  1.62it/s]Extractor Estimating: 116it [01:10,  1.67it/s]Extractor Estimating: 117it [01:11,  1.57it/s]Extractor Estimating: 118it [01:12,  1.52it/s]Extractor Estimating: 119it [01:12,  1.54it/s]Extractor Estimating: 120it [01:13,  1.58it/s]Extractor Estimating: 121it [01:14,  1.54it/s]Extractor Estimating: 122it [01:14,  1.55it/s]Extractor Estimating: 123it [01:15,  1.58it/s]Extractor Estimating: 124it [01:16,  1.45it/s]Extractor Estimating: 125it [01:16,  1.49it/s]Extractor Estimating: 126it [01:17,  1.51it/s]Extractor Estimating: 127it [01:18,  1.54it/s]Extractor Estimating: 128it [01:18,  1.49it/s]Extractor Estimating: 129it [01:19,  1.53it/s]Extractor Estimating: 130it [01:19,  1.59it/s]Extractor Estimating: 131it [01:20,  1.58it/s]Extractor Estimating: 132it [01:21,  1.60it/s]Extractor Estimating: 133it [01:21,  1.56it/s]Extractor Estimating: 134it [01:22,  1.58it/s]Extractor Estimating: 135it [01:23,  1.59it/s]Extractor Estimating: 136it [01:23,  1.61it/s]Extractor Estimating: 137it [01:24,  1.67it/s]Extractor Estimating: 138it [01:24,  1.57it/s]Extractor Estimating: 139it [01:25,  1.57it/s]Extractor Estimating: 140it [01:26,  1.65it/s]Extractor Estimating: 141it [01:26,  1.63it/s]Extractor Estimating: 142it [01:27,  1.59it/s]Extractor Estimating: 143it [01:28,  1.58it/s]Extractor Estimating: 144it [01:28,  1.64it/s]Extractor Estimating: 145it [01:29,  1.66it/s]Extractor Estimating: 146it [01:29,  1.70it/s]Extractor Estimating: 147it [01:30,  1.60it/s]Extractor Estimating: 148it [01:31,  1.56it/s]Extractor Estimating: 149it [01:31,  1.57it/s]Extractor Estimating: 150it [01:32,  1.57it/s]Extractor Estimating: 151it [01:33,  1.58it/s]Extractor Estimating: 152it [01:33,  1.57it/s]Extractor Estimating: 153it [01:34,  1.59it/s]Extractor Estimating: 154it [01:34,  1.62it/s]Extractor Estimating: 155it [01:35,  1.62it/s]Extractor Estimating: 156it [01:36,  1.61it/s]Extractor Estimating: 157it [01:36,  1.57it/s]Extractor Estimating: 158it [01:37,  1.57it/s]Extractor Estimating: 159it [01:38,  1.59it/s]Extractor Estimating: 160it [01:38,  1.61it/s]Extractor Estimating: 161it [01:39,  1.62it/s]Extractor Estimating: 162it [01:39,  1.61it/s]Extractor Estimating: 163it [01:40,  1.60it/s]Extractor Estimating: 164it [01:41,  1.59it/s]Extractor Estimating: 165it [01:41,  1.56it/s]Extractor Estimating: 166it [01:42,  1.63it/s]Extractor Estimating: 167it [01:42,  1.68it/s]Extractor Estimating: 168it [01:43,  1.70it/s]Extractor Estimating: 169it [01:44,  1.68it/s]Extractor Estimating: 170it [01:44,  1.65it/s]Extractor Estimating: 171it [01:45,  1.64it/s]Extractor Estimating: 172it [01:45,  1.65it/s]Extractor Estimating: 173it [01:46,  1.61it/s]Extractor Estimating: 174it [01:47,  1.54it/s]Extractor Estimating: 175it [01:47,  1.54it/s]Extractor Estimating: 176it [01:48,  1.54it/s]Extractor Estimating: 177it [01:49,  1.55it/s]Extractor Estimating: 178it [01:49,  1.56it/s]Extractor Estimating: 179it [01:50,  1.61it/s]Extractor Estimating: 180it [01:51,  1.65it/s]Extractor Estimating: 181it [01:51,  1.65it/s]Extractor Estimating: 182it [01:52,  1.65it/s]Extractor Estimating: 183it [01:52,  1.67it/s]Extractor Estimating: 184it [01:53,  1.64it/s]Extractor Estimating: 185it [01:54,  1.68it/s]Extractor Estimating: 186it [01:54,  1.66it/s]Extractor Estimating: 187it [01:55,  1.67it/s]Extractor Estimating: 188it [01:55,  1.67it/s]Extractor Estimating: 189it [01:56,  1.67it/s]Extractor Estimating: 190it [01:57,  1.63it/s]Extractor Estimating: 191it [01:57,  1.51it/s]Extractor Estimating: 192it [01:58,  1.49it/s]Extractor Estimating: 193it [01:59,  1.54it/s]Extractor Estimating: 194it [01:59,  1.55it/s]Extractor Estimating: 195it [02:00,  1.60it/s]Extractor Estimating: 196it [02:01,  1.59it/s]Extractor Estimating: 197it [02:01,  1.53it/s]Extractor Estimating: 198it [02:02,  1.57it/s]Extractor Estimating: 199it [02:02,  1.62it/s]Extractor Estimating: 200it [02:03,  1.64it/s]Extractor Estimating: 201it [02:04,  1.69it/s]Extractor Estimating: 202it [02:04,  1.67it/s]Extractor Estimating: 203it [02:05,  1.73it/s]Extractor Estimating: 204it [02:05,  1.78it/s]Extractor Estimating: 205it [02:06,  1.81it/s]Extractor Estimating: 206it [02:06,  1.84it/s]Extractor Estimating: 207it [02:07,  1.85it/s]Extractor Estimating: 208it [02:07,  1.87it/s]Extractor Estimating: 209it [02:08,  1.86it/s]Extractor Estimating: 210it [02:08,  1.91it/s]Extractor Estimating: 211it [02:09,  1.96it/s]Extractor Estimating: 212it [02:09,  1.93it/s]Extractor Estimating: 213it [02:10,  1.90it/s]Extractor Estimating: 214it [02:10,  1.87it/s]Extractor Estimating: 215it [02:11,  1.84it/s]Extractor Estimating: 216it [02:12,  1.84it/s]Extractor Estimating: 217it [02:12,  1.77it/s]Extractor Estimating: 218it [02:13,  1.82it/s]Extractor Estimating: 219it [02:13,  1.85it/s]Extractor Estimating: 220it [02:14,  1.84it/s]Extractor Estimating: 221it [02:14,  1.86it/s]Extractor Estimating: 222it [02:15,  1.90it/s]Extractor Estimating: 223it [02:15,  1.89it/s]Extractor Estimating: 224it [02:16,  1.83it/s]Extractor Estimating: 225it [02:16,  1.83it/s]Extractor Estimating: 226it [02:17,  1.86it/s]Extractor Estimating: 227it [02:18,  1.84it/s]Extractor Estimating: 228it [02:18,  1.87it/s]Extractor Estimating: 229it [02:19,  1.83it/s]Extractor Estimating: 230it [02:19,  1.75it/s]Extractor Estimating: 231it [02:20,  1.83it/s]Extractor Estimating: 232it [02:20,  1.88it/s]Extractor Estimating: 233it [02:21,  1.93it/s]Extractor Estimating: 234it [02:21,  1.95it/s]Extractor Estimating: 235it [02:22,  1.92it/s]Extractor Estimating: 236it [02:22,  1.98it/s]Extractor Estimating: 237it [02:23,  1.93it/s]Extractor Estimating: 238it [02:23,  1.89it/s]Extractor Estimating: 239it [02:24,  1.91it/s]Extractor Estimating: 240it [02:24,  1.90it/s]Extractor Estimating: 241it [02:25,  1.93it/s]Extractor Estimating: 242it [02:25,  1.96it/s]Extractor Estimating: 243it [02:26,  2.08it/s]Extractor Estimating: 244it [02:26,  2.05it/s]Extractor Estimating: 245it [02:27,  1.98it/s]Extractor Estimating: 246it [02:27,  2.01it/s]Extractor Estimating: 247it [02:28,  2.00it/s]Extractor Estimating: 248it [02:28,  1.94it/s]Extractor Estimating: 249it [02:29,  1.94it/s]Extractor Estimating: 250it [02:29,  1.91it/s]Extractor Estimating: 251it [02:30,  1.83it/s]Extractor Estimating: 252it [02:31,  1.79it/s]Extractor Estimating: 253it [02:31,  1.71it/s]Extractor Estimating: 254it [02:32,  1.70it/s]Extractor Estimating: 255it [02:32,  1.69it/s]Extractor Estimating: 256it [02:33,  1.70it/s]Extractor Estimating: 257it [02:34,  1.65it/s]Extractor Estimating: 258it [02:34,  1.64it/s]Extractor Estimating: 259it [02:35,  1.58it/s]Extractor Estimating: 260it [02:36,  1.56it/s]Extractor Estimating: 261it [02:36,  1.59it/s]Extractor Estimating: 262it [02:37,  1.59it/s]Extractor Estimating: 263it [02:37,  1.64it/s]Extractor Estimating: 264it [02:38,  1.62it/s]Extractor Estimating: 265it [02:39,  1.65it/s]Extractor Estimating: 266it [02:39,  1.64it/s]Extractor Estimating: 267it [02:40,  1.66it/s]Extractor Estimating: 268it [02:40,  1.67it/s]Extractor Estimating: 269it [02:41,  1.71it/s]Extractor Estimating: 270it [02:42,  1.72it/s]Extractor Estimating: 271it [02:42,  1.68it/s]Extractor Estimating: 272it [02:43,  1.70it/s]Extractor Estimating: 273it [02:43,  1.67it/s]Extractor Estimating: 274it [02:44,  1.71it/s]Extractor Estimating: 275it [02:45,  1.71it/s]Extractor Estimating: 276it [02:45,  1.72it/s]Extractor Estimating: 277it [02:46,  1.73it/s]Extractor Estimating: 278it [02:46,  1.70it/s]Extractor Estimating: 279it [02:47,  1.68it/s]Extractor Estimating: 280it [02:48,  1.67it/s]Extractor Estimating: 281it [02:48,  1.63it/s]Extractor Estimating: 282it [02:49,  1.65it/s]Extractor Estimating: 283it [02:49,  1.68it/s]Extractor Estimating: 284it [02:50,  1.66it/s]Extractor Estimating: 285it [02:51,  1.65it/s]Extractor Estimating: 286it [02:51,  1.61it/s]Extractor Estimating: 287it [02:52,  1.58it/s]Extractor Estimating: 288it [02:52,  1.62it/s]Extractor Estimating: 289it [02:53,  1.46it/s]Extractor Estimating: 290it [02:54,  1.50it/s]Extractor Estimating: 291it [02:55,  1.49it/s]Extractor Estimating: 292it [02:55,  1.53it/s]Extractor Estimating: 293it [02:56,  1.52it/s]Extractor Estimating: 294it [02:57,  1.53it/s]Extractor Estimating: 295it [02:57,  1.58it/s]Extractor Estimating: 296it [02:58,  1.58it/s]Extractor Estimating: 297it [02:58,  1.57it/s]Extractor Estimating: 298it [02:59,  1.57it/s]Extractor Estimating: 299it [03:00,  1.58it/s]Extractor Estimating: 300it [03:00,  1.62it/s]Extractor Estimating: 301it [03:01,  1.62it/s]Extractor Estimating: 302it [03:01,  1.63it/s]Extractor Estimating: 303it [03:02,  1.61it/s]Extractor Estimating: 304it [03:03,  1.61it/s]Extractor Estimating: 305it [03:03,  1.57it/s]Extractor Estimating: 306it [03:04,  1.56it/s]Extractor Estimating: 307it [03:05,  1.60it/s]Extractor Estimating: 308it [03:05,  1.66it/s]Extractor Estimating: 309it [03:06,  1.64it/s]Extractor Estimating: 310it [03:06,  1.59it/s]Extractor Estimating: 311it [03:07,  1.59it/s]Extractor Estimating: 312it [03:08,  1.65it/s]Extractor Estimating: 313it [03:08,  1.64it/s]Extractor Estimating: 314it [03:09,  1.56it/s]Extractor Estimating: 315it [03:10,  1.56it/s]Extractor Estimating: 316it [03:10,  1.58it/s]Extractor Estimating: 317it [03:11,  1.58it/s]Extractor Estimating: 318it [03:11,  1.60it/s]Extractor Estimating: 319it [03:12,  1.60it/s]Extractor Estimating: 320it [03:13,  1.59it/s]Extractor Estimating: 321it [03:13,  1.63it/s]Extractor Estimating: 322it [03:14,  1.57it/s]Extractor Estimating: 323it [03:15,  1.57it/s]Extractor Estimating: 324it [03:15,  1.55it/s]Extractor Estimating: 325it [03:16,  1.64it/s]Extractor Estimating: 326it [03:16,  1.74it/s]Extractor Estimating: 327it [03:17,  1.75it/s]Extractor Estimating: 328it [03:17,  1.79it/s]Extractor Estimating: 329it [03:18,  1.88it/s]Extractor Estimating: 330it [03:18,  1.86it/s]Extractor Estimating: 331it [03:19,  1.86it/s]Extractor Estimating: 332it [03:19,  1.94it/s]Extractor Estimating: 333it [03:20,  1.97it/s]Extractor Estimating: 334it [03:20,  1.95it/s]Extractor Estimating: 335it [03:21,  1.95it/s]Extractor Estimating: 336it [03:22,  1.80it/s]Extractor Estimating: 337it [03:22,  1.80it/s]Extractor Estimating: 338it [03:23,  1.86it/s]Extractor Estimating: 339it [03:23,  1.84it/s]Extractor Estimating: 340it [03:24,  1.84it/s]Extractor Estimating: 341it [03:24,  1.79it/s]Extractor Estimating: 342it [03:25,  1.80it/s]Extractor Estimating: 343it [03:25,  1.86it/s]Extractor Estimating: 344it [03:26,  1.86it/s]Extractor Estimating: 345it [03:26,  1.94it/s]Extractor Estimating: 346it [03:27,  1.97it/s]Extractor Estimating: 347it [03:27,  1.98it/s]Extractor Estimating: 348it [03:28,  1.97it/s]Extractor Estimating: 349it [03:29,  1.88it/s]Extractor Estimating: 350it [03:29,  1.81it/s]Extractor Estimating: 351it [03:30,  1.69it/s]Extractor Estimating: 352it [03:30,  1.70it/s]Extractor Estimating: 353it [03:31,  1.71it/s]Extractor Estimating: 354it [03:32,  1.72it/s]Extractor Estimating: 355it [03:32,  1.68it/s]Extractor Estimating: 356it [03:33,  1.67it/s]Extractor Estimating: 357it [03:33,  1.69it/s]Extractor Estimating: 358it [03:34,  1.67it/s]Extractor Estimating: 359it [03:35,  1.72it/s]Extractor Estimating: 360it [03:35,  1.68it/s]Extractor Estimating: 361it [03:36,  1.67it/s]Extractor Estimating: 362it [03:36,  1.62it/s]Extractor Estimating: 363it [03:37,  1.60it/s]Extractor Estimating: 364it [03:38,  1.64it/s]Extractor Estimating: 365it [03:38,  1.70it/s]Extractor Estimating: 366it [03:39,  1.72it/s]Extractor Estimating: 367it [03:39,  1.73it/s]Extractor Estimating: 368it [03:40,  1.65it/s]Extractor Estimating: 369it [03:41,  1.65it/s]Extractor Estimating: 370it [03:41,  1.66it/s]Extractor Estimating: 371it [03:42,  1.64it/s]Extractor Estimating: 372it [03:43,  1.51it/s]Extractor Estimating: 373it [03:43,  1.55it/s]Extractor Estimating: 374it [03:44,  1.59it/s]Extractor Estimating: 375it [03:44,  1.68it/s]Extractor Estimating: 375it [03:44,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:08,923 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:08,929 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:08,929 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:08,929 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:08,929 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:51:09,673 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:51:09,674 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:51:10,284 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:51:11,373 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:51:11,374 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:14,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:14,642 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:14,642 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:14,642 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:51:14,642 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:51:15,340 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:51:15,341 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:51:15,901 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:51:16,074 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:51:16,075 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 20:02:31,939 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 20:02:31,943 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7495 mean pseudo reward: 0.9208026113878975
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 23015
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23115, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23115, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.978, loss:875.1301
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.986, loss:847.6897
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.010, loss:875.6532
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.990, loss:810.9800
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.994, loss:867.5942
>> valid entity prec:0.5359, rec:0.4361, f1:0.4808
>> valid relation prec:0.0244, rec:0.0002, f1:0.0005
>> valid relation with NER prec:0.0244, rec:0.0002, f1:0.0005
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.394, loss:848.3534
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.980, loss:832.8519
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.997, loss:838.0948
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.988, loss:846.4143
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.976, loss:866.8232
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5336, rec:0.3507, f1:0.4233
>> valid relation prec:0.0805, rec:0.0016, f1:0.0032
>> valid relation with NER prec:0.0805, rec:0.0016, f1:0.0032
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.381, loss:847.4046
g_step 1200, step 261, avg_time 0.990, loss:815.0955
g_step 1300, step 48, avg_time 0.989, loss:807.5887
g_step 1400, step 148, avg_time 0.994, loss:802.6635
g_step 1500, step 248, avg_time 0.994, loss:790.4836
>> valid entity prec:0.4568, rec:0.4058, f1:0.4298
>> valid relation prec:0.0291, rec:0.0014, f1:0.0026
>> valid relation with NER prec:0.0291, rec:0.0014, f1:0.0026
g_step 1600, step 35, avg_time 2.385, loss:813.6116
g_step 1700, step 135, avg_time 0.984, loss:775.6762
g_step 1800, step 235, avg_time 0.988, loss:775.4714
g_step 1900, step 22, avg_time 1.000, loss:767.4378
g_step 2000, step 122, avg_time 0.991, loss:769.0500
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4738, rec:0.3898, f1:0.4277
>> valid relation prec:0.0124, rec:0.0009, f1:0.0017
>> valid relation with NER prec:0.0124, rec:0.0009, f1:0.0017
g_step 2100, step 222, avg_time 2.382, loss:751.5542
g_step 2200, step 9, avg_time 0.976, loss:733.1826
g_step 2300, step 109, avg_time 0.979, loss:716.6371
g_step 2400, step 209, avg_time 0.993, loss:735.4859
g_step 2500, step 309, avg_time 0.995, loss:732.7364
>> valid entity prec:0.4848, rec:0.4130, f1:0.4460
>> valid relation prec:0.0655, rec:0.0042, f1:0.0078
>> valid relation with NER prec:0.0655, rec:0.0042, f1:0.0078
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.389, loss:697.5545
g_step 2700, step 196, avg_time 0.992, loss:695.3697
g_step 2800, step 296, avg_time 0.992, loss:702.5201
g_step 2900, step 83, avg_time 0.978, loss:694.7603
g_step 3000, step 183, avg_time 0.983, loss:666.5887
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5238, rec:0.3883, f1:0.4460
>> valid relation prec:0.0784, rec:0.0055, f1:0.0103
>> valid relation with NER prec:0.0784, rec:0.0055, f1:0.0103
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 283, avg_time 2.391, loss:710.3279
g_step 3200, step 70, avg_time 0.996, loss:643.1096
g_step 3300, step 170, avg_time 0.983, loss:639.0882
g_step 3400, step 270, avg_time 0.982, loss:658.9244
g_step 3500, step 57, avg_time 0.984, loss:620.5313
>> valid entity prec:0.5256, rec:0.4682, f1:0.4952
>> valid relation prec:0.0615, rec:0.0065, f1:0.0117
>> valid relation with NER prec:0.0615, rec:0.0065, f1:0.0117
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 157, avg_time 2.390, loss:633.6063
g_step 3700, step 257, avg_time 0.993, loss:652.5635
g_step 3800, step 44, avg_time 0.989, loss:627.7731
g_step 3900, step 144, avg_time 0.989, loss:604.7131
g_step 4000, step 244, avg_time 0.987, loss:658.2620
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4903, rec:0.5323, f1:0.5104
>> valid relation prec:0.0581, rec:0.0074, f1:0.0131
>> valid relation with NER prec:0.0581, rec:0.0074, f1:0.0131
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 31, avg_time 2.413, loss:604.5893
g_step 4200, step 131, avg_time 0.991, loss:577.4943
g_step 4300, step 231, avg_time 0.999, loss:592.8667
g_step 4400, step 18, avg_time 0.964, loss:602.8686
g_step 4500, step 118, avg_time 0.984, loss:566.3462
>> valid entity prec:0.5106, rec:0.3890, f1:0.4416
>> valid relation prec:0.0761, rec:0.0069, f1:0.0127
>> valid relation with NER prec:0.0761, rec:0.0069, f1:0.0127
g_step 4600, step 218, avg_time 2.388, loss:583.5118
g_step 4700, step 5, avg_time 0.993, loss:589.2598
g_step 4800, step 105, avg_time 0.988, loss:542.3955
g_step 4900, step 205, avg_time 0.998, loss:569.1817
g_step 5000, step 305, avg_time 0.997, loss:575.4719
learning rate was adjusted to 0.0008
>> valid entity prec:0.5240, rec:0.2907, f1:0.3739
>> valid relation prec:0.0308, rec:0.0018, f1:0.0035
>> valid relation with NER prec:0.0308, rec:0.0018, f1:0.0035
g_step 5100, step 92, avg_time 2.375, loss:535.1899
g_step 5200, step 192, avg_time 0.985, loss:538.8537
g_step 5300, step 292, avg_time 0.996, loss:572.4577
g_step 5400, step 79, avg_time 0.990, loss:515.5508
g_step 5500, step 179, avg_time 0.997, loss:532.8472
>> valid entity prec:0.5194, rec:0.4368, f1:0.4745
>> valid relation prec:0.0309, rec:0.0039, f1:0.0070
>> valid relation with NER prec:0.0309, rec:0.0039, f1:0.0070
g_step 5600, step 279, avg_time 2.396, loss:540.6820
g_step 5700, step 66, avg_time 0.986, loss:493.5968
g_step 5800, step 166, avg_time 0.999, loss:505.9047
g_step 5900, step 266, avg_time 0.987, loss:532.2709
g_step 6000, step 53, avg_time 0.990, loss:509.3715
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5373, rec:0.3228, f1:0.4033
>> valid relation prec:0.0680, rec:0.0076, f1:0.0137
>> valid relation with NER prec:0.0680, rec:0.0076, f1:0.0137
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 153, avg_time 2.391, loss:493.7466
g_step 6200, step 253, avg_time 0.986, loss:504.1673
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 20:02:31 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 20:02:31 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_20-02-31_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 20:02:32 - WARNING - datasets.builder -   Using custom data configuration default-42aea1dc612746b2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-42aea1dc612746b2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 20:02:33,324 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:02:33,325 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:02:33,326 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:02:33,326 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:02:33,337 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:33,345 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:33,345 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:33,345 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:33,345 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:33,345 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:02:33,345 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 20:02:33,921 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:02:36,986 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 20:02:36,993 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-42aea1dc612746b2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.25ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.30ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.86ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.14ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.33ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.47ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.54ba/s]100%|██████████| 8/8 [00:01<00:00,  5.35ba/s]100%|██████████| 8/8 [00:01<00:00,  4.39ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.69ba/s] 40%|████      | 2/5 [00:00<00:00,  4.11ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.28ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.36ba/s]100%|██████████| 5/5 [00:01<00:00,  4.89ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  7.98ba/s] 38%|███▊      | 3/8 [00:00<00:00,  9.93ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.37ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.71ba/s]100%|██████████| 8/8 [00:00<00:00, 11.05ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  5.27ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.53ba/s]100%|██████████| 5/5 [00:00<00:00, 11.24ba/s]100%|██████████| 5/5 [00:00<00:00, 10.04ba/s]
[INFO|trainer.py:414] 2023-08-28 20:02:41,976 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 20:02:42,117 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 20:02:42,117 >>   Num examples = 7537
[INFO|trainer.py:1149] 2023-08-28 20:02:42,117 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 20:02:42,117 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 20:02:42,117 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 20:02:42,117 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 20:02:42,117 >>   Total optimization steps = 590
  0%|          | 0/590 [00:00<?, ?it/s]  0%|          | 1/590 [00:00<04:02,  2.43it/s]  0%|          | 2/590 [00:00<03:40,  2.66it/s]  1%|          | 3/590 [00:01<03:54,  2.50it/s]  1%|          | 4/590 [00:01<03:56,  2.48it/s]  1%|          | 5/590 [00:01<03:42,  2.63it/s]  1%|          | 6/590 [00:02<03:26,  2.83it/s]  1%|          | 7/590 [00:02<03:15,  2.98it/s]  1%|▏         | 8/590 [00:02<03:07,  3.10it/s]  2%|▏         | 9/590 [00:03<03:03,  3.16it/s]  2%|▏         | 10/590 [00:03<02:59,  3.23it/s]  2%|▏         | 11/590 [00:03<02:56,  3.28it/s]  2%|▏         | 12/590 [00:04<02:54,  3.31it/s]  2%|▏         | 13/590 [00:04<02:53,  3.33it/s]  2%|▏         | 14/590 [00:04<02:54,  3.31it/s]  3%|▎         | 15/590 [00:04<02:52,  3.33it/s]  3%|▎         | 16/590 [00:05<02:51,  3.35it/s]  3%|▎         | 17/590 [00:05<02:50,  3.36it/s]  3%|▎         | 18/590 [00:05<02:49,  3.37it/s]  3%|▎         | 19/590 [00:06<02:49,  3.38it/s]  3%|▎         | 20/590 [00:06<02:48,  3.38it/s]  4%|▎         | 21/590 [00:06<02:48,  3.38it/s]  4%|▎         | 22/590 [00:06<02:47,  3.39it/s]  4%|▍         | 23/590 [00:07<02:47,  3.39it/s]  4%|▍         | 24/590 [00:07<02:47,  3.39it/s]  4%|▍         | 25/590 [00:07<02:47,  3.37it/s]  4%|▍         | 26/590 [00:08<02:46,  3.38it/s]  5%|▍         | 27/590 [00:08<02:46,  3.38it/s]  5%|▍         | 28/590 [00:08<02:46,  3.38it/s]  5%|▍         | 29/590 [00:09<02:45,  3.38it/s]  5%|▌         | 30/590 [00:09<02:45,  3.39it/s]  5%|▌         | 31/590 [00:09<02:44,  3.39it/s]  5%|▌         | 32/590 [00:09<02:44,  3.39it/s]  6%|▌         | 33/590 [00:10<02:44,  3.39it/s]  6%|▌         | 34/590 [00:10<02:44,  3.39it/s]  6%|▌         | 35/590 [00:10<02:43,  3.39it/s]  6%|▌         | 36/590 [00:11<02:45,  3.34it/s]  6%|▋         | 37/590 [00:11<02:44,  3.36it/s]  6%|▋         | 38/590 [00:11<02:44,  3.37it/s]  7%|▋         | 39/590 [00:12<02:43,  3.37it/s]  7%|▋         | 40/590 [00:12<02:43,  3.37it/s]  7%|▋         | 41/590 [00:12<02:42,  3.38it/s]  7%|▋         | 42/590 [00:12<02:41,  3.38it/s]  7%|▋         | 43/590 [00:13<02:41,  3.38it/s]  7%|▋         | 44/590 [00:13<02:41,  3.38it/s]  8%|▊         | 45/590 [00:13<02:41,  3.38it/s]  8%|▊         | 46/590 [00:14<02:40,  3.39it/s]  8%|▊         | 47/590 [00:14<02:41,  3.36it/s]  8%|▊         | 48/590 [00:14<02:40,  3.37it/s]  8%|▊         | 49/590 [00:14<02:40,  3.37it/s]  8%|▊         | 50/590 [00:15<02:39,  3.38it/s]  9%|▊         | 51/590 [00:15<02:39,  3.38it/s]  9%|▉         | 52/590 [00:15<02:39,  3.38it/s]  9%|▉         | 53/590 [00:16<02:38,  3.38it/s]  9%|▉         | 54/590 [00:16<02:38,  3.38it/s]  9%|▉         | 55/590 [00:16<02:38,  3.38it/s]  9%|▉         | 56/590 [00:17<02:37,  3.40it/s] 10%|▉         | 57/590 [00:17<02:36,  3.41it/s] 10%|▉         | 58/590 [00:17<02:36,  3.39it/s] 10%|█         | 59/590 [00:17<02:35,  3.41it/s] 10%|█         | 60/590 [00:18<02:35,  3.41it/s] 10%|█         | 61/590 [00:18<02:34,  3.42it/s] 11%|█         | 62/590 [00:18<02:34,  3.42it/s] 11%|█         | 63/590 [00:19<02:33,  3.42it/s] 11%|█         | 64/590 [00:19<02:33,  3.43it/s] 11%|█         | 65/590 [00:19<02:33,  3.43it/s] 11%|█         | 66/590 [00:19<02:32,  3.43it/s] 11%|█▏        | 67/590 [00:20<02:32,  3.43it/s] 12%|█▏        | 68/590 [00:20<02:32,  3.43it/s] 12%|█▏        | 69/590 [00:20<02:33,  3.40it/s] 12%|█▏        | 70/590 [00:21<02:32,  3.41it/s] 12%|█▏        | 71/590 [00:21<02:31,  3.41it/s] 12%|█▏        | 72/590 [00:21<02:31,  3.42it/s] 12%|█▏        | 73/590 [00:22<02:30,  3.43it/s] 13%|█▎        | 74/590 [00:22<02:30,  3.43it/s] 13%|█▎        | 75/590 [00:22<02:30,  3.43it/s] 13%|█▎        | 76/590 [00:22<02:30,  3.42it/s] 13%|█▎        | 77/590 [00:23<02:29,  3.43it/s] 13%|█▎        | 78/590 [00:23<02:29,  3.43it/s] 13%|█▎        | 79/590 [00:23<02:29,  3.43it/s] 14%|█▎        | 80/590 [00:24<02:28,  3.43it/s] 14%|█▎        | 81/590 [00:24<02:28,  3.43it/s] 14%|█▍        | 82/590 [00:24<02:29,  3.40it/s] 14%|█▍        | 83/590 [00:24<02:28,  3.41it/s] 14%|█▍        | 84/590 [00:25<02:28,  3.42it/s] 14%|█▍        | 85/590 [00:25<02:27,  3.42it/s] 15%|█▍        | 86/590 [00:25<02:27,  3.43it/s] 15%|█▍        | 87/590 [00:26<02:26,  3.43it/s] 15%|█▍        | 88/590 [00:26<02:26,  3.43it/s] 15%|█▌        | 89/590 [00:26<02:26,  3.43it/s] 15%|█▌        | 90/590 [00:26<02:25,  3.43it/s] 15%|█▌        | 91/590 [00:27<02:25,  3.43it/s] 16%|█▌        | 92/590 [00:27<02:25,  3.43it/s] 16%|█▌        | 93/590 [00:27<02:25,  3.41it/s] 16%|█▌        | 94/590 [00:28<02:25,  3.41it/s] 16%|█▌        | 95/590 [00:28<02:24,  3.42it/s] 16%|█▋        | 96/590 [00:28<02:24,  3.42it/s] 16%|█▋        | 97/590 [00:29<02:24,  3.42it/s] 17%|█▋        | 98/590 [00:29<02:23,  3.43it/s] 17%|█▋        | 99/590 [00:29<02:23,  3.43it/s] 17%|█▋        | 100/590 [00:29<02:22,  3.43it/s] 17%|█▋        | 101/590 [00:30<02:22,  3.43it/s] 17%|█▋        | 102/590 [00:30<02:22,  3.43it/s] 17%|█▋        | 103/590 [00:30<02:22,  3.43it/s] 18%|█▊        | 104/590 [00:31<02:22,  3.42it/s] 18%|█▊        | 105/590 [00:31<02:21,  3.42it/s] 18%|█▊        | 106/590 [00:31<02:21,  3.42it/s] 18%|█▊        | 107/590 [00:31<02:21,  3.42it/s] 18%|█▊        | 108/590 [00:32<02:20,  3.42it/s] 18%|█▊        | 109/590 [00:32<02:20,  3.42it/s] 19%|█▊        | 110/590 [00:32<02:20,  3.42it/s] 19%|█▉        | 111/590 [00:33<02:19,  3.43it/s] 19%|█▉        | 112/590 [00:33<02:19,  3.42it/s] 19%|█▉        | 113/590 [00:33<02:19,  3.43it/s] 19%|█▉        | 114/590 [00:33<02:18,  3.43it/s] 19%|█▉        | 115/590 [00:34<02:21,  3.35it/s] 20%|█▉        | 116/590 [00:34<02:20,  3.37it/s] 20%|█▉        | 117/590 [00:34<02:19,  3.39it/s] 20%|██        | 118/590 [00:35<02:10,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 20:03:17,240 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:03:17,240 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:03:17,240 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.47it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.76it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.16it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.36it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.84it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.47it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.20it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.18it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.23it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.30it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.28it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.22it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.07it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.13it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.86it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.96it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.03it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.09it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.10it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.14it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.09it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.06it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.97it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.85it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.93it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.08it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.08it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.12it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.11it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.08it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.05it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.00it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.82it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.83it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.85it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.10it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.11it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.08it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.15it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.98it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.84it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.03it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.10it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.07it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.15it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.11it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.17it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.08it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.98it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.90it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.93it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.00it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.03it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.06it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.15it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.15it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.05it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.97it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.95it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.01it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.01it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.97it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.09it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.15it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.03it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.04it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.02it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.88it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.09it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.07it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.05it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.16it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.59it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 43.71it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 43.68it/s][A
 70%|███████   | 382/543 [00:08<00:03, 43.84it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.71it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 43.81it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 43.93it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.77it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 43.90it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.03it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 43.99it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.09it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 43.91it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.03it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.07it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.02it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.01it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.06it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.08it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.14it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.07it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.03it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.02it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.92it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.04it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.94it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.14it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.08it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.07it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.97it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.97it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.03it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.91it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.84it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.12it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.12it/s][A 20%|██        | 118/590 [00:47<02:10,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:03:29,814 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-118
[INFO|configuration_utils.py:351] 2023-08-28 20:03:29,895 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-118/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:03:34,076 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-118/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:03:34,192 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-118/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:03:34,222 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-118/special_tokens_map.json
 20%|██        | 119/590 [01:02<1:04:59,  8.28s/it] 20%|██        | 120/590 [01:02<46:05,  5.88s/it]   21%|██        | 121/590 [01:02<32:53,  4.21s/it] 21%|██        | 122/590 [01:02<23:39,  3.03s/it] 21%|██        | 123/590 [01:03<17:13,  2.21s/it] 21%|██        | 124/590 [01:03<12:42,  1.64s/it] 21%|██        | 125/590 [01:03<09:33,  1.23s/it] 21%|██▏       | 126/590 [01:04<07:22,  1.05it/s] 22%|██▏       | 127/590 [01:04<05:49,  1.32it/s] 22%|██▏       | 128/590 [01:04<04:45,  1.62it/s] 22%|██▏       | 129/590 [01:05<03:59,  1.92it/s] 22%|██▏       | 130/590 [01:05<03:28,  2.21it/s] 22%|██▏       | 131/590 [01:05<03:08,  2.43it/s] 22%|██▏       | 132/590 [01:05<02:52,  2.66it/s] 23%|██▎       | 133/590 [01:06<02:40,  2.84it/s] 23%|██▎       | 134/590 [01:06<02:32,  2.99it/s] 23%|██▎       | 135/590 [01:06<02:26,  3.10it/s] 23%|██▎       | 136/590 [01:07<02:22,  3.18it/s] 23%|██▎       | 137/590 [01:07<02:19,  3.24it/s] 23%|██▎       | 138/590 [01:07<02:17,  3.28it/s] 24%|██▎       | 139/590 [01:07<02:15,  3.32it/s] 24%|██▎       | 140/590 [01:08<02:14,  3.34it/s] 24%|██▍       | 141/590 [01:08<02:13,  3.35it/s] 24%|██▍       | 142/590 [01:08<02:13,  3.35it/s] 24%|██▍       | 143/590 [01:09<02:13,  3.36it/s] 24%|██▍       | 144/590 [01:09<02:12,  3.37it/s] 25%|██▍       | 145/590 [01:09<02:11,  3.37it/s] 25%|██▍       | 146/590 [01:10<02:11,  3.38it/s] 25%|██▍       | 147/590 [01:10<02:11,  3.38it/s] 25%|██▌       | 148/590 [01:10<02:10,  3.38it/s] 25%|██▌       | 149/590 [01:10<02:10,  3.39it/s] 25%|██▌       | 150/590 [01:11<02:10,  3.38it/s] 26%|██▌       | 151/590 [01:11<02:09,  3.38it/s] 26%|██▌       | 152/590 [01:11<02:09,  3.38it/s] 26%|██▌       | 153/590 [01:12<02:12,  3.30it/s] 26%|██▌       | 154/590 [01:12<02:11,  3.33it/s] 26%|██▋       | 155/590 [01:12<02:10,  3.34it/s] 26%|██▋       | 156/590 [01:13<02:09,  3.35it/s] 27%|██▋       | 157/590 [01:13<02:08,  3.36it/s] 27%|██▋       | 158/590 [01:13<02:08,  3.37it/s] 27%|██▋       | 159/590 [01:13<02:07,  3.38it/s] 27%|██▋       | 160/590 [01:14<02:07,  3.38it/s] 27%|██▋       | 161/590 [01:14<02:06,  3.38it/s] 27%|██▋       | 162/590 [01:14<02:06,  3.38it/s] 28%|██▊       | 163/590 [01:15<02:06,  3.38it/s] 28%|██▊       | 164/590 [01:15<02:06,  3.36it/s] 28%|██▊       | 165/590 [01:15<02:06,  3.37it/s] 28%|██▊       | 166/590 [01:16<02:05,  3.37it/s] 28%|██▊       | 167/590 [01:16<02:05,  3.37it/s] 28%|██▊       | 168/590 [01:16<02:04,  3.38it/s] 29%|██▊       | 169/590 [01:16<02:05,  3.36it/s] 29%|██▉       | 170/590 [01:17<02:04,  3.37it/s] 29%|██▉       | 171/590 [01:17<02:04,  3.37it/s] 29%|██▉       | 172/590 [01:17<02:03,  3.38it/s] 29%|██▉       | 173/590 [01:18<02:03,  3.38it/s] 29%|██▉       | 174/590 [01:18<02:03,  3.38it/s] 30%|██▉       | 175/590 [01:18<02:04,  3.35it/s] 30%|██▉       | 176/590 [01:18<02:03,  3.36it/s] 30%|███       | 177/590 [01:19<02:02,  3.36it/s] 30%|███       | 178/590 [01:19<02:02,  3.37it/s] 30%|███       | 179/590 [01:19<02:01,  3.37it/s] 31%|███       | 180/590 [01:20<02:01,  3.37it/s] 31%|███       | 181/590 [01:20<02:01,  3.37it/s] 31%|███       | 182/590 [01:20<02:00,  3.37it/s] 31%|███       | 183/590 [01:21<02:00,  3.38it/s] 31%|███       | 184/590 [01:21<02:00,  3.38it/s] 31%|███▏      | 185/590 [01:21<01:59,  3.38it/s] 32%|███▏      | 186/590 [01:21<02:03,  3.27it/s] 32%|███▏      | 187/590 [01:22<02:01,  3.30it/s] 32%|███▏      | 188/590 [01:22<02:00,  3.33it/s] 32%|███▏      | 189/590 [01:22<01:59,  3.34it/s] 32%|███▏      | 190/590 [01:23<01:59,  3.36it/s] 32%|███▏      | 191/590 [01:23<01:58,  3.37it/s] 33%|███▎      | 192/590 [01:23<01:58,  3.37it/s] 33%|███▎      | 193/590 [01:24<01:57,  3.37it/s] 33%|███▎      | 194/590 [01:24<01:57,  3.37it/s] 33%|███▎      | 195/590 [01:24<01:56,  3.38it/s] 33%|███▎      | 196/590 [01:24<01:55,  3.40it/s] 33%|███▎      | 197/590 [01:25<01:55,  3.41it/s] 34%|███▎      | 198/590 [01:25<01:54,  3.41it/s] 34%|███▎      | 199/590 [01:25<01:54,  3.42it/s] 34%|███▍      | 200/590 [01:26<01:55,  3.39it/s] 34%|███▍      | 201/590 [01:26<01:54,  3.40it/s] 34%|███▍      | 202/590 [01:26<01:53,  3.41it/s] 34%|███▍      | 203/590 [01:26<01:53,  3.41it/s] 35%|███▍      | 204/590 [01:27<01:52,  3.42it/s] 35%|███▍      | 205/590 [01:27<01:52,  3.41it/s] 35%|███▍      | 206/590 [01:27<01:52,  3.42it/s] 35%|███▌      | 207/590 [01:28<01:51,  3.42it/s] 35%|███▌      | 208/590 [01:28<01:51,  3.43it/s] 35%|███▌      | 209/590 [01:28<01:51,  3.43it/s] 36%|███▌      | 210/590 [01:29<01:50,  3.43it/s] 36%|███▌      | 211/590 [01:29<01:51,  3.39it/s] 36%|███▌      | 212/590 [01:29<01:51,  3.40it/s] 36%|███▌      | 213/590 [01:29<01:50,  3.41it/s] 36%|███▋      | 214/590 [01:30<01:50,  3.41it/s] 36%|███▋      | 215/590 [01:30<01:49,  3.42it/s] 37%|███▋      | 216/590 [01:30<01:49,  3.42it/s] 37%|███▋      | 217/590 [01:31<01:48,  3.42it/s] 37%|███▋      | 218/590 [01:31<01:48,  3.42it/s] 37%|███▋      | 219/590 [01:31<01:48,  3.42it/s] 37%|███▋      | 220/590 [01:31<01:47,  3.43it/s] 37%|███▋      | 221/590 [01:32<01:47,  3.43it/s] 38%|███▊      | 222/590 [01:32<01:49,  3.37it/s] 38%|███▊      | 223/590 [01:32<01:48,  3.39it/s] 38%|███▊      | 224/590 [01:33<01:47,  3.40it/s] 38%|███▊      | 225/590 [01:33<01:47,  3.41it/s] 38%|███▊      | 226/590 [01:33<01:46,  3.41it/s] 38%|███▊      | 227/590 [01:33<01:46,  3.41it/s] 39%|███▊      | 228/590 [01:34<01:45,  3.42it/s] 39%|███▉      | 229/590 [01:34<01:45,  3.42it/s] 39%|███▉      | 230/590 [01:34<01:45,  3.42it/s] 39%|███▉      | 231/590 [01:35<01:44,  3.42it/s] 39%|███▉      | 232/590 [01:35<01:44,  3.43it/s] 39%|███▉      | 233/590 [01:35<01:45,  3.38it/s] 40%|███▉      | 234/590 [01:36<01:44,  3.40it/s] 40%|███▉      | 235/590 [01:36<01:44,  3.41it/s] 40%|████      | 236/590 [01:36<01:37,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 20:04:18,700 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:04:18,700 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:04:18,700 >>   Batch size = 8
{'eval_loss': 0.9793775081634521, 'eval_runtime': 12.3359, 'eval_samples_per_second': 351.981, 'eval_steps_per_second': 44.018, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.28it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.05it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.44it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.33it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.91it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.55it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.37it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.17it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.16it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.29it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.35it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.26it/s][A
 12%|█▏        | 67/543 [00:01<00:11, 41.05it/s][A
 13%|█▎        | 72/543 [00:01<00:11, 42.04it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 42.60it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 42.93it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.28it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.54it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.78it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 43.95it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 43.69it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.82it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.09it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.04it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.98it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.09it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.07it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.21it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.13it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 43.97it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.03it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.07it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.11it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.09it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 44.16it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.25it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.22it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.13it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 43.99it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.03it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.08it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.14it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.88it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.13it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.17it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.11it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.11it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.05it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.10it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.09it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.11it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.03it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.21it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.14it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.03it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.03it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.09it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.12it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.16it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.00it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.10it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.15it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.14it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.00it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.92it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.91it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.89it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.85it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.95it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.90it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.03it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.03it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.97it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.02it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.08it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.15it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.16it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.07it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 44.14it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.10it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.06it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 43.98it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.01it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.07it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.14it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.03it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.07it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.18it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.12it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.01it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.00it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.02it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.01it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.09it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.17it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.13it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.13it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.09it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 43.94it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.08it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.89it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.15it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.18it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.16it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.05it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.07it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.04it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 43.92it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 43.92it/s][A 40%|████      | 236/590 [01:48<01:37,  3.63it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:04:31,123 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-236
[INFO|configuration_utils.py:351] 2023-08-28 20:04:31,181 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-236/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:04:37,235 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-236/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:04:37,303 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-236/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:04:37,320 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-236/special_tokens_map.json
 40%|████      | 237/590 [02:06<54:29,  9.26s/it] 40%|████      | 238/590 [02:07<38:34,  6.57s/it] 41%|████      | 239/590 [02:07<27:26,  4.69s/it] 41%|████      | 240/590 [02:07<19:40,  3.37s/it] 41%|████      | 241/590 [02:07<14:14,  2.45s/it] 41%|████      | 242/590 [02:08<10:27,  1.80s/it] 41%|████      | 243/590 [02:08<07:48,  1.35s/it] 41%|████▏     | 244/590 [02:08<05:57,  1.03s/it] 42%|████▏     | 245/590 [02:09<04:40,  1.23it/s] 42%|████▏     | 246/590 [02:09<03:45,  1.52it/s] 42%|████▏     | 247/590 [02:09<03:07,  1.82it/s] 42%|████▏     | 248/590 [02:10<02:41,  2.12it/s] 42%|████▏     | 249/590 [02:10<02:22,  2.39it/s] 42%|████▏     | 250/590 [02:10<02:09,  2.62it/s] 43%|████▎     | 251/590 [02:10<02:00,  2.81it/s] 43%|████▎     | 252/590 [02:11<01:54,  2.96it/s] 43%|████▎     | 253/590 [02:11<01:49,  3.08it/s] 43%|████▎     | 254/590 [02:11<01:46,  3.17it/s] 43%|████▎     | 255/590 [02:12<01:46,  3.15it/s] 43%|████▎     | 256/590 [02:12<01:44,  3.20it/s] 44%|████▎     | 257/590 [02:12<01:42,  3.26it/s] 44%|████▎     | 258/590 [02:13<01:40,  3.30it/s] 44%|████▍     | 259/590 [02:13<01:39,  3.32it/s] 44%|████▍     | 260/590 [02:13<01:39,  3.32it/s] 44%|████▍     | 261/590 [02:13<01:38,  3.34it/s] 44%|████▍     | 262/590 [02:14<01:37,  3.36it/s] 45%|████▍     | 263/590 [02:14<01:37,  3.37it/s] 45%|████▍     | 264/590 [02:14<01:36,  3.37it/s] 45%|████▍     | 265/590 [02:15<01:36,  3.38it/s] 45%|████▌     | 266/590 [02:15<01:45,  3.08it/s] 45%|████▌     | 267/590 [02:15<01:42,  3.16it/s] 45%|████▌     | 268/590 [02:16<01:40,  3.21it/s] 46%|████▌     | 269/590 [02:16<01:38,  3.26it/s] 46%|████▌     | 270/590 [02:16<01:37,  3.30it/s] 46%|████▌     | 271/590 [02:16<01:36,  3.32it/s] 46%|████▌     | 272/590 [02:17<01:35,  3.34it/s] 46%|████▋     | 273/590 [02:17<01:34,  3.36it/s] 46%|████▋     | 274/590 [02:17<01:33,  3.36it/s] 47%|████▋     | 275/590 [02:18<01:33,  3.37it/s] 47%|████▋     | 276/590 [02:18<01:33,  3.37it/s] 47%|████▋     | 277/590 [02:18<01:32,  3.37it/s] 47%|████▋     | 278/590 [02:19<01:32,  3.38it/s] 47%|████▋     | 279/590 [02:19<01:32,  3.38it/s] 47%|████▋     | 280/590 [02:19<01:31,  3.39it/s] 48%|████▊     | 281/590 [02:19<01:31,  3.39it/s] 48%|████▊     | 282/590 [02:20<01:30,  3.39it/s] 48%|████▊     | 283/590 [02:20<01:30,  3.39it/s] 48%|████▊     | 284/590 [02:20<01:30,  3.39it/s] 48%|████▊     | 285/590 [02:21<01:29,  3.40it/s] 48%|████▊     | 286/590 [02:21<01:29,  3.41it/s] 49%|████▊     | 287/590 [02:21<01:29,  3.37it/s] 49%|████▉     | 288/590 [02:22<01:28,  3.40it/s] 49%|████▉     | 289/590 [02:22<01:28,  3.41it/s] 49%|████▉     | 290/590 [02:22<01:27,  3.42it/s] 49%|████▉     | 291/590 [02:22<01:27,  3.42it/s] 49%|████▉     | 292/590 [02:23<01:26,  3.43it/s] 50%|████▉     | 293/590 [02:23<01:26,  3.43it/s] 50%|████▉     | 294/590 [02:23<01:26,  3.43it/s] 50%|█████     | 295/590 [02:24<01:25,  3.43it/s] 50%|█████     | 296/590 [02:24<01:25,  3.43it/s] 50%|█████     | 297/590 [02:24<01:25,  3.43it/s] 51%|█████     | 298/590 [02:24<01:25,  3.41it/s] 51%|█████     | 299/590 [02:25<01:25,  3.41it/s] 51%|█████     | 300/590 [02:25<01:24,  3.42it/s] 51%|█████     | 301/590 [02:25<01:24,  3.42it/s] 51%|█████     | 302/590 [02:26<01:24,  3.43it/s] 51%|█████▏    | 303/590 [02:26<01:23,  3.43it/s] 52%|█████▏    | 304/590 [02:26<01:23,  3.43it/s] 52%|█████▏    | 305/590 [02:26<01:22,  3.44it/s] 52%|█████▏    | 306/590 [02:27<01:22,  3.43it/s] 52%|█████▏    | 307/590 [02:27<01:22,  3.43it/s] 52%|█████▏    | 308/590 [02:27<01:22,  3.43it/s] 52%|█████▏    | 309/590 [02:28<01:24,  3.34it/s] 53%|█████▎    | 310/590 [02:28<01:23,  3.37it/s] 53%|█████▎    | 311/590 [02:28<01:22,  3.39it/s] 53%|█████▎    | 312/590 [02:29<01:21,  3.40it/s] 53%|█████▎    | 313/590 [02:29<01:21,  3.41it/s] 53%|█████▎    | 314/590 [02:29<01:20,  3.41it/s] 53%|█████▎    | 315/590 [02:29<01:20,  3.42it/s] 54%|█████▎    | 316/590 [02:30<01:20,  3.42it/s] 54%|█████▎    | 317/590 [02:30<01:19,  3.42it/s] 54%|█████▍    | 318/590 [02:30<01:19,  3.42it/s] 54%|█████▍    | 319/590 [02:31<01:19,  3.43it/s] 54%|█████▍    | 320/590 [02:31<01:20,  3.35it/s] 54%|█████▍    | 321/590 [02:31<01:19,  3.38it/s] 55%|█████▍    | 322/590 [02:31<01:18,  3.39it/s] 55%|█████▍    | 323/590 [02:32<01:18,  3.40it/s] 55%|█████▍    | 324/590 [02:32<01:17,  3.41it/s] 55%|█████▌    | 325/590 [02:32<01:17,  3.42it/s] 55%|█████▌    | 326/590 [02:33<01:17,  3.42it/s] 55%|█████▌    | 327/590 [02:33<01:16,  3.42it/s] 56%|█████▌    | 328/590 [02:33<01:16,  3.43it/s] 56%|█████▌    | 329/590 [02:34<01:16,  3.43it/s] 56%|█████▌    | 330/590 [02:34<01:15,  3.43it/s] 56%|█████▌    | 331/590 [02:34<01:16,  3.40it/s] 56%|█████▋    | 332/590 [02:34<01:15,  3.41it/s] 56%|█████▋    | 333/590 [02:35<01:15,  3.42it/s] 57%|█████▋    | 334/590 [02:35<01:14,  3.42it/s] 57%|█████▋    | 335/590 [02:35<01:14,  3.42it/s] 57%|█████▋    | 336/590 [02:36<01:14,  3.42it/s] 57%|█████▋    | 337/590 [02:36<01:13,  3.42it/s] 57%|█████▋    | 338/590 [02:36<01:13,  3.43it/s] 57%|█████▋    | 339/590 [02:36<01:13,  3.43it/s] 58%|█████▊    | 340/590 [02:37<01:12,  3.43it/s] 58%|█████▊    | 341/590 [02:37<01:12,  3.43it/s] 58%|█████▊    | 342/590 [02:37<01:14,  3.34it/s] 58%|█████▊    | 343/590 [02:38<01:13,  3.35it/s] 58%|█████▊    | 344/590 [02:38<01:13,  3.36it/s] 58%|█████▊    | 345/590 [02:38<01:12,  3.37it/s] 59%|█████▊    | 346/590 [02:39<01:12,  3.37it/s] 59%|█████▉    | 347/590 [02:39<01:12,  3.37it/s] 59%|█████▉    | 348/590 [02:39<01:11,  3.38it/s] 59%|█████▉    | 349/590 [02:39<01:11,  3.38it/s] 59%|█████▉    | 350/590 [02:40<01:11,  3.38it/s] 59%|█████▉    | 351/590 [02:40<01:10,  3.37it/s] 60%|█████▉    | 352/590 [02:40<01:10,  3.37it/s] 60%|█████▉    | 353/590 [02:41<01:10,  3.38it/s] 60%|██████    | 354/590 [02:41<01:05,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 20:05:23,442 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:05:23,442 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:05:23,442 >>   Batch size = 8
{'eval_loss': 0.9988542795181274, 'eval_runtime': 12.3445, 'eval_samples_per_second': 351.737, 'eval_steps_per_second': 43.987, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.96it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.48it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.00it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.28it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.87it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.48it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.44it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.26it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.31it/s][A
 10%|▉         | 52/543 [00:01<00:12, 40.13it/s][A
 10%|█         | 57/543 [00:01<00:11, 41.46it/s][A
 11%|█▏        | 62/543 [00:01<00:11, 42.37it/s][A
 12%|█▏        | 67/543 [00:01<00:11, 42.92it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.21it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.56it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.64it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.75it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 43.53it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 43.78it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 43.92it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.20it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.22it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.22it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.10it/s][A
 24%|██▍       | 132/543 [00:03<00:09, 43.95it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 43.75it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 43.89it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.07it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.20it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.27it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.08it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.19it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.07it/s][A
 33%|███▎      | 177/543 [00:04<00:08, 43.88it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 43.76it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 43.75it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.06it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.10it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.24it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.25it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.17it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.04it/s][A
 41%|████      | 222/543 [00:05<00:07, 43.95it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 43.90it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 43.94it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.06it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.13it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.26it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.23it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.12it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.08it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.04it/s][A
 50%|█████     | 272/543 [00:06<00:06, 43.97it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.15it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.17it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.18it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.21it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.21it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.02it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.00it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 43.92it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 43.96it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.46it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.79it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.94it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.99it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.89it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.87it/s][A
 65%|██████▍   | 352/543 [00:08<00:04, 43.92it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.94it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.98it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.08it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.21it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.07it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.09it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.11it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.05it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 44.02it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.95it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.02it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.13it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.24it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.15it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.18it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.10it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.02it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.99it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 43.86it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.99it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 43.87it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.04it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.11it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.06it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.99it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.01it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.96it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.96it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 43.98it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.06it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.05it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.14it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.24it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.08it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.92it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.98it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.02it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.01it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.01it/s][A 60%|██████    | 354/590 [02:53<01:05,  3.60it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:05:36,098 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-354
[INFO|configuration_utils.py:351] 2023-08-28 20:05:36,178 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-354/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:05:41,798 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-354/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:05:41,849 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-354/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:05:41,866 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-354/special_tokens_map.json
 60%|██████    | 355/590 [03:09<34:25,  8.79s/it] 60%|██████    | 356/590 [03:10<24:21,  6.24s/it] 61%|██████    | 357/590 [03:10<17:18,  4.46s/it] 61%|██████    | 358/590 [03:10<12:24,  3.21s/it] 61%|██████    | 359/590 [03:11<08:59,  2.34s/it] 61%|██████    | 360/590 [03:11<06:36,  1.72s/it] 61%|██████    | 361/590 [03:11<04:56,  1.29s/it] 61%|██████▏   | 362/590 [03:12<03:46,  1.01it/s] 62%|██████▏   | 363/590 [03:12<02:58,  1.27it/s] 62%|██████▏   | 364/590 [03:12<02:24,  1.57it/s] 62%|██████▏   | 365/590 [03:12<02:00,  1.87it/s] 62%|██████▏   | 366/590 [03:13<01:43,  2.16it/s] 62%|██████▏   | 367/590 [03:13<01:32,  2.41it/s] 62%|██████▏   | 368/590 [03:13<01:23,  2.64it/s] 63%|██████▎   | 369/590 [03:14<01:18,  2.83it/s] 63%|██████▎   | 370/590 [03:14<01:13,  2.98it/s] 63%|██████▎   | 371/590 [03:14<01:10,  3.09it/s] 63%|██████▎   | 372/590 [03:14<01:08,  3.18it/s] 63%|██████▎   | 373/590 [03:15<01:06,  3.24it/s] 63%|██████▎   | 374/590 [03:15<01:05,  3.29it/s] 64%|██████▎   | 375/590 [03:15<01:04,  3.32it/s] 64%|██████▎   | 376/590 [03:16<01:04,  3.34it/s] 64%|██████▍   | 377/590 [03:16<01:03,  3.35it/s] 64%|██████▍   | 378/590 [03:16<01:04,  3.27it/s] 64%|██████▍   | 379/590 [03:17<01:03,  3.31it/s] 64%|██████▍   | 380/590 [03:17<01:02,  3.33it/s] 65%|██████▍   | 381/590 [03:17<01:02,  3.35it/s] 65%|██████▍   | 382/590 [03:17<01:01,  3.36it/s] 65%|██████▍   | 383/590 [03:18<01:01,  3.37it/s] 65%|██████▌   | 384/590 [03:18<01:01,  3.38it/s] 65%|██████▌   | 385/590 [03:18<01:00,  3.38it/s] 65%|██████▌   | 386/590 [03:19<01:00,  3.38it/s] 66%|██████▌   | 387/590 [03:19<00:59,  3.39it/s] 66%|██████▌   | 388/590 [03:19<00:59,  3.38it/s] 66%|██████▌   | 389/590 [03:20<00:59,  3.36it/s] 66%|██████▌   | 390/590 [03:20<00:59,  3.36it/s] 66%|██████▋   | 391/590 [03:20<00:59,  3.37it/s] 66%|██████▋   | 392/590 [03:20<00:58,  3.37it/s] 67%|██████▋   | 393/590 [03:21<00:58,  3.38it/s] 67%|██████▋   | 394/590 [03:21<00:57,  3.38it/s] 67%|██████▋   | 395/590 [03:21<00:57,  3.38it/s] 67%|██████▋   | 396/590 [03:22<00:57,  3.38it/s] 67%|██████▋   | 397/590 [03:22<00:56,  3.39it/s] 67%|██████▋   | 398/590 [03:22<00:56,  3.39it/s] 68%|██████▊   | 399/590 [03:22<00:56,  3.38it/s] 68%|██████▊   | 400/590 [03:23<00:56,  3.36it/s] 68%|██████▊   | 401/590 [03:23<00:56,  3.36it/s] 68%|██████▊   | 402/590 [03:23<00:55,  3.37it/s] 68%|██████▊   | 403/590 [03:24<00:55,  3.38it/s] 68%|██████▊   | 404/590 [03:24<00:55,  3.38it/s] 69%|██████▊   | 405/590 [03:24<00:54,  3.38it/s] 69%|██████▉   | 406/590 [03:25<00:54,  3.38it/s] 69%|██████▉   | 407/590 [03:25<00:54,  3.38it/s] 69%|██████▉   | 408/590 [03:25<00:53,  3.39it/s] 69%|██████▉   | 409/590 [03:25<00:53,  3.40it/s] 69%|██████▉   | 410/590 [03:26<00:52,  3.41it/s] 70%|██████▉   | 411/590 [03:26<00:52,  3.39it/s] 70%|██████▉   | 412/590 [03:26<00:52,  3.40it/s] 70%|███████   | 413/590 [03:27<00:51,  3.41it/s] 70%|███████   | 414/590 [03:27<00:51,  3.42it/s] 70%|███████   | 415/590 [03:27<00:51,  3.42it/s] 71%|███████   | 416/590 [03:28<00:50,  3.43it/s] 71%|███████   | 417/590 [03:28<00:50,  3.43it/s] 71%|███████   | 418/590 [03:28<00:50,  3.43it/s] 71%|███████   | 419/590 [03:28<00:49,  3.43it/s] 71%|███████   | 420/590 [03:29<00:49,  3.43it/s] 71%|███████▏  | 421/590 [03:29<00:49,  3.43it/s] 72%|███████▏  | 422/590 [03:29<00:50,  3.31it/s] 72%|███████▏  | 423/590 [03:30<00:49,  3.35it/s] 72%|███████▏  | 424/590 [03:30<00:49,  3.38it/s] 72%|███████▏  | 425/590 [03:30<00:48,  3.39it/s] 72%|███████▏  | 426/590 [03:30<00:48,  3.40it/s] 72%|███████▏  | 427/590 [03:31<00:47,  3.41it/s] 73%|███████▎  | 428/590 [03:31<00:47,  3.42it/s] 73%|███████▎  | 429/590 [03:31<00:47,  3.42it/s] 73%|███████▎  | 430/590 [03:32<00:46,  3.43it/s] 73%|███████▎  | 431/590 [03:32<00:46,  3.43it/s] 73%|███████▎  | 432/590 [03:32<00:46,  3.43it/s] 73%|███████▎  | 433/590 [03:33<00:46,  3.39it/s] 74%|███████▎  | 434/590 [03:33<00:45,  3.41it/s] 74%|███████▎  | 435/590 [03:33<00:45,  3.41it/s] 74%|███████▍  | 436/590 [03:33<00:45,  3.42it/s] 74%|███████▍  | 437/590 [03:34<00:44,  3.42it/s] 74%|███████▍  | 438/590 [03:34<00:44,  3.43it/s] 74%|███████▍  | 439/590 [03:34<00:44,  3.43it/s] 75%|███████▍  | 440/590 [03:35<00:43,  3.43it/s] 75%|███████▍  | 441/590 [03:35<00:43,  3.43it/s] 75%|███████▍  | 442/590 [03:35<00:43,  3.43it/s] 75%|███████▌  | 443/590 [03:35<00:42,  3.43it/s] 75%|███████▌  | 444/590 [03:36<00:42,  3.42it/s] 75%|███████▌  | 445/590 [03:36<00:42,  3.42it/s] 76%|███████▌  | 446/590 [03:36<00:42,  3.42it/s] 76%|███████▌  | 447/590 [03:37<00:41,  3.43it/s] 76%|███████▌  | 448/590 [03:37<00:41,  3.43it/s] 76%|███████▌  | 449/590 [03:37<00:41,  3.43it/s] 76%|███████▋  | 450/590 [03:37<00:40,  3.43it/s] 76%|███████▋  | 451/590 [03:38<00:40,  3.43it/s] 77%|███████▋  | 452/590 [03:38<00:40,  3.43it/s] 77%|███████▋  | 453/590 [03:38<00:39,  3.43it/s] 77%|███████▋  | 454/590 [03:39<00:39,  3.43it/s] 77%|███████▋  | 455/590 [03:39<00:39,  3.41it/s] 77%|███████▋  | 456/590 [03:39<00:39,  3.41it/s] 77%|███████▋  | 457/590 [03:40<00:38,  3.42it/s] 78%|███████▊  | 458/590 [03:40<00:38,  3.42it/s] 78%|███████▊  | 459/590 [03:40<00:38,  3.40it/s] 78%|███████▊  | 460/590 [03:40<00:38,  3.40it/s] 78%|███████▊  | 461/590 [03:41<00:37,  3.41it/s] 78%|███████▊  | 462/590 [03:41<00:37,  3.41it/s] 78%|███████▊  | 463/590 [03:41<00:37,  3.42it/s] 79%|███████▊  | 464/590 [03:42<00:36,  3.42it/s] 79%|███████▉  | 465/590 [03:42<00:36,  3.42it/s] 79%|███████▉  | 466/590 [03:42<00:36,  3.42it/s] 79%|███████▉  | 467/590 [03:42<00:35,  3.43it/s] 79%|███████▉  | 468/590 [03:43<00:35,  3.42it/s] 79%|███████▉  | 469/590 [03:43<00:35,  3.43it/s] 80%|███████▉  | 470/590 [03:43<00:35,  3.43it/s] 80%|███████▉  | 471/590 [03:44<00:34,  3.40it/s] 80%|████████  | 472/590 [03:44<00:32,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 20:06:26,462 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:06:26,462 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:06:26,462 >>   Batch size = 8
{'eval_loss': 1.0122992992401123, 'eval_runtime': 12.3606, 'eval_samples_per_second': 351.278, 'eval_steps_per_second': 43.93, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.51it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.73it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.99it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.15it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.80it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.51it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.31it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.28it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.34it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.32it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.22it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.16it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.13it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.06it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.02it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.09it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.12it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.07it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.22it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 43.96it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 43.96it/s][A
 21%|██        | 112/543 [00:02<00:09, 43.98it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 43.72it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.93it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.87it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.97it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.07it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.05it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.01it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.01it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 43.93it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.90it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.89it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.04it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.13it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.12it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.17it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.10it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.14it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.02it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.01it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.08it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.96it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.20it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.08it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.03it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.11it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.07it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.02it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.06it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.07it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.12it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.13it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.08it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.07it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.05it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.03it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.02it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.04it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.05it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.06it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.05it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.04it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.97it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.98it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.00it/s][A
 63%|██████▎   | 342/543 [00:11<00:46,  4.32it/s][A
 64%|██████▎   | 346/543 [00:11<00:36,  5.40it/s][A
 65%|██████▍   | 351/543 [00:11<00:25,  7.45it/s][A
 66%|██████▌   | 356/543 [00:11<00:18, 10.05it/s][A
 66%|██████▋   | 361/543 [00:11<00:13, 13.17it/s][A
 67%|██████▋   | 366/543 [00:11<00:10, 16.77it/s][A
 68%|██████▊   | 371/543 [00:11<00:08, 20.68it/s][A
 69%|██████▉   | 376/543 [00:12<00:06, 24.70it/s][A
 70%|███████   | 381/543 [00:12<00:05, 28.42it/s][A
 71%|███████   | 386/543 [00:12<00:04, 31.64it/s][A
 72%|███████▏  | 391/543 [00:12<00:04, 34.32it/s][A
 73%|███████▎  | 396/543 [00:12<00:04, 36.63it/s][A
 74%|███████▍  | 401/543 [00:12<00:03, 38.53it/s][A
 75%|███████▍  | 406/543 [00:12<00:03, 40.15it/s][A
 76%|███████▌  | 411/543 [00:12<00:03, 41.44it/s][A
 77%|███████▋  | 416/543 [00:13<00:02, 42.34it/s][A
 78%|███████▊  | 421/543 [00:13<00:02, 42.79it/s][A
 78%|███████▊  | 426/543 [00:13<00:02, 43.23it/s][A
 79%|███████▉  | 431/543 [00:13<00:02, 43.20it/s][A
 80%|████████  | 436/543 [00:13<00:02, 43.17it/s][A
 81%|████████  | 441/543 [00:13<00:02, 43.24it/s][A
 82%|████████▏ | 446/543 [00:13<00:02, 43.53it/s][A
 83%|████████▎ | 451/543 [00:13<00:02, 43.84it/s][A
 84%|████████▍ | 456/543 [00:13<00:01, 44.01it/s][A
 85%|████████▍ | 461/543 [00:14<00:01, 44.09it/s][A
 86%|████████▌ | 466/543 [00:14<00:01, 44.07it/s][A
 87%|████████▋ | 471/543 [00:14<00:01, 44.07it/s][A
 88%|████████▊ | 476/543 [00:14<00:01, 43.82it/s][A
 89%|████████▊ | 481/543 [00:14<00:01, 43.62it/s][A
 90%|████████▉ | 486/543 [00:14<00:01, 43.58it/s][A
 90%|█████████ | 491/543 [00:14<00:01, 43.76it/s][A
 91%|█████████▏| 496/543 [00:14<00:01, 43.95it/s][A
 92%|█████████▏| 501/543 [00:14<00:00, 44.03it/s][A
 93%|█████████▎| 506/543 [00:15<00:00, 44.19it/s][A
 94%|█████████▍| 511/543 [00:15<00:00, 44.12it/s][A
 95%|█████████▌| 516/543 [00:15<00:00, 44.00it/s][A
 96%|█████████▌| 521/543 [00:15<00:00, 43.83it/s][A
 97%|█████████▋| 526/543 [00:15<00:00, 43.70it/s][A
 98%|█████████▊| 531/543 [00:15<00:00, 43.70it/s][A
 99%|█████████▊| 536/543 [00:15<00:00, 43.82it/s][A
100%|█████████▉| 541/543 [00:15<00:00, 44.00it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:16<00:00, 44.00it/s][A 80%|████████  | 472/590 [04:00<00:32,  3.63it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:06:42,637 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-472
[INFO|configuration_utils.py:351] 2023-08-28 20:06:42,716 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-472/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:06:46,721 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-472/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:06:46,755 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-472/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:06:46,779 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-472/special_tokens_map.json
 80%|████████  | 473/590 [04:16<19:09,  9.83s/it] 80%|████████  | 474/590 [04:16<13:28,  6.97s/it] 81%|████████  | 475/590 [04:17<09:31,  4.97s/it] 81%|████████  | 476/590 [04:17<06:46,  3.57s/it] 81%|████████  | 477/590 [04:17<04:52,  2.59s/it] 81%|████████  | 478/590 [04:17<03:32,  1.90s/it] 81%|████████  | 479/590 [04:18<02:37,  1.42s/it] 81%|████████▏ | 480/590 [04:18<01:58,  1.08s/it] 82%|████████▏ | 481/590 [04:18<01:32,  1.18it/s] 82%|████████▏ | 482/590 [04:19<01:13,  1.47it/s] 82%|████████▏ | 483/590 [04:19<01:00,  1.77it/s] 82%|████████▏ | 484/590 [04:19<00:51,  2.07it/s] 82%|████████▏ | 485/590 [04:20<00:45,  2.33it/s] 82%|████████▏ | 486/590 [04:20<00:40,  2.57it/s] 83%|████████▎ | 487/590 [04:20<00:37,  2.78it/s] 83%|████████▎ | 488/590 [04:20<00:34,  2.94it/s] 83%|████████▎ | 489/590 [04:21<00:33,  3.06it/s] 83%|████████▎ | 490/590 [04:21<00:31,  3.15it/s] 83%|████████▎ | 491/590 [04:21<00:30,  3.22it/s] 83%|████████▎ | 492/590 [04:22<00:29,  3.27it/s] 84%|████████▎ | 493/590 [04:22<00:29,  3.30it/s] 84%|████████▎ | 494/590 [04:22<00:28,  3.33it/s] 84%|████████▍ | 495/590 [04:22<00:28,  3.35it/s] 84%|████████▍ | 496/590 [04:23<00:28,  3.35it/s] 84%|████████▍ | 497/590 [04:23<00:27,  3.36it/s] 84%|████████▍ | 498/590 [04:23<00:27,  3.37it/s] 85%|████████▍ | 499/590 [04:24<00:26,  3.38it/s] 85%|████████▍ | 500/590 [04:24<00:26,  3.38it/s]                                                  85%|████████▍ | 500/590 [04:24<00:26,  3.38it/s] 85%|████████▍ | 501/590 [04:24<00:26,  3.38it/s] 85%|████████▌ | 502/590 [04:25<00:25,  3.39it/s] 85%|████████▌ | 503/590 [04:25<00:25,  3.39it/s] 85%|████████▌ | 504/590 [04:25<00:25,  3.39it/s] 86%|████████▌ | 505/590 [04:25<00:25,  3.38it/s] 86%|████████▌ | 506/590 [04:26<00:24,  3.38it/s] 86%|████████▌ | 507/590 [04:26<00:24,  3.37it/s] 86%|████████▌ | 508/590 [04:26<00:24,  3.38it/s] 86%|████████▋ | 509/590 [04:27<00:23,  3.38it/s] 86%|████████▋ | 510/590 [04:27<00:23,  3.38it/s] 87%|████████▋ | 511/590 [04:27<00:23,  3.38it/s] 87%|████████▋ | 512/590 [04:27<00:23,  3.39it/s] 87%|████████▋ | 513/590 [04:28<00:22,  3.39it/s] 87%|████████▋ | 514/590 [04:28<00:22,  3.39it/s] 87%|████████▋ | 515/590 [04:28<00:22,  3.39it/s] 87%|████████▋ | 516/590 [04:29<00:21,  3.39it/s] 88%|████████▊ | 517/590 [04:29<00:21,  3.39it/s] 88%|████████▊ | 518/590 [04:29<00:21,  3.39it/s] 88%|████████▊ | 519/590 [04:30<00:20,  3.40it/s] 88%|████████▊ | 520/590 [04:30<00:20,  3.42it/s] 88%|████████▊ | 521/590 [04:30<00:20,  3.42it/s] 88%|████████▊ | 522/590 [04:30<00:19,  3.42it/s] 89%|████████▊ | 523/590 [04:31<00:19,  3.43it/s] 89%|████████▉ | 524/590 [04:31<00:19,  3.43it/s] 89%|████████▉ | 525/590 [04:31<00:18,  3.43it/s] 89%|████████▉ | 526/590 [04:32<00:18,  3.43it/s] 89%|████████▉ | 527/590 [04:32<00:18,  3.43it/s] 89%|████████▉ | 528/590 [04:32<00:18,  3.43it/s] 90%|████████▉ | 529/590 [04:32<00:17,  3.42it/s] 90%|████████▉ | 530/590 [04:33<00:17,  3.43it/s] 90%|█████████ | 531/590 [04:33<00:17,  3.43it/s] 90%|█████████ | 532/590 [04:33<00:16,  3.43it/s] 90%|█████████ | 533/590 [04:34<00:16,  3.43it/s] 91%|█████████ | 534/590 [04:34<00:16,  3.43it/s] 91%|█████████ | 535/590 [04:34<00:16,  3.43it/s] 91%|█████████ | 536/590 [04:34<00:15,  3.44it/s] 91%|█████████ | 537/590 [04:35<00:15,  3.44it/s] 91%|█████████ | 538/590 [04:35<00:15,  3.44it/s] 91%|█████████▏| 539/590 [04:35<00:14,  3.43it/s] 92%|█████████▏| 540/590 [04:36<00:14,  3.42it/s] 92%|█████████▏| 541/590 [04:36<00:14,  3.43it/s] 92%|█████████▏| 542/590 [04:36<00:14,  3.43it/s] 92%|█████████▏| 543/590 [04:37<00:13,  3.43it/s] 92%|█████████▏| 544/590 [04:37<00:13,  3.43it/s] 92%|█████████▏| 545/590 [04:37<00:13,  3.43it/s] 93%|█████████▎| 546/590 [04:37<00:12,  3.43it/s] 93%|█████████▎| 547/590 [04:38<00:12,  3.43it/s] 93%|█████████▎| 548/590 [04:38<00:12,  3.43it/s] 93%|█████████▎| 549/590 [04:38<00:11,  3.43it/s] 93%|█████████▎| 550/590 [04:39<00:11,  3.43it/s] 93%|█████████▎| 551/590 [04:39<00:11,  3.39it/s] 94%|█████████▎| 552/590 [04:39<00:11,  3.40it/s] 94%|█████████▎| 553/590 [04:39<00:10,  3.41it/s] 94%|█████████▍| 554/590 [04:40<00:10,  3.42it/s] 94%|█████████▍| 555/590 [04:40<00:10,  3.42it/s] 94%|█████████▍| 556/590 [04:40<00:09,  3.43it/s] 94%|█████████▍| 557/590 [04:41<00:09,  3.43it/s] 95%|█████████▍| 558/590 [04:41<00:09,  3.43it/s] 95%|█████████▍| 559/590 [04:41<00:09,  3.43it/s] 95%|█████████▍| 560/590 [04:42<00:08,  3.43it/s] 95%|█████████▌| 561/590 [04:42<00:08,  3.43it/s] 95%|█████████▌| 562/590 [04:42<00:08,  3.43it/s] 95%|█████████▌| 563/590 [04:42<00:07,  3.43it/s] 96%|█████████▌| 564/590 [04:43<00:07,  3.43it/s] 96%|█████████▌| 565/590 [04:43<00:07,  3.43it/s] 96%|█████████▌| 566/590 [04:43<00:06,  3.43it/s] 96%|█████████▌| 567/590 [04:44<00:06,  3.43it/s] 96%|█████████▋| 568/590 [04:44<00:06,  3.43it/s] 96%|█████████▋| 569/590 [04:44<00:06,  3.43it/s] 97%|█████████▋| 570/590 [04:44<00:05,  3.43it/s] 97%|█████████▋| 571/590 [04:45<00:05,  3.43it/s] 97%|█████████▋| 572/590 [04:45<00:05,  3.43it/s] 97%|█████████▋| 573/590 [04:45<00:04,  3.41it/s] 97%|█████████▋| 574/590 [04:46<00:04,  3.42it/s] 97%|█████████▋| 575/590 [04:46<00:04,  3.42it/s] 98%|█████████▊| 576/590 [04:46<00:04,  3.42it/s] 98%|█████████▊| 577/590 [04:46<00:03,  3.43it/s] 98%|█████████▊| 578/590 [04:47<00:03,  3.43it/s] 98%|█████████▊| 579/590 [04:47<00:03,  3.43it/s] 98%|█████████▊| 580/590 [04:47<00:02,  3.43it/s] 98%|█████████▊| 581/590 [04:48<00:02,  3.43it/s] 99%|█████████▊| 582/590 [04:48<00:02,  3.43it/s] 99%|█████████▉| 583/590 [04:48<00:02,  3.43it/s] 99%|█████████▉| 584/590 [04:49<00:01,  3.41it/s] 99%|█████████▉| 585/590 [04:49<00:01,  3.41it/s] 99%|█████████▉| 586/590 [04:49<00:01,  3.42it/s] 99%|█████████▉| 587/590 [04:49<00:00,  3.42it/s]100%|█████████▉| 588/590 [04:50<00:00,  3.43it/s]100%|█████████▉| 589/590 [04:50<00:00,  3.43it/s]100%|██████████| 590/590 [04:50<00:00,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 20:07:33,091 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:07:33,091 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:07:33,091 >>   Batch size = 8
{'eval_loss': 1.0197653770446777, 'eval_runtime': 16.0408, 'eval_samples_per_second': 270.685, 'eval_steps_per_second': 33.851, 'epoch': 4.0}
{'loss': 0.6266, 'learning_rate': 5.720338983050847e-06, 'epoch': 4.24}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.69it/s][A
  2%|▏         | 12/543 [00:00<00:10, 49.32it/s][A
  3%|▎         | 18/543 [00:00<00:11, 47.04it/s][A
  4%|▍         | 23/543 [00:00<00:11, 46.27it/s][A
  5%|▌         | 28/543 [00:00<00:11, 45.60it/s][A
  6%|▌         | 33/543 [00:00<00:11, 45.24it/s][A
  7%|▋         | 38/543 [00:00<00:11, 43.56it/s][A
  8%|▊         | 43/543 [00:00<00:11, 43.39it/s][A
  9%|▉         | 48/543 [00:01<00:11, 43.04it/s][A
 10%|▉         | 53/543 [00:01<00:11, 43.23it/s][A
 11%|█         | 58/543 [00:01<00:11, 43.66it/s][A
 12%|█▏        | 63/543 [00:01<00:10, 44.02it/s][A
 13%|█▎        | 68/543 [00:01<00:10, 44.19it/s][A
 13%|█▎        | 73/543 [00:01<00:10, 44.17it/s][A
 14%|█▍        | 78/543 [00:01<00:10, 44.23it/s][A
 15%|█▌        | 83/543 [00:01<00:10, 44.15it/s][A
 16%|█▌        | 88/543 [00:01<00:10, 43.87it/s][A
 17%|█▋        | 93/543 [00:02<00:10, 43.78it/s][A
 18%|█▊        | 98/543 [00:02<00:10, 43.93it/s][A
 19%|█▉        | 103/543 [00:02<00:09, 44.06it/s][A
 20%|█▉        | 108/543 [00:02<00:09, 44.18it/s][A
 21%|██        | 113/543 [00:02<00:09, 44.33it/s][A
 22%|██▏       | 118/543 [00:02<00:09, 44.27it/s][A
 23%|██▎       | 123/543 [00:02<00:09, 44.26it/s][A
 24%|██▎       | 128/543 [00:02<00:09, 44.07it/s][A
 24%|██▍       | 133/543 [00:02<00:09, 43.96it/s][A
 25%|██▌       | 138/543 [00:03<00:09, 43.86it/s][A
 26%|██▋       | 143/543 [00:03<00:09, 44.00it/s][A
 27%|██▋       | 148/543 [00:03<00:08, 44.05it/s][A
 28%|██▊       | 153/543 [00:03<00:08, 44.10it/s][A
 29%|██▉       | 158/543 [00:03<00:08, 44.31it/s][A
 30%|███       | 163/543 [00:03<00:08, 44.29it/s][A
 31%|███       | 168/543 [00:03<00:08, 44.11it/s][A
 32%|███▏      | 173/543 [00:03<00:08, 44.02it/s][A
 33%|███▎      | 178/543 [00:04<00:08, 43.99it/s][A
 34%|███▎      | 183/543 [00:04<00:08, 43.94it/s][A
 35%|███▍      | 188/543 [00:04<00:08, 44.09it/s][A
 36%|███▌      | 193/543 [00:04<00:07, 44.18it/s][A
 36%|███▋      | 198/543 [00:04<00:07, 44.22it/s][A
 37%|███▋      | 203/543 [00:04<00:07, 44.17it/s][A
 38%|███▊      | 208/543 [00:04<00:07, 44.29it/s][A
 39%|███▉      | 213/543 [00:04<00:07, 43.78it/s][A
 40%|████      | 218/543 [00:04<00:07, 43.98it/s][A
 41%|████      | 223/543 [00:05<00:07, 43.92it/s][A
 42%|████▏     | 228/543 [00:05<00:07, 43.78it/s][A
 43%|████▎     | 233/543 [00:05<00:07, 43.96it/s][A
 44%|████▍     | 238/543 [00:05<00:06, 44.14it/s][A
 45%|████▍     | 243/543 [00:05<00:06, 44.16it/s][A
 46%|████▌     | 248/543 [00:05<00:06, 44.19it/s][A
 47%|████▋     | 253/543 [00:05<00:06, 44.16it/s][A
 48%|████▊     | 258/543 [00:05<00:06, 44.17it/s][A
 48%|████▊     | 263/543 [00:05<00:06, 44.06it/s][A
 49%|████▉     | 268/543 [00:06<00:06, 44.00it/s][A
 50%|█████     | 273/543 [00:06<00:06, 43.95it/s][A
 51%|█████     | 278/543 [00:06<00:06, 44.04it/s][A
 52%|█████▏    | 283/543 [00:06<00:05, 44.17it/s][A
 53%|█████▎    | 288/543 [00:06<00:05, 44.16it/s][A
 54%|█████▍    | 293/543 [00:06<00:05, 44.12it/s][A
 55%|█████▍    | 298/543 [00:06<00:05, 44.15it/s][A
 56%|█████▌    | 303/543 [00:06<00:05, 44.09it/s][A
 57%|█████▋    | 308/543 [00:06<00:05, 43.95it/s][A
 58%|█████▊    | 313/543 [00:07<00:05, 44.00it/s][A
 59%|█████▊    | 318/543 [00:07<00:05, 44.00it/s][A
 59%|█████▉    | 323/543 [00:07<00:04, 44.08it/s][A
 60%|██████    | 328/543 [00:07<00:04, 44.20it/s][A
 61%|██████▏   | 333/543 [00:07<00:04, 44.14it/s][A
 62%|██████▏   | 338/543 [00:07<00:04, 44.25it/s][A
 63%|██████▎   | 343/543 [00:07<00:04, 44.23it/s][A
 64%|██████▍   | 348/543 [00:07<00:04, 44.08it/s][A
 65%|██████▌   | 353/543 [00:07<00:04, 44.03it/s][A
 66%|██████▌   | 358/543 [00:08<00:04, 43.96it/s][A
 67%|██████▋   | 363/543 [00:08<00:04, 44.04it/s][A
 68%|██████▊   | 368/543 [00:08<00:03, 44.14it/s][A
 69%|██████▊   | 373/543 [00:08<00:03, 44.07it/s][A
 70%|██████▉   | 378/543 [00:08<00:03, 44.10it/s][A
 71%|███████   | 383/543 [00:08<00:03, 44.19it/s][A
 71%|███████▏  | 388/543 [00:08<00:03, 44.14it/s][A
 72%|███████▏  | 393/543 [00:08<00:03, 44.04it/s][A
 73%|███████▎  | 398/543 [00:09<00:03, 44.03it/s][A
 74%|███████▍  | 403/543 [00:09<00:03, 44.09it/s][A
 75%|███████▌  | 408/543 [00:09<00:03, 44.14it/s][A
 76%|███████▌  | 413/543 [00:09<00:02, 44.18it/s][A
 77%|███████▋  | 418/543 [00:09<00:02, 44.12it/s][A
 78%|███████▊  | 423/543 [00:09<00:02, 44.16it/s][A
 79%|███████▉  | 428/543 [00:09<00:02, 44.27it/s][A
 80%|███████▉  | 433/543 [00:09<00:02, 44.13it/s][A
 81%|████████  | 438/543 [00:09<00:02, 44.03it/s][A
 82%|████████▏ | 443/543 [00:10<00:02, 43.94it/s][A
 83%|████████▎ | 448/543 [00:10<00:02, 44.07it/s][A
 83%|████████▎ | 453/543 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 458/543 [00:10<00:01, 44.16it/s][A
 85%|████████▌ | 463/543 [00:10<00:01, 44.09it/s][A
 86%|████████▌ | 468/543 [00:10<00:01, 44.12it/s][A
 87%|████████▋ | 473/543 [00:10<00:01, 44.16it/s][A
 88%|████████▊ | 478/543 [00:10<00:01, 44.13it/s][A
 89%|████████▉ | 483/543 [00:10<00:01, 44.05it/s][A
 90%|████████▉ | 488/543 [00:11<00:01, 43.98it/s][A
 91%|█████████ | 493/543 [00:11<00:01, 44.02it/s][A
 92%|█████████▏| 498/543 [00:11<00:01, 43.99it/s][A
 93%|█████████▎| 503/543 [00:11<00:00, 44.05it/s][A
 94%|█████████▎| 508/543 [00:11<00:00, 44.09it/s][A
 94%|█████████▍| 513/543 [00:11<00:00, 44.15it/s][A
 95%|█████████▌| 518/543 [00:11<00:00, 44.24it/s][A
 96%|█████████▋| 523/543 [00:11<00:00, 44.14it/s][A
 97%|█████████▋| 528/543 [00:11<00:00, 44.06it/s][A
 98%|█████████▊| 533/543 [00:12<00:00, 44.02it/s][A
 99%|█████████▉| 538/543 [00:12<00:00, 44.09it/s][A
100%|██████████| 543/543 [00:12<00:00, 44.06it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.06it/s][A100%|██████████| 590/590 [05:03<00:00,  3.65it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:07:45,955 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-590
[INFO|configuration_utils.py:351] 2023-08-28 20:07:46,035 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-590/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:07:50,542 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-590/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:07:50,583 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-590/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:07:50,612 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-590/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:08:00,051 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:08:00,061 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-118 (score: 0.9793775081634521).
                                                 100%|██████████| 590/590 [05:24<00:00,  3.65it/s]100%|██████████| 590/590 [05:24<00:00,  1.82it/s]
[INFO|trainer.py:1894] 2023-08-28 20:08:06,134 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 20:08:06,156 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:08:11,719 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:08:11,739 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:08:11,747 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:08:12,070 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:12,071 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:12,071 >>   train_loss               =     0.6219
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:12,071 >>   train_runtime            = 0:05:23.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:12,071 >>   train_samples            =       7537
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:12,071 >>   train_samples_per_second =    116.314
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:12,071 >>   train_steps_per_second   =      1.821
{'eval_loss': 1.0213325023651123, 'eval_runtime': 12.3367, 'eval_samples_per_second': 351.958, 'eval_steps_per_second': 44.015, 'epoch': 5.0}
{'train_runtime': 323.9943, 'train_samples_per_second': 116.314, 'train_steps_per_second': 1.821, 'train_loss': 0.6218884936833786, 'epoch': 5.0}
08/28/2023 20:08:12 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:08:12,139 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:08:12,139 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 20:08:12,139 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 56.23it/s]  2%|▏         | 12/543 [00:00<00:10, 48.87it/s]  3%|▎         | 17/543 [00:00<00:11, 47.40it/s]  4%|▍         | 22/543 [00:00<00:11, 46.62it/s]  5%|▍         | 27/543 [00:00<00:11, 46.07it/s]  6%|▌         | 32/543 [00:00<00:11, 45.74it/s]  7%|▋         | 37/543 [00:00<00:11, 45.64it/s]  8%|▊         | 42/543 [00:00<00:11, 45.04it/s]  9%|▊         | 47/543 [00:01<00:11, 44.47it/s] 10%|▉         | 52/543 [00:01<00:11, 44.11it/s] 10%|█         | 57/543 [00:01<00:11, 44.14it/s] 11%|█▏        | 62/543 [00:01<00:10, 44.36it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.54it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.80it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.93it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.82it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.54it/s] 17%|█▋        | 92/543 [00:02<00:10, 44.26it/s] 18%|█▊        | 97/543 [00:02<00:10, 44.03it/s] 19%|█▉        | 102/543 [00:02<00:10, 44.08it/s] 20%|█▉        | 107/543 [00:02<00:09, 44.22it/s] 21%|██        | 112/543 [00:02<00:09, 44.46it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.58it/s] 22%|██▏       | 122/543 [00:02<00:09, 42.53it/s] 23%|██▎       | 127/543 [00:02<00:09, 42.97it/s] 24%|██▍       | 132/543 [00:02<00:09, 43.57it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.73it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.86it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.88it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.09it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.21it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.14it/s] 31%|███       | 167/543 [00:03<00:08, 44.34it/s] 32%|███▏      | 172/543 [00:03<00:08, 44.28it/s] 33%|███▎      | 177/543 [00:03<00:08, 44.26it/s] 34%|███▎      | 182/543 [00:04<00:08, 44.32it/s] 34%|███▍      | 187/543 [00:04<00:08, 44.26it/s] 35%|███▌      | 192/543 [00:04<00:07, 44.15it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.26it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.28it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.29it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.27it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.34it/s] 41%|████      | 222/543 [00:04<00:07, 44.30it/s] 42%|████▏     | 227/543 [00:05<00:07, 44.36it/s] 43%|████▎     | 232/543 [00:05<00:07, 44.16it/s] 44%|████▎     | 237/543 [00:05<00:06, 44.19it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.26it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.20it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.30it/s] 47%|████▋     | 257/543 [00:05<00:06, 43.14it/s] 48%|████▊     | 262/543 [00:05<00:06, 43.61it/s] 49%|████▉     | 267/543 [00:06<00:06, 43.72it/s] 50%|█████     | 272/543 [00:06<00:06, 43.86it/s] 51%|█████     | 277/543 [00:06<00:06, 43.97it/s] 52%|█████▏    | 282/543 [00:06<00:05, 43.95it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.03it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.11it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.11it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.31it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.35it/s] 57%|█████▋    | 312/543 [00:07<00:05, 44.33it/s] 58%|█████▊    | 317/543 [00:07<00:05, 44.32it/s] 59%|█████▉    | 322/543 [00:07<00:04, 44.20it/s] 60%|██████    | 327/543 [00:07<00:04, 44.10it/s] 61%|██████    | 332/543 [00:07<00:04, 44.24it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.05it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.15it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.34it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.36it/s] 66%|██████▌   | 357/543 [00:08<00:04, 44.25it/s] 67%|██████▋   | 362/543 [00:08<00:04, 44.28it/s] 68%|██████▊   | 367/543 [00:08<00:03, 44.27it/s] 69%|██████▊   | 372/543 [00:08<00:03, 44.19it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.15it/s] 70%|███████   | 382/543 [00:08<00:03, 44.21it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.19it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.42it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.32it/s] 74%|███████▍  | 402/543 [00:09<00:03, 44.28it/s] 75%|███████▍  | 407/543 [00:09<00:03, 44.23it/s] 76%|███████▌  | 412/543 [00:09<00:02, 44.18it/s] 77%|███████▋  | 417/543 [00:09<00:02, 44.15it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.04it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.19it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.31it/s] 80%|████████  | 437/543 [00:09<00:02, 44.26it/s] 81%|████████▏ | 442/543 [00:09<00:02, 44.31it/s] 82%|████████▏ | 447/543 [00:10<00:02, 44.29it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.22it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.15it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.09it/s] 86%|████████▌ | 467/543 [00:10<00:01, 43.99it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.08it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.26it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.32it/s] 90%|████████▉ | 487/543 [00:10<00:01, 44.22it/s] 91%|█████████ | 492/543 [00:11<00:01, 44.23it/s] 92%|█████████▏| 497/543 [00:11<00:01, 44.25it/s] 92%|█████████▏| 502/543 [00:11<00:00, 44.07it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.20it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.09it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.10it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.29it/s] 97%|█████████▋| 527/543 [00:11<00:00, 44.18it/s] 98%|█████████▊| 532/543 [00:12<00:00, 44.23it/s] 99%|█████████▉| 537/543 [00:12<00:00, 44.17it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.16it/s]100%|██████████| 543/543 [00:12<00:00, 44.28it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:08:24,419 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:24,420 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:24,420 >>   eval_loss               =     0.9794
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:24,420 >>   eval_runtime            = 0:00:12.28
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:24,420 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:24,420 >>   eval_samples_per_second =    353.577
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:24,420 >>   eval_steps_per_second   =     44.218
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:08:24,420 >>   perplexity              =     2.6628
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:31,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:31,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:31,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:31,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:31,715 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:08:32,074 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:08:32,075 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:08:32,350 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:08:33,485 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:08:33,485 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:36,366 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:36,374 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:36,374 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:36,374 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:08:36,375 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:08:37,078 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:08:37,080 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:08:37,658 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:08:37,834 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:08:37,834 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-118
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-590
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-472
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-354
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-236
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.73it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.77it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.67it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.75it/s]Extractor Predicting: 10it [00:05,  1.76it/s]Extractor Predicting: 11it [00:06,  1.78it/s]Extractor Predicting: 12it [00:06,  1.79it/s]Extractor Predicting: 13it [00:07,  1.70it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:08,  1.61it/s]Extractor Predicting: 16it [00:09,  1.61it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:10,  1.55it/s]Extractor Predicting: 19it [00:11,  1.51it/s]Extractor Predicting: 20it [00:12,  1.50it/s]Extractor Predicting: 21it [00:12,  1.53it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:15,  1.57it/s]Extractor Predicting: 27it [00:16,  1.56it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:17,  1.55it/s]Extractor Predicting: 30it [00:18,  1.56it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.48it/s]Extractor Predicting: 34it [00:21,  1.48it/s]Extractor Predicting: 35it [00:21,  1.49it/s]Extractor Predicting: 36it [00:22,  1.48it/s]Extractor Predicting: 37it [00:23,  1.48it/s]Extractor Predicting: 38it [00:23,  1.49it/s]Extractor Predicting: 39it [00:24,  1.52it/s]Extractor Predicting: 40it [00:25,  1.54it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:26,  1.51it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.57it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:30,  1.55it/s]Extractor Predicting: 50it [00:31,  1.57it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:32,  1.56it/s]Extractor Predicting: 53it [00:33,  1.53it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:34,  1.59it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:36,  1.59it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:37,  1.55it/s]Extractor Predicting: 61it [00:38,  1.57it/s]Extractor Predicting: 62it [00:39,  1.58it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:40,  1.58it/s]Extractor Predicting: 65it [00:41,  1.62it/s]Extractor Predicting: 66it [00:41,  1.63it/s]Extractor Predicting: 67it [00:42,  1.62it/s]Extractor Predicting: 68it [00:42,  1.59it/s]Extractor Predicting: 69it [00:43,  1.60it/s]Extractor Predicting: 70it [00:44,  1.59it/s]Extractor Predicting: 71it [00:44,  1.58it/s]Extractor Predicting: 72it [00:45,  1.59it/s]Extractor Predicting: 73it [00:46,  1.58it/s]Extractor Predicting: 74it [00:46,  1.55it/s]Extractor Predicting: 75it [00:47,  1.52it/s]Extractor Predicting: 76it [00:48,  1.55it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:49,  1.57it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:50,  1.57it/s]Extractor Predicting: 81it [00:51,  1.57it/s]Extractor Predicting: 82it [00:51,  1.58it/s]Extractor Predicting: 83it [00:52,  1.59it/s]Extractor Predicting: 84it [00:53,  1.60it/s]Extractor Predicting: 85it [00:53,  1.61it/s]Extractor Predicting: 86it [00:54,  1.59it/s]Extractor Predicting: 87it [00:55,  1.59it/s]Extractor Predicting: 88it [00:55,  1.61it/s]Extractor Predicting: 89it [00:56,  1.62it/s]Extractor Predicting: 90it [00:56,  1.60it/s]Extractor Predicting: 91it [00:57,  1.54it/s]Extractor Predicting: 92it [00:58,  1.53it/s]Extractor Predicting: 93it [00:58,  1.59it/s]Extractor Predicting: 94it [00:59,  1.60it/s]Extractor Predicting: 95it [01:00,  1.59it/s]Extractor Predicting: 96it [01:00,  1.59it/s]Extractor Predicting: 97it [01:01,  1.62it/s]Extractor Predicting: 98it [01:01,  1.60it/s]Extractor Predicting: 99it [01:02,  1.59it/s]Extractor Predicting: 100it [01:03,  1.59it/s]Extractor Predicting: 101it [01:03,  1.59it/s]Extractor Predicting: 102it [01:04,  1.59it/s]Extractor Predicting: 103it [01:05,  1.60it/s]Extractor Predicting: 104it [01:05,  1.61it/s]Extractor Predicting: 105it [01:06,  1.58it/s]Extractor Predicting: 106it [01:06,  1.60it/s]Extractor Predicting: 107it [01:07,  1.60it/s]Extractor Predicting: 108it [01:08,  1.60it/s]Extractor Predicting: 109it [01:08,  1.59it/s]Extractor Predicting: 110it [01:09,  1.58it/s]Extractor Predicting: 111it [01:10,  1.59it/s]Extractor Predicting: 112it [01:10,  1.62it/s]Extractor Predicting: 113it [01:11,  1.62it/s]Extractor Predicting: 114it [01:11,  1.61it/s]Extractor Predicting: 115it [01:12,  1.60it/s]Extractor Predicting: 116it [01:13,  1.59it/s]Extractor Predicting: 117it [01:13,  1.61it/s]Extractor Predicting: 118it [01:14,  1.57it/s]Extractor Predicting: 119it [01:15,  1.58it/s]Extractor Predicting: 120it [01:15,  1.57it/s]Extractor Predicting: 121it [01:16,  1.54it/s]Extractor Predicting: 122it [01:17,  1.54it/s]Extractor Predicting: 123it [01:17,  1.58it/s]Extractor Predicting: 124it [01:18,  1.59it/s]Extractor Predicting: 125it [01:18,  1.61it/s]Extractor Predicting: 126it [01:19,  1.56it/s]Extractor Predicting: 127it [01:20,  1.56it/s]Extractor Predicting: 128it [01:20,  1.58it/s]Extractor Predicting: 129it [01:21,  1.59it/s]Extractor Predicting: 130it [01:22,  1.64it/s]Extractor Predicting: 131it [01:22,  1.59it/s]Extractor Predicting: 132it [01:23,  1.59it/s]Extractor Predicting: 133it [01:23,  1.59it/s]Extractor Predicting: 134it [01:24,  1.60it/s]Extractor Predicting: 135it [01:25,  1.60it/s]Extractor Predicting: 136it [01:25,  1.60it/s]Extractor Predicting: 137it [01:26,  1.62it/s]Extractor Predicting: 138it [01:27,  1.58it/s]Extractor Predicting: 139it [01:27,  1.57it/s]Extractor Predicting: 140it [01:28,  1.60it/s]Extractor Predicting: 141it [01:28,  1.60it/s]Extractor Predicting: 142it [01:29,  1.63it/s]Extractor Predicting: 143it [01:30,  1.64it/s]Extractor Predicting: 144it [01:30,  1.48it/s]Extractor Predicting: 145it [01:31,  1.49it/s]Extractor Predicting: 146it [01:32,  1.52it/s]Extractor Predicting: 147it [01:32,  1.54it/s]Extractor Predicting: 148it [01:33,  1.55it/s]Extractor Predicting: 149it [01:34,  1.58it/s]Extractor Predicting: 150it [01:34,  1.51it/s]Extractor Predicting: 151it [01:35,  1.54it/s]Extractor Predicting: 152it [01:36,  1.55it/s]Extractor Predicting: 153it [01:36,  1.54it/s]Extractor Predicting: 154it [01:37,  1.56it/s]Extractor Predicting: 155it [01:38,  1.54it/s]Extractor Predicting: 156it [01:38,  1.53it/s]Extractor Predicting: 157it [01:39,  1.54it/s]Extractor Predicting: 158it [01:39,  1.55it/s]Extractor Predicting: 159it [01:40,  1.57it/s]Extractor Predicting: 160it [01:41,  1.59it/s]Extractor Predicting: 161it [01:41,  1.60it/s]Extractor Predicting: 162it [01:42,  1.61it/s]Extractor Predicting: 163it [01:43,  1.51it/s]Extractor Predicting: 163it [01:43,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:31,378 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:31,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:31,381 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:31,381 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:31,381 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:10:32,139 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:10:32,157 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:10:32,719 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:10:33,749 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:10:33,750 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:35,811 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:35,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:35,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:35,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:10:35,826 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:10:36,225 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:10:36,227 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:10:36,507 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:10:36,691 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:10:36,691 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.14457831325301204,
  "recall": 0.011054813450023031,
  "score": 0.02053915275994865,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.63it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.62it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.67it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.66it/s]Extractor Predicting: 33it [00:20,  1.66it/s]Extractor Predicting: 34it [00:20,  1.64it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:22,  1.51it/s]Extractor Predicting: 37it [00:22,  1.54it/s]Extractor Predicting: 38it [00:23,  1.55it/s]Extractor Predicting: 39it [00:23,  1.62it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:25,  1.55it/s]Extractor Predicting: 42it [00:25,  1.56it/s]Extractor Predicting: 43it [00:26,  1.56it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:27,  1.54it/s]Extractor Predicting: 46it [00:28,  1.53it/s]Extractor Predicting: 47it [00:29,  1.51it/s]Extractor Predicting: 48it [00:29,  1.54it/s]Extractor Predicting: 49it [00:30,  1.53it/s]Extractor Predicting: 50it [00:31,  1.53it/s]Extractor Predicting: 51it [00:31,  1.54it/s]Extractor Predicting: 52it [00:32,  1.54it/s]Extractor Predicting: 53it [00:32,  1.52it/s]Extractor Predicting: 54it [00:33,  1.48it/s]Extractor Predicting: 55it [00:34,  1.52it/s]Extractor Predicting: 56it [00:34,  1.57it/s]Extractor Predicting: 57it [00:35,  1.55it/s]Extractor Predicting: 58it [00:36,  1.55it/s]Extractor Predicting: 59it [00:36,  1.52it/s]Extractor Predicting: 60it [00:37,  1.52it/s]Extractor Predicting: 61it [00:38,  1.55it/s]Extractor Predicting: 62it [00:38,  1.55it/s]Extractor Predicting: 63it [00:39,  1.50it/s]Extractor Predicting: 64it [00:40,  1.52it/s]Extractor Predicting: 65it [00:40,  1.51it/s]Extractor Predicting: 66it [00:41,  1.50it/s]Extractor Predicting: 67it [00:42,  1.52it/s]Extractor Predicting: 68it [00:42,  1.50it/s]Extractor Predicting: 69it [00:43,  1.52it/s]Extractor Predicting: 70it [00:44,  1.52it/s]Extractor Predicting: 71it [00:44,  1.54it/s]Extractor Predicting: 72it [00:45,  1.54it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:48,  1.60it/s]Extractor Predicting: 78it [00:49,  1.59it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:50,  1.58it/s]Extractor Predicting: 81it [00:51,  1.46it/s]Extractor Predicting: 82it [00:51,  1.51it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:53,  1.52it/s]Extractor Predicting: 85it [00:53,  1.53it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:55,  1.58it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:56,  1.60it/s]Extractor Predicting: 90it [00:56,  1.60it/s]Extractor Predicting: 91it [00:57,  1.63it/s]Extractor Predicting: 92it [00:58,  1.61it/s]Extractor Predicting: 93it [00:58,  1.62it/s]Extractor Predicting: 94it [00:59,  1.60it/s]Extractor Predicting: 95it [01:00,  1.62it/s]Extractor Predicting: 96it [01:00,  1.64it/s]Extractor Predicting: 97it [01:01,  1.63it/s]Extractor Predicting: 98it [01:01,  1.57it/s]Extractor Predicting: 99it [01:02,  1.57it/s]Extractor Predicting: 100it [01:03,  1.57it/s]Extractor Predicting: 101it [01:03,  1.56it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:05,  1.56it/s]Extractor Predicting: 104it [01:05,  1.59it/s]Extractor Predicting: 105it [01:06,  1.58it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:07,  1.48it/s]Extractor Predicting: 108it [01:08,  1.52it/s]Extractor Predicting: 109it [01:09,  1.48it/s]Extractor Predicting: 110it [01:09,  1.50it/s]Extractor Predicting: 111it [01:10,  1.51it/s]Extractor Predicting: 112it [01:11,  1.53it/s]Extractor Predicting: 113it [01:11,  1.52it/s]Extractor Predicting: 114it [01:12,  1.54it/s]Extractor Predicting: 115it [01:13,  1.52it/s]Extractor Predicting: 116it [01:13,  1.51it/s]Extractor Predicting: 117it [01:14,  1.45it/s]Extractor Predicting: 118it [01:15,  1.52it/s]Extractor Predicting: 119it [01:15,  1.53it/s]Extractor Predicting: 120it [01:16,  1.59it/s]Extractor Predicting: 121it [01:16,  1.58it/s]Extractor Predicting: 122it [01:17,  1.59it/s]Extractor Predicting: 123it [01:18,  1.62it/s]Extractor Predicting: 124it [01:18,  1.65it/s]Extractor Predicting: 125it [01:19,  1.63it/s]Extractor Predicting: 126it [01:20,  1.43it/s]Extractor Predicting: 127it [01:20,  1.46it/s]Extractor Predicting: 128it [01:21,  1.49it/s]Extractor Predicting: 129it [01:22,  1.51it/s]Extractor Predicting: 130it [01:22,  1.53it/s]Extractor Predicting: 131it [01:23,  1.60it/s]Extractor Predicting: 132it [01:23,  1.58it/s]Extractor Predicting: 133it [01:24,  1.59it/s]Extractor Predicting: 134it [01:25,  1.62it/s]Extractor Predicting: 135it [01:25,  1.62it/s]Extractor Predicting: 136it [01:26,  1.62it/s]Extractor Predicting: 137it [01:27,  1.66it/s]Extractor Predicting: 138it [01:27,  1.64it/s]Extractor Predicting: 139it [01:28,  1.65it/s]Extractor Predicting: 140it [01:28,  1.62it/s]Extractor Predicting: 141it [01:29,  1.64it/s]Extractor Predicting: 142it [01:30,  1.61it/s]Extractor Predicting: 143it [01:30,  1.59it/s]Extractor Predicting: 144it [01:31,  1.59it/s]Extractor Predicting: 145it [01:32,  1.55it/s]Extractor Predicting: 146it [01:32,  1.50it/s]Extractor Predicting: 147it [01:33,  1.53it/s]Extractor Predicting: 148it [01:34,  1.52it/s]Extractor Predicting: 149it [01:34,  1.55it/s]Extractor Predicting: 150it [01:35,  1.47it/s]Extractor Predicting: 151it [01:36,  1.50it/s]Extractor Predicting: 152it [01:36,  1.51it/s]Extractor Predicting: 153it [01:37,  1.55it/s]Extractor Predicting: 154it [01:37,  1.58it/s]Extractor Predicting: 155it [01:38,  1.57it/s]Extractor Predicting: 156it [01:39,  1.54it/s]Extractor Predicting: 157it [01:39,  1.56it/s]Extractor Predicting: 158it [01:40,  1.57it/s]Extractor Predicting: 159it [01:41,  1.57it/s]Extractor Predicting: 160it [01:41,  1.59it/s]Extractor Predicting: 161it [01:42,  1.52it/s]Extractor Predicting: 162it [01:43,  1.58it/s]Extractor Predicting: 163it [01:43,  1.61it/s]Extractor Predicting: 164it [01:44,  1.59it/s]Extractor Predicting: 165it [01:44,  1.65it/s]Extractor Predicting: 166it [01:45,  1.66it/s]Extractor Predicting: 167it [01:46,  1.65it/s]Extractor Predicting: 168it [01:46,  1.70it/s]Extractor Predicting: 169it [01:47,  1.71it/s]Extractor Predicting: 170it [01:47,  1.67it/s]Extractor Predicting: 171it [01:48,  1.65it/s]Extractor Predicting: 172it [01:49,  1.64it/s]Extractor Predicting: 173it [01:49,  1.61it/s]Extractor Predicting: 174it [01:50,  1.66it/s]Extractor Predicting: 175it [01:50,  1.65it/s]Extractor Predicting: 176it [01:51,  1.62it/s]Extractor Predicting: 177it [01:52,  1.60it/s]Extractor Predicting: 178it [01:52,  1.61it/s]Extractor Predicting: 179it [01:53,  1.68it/s]Extractor Predicting: 180it [01:53,  1.66it/s]Extractor Predicting: 181it [01:54,  1.65it/s]Extractor Predicting: 182it [01:55,  1.63it/s]Extractor Predicting: 183it [01:55,  1.62it/s]Extractor Predicting: 184it [01:56,  1.61it/s]Extractor Predicting: 185it [01:57,  1.64it/s]Extractor Predicting: 186it [01:57,  1.61it/s]Extractor Predicting: 187it [01:58,  1.61it/s]Extractor Predicting: 188it [01:58,  1.59it/s]Extractor Predicting: 189it [01:59,  1.58it/s]Extractor Predicting: 190it [02:00,  1.55it/s]Extractor Predicting: 191it [02:00,  1.55it/s]Extractor Predicting: 192it [02:01,  1.58it/s]Extractor Predicting: 193it [02:02,  1.61it/s]Extractor Predicting: 194it [02:02,  1.62it/s]Extractor Predicting: 195it [02:03,  1.59it/s]Extractor Predicting: 196it [02:03,  1.61it/s]Extractor Predicting: 197it [02:04,  1.60it/s]Extractor Predicting: 198it [02:05,  1.61it/s]Extractor Predicting: 199it [02:05,  1.60it/s]Extractor Predicting: 200it [02:06,  1.59it/s]Extractor Predicting: 201it [02:07,  1.59it/s]Extractor Predicting: 202it [02:07,  1.60it/s]Extractor Predicting: 203it [02:08,  1.60it/s]Extractor Predicting: 204it [02:08,  1.58it/s]Extractor Predicting: 205it [02:09,  1.57it/s]Extractor Predicting: 206it [02:10,  1.58it/s]Extractor Predicting: 207it [02:10,  1.60it/s]Extractor Predicting: 208it [02:11,  1.61it/s]Extractor Predicting: 209it [02:12,  1.60it/s]Extractor Predicting: 210it [02:12,  1.60it/s]Extractor Predicting: 211it [02:13,  1.59it/s]Extractor Predicting: 212it [02:13,  1.61it/s]Extractor Predicting: 213it [02:14,  1.60it/s]Extractor Predicting: 214it [02:15,  1.59it/s]Extractor Predicting: 215it [02:15,  1.55it/s]Extractor Predicting: 216it [02:16,  1.58it/s]Extractor Predicting: 217it [02:17,  1.61it/s]Extractor Predicting: 218it [02:17,  1.61it/s]Extractor Predicting: 219it [02:18,  1.58it/s]Extractor Predicting: 220it [02:19,  1.60it/s]Extractor Predicting: 221it [02:19,  1.58it/s]Extractor Predicting: 222it [02:20,  1.60it/s]Extractor Predicting: 223it [02:20,  1.53it/s]Extractor Predicting: 224it [02:21,  1.55it/s]Extractor Predicting: 225it [02:22,  1.53it/s]Extractor Predicting: 226it [02:22,  1.54it/s]Extractor Predicting: 227it [02:23,  1.50it/s]Extractor Predicting: 228it [02:24,  1.46it/s]Extractor Predicting: 229it [02:25,  1.48it/s]Extractor Predicting: 230it [02:25,  1.50it/s]Extractor Predicting: 231it [02:26,  1.51it/s]Extractor Predicting: 232it [02:26,  1.52it/s]Extractor Predicting: 233it [02:27,  1.51it/s]Extractor Predicting: 234it [02:28,  1.53it/s]Extractor Predicting: 235it [02:28,  1.54it/s]Extractor Predicting: 236it [02:29,  1.56it/s]Extractor Predicting: 237it [02:30,  1.56it/s]Extractor Predicting: 238it [02:31,  1.44it/s]Extractor Predicting: 239it [02:31,  1.50it/s]Extractor Predicting: 240it [02:32,  1.58it/s]Extractor Predicting: 241it [02:32,  1.60it/s]Extractor Predicting: 242it [02:33,  1.61it/s]Extractor Predicting: 243it [02:34,  1.57it/s]Extractor Predicting: 244it [02:34,  1.59it/s]Extractor Predicting: 245it [02:35,  1.62it/s]Extractor Predicting: 246it [02:35,  1.61it/s]Extractor Predicting: 247it [02:36,  1.63it/s]Extractor Predicting: 248it [02:37,  1.64it/s]Extractor Predicting: 249it [02:37,  1.69it/s]Extractor Predicting: 250it [02:38,  1.69it/s]Extractor Predicting: 251it [02:38,  1.75it/s]Extractor Predicting: 252it [02:39,  1.76it/s]Extractor Predicting: 253it [02:39,  1.73it/s]Extractor Predicting: 254it [02:40,  1.68it/s]Extractor Predicting: 255it [02:41,  1.64it/s]Extractor Predicting: 256it [02:41,  1.69it/s]Extractor Predicting: 257it [02:42,  1.66it/s]Extractor Predicting: 258it [02:43,  1.62it/s]Extractor Predicting: 259it [02:43,  1.54it/s]Extractor Predicting: 260it [02:44,  1.61it/s]Extractor Predicting: 261it [02:44,  1.60it/s]Extractor Predicting: 262it [02:45,  1.61it/s]Extractor Predicting: 263it [02:46,  1.66it/s]Extractor Predicting: 264it [02:46,  1.67it/s]Extractor Predicting: 265it [02:47,  1.67it/s]Extractor Predicting: 266it [02:47,  1.69it/s]Extractor Predicting: 267it [02:48,  1.72it/s]Extractor Predicting: 268it [02:49,  1.71it/s]Extractor Predicting: 269it [02:49,  1.68it/s]Extractor Predicting: 270it [02:50,  1.54it/s]Extractor Predicting: 271it [02:51,  1.58it/s]Extractor Predicting: 272it [02:51,  1.59it/s]Extractor Predicting: 273it [02:52,  1.65it/s]Extractor Predicting: 274it [02:52,  1.67it/s]Extractor Predicting: 275it [02:53,  1.45it/s]Extractor Predicting: 276it [02:54,  1.50it/s]Extractor Predicting: 277it [02:54,  1.55it/s]Extractor Predicting: 278it [02:55,  1.61it/s]Extractor Predicting: 279it [02:56,  1.59it/s]Extractor Predicting: 280it [02:56,  1.67it/s]Extractor Predicting: 281it [02:57,  1.69it/s]Extractor Predicting: 282it [02:57,  1.71it/s]Extractor Predicting: 283it [02:58,  1.73it/s]Extractor Predicting: 284it [02:58,  1.72it/s]Extractor Predicting: 285it [02:59,  1.73it/s]Extractor Predicting: 286it [03:00,  1.73it/s]Extractor Predicting: 287it [03:00,  1.73it/s]Extractor Predicting: 288it [03:01,  1.69it/s]Extractor Predicting: 289it [03:01,  1.69it/s]Extractor Predicting: 290it [03:02,  1.61it/s]Extractor Predicting: 291it [03:03,  1.64it/s]Extractor Predicting: 292it [03:03,  1.68it/s]Extractor Predicting: 293it [03:04,  1.69it/s]Extractor Predicting: 294it [03:04,  1.70it/s]Extractor Predicting: 295it [03:05,  1.71it/s]Extractor Predicting: 296it [03:06,  1.63it/s]Extractor Predicting: 297it [03:06,  1.62it/s]Extractor Predicting: 298it [03:07,  1.56it/s]Extractor Predicting: 299it [03:08,  1.56it/s]Extractor Predicting: 300it [03:08,  1.58it/s]Extractor Predicting: 301it [03:09,  1.59it/s]Extractor Predicting: 302it [03:09,  1.60it/s]Extractor Predicting: 303it [03:10,  1.55it/s]Extractor Predicting: 304it [03:11,  1.56it/s]Extractor Predicting: 305it [03:11,  1.60it/s]Extractor Predicting: 306it [03:12,  1.56it/s]Extractor Predicting: 307it [03:13,  1.52it/s]Extractor Predicting: 308it [03:13,  1.53it/s]Extractor Predicting: 309it [03:14,  1.56it/s]Extractor Predicting: 310it [03:15,  1.59it/s]Extractor Predicting: 311it [03:15,  1.61it/s]Extractor Predicting: 312it [03:16,  1.59it/s]Extractor Predicting: 313it [03:16,  1.55it/s]Extractor Predicting: 314it [03:17,  1.53it/s]Extractor Predicting: 315it [03:18,  1.53it/s]Extractor Predicting: 316it [03:18,  1.52it/s]Extractor Predicting: 317it [03:19,  1.53it/s]Extractor Predicting: 318it [03:20,  1.54it/s]Extractor Predicting: 319it [03:20,  1.56it/s]Extractor Predicting: 320it [03:21,  1.56it/s]Extractor Predicting: 321it [03:22,  1.62it/s]Extractor Predicting: 322it [03:22,  1.62it/s]Extractor Predicting: 323it [03:23,  1.58it/s]Extractor Predicting: 324it [03:24,  1.57it/s]Extractor Predicting: 325it [03:24,  1.56it/s]Extractor Predicting: 326it [03:25,  1.55it/s]Extractor Predicting: 327it [03:26,  1.53it/s]Extractor Predicting: 328it [03:26,  1.54it/s]Extractor Predicting: 329it [03:27,  1.58it/s]Extractor Predicting: 330it [03:27,  1.61it/s]Extractor Predicting: 331it [03:28,  1.77it/s]Extractor Predicting: 331it [03:28,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:14,170 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:14,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:14,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:14,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:14,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:14:14,508 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:14:14,509 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:14:14,796 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:14:15,886 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:14:15,886 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:18,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:18,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:18,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:18,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:14:18,582 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:14:19,214 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:14:19,215 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:14:19,919 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:14:20,132 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:14:20,132 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.47690217391304346,
  "recall": 0.08851342831925356,
  "score": 0.1493140487078592,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:10,  1.48it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:11,  1.36it/s]Extractor Predicting: 18it [00:12,  1.42it/s]Extractor Predicting: 19it [00:12,  1.43it/s]Extractor Predicting: 20it [00:13,  1.43it/s]Extractor Predicting: 21it [00:14,  1.43it/s]Extractor Predicting: 22it [00:14,  1.43it/s]Extractor Predicting: 23it [00:15,  1.43it/s]Extractor Predicting: 24it [00:16,  1.44it/s]Extractor Predicting: 25it [00:17,  1.22it/s]Extractor Predicting: 26it [00:18,  1.31it/s]Extractor Predicting: 27it [00:18,  1.39it/s]Extractor Predicting: 28it [00:19,  1.41it/s]Extractor Predicting: 29it [00:20,  1.37it/s]Extractor Predicting: 30it [00:20,  1.42it/s]Extractor Predicting: 31it [00:21,  1.44it/s]Extractor Predicting: 32it [00:22,  1.48it/s]Extractor Predicting: 33it [00:22,  1.52it/s]Extractor Predicting: 34it [00:23,  1.51it/s]Extractor Predicting: 35it [00:24,  1.54it/s]Extractor Predicting: 36it [00:24,  1.55it/s]Extractor Predicting: 37it [00:25,  1.56it/s]Extractor Predicting: 38it [00:25,  1.57it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.52it/s]Extractor Predicting: 41it [00:27,  1.54it/s]Extractor Predicting: 42it [00:28,  1.55it/s]Extractor Predicting: 43it [00:29,  1.55it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:30,  1.53it/s]Extractor Predicting: 46it [00:31,  1.55it/s]Extractor Predicting: 47it [00:31,  1.54it/s]Extractor Predicting: 48it [00:32,  1.57it/s]Extractor Predicting: 49it [00:33,  1.59it/s]Extractor Predicting: 50it [00:33,  1.61it/s]Extractor Predicting: 51it [00:34,  1.61it/s]Extractor Predicting: 52it [00:34,  1.60it/s]Extractor Predicting: 53it [00:35,  1.53it/s]Extractor Predicting: 54it [00:36,  1.55it/s]Extractor Predicting: 55it [00:36,  1.58it/s]Extractor Predicting: 56it [00:37,  1.64it/s]Extractor Predicting: 57it [00:37,  1.69it/s]Extractor Predicting: 58it [00:38,  1.74it/s]Extractor Predicting: 59it [00:39,  1.83it/s]Extractor Predicting: 60it [00:39,  1.90it/s]Extractor Predicting: 61it [00:39,  1.94it/s]Extractor Predicting: 62it [00:40,  1.93it/s]Extractor Predicting: 63it [00:40,  1.95it/s]Extractor Predicting: 64it [00:41,  1.93it/s]Extractor Predicting: 65it [00:42,  1.92it/s]Extractor Predicting: 66it [00:42,  1.90it/s]Extractor Predicting: 67it [00:43,  1.90it/s]Extractor Predicting: 68it [00:43,  1.92it/s]Extractor Predicting: 69it [00:44,  1.95it/s]Extractor Predicting: 70it [00:44,  1.92it/s]Extractor Predicting: 71it [00:45,  1.91it/s]Extractor Predicting: 72it [00:45,  1.93it/s]Extractor Predicting: 73it [00:46,  1.97it/s]Extractor Predicting: 74it [00:46,  1.99it/s]Extractor Predicting: 75it [00:47,  1.97it/s]Extractor Predicting: 76it [00:47,  1.72it/s]Extractor Predicting: 77it [00:48,  1.83it/s]Extractor Predicting: 78it [00:48,  1.82it/s]Extractor Predicting: 79it [00:49,  1.85it/s]Extractor Predicting: 80it [00:50,  1.86it/s]Extractor Predicting: 81it [00:50,  1.84it/s]Extractor Predicting: 82it [00:51,  1.87it/s]Extractor Predicting: 83it [00:51,  1.88it/s]Extractor Predicting: 84it [00:52,  1.89it/s]Extractor Predicting: 85it [00:52,  1.91it/s]Extractor Predicting: 86it [00:53,  1.77it/s]Extractor Predicting: 87it [00:53,  1.72it/s]Extractor Predicting: 88it [00:54,  1.68it/s]Extractor Predicting: 89it [00:55,  1.67it/s]Extractor Predicting: 90it [00:55,  1.68it/s]Extractor Predicting: 91it [00:56,  1.65it/s]Extractor Predicting: 92it [00:57,  1.63it/s]Extractor Predicting: 93it [00:57,  1.64it/s]Extractor Predicting: 94it [00:58,  1.62it/s]Extractor Predicting: 95it [00:58,  1.65it/s]Extractor Predicting: 96it [00:59,  1.66it/s]Extractor Predicting: 97it [01:00,  1.66it/s]Extractor Predicting: 98it [01:00,  1.65it/s]Extractor Predicting: 99it [01:01,  1.63it/s]Extractor Predicting: 100it [01:01,  1.59it/s]Extractor Predicting: 101it [01:02,  1.61it/s]Extractor Predicting: 102it [01:03,  1.61it/s]Extractor Predicting: 103it [01:03,  1.56it/s]Extractor Predicting: 104it [01:04,  1.55it/s]Extractor Predicting: 105it [01:05,  1.54it/s]Extractor Predicting: 106it [01:05,  1.53it/s]Extractor Predicting: 107it [01:06,  1.51it/s]Extractor Predicting: 108it [01:07,  1.47it/s]Extractor Predicting: 108it [01:07,  1.61it/s]
[INFO|configuration_utils.py:515] 2023-08-28 20:15:29,395 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:15:29,396 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:15:29,398 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:15:29,399 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 20:15:29,400 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:15:36,695 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 20:15:36,695 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 20:15:36,709 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:15:36,710 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:15:36,717 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:15:36,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:15:36,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:15:36,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:15:36,723 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:15:36,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:15:36,723 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8134034165571616,
  "recall": 0.09915104917507608,
  "score": 0.17675613934894346,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 20:15:36,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:37,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:38,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:39,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:39,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:40,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:41,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:41,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:42,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:42,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:43,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:44,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:45,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:45,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:46,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:46,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:47,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:48,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:48,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:49,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:49,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:13<03:09, 13.53s/it][WARNING|generation_utils.py:914] 2023-08-28 20:15:50,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:51,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:51,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:52,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:52,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:53,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:54,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:55,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:55,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:56,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:57,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:57,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:58,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:59,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:15:59,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:00,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:01,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:01,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:02,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:02,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:03,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<03:00, 13.86s/it][WARNING|generation_utils.py:914] 2023-08-28 20:16:04,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:05,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:05,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:06,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:06,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:07,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:08,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:08,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:09,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:09,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:10,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:10,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:11,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:12,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:12,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:13,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:14,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:14,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:15,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:15,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:16,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:40<02:38, 13.21s/it][WARNING|generation_utils.py:914] 2023-08-28 20:16:17,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:17,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:18,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:18,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:19,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:20,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:20,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:21,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:21,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:22,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:23,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:23,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:24,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:24,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:25,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:25,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:26,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:27,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:27,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:28,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:29,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:52<02:23, 13.03s/it][WARNING|generation_utils.py:914] 2023-08-28 20:16:29,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:30,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:30,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:31,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:32,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:32,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:33,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:34,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:34,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:35,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:35,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:36,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:37,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:37,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:38,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:38,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:39,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:40,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:40,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:41,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:41,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:42,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:43,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:06<02:13, 13.33s/it][WARNING|generation_utils.py:914] 2023-08-28 20:16:43,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:44,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:44,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:45,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:45,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:46,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:46,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:47,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:48,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:48,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:49,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:49,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:50,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:51,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:51,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:52,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:52,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:53,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:54,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:54,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:55,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:55,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:19<01:58, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-28 20:16:56,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:57,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:58,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:58,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:16:59,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:00,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:00,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:01,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:02,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:02,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:03,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:04,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:05,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:05,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:06,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:07,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:08,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:08,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:09,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:10,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:10,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:11,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:12,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:35<01:53, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-28 20:17:12,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:13,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:13,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:14,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:14,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:15,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:16,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:16,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:17,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:17,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:18,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:18,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:19,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:20,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:20,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:21,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:21,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:22,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:22,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:23,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:23,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:24,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:47<01:34, 13.51s/it][WARNING|generation_utils.py:914] 2023-08-28 20:17:24,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:25,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:25,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:26,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:26,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:27,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:27,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:28,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:28,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:29,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:29,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:30,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:30,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:31,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:31,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:32,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:32,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:33,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:33,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:34,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:34,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:58<01:15, 12.61s/it][WARNING|generation_utils.py:914] 2023-08-28 20:17:35,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:36,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:36,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:37,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:37,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:38,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:38,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:39,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:39,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:40,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:40,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:41,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:41,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:42,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:42,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:43,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:43,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:44,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:45,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:45,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:46,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:09<01:00, 12.15s/it][WARNING|generation_utils.py:914] 2023-08-28 20:17:46,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:47,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:47,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:48,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:48,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:49,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:49,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:50,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:50,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:51,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:52,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:52,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:53,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:53,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:54,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:55,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:55,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:56,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:56,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:57,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:57,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:58,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:21<00:48, 12.20s/it][WARNING|generation_utils.py:914] 2023-08-28 20:17:58,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:17:59,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:00,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:00,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:01,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:02,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:02,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:03,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:04,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:05,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:05,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:06,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:06,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:07,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:08,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:08,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:09,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:10,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:10,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:11,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:12,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:12,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:13,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:37<00:39, 13.15s/it][WARNING|generation_utils.py:914] 2023-08-28 20:18:14,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:14,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:15,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:16,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:16,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:17,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:17,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:18,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:19,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:19,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:20,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:20,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:21,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:22,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:23,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:23,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:24,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:24,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:25,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:26,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:26,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:27,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:51<00:26, 13.40s/it][WARNING|generation_utils.py:914] 2023-08-28 20:18:28,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:28,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:29,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:29,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:30,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:30,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:31,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:31,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:32,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:33,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:33,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:34,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:35,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:35,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:36,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:36,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:37,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:37,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:38,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:38,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:39,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:02<00:12, 12.89s/it][WARNING|generation_utils.py:914] 2023-08-28 20:18:39,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:40,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:41,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:41,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:42,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:43,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:43,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:44,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:44,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:45,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:46,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:46,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:47,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:47,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:48,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:49,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:49,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:50,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:50,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:51,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:52,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:52,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:18:53,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:17<00:00, 13.32s/it]Generating: 100%|██████████| 15/15 [03:17<00:00, 13.15s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:01,418 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:01,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:01,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:01,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:01,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:19:02,135 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:19:02,136 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:19:02,711 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:19:03,795 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:19:03,795 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:06,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:06,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:06,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:06,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:19:06,780 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:19:07,447 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:19:07,448 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:19:08,021 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:19:08,195 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:19:08,195 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : characters . Context : Later in the film , the character of the villainous villainous villainous character of the same name had appeared in a role in the first film , titled The Wolf of Wall Street . Head Entity : The Wolf of Wall Street , Tail Entity : villainous villainous villainous villain .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 628, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8532608695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8821022727272727, 'errors': {''}}
['Relation : follows . Context : Later in the year ( 1151 ) , he married Elizabeth I of Spain whom at the end of 1492 he married to a daughter , Mary I , daughter of Thomas II , queen of Spain . Head Entity : Margaret I , Tail Entity : Spain .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('The Lord of the Rings', 'follows', '', 'It was written by Michael D. Wilson , who also wrote the novel , The Lord of the Rings .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : league .', 'success_rate': 0.8565340909090909, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Jupiter', 'located on astronomical body', '', 'It is most commonly found on Jupiter , where it is found in the constellation of Cephei .')", 'too many values to unpack (expected 2)'}}
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major in the Commons . Head Entity : John Major , Tail Entity : Liberal Party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a parliamentary majority government under Prime Minister John Major in the Commons . Head Entity : John Major , Tail Entity : Liberal Party .\n', 'Relation : member of political party . Context : After the 2008 election , Prime Minister Paul Martin was elected to the House of Commons , where he was the Conservative MP for Port Talbot from 2010 until he was elected Liberal MP for Victoria from 2012 . Head Entity : Premier , Tail Entity : Liberal MP .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8565340909090909, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 9857
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9957, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.63it/s]Extractor Estimating: 2it [00:01,  1.58it/s]Extractor Estimating: 3it [00:02,  1.46it/s]Extractor Estimating: 4it [00:02,  1.51it/s]Extractor Estimating: 5it [00:03,  1.60it/s]Extractor Estimating: 6it [00:03,  1.66it/s]Extractor Estimating: 7it [00:04,  1.67it/s]Extractor Estimating: 8it [00:04,  1.64it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.66it/s]Extractor Estimating: 11it [00:06,  1.62it/s]Extractor Estimating: 12it [00:07,  1.62it/s]Extractor Estimating: 13it [00:08,  1.60it/s]Extractor Estimating: 14it [00:08,  1.53it/s]Extractor Estimating: 15it [00:09,  1.59it/s]Extractor Estimating: 16it [00:09,  1.61it/s]Extractor Estimating: 17it [00:10,  1.63it/s]Extractor Estimating: 18it [00:11,  1.59it/s]Extractor Estimating: 19it [00:11,  1.65it/s]Extractor Estimating: 20it [00:12,  1.65it/s]Extractor Estimating: 21it [00:12,  1.69it/s]Extractor Estimating: 22it [00:13,  1.68it/s]Extractor Estimating: 23it [00:14,  1.68it/s]Extractor Estimating: 24it [00:14,  1.68it/s]Extractor Estimating: 25it [00:15,  1.69it/s]Extractor Estimating: 26it [00:15,  1.67it/s]Extractor Estimating: 27it [00:16,  1.63it/s]Extractor Estimating: 28it [00:17,  1.66it/s]Extractor Estimating: 29it [00:17,  1.63it/s]Extractor Estimating: 30it [00:18,  1.60it/s]Extractor Estimating: 31it [00:19,  1.63it/s]Extractor Estimating: 32it [00:19,  1.61it/s]Extractor Estimating: 33it [00:20,  1.61it/s]Extractor Estimating: 34it [00:20,  1.59it/s]Extractor Estimating: 35it [00:21,  1.66it/s]Extractor Estimating: 36it [00:22,  1.65it/s]Extractor Estimating: 37it [00:22,  1.63it/s]Extractor Estimating: 38it [00:23,  1.66it/s]Extractor Estimating: 39it [00:23,  1.63it/s]Extractor Estimating: 40it [00:24,  1.63it/s]Extractor Estimating: 41it [00:25,  1.64it/s]Extractor Estimating: 42it [00:25,  1.66it/s]Extractor Estimating: 43it [00:26,  1.67it/s]Extractor Estimating: 44it [00:27,  1.55it/s]Extractor Estimating: 45it [00:27,  1.59it/s]Extractor Estimating: 46it [00:28,  1.58it/s]Extractor Estimating: 47it [00:28,  1.59it/s]Extractor Estimating: 48it [00:29,  1.61it/s]Extractor Estimating: 49it [00:30,  1.58it/s]Extractor Estimating: 50it [00:30,  1.63it/s]Extractor Estimating: 51it [00:31,  1.67it/s]Extractor Estimating: 52it [00:31,  1.68it/s]Extractor Estimating: 53it [00:32,  1.63it/s]Extractor Estimating: 54it [00:33,  1.66it/s]Extractor Estimating: 55it [00:33,  1.71it/s]Extractor Estimating: 56it [00:34,  1.78it/s]Extractor Estimating: 57it [00:34,  1.71it/s]Extractor Estimating: 58it [00:35,  1.73it/s]Extractor Estimating: 59it [00:36,  1.73it/s]Extractor Estimating: 60it [00:36,  1.78it/s]Extractor Estimating: 61it [00:37,  1.77it/s]Extractor Estimating: 62it [00:37,  1.66it/s]Extractor Estimating: 63it [00:38,  1.75it/s]Extractor Estimating: 64it [00:38,  1.72it/s]Extractor Estimating: 65it [00:39,  1.75it/s]Extractor Estimating: 66it [00:40,  1.76it/s]Extractor Estimating: 67it [00:40,  1.74it/s]Extractor Estimating: 68it [00:41,  1.76it/s]Extractor Estimating: 69it [00:41,  1.75it/s]Extractor Estimating: 70it [00:42,  1.73it/s]Extractor Estimating: 71it [00:42,  1.73it/s]Extractor Estimating: 72it [00:43,  1.74it/s]Extractor Estimating: 73it [00:44,  1.71it/s]Extractor Estimating: 74it [00:44,  1.72it/s]Extractor Estimating: 75it [00:45,  1.72it/s]Extractor Estimating: 76it [00:45,  1.68it/s]Extractor Estimating: 77it [00:46,  1.60it/s]Extractor Estimating: 78it [00:47,  1.65it/s]Extractor Estimating: 79it [00:47,  1.62it/s]Extractor Estimating: 80it [00:48,  1.56it/s]Extractor Estimating: 81it [00:49,  1.59it/s]Extractor Estimating: 82it [00:49,  1.57it/s]Extractor Estimating: 83it [00:50,  1.62it/s]Extractor Estimating: 84it [00:50,  1.67it/s]Extractor Estimating: 85it [00:51,  1.74it/s]Extractor Estimating: 86it [00:51,  1.77it/s]Extractor Estimating: 87it [00:52,  1.75it/s]Extractor Estimating: 88it [00:53,  1.71it/s]Extractor Estimating: 89it [00:53,  1.74it/s]Extractor Estimating: 90it [00:54,  1.70it/s]Extractor Estimating: 91it [00:54,  1.66it/s]Extractor Estimating: 92it [00:55,  1.69it/s]Extractor Estimating: 93it [00:56,  1.69it/s]Extractor Estimating: 94it [00:56,  1.69it/s]Extractor Estimating: 95it [00:57,  1.67it/s]Extractor Estimating: 96it [00:57,  1.63it/s]Extractor Estimating: 97it [00:58,  1.61it/s]Extractor Estimating: 98it [00:59,  1.63it/s]Extractor Estimating: 99it [00:59,  1.60it/s]Extractor Estimating: 100it [01:00,  1.62it/s]Extractor Estimating: 101it [01:01,  1.61it/s]Extractor Estimating: 102it [01:01,  1.61it/s]Extractor Estimating: 103it [01:02,  1.61it/s]Extractor Estimating: 104it [01:03,  1.48it/s]Extractor Estimating: 105it [01:03,  1.48it/s]Extractor Estimating: 106it [01:04,  1.46it/s]Extractor Estimating: 107it [01:05,  1.51it/s]Extractor Estimating: 108it [01:05,  1.40it/s]Extractor Estimating: 109it [01:06,  1.45it/s]Extractor Estimating: 110it [01:07,  1.50it/s]Extractor Estimating: 111it [01:07,  1.56it/s]Extractor Estimating: 112it [01:08,  1.53it/s]Extractor Estimating: 113it [01:09,  1.55it/s]Extractor Estimating: 114it [01:09,  1.52it/s]Extractor Estimating: 115it [01:10,  1.56it/s]Extractor Estimating: 116it [01:10,  1.58it/s]Extractor Estimating: 117it [01:11,  1.61it/s]Extractor Estimating: 118it [01:12,  1.64it/s]Extractor Estimating: 119it [01:12,  1.67it/s]Extractor Estimating: 120it [01:13,  1.65it/s]Extractor Estimating: 121it [01:13,  1.67it/s]Extractor Estimating: 122it [01:14,  1.66it/s]Extractor Estimating: 123it [01:15,  1.64it/s]Extractor Estimating: 124it [01:15,  1.61it/s]Extractor Estimating: 125it [01:16,  1.59it/s]Extractor Estimating: 126it [01:17,  1.62it/s]Extractor Estimating: 127it [01:17,  1.66it/s]Extractor Estimating: 128it [01:18,  1.68it/s]Extractor Estimating: 129it [01:18,  1.67it/s]Extractor Estimating: 130it [01:19,  1.66it/s]Extractor Estimating: 131it [01:19,  1.67it/s]Extractor Estimating: 132it [01:20,  1.67it/s]Extractor Estimating: 133it [01:21,  1.71it/s]Extractor Estimating: 134it [01:21,  1.74it/s]Extractor Estimating: 135it [01:22,  1.69it/s]Extractor Estimating: 136it [01:22,  1.68it/s]Extractor Estimating: 137it [01:23,  1.65it/s]Extractor Estimating: 138it [01:24,  1.68it/s]Extractor Estimating: 139it [01:24,  1.66it/s]Extractor Estimating: 140it [01:25,  1.66it/s]Extractor Estimating: 141it [01:25,  1.68it/s]Extractor Estimating: 142it [01:26,  1.73it/s]Extractor Estimating: 143it [01:27,  1.64it/s]Extractor Estimating: 144it [01:27,  1.65it/s]Extractor Estimating: 145it [01:28,  1.65it/s]Extractor Estimating: 146it [01:28,  1.66it/s]Extractor Estimating: 147it [01:29,  1.55it/s]Extractor Estimating: 148it [01:30,  1.59it/s]Extractor Estimating: 149it [01:30,  1.66it/s]Extractor Estimating: 150it [01:31,  1.65it/s]Extractor Estimating: 151it [01:32,  1.65it/s]Extractor Estimating: 152it [01:32,  1.60it/s]Extractor Estimating: 153it [01:33,  1.59it/s]Extractor Estimating: 154it [01:33,  1.61it/s]Extractor Estimating: 155it [01:34,  1.59it/s]Extractor Estimating: 156it [01:35,  1.65it/s]Extractor Estimating: 157it [01:35,  1.63it/s]Extractor Estimating: 158it [01:36,  1.60it/s]Extractor Estimating: 159it [01:37,  1.58it/s]Extractor Estimating: 160it [01:37,  1.58it/s]Extractor Estimating: 161it [01:38,  1.48it/s]Extractor Estimating: 162it [01:39,  1.51it/s]Extractor Estimating: 163it [01:39,  1.53it/s]Extractor Estimating: 164it [01:40,  1.50it/s]Extractor Estimating: 165it [01:41,  1.53it/s]Extractor Estimating: 166it [01:41,  1.53it/s]Extractor Estimating: 167it [01:42,  1.52it/s]Extractor Estimating: 168it [01:43,  1.34it/s]Extractor Estimating: 169it [01:44,  1.35it/s]Extractor Estimating: 170it [01:44,  1.44it/s]Extractor Estimating: 171it [01:45,  1.49it/s]Extractor Estimating: 172it [01:45,  1.53it/s]Extractor Estimating: 173it [01:46,  1.54it/s]Extractor Estimating: 174it [01:47,  1.58it/s]Extractor Estimating: 175it [01:47,  1.60it/s]Extractor Estimating: 176it [01:48,  1.61it/s]Extractor Estimating: 177it [01:48,  1.65it/s]Extractor Estimating: 178it [01:49,  1.64it/s]Extractor Estimating: 179it [01:50,  1.65it/s]Extractor Estimating: 180it [01:50,  1.66it/s]Extractor Estimating: 181it [01:51,  1.69it/s]Extractor Estimating: 182it [01:51,  1.67it/s]Extractor Estimating: 183it [01:52,  1.66it/s]Extractor Estimating: 184it [01:53,  1.68it/s]Extractor Estimating: 185it [01:53,  1.66it/s]Extractor Estimating: 186it [01:54,  1.67it/s]Extractor Estimating: 187it [01:54,  1.68it/s]Extractor Estimating: 188it [01:55,  1.69it/s]Extractor Estimating: 189it [01:56,  1.59it/s]Extractor Estimating: 190it [01:56,  1.62it/s]Extractor Estimating: 191it [01:57,  1.61it/s]Extractor Estimating: 192it [01:58,  1.49it/s]Extractor Estimating: 193it [01:58,  1.57it/s]Extractor Estimating: 194it [01:59,  1.62it/s]Extractor Estimating: 195it [02:00,  1.59it/s]Extractor Estimating: 196it [02:00,  1.59it/s]Extractor Estimating: 197it [02:01,  1.64it/s]Extractor Estimating: 198it [02:01,  1.63it/s]Extractor Estimating: 199it [02:02,  1.62it/s]Extractor Estimating: 200it [02:03,  1.62it/s]Extractor Estimating: 201it [02:03,  1.72it/s]Extractor Estimating: 202it [02:04,  1.78it/s]Extractor Estimating: 203it [02:04,  1.81it/s]Extractor Estimating: 204it [02:05,  1.83it/s]Extractor Estimating: 205it [02:05,  1.92it/s]Extractor Estimating: 206it [02:06,  1.96it/s]Extractor Estimating: 207it [02:06,  2.02it/s]Extractor Estimating: 208it [02:07,  1.94it/s]Extractor Estimating: 209it [02:07,  1.95it/s]Extractor Estimating: 210it [02:08,  1.94it/s]Extractor Estimating: 211it [02:08,  1.96it/s]Extractor Estimating: 212it [02:09,  1.96it/s]Extractor Estimating: 213it [02:09,  1.90it/s]Extractor Estimating: 214it [02:10,  1.99it/s]Extractor Estimating: 215it [02:10,  1.99it/s]Extractor Estimating: 216it [02:11,  1.86it/s]Extractor Estimating: 217it [02:11,  1.90it/s]Extractor Estimating: 218it [02:12,  1.95it/s]Extractor Estimating: 219it [02:12,  1.98it/s]Extractor Estimating: 220it [02:13,  1.91it/s]Extractor Estimating: 221it [02:13,  1.94it/s]Extractor Estimating: 222it [02:14,  1.98it/s]Extractor Estimating: 223it [02:14,  1.90it/s]Extractor Estimating: 224it [02:15,  1.97it/s]Extractor Estimating: 225it [02:15,  1.93it/s]Extractor Estimating: 226it [02:16,  1.92it/s]Extractor Estimating: 227it [02:16,  1.88it/s]Extractor Estimating: 228it [02:17,  1.89it/s]Extractor Estimating: 229it [02:18,  1.90it/s]Extractor Estimating: 230it [02:18,  1.92it/s]Extractor Estimating: 231it [02:19,  1.92it/s]Extractor Estimating: 232it [02:19,  1.95it/s]Extractor Estimating: 233it [02:20,  1.90it/s]Extractor Estimating: 234it [02:20,  1.96it/s]Extractor Estimating: 235it [02:21,  2.03it/s]Extractor Estimating: 236it [02:21,  1.94it/s]Extractor Estimating: 237it [02:22,  1.98it/s]Extractor Estimating: 238it [02:22,  2.03it/s]Extractor Estimating: 239it [02:23,  2.01it/s]Extractor Estimating: 240it [02:23,  2.02it/s]Extractor Estimating: 241it [02:24,  1.97it/s]Extractor Estimating: 242it [02:24,  1.90it/s]Extractor Estimating: 243it [02:25,  1.81it/s]Extractor Estimating: 244it [02:25,  1.87it/s]Extractor Estimating: 245it [02:26,  1.80it/s]Extractor Estimating: 246it [02:26,  1.89it/s]Extractor Estimating: 247it [02:27,  1.88it/s]Extractor Estimating: 248it [02:27,  1.90it/s]Extractor Estimating: 249it [02:28,  1.94it/s]Extractor Estimating: 250it [02:28,  1.92it/s]Extractor Estimating: 251it [02:29,  1.90it/s]Extractor Estimating: 252it [02:29,  1.85it/s]Extractor Estimating: 253it [02:30,  1.79it/s]Extractor Estimating: 254it [02:31,  1.74it/s]Extractor Estimating: 255it [02:31,  1.70it/s]Extractor Estimating: 256it [02:32,  1.69it/s]Extractor Estimating: 257it [02:33,  1.68it/s]Extractor Estimating: 258it [02:33,  1.66it/s]Extractor Estimating: 259it [02:34,  1.66it/s]Extractor Estimating: 260it [02:34,  1.64it/s]Extractor Estimating: 261it [02:35,  1.55it/s]Extractor Estimating: 262it [02:36,  1.55it/s]Extractor Estimating: 263it [02:36,  1.60it/s]Extractor Estimating: 264it [02:37,  1.62it/s]Extractor Estimating: 265it [02:38,  1.64it/s]Extractor Estimating: 266it [02:38,  1.69it/s]Extractor Estimating: 267it [02:39,  1.70it/s]Extractor Estimating: 268it [02:39,  1.71it/s]Extractor Estimating: 269it [02:40,  1.75it/s]Extractor Estimating: 270it [02:40,  1.75it/s]Extractor Estimating: 271it [02:41,  1.68it/s]Extractor Estimating: 272it [02:42,  1.66it/s]Extractor Estimating: 273it [02:42,  1.68it/s]Extractor Estimating: 274it [02:43,  1.52it/s]Extractor Estimating: 275it [02:44,  1.55it/s]Extractor Estimating: 276it [02:44,  1.56it/s]Extractor Estimating: 277it [02:45,  1.61it/s]Extractor Estimating: 278it [02:45,  1.61it/s]Extractor Estimating: 279it [02:46,  1.56it/s]Extractor Estimating: 280it [02:47,  1.56it/s]Extractor Estimating: 281it [02:47,  1.56it/s]Extractor Estimating: 282it [02:48,  1.56it/s]Extractor Estimating: 283it [02:49,  1.51it/s]Extractor Estimating: 284it [02:49,  1.54it/s]Extractor Estimating: 285it [02:50,  1.55it/s]Extractor Estimating: 286it [02:51,  1.54it/s]Extractor Estimating: 287it [02:51,  1.56it/s]Extractor Estimating: 288it [02:52,  1.54it/s]Extractor Estimating: 289it [02:53,  1.54it/s]Extractor Estimating: 290it [02:53,  1.58it/s]Extractor Estimating: 291it [02:54,  1.59it/s]Extractor Estimating: 292it [02:54,  1.56it/s]Extractor Estimating: 293it [02:55,  1.53it/s]Extractor Estimating: 294it [02:56,  1.59it/s]Extractor Estimating: 295it [02:56,  1.63it/s]Extractor Estimating: 296it [02:57,  1.59it/s]Extractor Estimating: 297it [02:58,  1.49it/s]Extractor Estimating: 298it [02:58,  1.52it/s]Extractor Estimating: 299it [02:59,  1.58it/s]Extractor Estimating: 300it [03:00,  1.61it/s]Extractor Estimating: 301it [03:00,  1.64it/s]Extractor Estimating: 302it [03:01,  1.55it/s]Extractor Estimating: 303it [03:01,  1.59it/s]Extractor Estimating: 304it [03:02,  1.56it/s]Extractor Estimating: 305it [03:03,  1.54it/s]Extractor Estimating: 306it [03:03,  1.59it/s]Extractor Estimating: 307it [03:04,  1.60it/s]Extractor Estimating: 308it [03:05,  1.64it/s]Extractor Estimating: 309it [03:05,  1.65it/s]Extractor Estimating: 310it [03:06,  1.67it/s]Extractor Estimating: 311it [03:06,  1.66it/s]Extractor Estimating: 312it [03:07,  1.66it/s]Extractor Estimating: 313it [03:08,  1.60it/s]Extractor Estimating: 314it [03:08,  1.62it/s]Extractor Estimating: 315it [03:09,  1.55it/s]Extractor Estimating: 316it [03:10,  1.56it/s]Extractor Estimating: 317it [03:10,  1.55it/s]Extractor Estimating: 318it [03:11,  1.51it/s]Extractor Estimating: 319it [03:12,  1.54it/s]Extractor Estimating: 320it [03:12,  1.57it/s]Extractor Estimating: 321it [03:13,  1.57it/s]Extractor Estimating: 322it [03:13,  1.57it/s]Extractor Estimating: 323it [03:14,  1.57it/s]Extractor Estimating: 324it [03:15,  1.55it/s]Extractor Estimating: 325it [03:15,  1.61it/s]Extractor Estimating: 326it [03:16,  1.79it/s]Extractor Estimating: 327it [03:16,  1.88it/s]Extractor Estimating: 328it [03:17,  1.94it/s]Extractor Estimating: 329it [03:17,  2.01it/s]Extractor Estimating: 330it [03:18,  2.10it/s]Extractor Estimating: 331it [03:18,  2.12it/s]Extractor Estimating: 332it [03:19,  1.97it/s]Extractor Estimating: 333it [03:19,  1.99it/s]Extractor Estimating: 334it [03:20,  1.98it/s]Extractor Estimating: 335it [03:20,  2.05it/s]Extractor Estimating: 336it [03:21,  2.03it/s]Extractor Estimating: 337it [03:21,  2.03it/s]Extractor Estimating: 338it [03:22,  1.96it/s]Extractor Estimating: 339it [03:22,  1.97it/s]Extractor Estimating: 340it [03:23,  1.94it/s]Extractor Estimating: 341it [03:23,  2.05it/s]Extractor Estimating: 342it [03:24,  1.99it/s]Extractor Estimating: 343it [03:24,  2.01it/s]Extractor Estimating: 344it [03:25,  2.07it/s]Extractor Estimating: 345it [03:25,  2.07it/s]Extractor Estimating: 346it [03:25,  2.07it/s]Extractor Estimating: 347it [03:26,  2.11it/s]Extractor Estimating: 348it [03:26,  2.07it/s]Extractor Estimating: 349it [03:27,  2.09it/s]Extractor Estimating: 350it [03:28,  1.89it/s]Extractor Estimating: 351it [03:28,  1.82it/s]Extractor Estimating: 352it [03:29,  1.81it/s]Extractor Estimating: 353it [03:29,  1.78it/s]Extractor Estimating: 354it [03:30,  1.71it/s]Extractor Estimating: 355it [03:31,  1.71it/s]Extractor Estimating: 356it [03:31,  1.59it/s]Extractor Estimating: 357it [03:32,  1.64it/s]Extractor Estimating: 358it [03:32,  1.73it/s]Extractor Estimating: 359it [03:33,  1.74it/s]Extractor Estimating: 360it [03:33,  1.73it/s]Extractor Estimating: 361it [03:34,  1.74it/s]Extractor Estimating: 362it [03:35,  1.77it/s]Extractor Estimating: 363it [03:35,  1.72it/s]Extractor Estimating: 364it [03:36,  1.73it/s]Extractor Estimating: 365it [03:36,  1.71it/s]Extractor Estimating: 366it [03:37,  1.71it/s]Extractor Estimating: 367it [03:38,  1.73it/s]Extractor Estimating: 368it [03:38,  1.74it/s]Extractor Estimating: 369it [03:39,  1.71it/s]Extractor Estimating: 370it [03:39,  1.65it/s]Extractor Estimating: 371it [03:40,  1.68it/s]Extractor Estimating: 372it [03:41,  1.64it/s]Extractor Estimating: 373it [03:41,  1.66it/s]Extractor Estimating: 374it [03:42,  1.66it/s]Extractor Estimating: 375it [03:42,  1.59it/s]Extractor Estimating: 375it [03:42,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:10,565 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:10,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:10,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:10,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:10,574 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:23:11,304 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:23:11,305 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:23:11,880 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:23:12,962 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:23:12,962 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:15,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:15,908 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:15,908 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:15,908 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:23:15,908 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:23:16,664 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:23:16,718 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:23:17,378 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:23:17,551 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:23:17,551 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 22:34:18,277 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 22:34:18,288 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7484 mean pseudo reward: 0.9301735685329644
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 19972
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20072, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20072, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.007, loss:628.5177
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.994, loss:601.8509
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.995, loss:621.8476
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.976, loss:585.2514
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.981, loss:584.3345
>> valid entity prec:0.5375, rec:0.2987, f1:0.3840
>> valid relation prec:0.0462, rec:0.0032, f1:0.0060
>> valid relation with NER prec:0.0462, rec:0.0032, f1:0.0060
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.394, loss:588.4386
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.016, loss:587.7516
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.993, loss:575.1123
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.989, loss:606.5917
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.986, loss:584.0083
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5244, rec:0.4315, f1:0.4734
>> valid relation prec:0.1101, rec:0.0088, f1:0.0162
>> valid relation with NER prec:0.1101, rec:0.0088, f1:0.0162
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.404, loss:592.2920
g_step 1200, step 264, avg_time 0.996, loss:605.9533
g_step 1300, step 52, avg_time 0.979, loss:572.8662
g_step 1400, step 152, avg_time 0.997, loss:565.7339
g_step 1500, step 252, avg_time 1.002, loss:570.2167
>> valid entity prec:0.4979, rec:0.3379, f1:0.4026
>> valid relation prec:0.0593, rec:0.0048, f1:0.0090
>> valid relation with NER prec:0.0593, rec:0.0048, f1:0.0090
g_step 1600, step 40, avg_time 2.391, loss:560.0802
g_step 1700, step 140, avg_time 0.998, loss:527.9082
g_step 1800, step 240, avg_time 0.967, loss:552.4006
g_step 1900, step 28, avg_time 0.991, loss:542.6178
g_step 2000, step 128, avg_time 0.984, loss:509.8205
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4915, rec:0.4617, f1:0.4762
>> valid relation prec:0.0676, rec:0.0085, f1:0.0152
>> valid relation with NER prec:0.0676, rec:0.0085, f1:0.0152
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.417, loss:549.5829
g_step 2200, step 16, avg_time 0.994, loss:533.0186
g_step 2300, step 116, avg_time 0.995, loss:501.6405
g_step 2400, step 216, avg_time 0.992, loss:496.5500
g_step 2500, step 4, avg_time 0.991, loss:513.2766
>> valid entity prec:0.4825, rec:0.4383, f1:0.4593
>> valid relation prec:0.0262, rec:0.0032, f1:0.0057
>> valid relation with NER prec:0.0262, rec:0.0032, f1:0.0057
g_step 2600, step 104, avg_time 2.386, loss:473.9062
g_step 2700, step 204, avg_time 0.998, loss:482.3856
g_step 2800, step 304, avg_time 0.992, loss:509.5049
g_step 2900, step 92, avg_time 0.986, loss:465.4836
g_step 3000, step 192, avg_time 1.003, loss:486.0959
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4966, rec:0.4372, f1:0.4650
>> valid relation prec:0.0701, rec:0.0092, f1:0.0163
>> valid relation with NER prec:0.0701, rec:0.0092, f1:0.0163
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 292, avg_time 2.381, loss:482.3747
g_step 3200, step 80, avg_time 0.984, loss:444.3062
g_step 3300, step 180, avg_time 0.998, loss:452.3937
g_step 3400, step 280, avg_time 0.997, loss:474.5649
g_step 3500, step 68, avg_time 0.989, loss:444.6627
>> valid entity prec:0.5026, rec:0.3731, f1:0.4283
>> valid relation prec:0.0791, rec:0.0097, f1:0.0173
>> valid relation with NER prec:0.0791, rec:0.0097, f1:0.0173
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 168, avg_time 2.382, loss:441.5484
g_step 3700, step 268, avg_time 0.995, loss:460.8344
g_step 3800, step 56, avg_time 1.011, loss:441.9298
g_step 3900, step 156, avg_time 0.990, loss:428.7917
g_step 4000, step 256, avg_time 0.985, loss:445.6197
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5025, rec:0.3404, f1:0.4059
>> valid relation prec:0.0924, rec:0.0104, f1:0.0187
>> valid relation with NER prec:0.0924, rec:0.0104, f1:0.0187
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 44, avg_time 2.376, loss:439.8427
g_step 4200, step 144, avg_time 1.003, loss:410.7083
g_step 4300, step 244, avg_time 0.975, loss:429.5357
g_step 4400, step 32, avg_time 0.973, loss:408.0251
g_step 4500, step 132, avg_time 0.975, loss:387.8614
>> valid entity prec:0.4908, rec:0.3950, f1:0.4377
>> valid relation prec:0.0507, rec:0.0065, f1:0.0115
>> valid relation with NER prec:0.0507, rec:0.0065, f1:0.0115
g_step 4600, step 232, avg_time 2.394, loss:413.7169
g_step 4700, step 20, avg_time 1.006, loss:424.0311
g_step 4800, step 120, avg_time 0.990, loss:382.7572
g_step 4900, step 220, avg_time 0.973, loss:408.6490
g_step 5000, step 8, avg_time 0.985, loss:409.1575
learning rate was adjusted to 0.0008
>> valid entity prec:0.4938, rec:0.4322, f1:0.4610
>> valid relation prec:0.1229, rec:0.0221, f1:0.0375
>> valid relation with NER prec:0.1229, rec:0.0221, f1:0.0375
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5100, step 108, avg_time 2.392, loss:373.0913
g_step 5200, step 208, avg_time 0.986, loss:381.3123
g_step 5300, step 308, avg_time 0.976, loss:397.1447
g_step 5400, step 96, avg_time 1.000, loss:364.8685
g_step 5500, step 196, avg_time 0.981, loss:377.3412
>> valid entity prec:0.4797, rec:0.3736, f1:0.4201
>> valid relation prec:0.0770, rec:0.0111, f1:0.0194
>> valid relation with NER prec:0.0770, rec:0.0111, f1:0.0194
g_step 5600, step 296, avg_time 2.380, loss:378.7167
g_step 5700, step 84, avg_time 0.984, loss:359.0327
g_step 5800, step 184, avg_time 0.975, loss:368.2033
g_step 5900, step 284, avg_time 0.997, loss:370.5043
g_step 6000, step 72, avg_time 0.991, loss:346.2065
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4531, rec:0.3456, f1:0.3921
>> valid relation prec:0.0909, rec:0.0166, f1:0.0281
>> valid relation with NER prec:0.0909, rec:0.0166, f1:0.0281
g_step 6100, step 172, avg_time 2.370, loss:353.9740
g_step 6200, step 272, avg_time 0.994, loss:364.2346
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 22:34:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 22:34:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_22-34-18_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 22:34:20 - WARNING - datasets.builder -   Using custom data configuration default-2a96a51f0799cf83
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2a96a51f0799cf83/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 22:34:21,419 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:34:21,424 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:34:21,424 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:34:21,425 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:34:21,438 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:34:21,446 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:34:21,446 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:34:21,446 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:34:21,446 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:34:21,446 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:34:21,446 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 22:34:21,699 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:34:24,861 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 22:34:24,861 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2a96a51f0799cf83/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.24ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.06ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.70ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.12ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.36ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.53ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.63ba/s]100%|██████████| 8/8 [00:01<00:00,  5.51ba/s]100%|██████████| 8/8 [00:01<00:00,  4.51ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.48ba/s] 40%|████      | 2/5 [00:00<00:00,  3.97ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.26ba/s]100%|██████████| 5/5 [00:01<00:00,  4.77ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  8.02ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.15ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.57ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.77ba/s]100%|██████████| 8/8 [00:00<00:00, 11.21ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.98ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.60ba/s]100%|██████████| 5/5 [00:00<00:00, 12.28ba/s]100%|██████████| 5/5 [00:00<00:00, 11.25ba/s]
[INFO|trainer.py:414] 2023-08-28 22:34:29,548 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 22:34:29,571 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 22:34:29,572 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-28 22:34:29,572 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 22:34:29,572 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 22:34:29,572 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 22:34:29,572 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 22:34:29,572 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:57,  3.30it/s]  0%|          | 2/585 [00:00<02:52,  3.38it/s]  1%|          | 3/585 [00:00<02:50,  3.41it/s]  1%|          | 4/585 [00:01<02:49,  3.43it/s]  1%|          | 5/585 [00:01<02:48,  3.43it/s]  1%|          | 6/585 [00:01<02:48,  3.44it/s]  1%|          | 7/585 [00:02<02:47,  3.44it/s]  1%|▏         | 8/585 [00:02<02:48,  3.43it/s]  2%|▏         | 9/585 [00:02<02:48,  3.42it/s]  2%|▏         | 10/585 [00:02<02:48,  3.41it/s]  2%|▏         | 11/585 [00:03<02:48,  3.41it/s]  2%|▏         | 12/585 [00:03<02:48,  3.40it/s]  2%|▏         | 13/585 [00:03<02:48,  3.40it/s]  2%|▏         | 14/585 [00:04<02:47,  3.40it/s]  3%|▎         | 15/585 [00:04<02:47,  3.40it/s]  3%|▎         | 16/585 [00:04<02:47,  3.40it/s]  3%|▎         | 17/585 [00:04<02:47,  3.40it/s]  3%|▎         | 18/585 [00:05<02:46,  3.40it/s]  3%|▎         | 19/585 [00:05<02:46,  3.40it/s]  3%|▎         | 20/585 [00:05<02:47,  3.38it/s]  4%|▎         | 21/585 [00:06<02:46,  3.38it/s]  4%|▍         | 22/585 [00:06<02:46,  3.39it/s]  4%|▍         | 23/585 [00:06<02:45,  3.39it/s]  4%|▍         | 24/585 [00:07<02:45,  3.39it/s]  4%|▍         | 25/585 [00:07<02:45,  3.39it/s]  4%|▍         | 26/585 [00:07<02:44,  3.40it/s]  5%|▍         | 27/585 [00:07<02:43,  3.41it/s]  5%|▍         | 28/585 [00:08<02:42,  3.42it/s]  5%|▍         | 29/585 [00:08<02:42,  3.43it/s]  5%|▌         | 30/585 [00:08<02:41,  3.43it/s]  5%|▌         | 31/585 [00:09<02:41,  3.43it/s]  5%|▌         | 32/585 [00:09<02:41,  3.43it/s]  6%|▌         | 33/585 [00:09<02:40,  3.44it/s]  6%|▌         | 34/585 [00:09<02:40,  3.44it/s]  6%|▌         | 35/585 [00:10<02:49,  3.24it/s]  6%|▌         | 36/585 [00:10<02:46,  3.29it/s]  6%|▋         | 37/585 [00:10<02:44,  3.34it/s]  6%|▋         | 38/585 [00:11<02:42,  3.36it/s]  7%|▋         | 39/585 [00:11<02:41,  3.39it/s]  7%|▋         | 40/585 [00:11<02:40,  3.40it/s]  7%|▋         | 41/585 [00:12<02:39,  3.41it/s]  7%|▋         | 42/585 [00:12<02:38,  3.42it/s]  7%|▋         | 43/585 [00:12<02:38,  3.43it/s]  8%|▊         | 44/585 [00:12<02:37,  3.43it/s]  8%|▊         | 45/585 [00:13<02:37,  3.43it/s]  8%|▊         | 46/585 [00:13<02:36,  3.43it/s]  8%|▊         | 47/585 [00:13<02:36,  3.43it/s]  8%|▊         | 48/585 [00:14<02:36,  3.44it/s]  8%|▊         | 49/585 [00:14<02:37,  3.41it/s]  9%|▊         | 50/585 [00:14<02:36,  3.42it/s]  9%|▊         | 51/585 [00:14<02:35,  3.43it/s]  9%|▉         | 52/585 [00:15<02:35,  3.43it/s]  9%|▉         | 53/585 [00:15<02:35,  3.43it/s]  9%|▉         | 54/585 [00:15<02:34,  3.44it/s]  9%|▉         | 55/585 [00:16<02:34,  3.44it/s] 10%|▉         | 56/585 [00:16<02:34,  3.43it/s] 10%|▉         | 57/585 [00:16<02:33,  3.43it/s] 10%|▉         | 58/585 [00:17<02:33,  3.44it/s] 10%|█         | 59/585 [00:17<02:33,  3.43it/s] 10%|█         | 60/585 [00:17<02:32,  3.43it/s] 10%|█         | 61/585 [00:17<02:32,  3.43it/s] 11%|█         | 62/585 [00:18<02:32,  3.43it/s] 11%|█         | 63/585 [00:18<02:32,  3.43it/s] 11%|█         | 64/585 [00:18<02:31,  3.44it/s] 11%|█         | 65/585 [00:19<02:31,  3.44it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.43it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 72/585 [00:21<02:29,  3.43it/s] 12%|█▏        | 73/585 [00:21<02:29,  3.43it/s] 13%|█▎        | 74/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 75/585 [00:21<02:28,  3.43it/s] 13%|█▎        | 76/585 [00:22<02:28,  3.44it/s] 13%|█▎        | 77/585 [00:22<02:27,  3.43it/s] 13%|█▎        | 78/585 [00:22<02:27,  3.43it/s] 14%|█▎        | 79/585 [00:23<02:27,  3.44it/s] 14%|█▎        | 80/585 [00:23<02:26,  3.44it/s] 14%|█▍        | 81/585 [00:23<02:26,  3.43it/s] 14%|█▍        | 82/585 [00:24<02:26,  3.43it/s] 14%|█▍        | 83/585 [00:24<02:26,  3.44it/s] 14%|█▍        | 84/585 [00:24<02:26,  3.43it/s] 15%|█▍        | 85/585 [00:24<02:25,  3.43it/s] 15%|█▍        | 86/585 [00:25<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:25<02:25,  3.43it/s] 15%|█▌        | 88/585 [00:25<02:24,  3.43it/s] 15%|█▌        | 89/585 [00:26<02:24,  3.43it/s] 15%|█▌        | 90/585 [00:26<02:24,  3.44it/s] 16%|█▌        | 91/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 92/585 [00:26<02:23,  3.43it/s] 16%|█▌        | 93/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 94/585 [00:27<02:22,  3.44it/s] 16%|█▌        | 95/585 [00:27<02:22,  3.44it/s] 16%|█▋        | 96/585 [00:28<02:22,  3.43it/s] 17%|█▋        | 97/585 [00:28<02:22,  3.44it/s] 17%|█▋        | 98/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 99/585 [00:28<02:21,  3.43it/s] 17%|█▋        | 100/585 [00:29<02:21,  3.43it/s] 17%|█▋        | 101/585 [00:29<02:20,  3.43it/s] 17%|█▋        | 102/585 [00:29<02:20,  3.43it/s] 18%|█▊        | 103/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 104/585 [00:30<02:20,  3.43it/s] 18%|█▊        | 105/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 106/585 [00:30<02:19,  3.43it/s] 18%|█▊        | 107/585 [00:31<02:19,  3.43it/s] 18%|█▊        | 108/585 [00:31<02:19,  3.43it/s] 19%|█▊        | 109/585 [00:31<02:18,  3.43it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.43it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 113/585 [00:33<02:17,  3.43it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.43it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.43it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.43it/s] 20%|██        | 117/585 [00:34<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 22:35:03,812 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:35:03,812 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:35:03,812 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.96it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.57it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.05it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.37it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.65it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.55it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.42it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.22it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.50it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.45it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.30it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.25it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.26it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 43.99it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.09it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.07it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.21it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.35it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.25it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.24it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.20it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.06it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.07it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.98it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.09it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.29it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.26it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 43.94it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.09it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.19it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.02it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.88it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.94it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.10it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.17it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.25it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.21it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.22it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.11it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.17it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.13it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.12it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.08it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.18it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.17it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.14it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.16it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.17it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.16it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.96it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.10it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.09it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.23it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.17it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.21it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.12it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.18it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.14it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.05it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.13it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.16it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.18it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.12it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.18it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.23it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.02it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.98it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.97it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.05it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.16it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.08it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.05it/s][A
 68%|██████▊   | 367/543 [00:08<00:04, 43.99it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.00it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.10it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.10it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.96it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.08it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.03it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.05it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.21it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.21it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.03it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.08it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.15it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.13it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.18it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 40.43it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 41.44it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 42.47it/s][A
 84%|████████▍ | 457/543 [00:10<00:02, 42.99it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 43.38it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.66it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.68it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.76it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.58it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.74it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 43.95it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.04it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.23it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.33it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.28it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.15it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.94it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 43.85it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.88it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.11it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.22it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.22it/s][A 20%|██        | 117/585 [00:46<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:35:16,182 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 22:35:16,219 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:35:19,272 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:35:19,493 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:35:19,515 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:59<59:41,  7.67s/it] 20%|██        | 119/585 [00:59<42:24,  5.46s/it] 21%|██        | 120/585 [00:59<30:18,  3.91s/it] 21%|██        | 121/585 [00:59<21:54,  2.83s/it] 21%|██        | 122/585 [01:00<15:58,  2.07s/it] 21%|██        | 123/585 [01:00<11:50,  1.54s/it] 21%|██        | 124/585 [01:00<08:57,  1.16s/it] 21%|██▏       | 125/585 [01:01<06:55,  1.11it/s] 22%|██▏       | 126/585 [01:01<05:31,  1.39it/s] 22%|██▏       | 127/585 [01:01<04:31,  1.68it/s] 22%|██▏       | 128/585 [01:02<03:50,  1.99it/s] 22%|██▏       | 129/585 [01:02<03:21,  2.27it/s] 22%|██▏       | 130/585 [01:02<03:03,  2.47it/s] 22%|██▏       | 131/585 [01:02<02:48,  2.69it/s] 23%|██▎       | 132/585 [01:03<02:37,  2.87it/s] 23%|██▎       | 133/585 [01:03<02:30,  3.01it/s] 23%|██▎       | 134/585 [01:03<02:24,  3.11it/s] 23%|██▎       | 135/585 [01:04<02:21,  3.19it/s] 23%|██▎       | 136/585 [01:04<02:18,  3.25it/s] 23%|██▎       | 137/585 [01:04<02:16,  3.29it/s] 24%|██▎       | 138/585 [01:05<02:14,  3.32it/s] 24%|██▍       | 139/585 [01:05<02:13,  3.34it/s] 24%|██▍       | 140/585 [01:05<02:12,  3.36it/s] 24%|██▍       | 141/585 [01:05<02:12,  3.35it/s] 24%|██▍       | 142/585 [01:06<02:11,  3.36it/s] 24%|██▍       | 143/585 [01:06<02:11,  3.37it/s] 25%|██▍       | 144/585 [01:06<02:10,  3.38it/s] 25%|██▍       | 145/585 [01:07<02:09,  3.40it/s] 25%|██▍       | 146/585 [01:07<02:08,  3.40it/s] 25%|██▌       | 147/585 [01:07<02:08,  3.41it/s] 25%|██▌       | 148/585 [01:07<02:07,  3.42it/s] 25%|██▌       | 149/585 [01:08<02:07,  3.42it/s] 26%|██▌       | 150/585 [01:08<02:06,  3.43it/s] 26%|██▌       | 151/585 [01:08<02:06,  3.43it/s] 26%|██▌       | 152/585 [01:09<02:06,  3.42it/s] 26%|██▌       | 153/585 [01:09<02:06,  3.42it/s] 26%|██▋       | 154/585 [01:09<02:05,  3.43it/s] 26%|██▋       | 155/585 [01:10<02:05,  3.43it/s] 27%|██▋       | 156/585 [01:10<02:05,  3.43it/s] 27%|██▋       | 157/585 [01:10<02:04,  3.43it/s] 27%|██▋       | 158/585 [01:10<02:04,  3.43it/s] 27%|██▋       | 159/585 [01:11<02:04,  3.43it/s] 27%|██▋       | 160/585 [01:11<02:03,  3.43it/s] 28%|██▊       | 161/585 [01:11<02:03,  3.43it/s] 28%|██▊       | 162/585 [01:12<02:03,  3.43it/s] 28%|██▊       | 163/585 [01:12<02:08,  3.28it/s] 28%|██▊       | 164/585 [01:12<02:06,  3.33it/s] 28%|██▊       | 165/585 [01:12<02:05,  3.36it/s] 28%|██▊       | 166/585 [01:13<02:03,  3.38it/s] 29%|██▊       | 167/585 [01:13<02:02,  3.40it/s] 29%|██▊       | 168/585 [01:13<02:02,  3.41it/s] 29%|██▉       | 169/585 [01:14<02:01,  3.42it/s] 29%|██▉       | 170/585 [01:14<02:01,  3.42it/s] 29%|██▉       | 171/585 [01:14<02:00,  3.42it/s] 29%|██▉       | 172/585 [01:15<02:00,  3.42it/s] 30%|██▉       | 173/585 [01:15<02:00,  3.43it/s] 30%|██▉       | 174/585 [01:15<02:00,  3.42it/s] 30%|██▉       | 175/585 [01:15<01:59,  3.42it/s] 30%|███       | 176/585 [01:16<01:59,  3.43it/s] 30%|███       | 177/585 [01:16<01:58,  3.43it/s] 30%|███       | 178/585 [01:16<01:58,  3.43it/s] 31%|███       | 179/585 [01:17<01:58,  3.43it/s] 31%|███       | 180/585 [01:17<01:58,  3.43it/s] 31%|███       | 181/585 [01:17<01:57,  3.43it/s] 31%|███       | 182/585 [01:17<01:57,  3.43it/s] 31%|███▏      | 183/585 [01:18<01:57,  3.43it/s] 31%|███▏      | 184/585 [01:18<01:56,  3.43it/s] 32%|███▏      | 185/585 [01:18<01:56,  3.42it/s] 32%|███▏      | 186/585 [01:19<01:56,  3.43it/s] 32%|███▏      | 187/585 [01:19<01:56,  3.43it/s] 32%|███▏      | 188/585 [01:19<01:55,  3.43it/s] 32%|███▏      | 189/585 [01:19<01:55,  3.43it/s] 32%|███▏      | 190/585 [01:20<01:55,  3.43it/s] 33%|███▎      | 191/585 [01:20<01:54,  3.43it/s] 33%|███▎      | 192/585 [01:20<01:54,  3.43it/s] 33%|███▎      | 193/585 [01:21<01:54,  3.43it/s] 33%|███▎      | 194/585 [01:21<01:54,  3.43it/s] 33%|███▎      | 195/585 [01:21<01:53,  3.43it/s] 34%|███▎      | 196/585 [01:22<01:53,  3.43it/s] 34%|███▎      | 197/585 [01:22<01:53,  3.43it/s] 34%|███▍      | 198/585 [01:22<01:52,  3.43it/s] 34%|███▍      | 199/585 [01:22<01:52,  3.43it/s] 34%|███▍      | 200/585 [01:23<01:52,  3.43it/s] 34%|███▍      | 201/585 [01:23<01:52,  3.43it/s] 35%|███▍      | 202/585 [01:23<01:51,  3.43it/s] 35%|███▍      | 203/585 [01:24<01:51,  3.43it/s] 35%|███▍      | 204/585 [01:24<01:51,  3.43it/s] 35%|███▌      | 205/585 [01:24<01:50,  3.43it/s] 35%|███▌      | 206/585 [01:24<01:52,  3.37it/s] 35%|███▌      | 207/585 [01:25<01:51,  3.38it/s] 36%|███▌      | 208/585 [01:25<01:51,  3.40it/s] 36%|███▌      | 209/585 [01:25<01:50,  3.40it/s] 36%|███▌      | 210/585 [01:26<01:49,  3.41it/s] 36%|███▌      | 211/585 [01:26<01:49,  3.42it/s] 36%|███▌      | 212/585 [01:26<01:49,  3.42it/s] 36%|███▋      | 213/585 [01:26<01:48,  3.42it/s] 37%|███▋      | 214/585 [01:27<01:48,  3.42it/s] 37%|███▋      | 215/585 [01:27<01:48,  3.42it/s] 37%|███▋      | 216/585 [01:27<01:47,  3.42it/s] 37%|███▋      | 217/585 [01:28<01:49,  3.38it/s] 37%|███▋      | 218/585 [01:28<01:48,  3.39it/s] 37%|███▋      | 219/585 [01:28<01:47,  3.40it/s] 38%|███▊      | 220/585 [01:29<01:47,  3.41it/s] 38%|███▊      | 221/585 [01:29<01:46,  3.42it/s] 38%|███▊      | 222/585 [01:29<01:46,  3.42it/s] 38%|███▊      | 223/585 [01:29<01:45,  3.42it/s] 38%|███▊      | 224/585 [01:30<01:45,  3.42it/s] 38%|███▊      | 225/585 [01:30<01:45,  3.42it/s] 39%|███▊      | 226/585 [01:30<01:45,  3.41it/s] 39%|███▉      | 227/585 [01:31<01:44,  3.42it/s] 39%|███▉      | 228/585 [01:31<01:44,  3.41it/s] 39%|███▉      | 229/585 [01:31<01:44,  3.41it/s] 39%|███▉      | 230/585 [01:31<01:43,  3.42it/s] 39%|███▉      | 231/585 [01:32<01:43,  3.42it/s] 40%|███▉      | 232/585 [01:32<01:43,  3.42it/s] 40%|███▉      | 233/585 [01:32<01:42,  3.42it/s] 40%|████      | 234/585 [01:33<01:42,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 22:36:02,753 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:36:02,753 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:36:02,753 >>   Batch size = 8
{'eval_loss': 1.012073040008545, 'eval_runtime': 12.3501, 'eval_samples_per_second': 351.577, 'eval_steps_per_second': 43.967, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.05it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.59it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.82it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.11it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.77it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.51it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.36it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.32it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.41it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.51it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.34it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.12it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.04it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.04it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.12it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.11it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.25it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.25it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.22it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.29it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.18it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.04it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.03it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.18it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.20it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.30it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.37it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.14it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.20it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.09it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.99it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.98it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.06it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.15it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.27it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.22it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.30it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.23it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.09it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.06it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.02it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.16it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.24it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.26it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.15it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.24it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.28it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.08it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.08it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.12it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.17it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 43.99it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.03it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.10it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.24it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 43.94it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.96it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.93it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.01it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.12it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.06it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 40.95it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 42.03it/s][A
 60%|██████    | 327/543 [00:07<00:05, 42.84it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.23it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.43it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.63it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.82it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 43.89it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 43.69it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 43.80it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.12it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.20it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.23it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.22it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.23it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.23it/s][A
 73%|███████▎  | 397/543 [00:09<00:03, 43.87it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 43.86it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.02it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.22it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.30it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.23it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.33it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.22it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.10it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 43.91it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.00it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 43.99it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.14it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.06it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.18it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.22it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.20it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.05it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.85it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.01it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 43.79it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.85it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.08it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.14it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.02it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.05it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.03it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 43.95it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.01it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.01it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.01it/s][A 40%|████      | 234/585 [01:45<01:42,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:36:15,239 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 22:36:15,277 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:36:18,811 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:36:18,855 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:36:18,875 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:55<41:07,  7.05s/it] 40%|████      | 236/585 [01:56<29:13,  5.03s/it] 41%|████      | 237/585 [01:56<20:54,  3.61s/it] 41%|████      | 238/585 [01:56<15:06,  2.61s/it] 41%|████      | 239/585 [01:57<11:03,  1.92s/it] 41%|████      | 240/585 [01:57<08:13,  1.43s/it] 41%|████      | 241/585 [01:57<06:14,  1.09s/it] 41%|████▏     | 242/585 [01:58<04:52,  1.17it/s] 42%|████▏     | 243/585 [01:58<03:54,  1.46it/s] 42%|████▏     | 244/585 [01:58<03:13,  1.76it/s] 42%|████▏     | 245/585 [01:58<02:45,  2.06it/s] 42%|████▏     | 246/585 [01:59<02:25,  2.33it/s] 42%|████▏     | 247/585 [01:59<02:11,  2.57it/s] 42%|████▏     | 248/585 [01:59<02:01,  2.78it/s] 43%|████▎     | 249/585 [02:00<01:57,  2.85it/s] 43%|████▎     | 250/585 [02:00<01:51,  3.00it/s] 43%|████▎     | 251/585 [02:00<01:46,  3.12it/s] 43%|████▎     | 252/585 [02:01<01:43,  3.21it/s] 43%|████▎     | 253/585 [02:01<01:41,  3.28it/s] 43%|████▎     | 254/585 [02:01<01:39,  3.32it/s] 44%|████▎     | 255/585 [02:01<01:38,  3.36it/s] 44%|████▍     | 256/585 [02:02<01:37,  3.38it/s] 44%|████▍     | 257/585 [02:02<01:36,  3.40it/s] 44%|████▍     | 258/585 [02:02<01:37,  3.36it/s] 44%|████▍     | 259/585 [02:03<01:36,  3.38it/s] 44%|████▍     | 260/585 [02:03<01:35,  3.40it/s] 45%|████▍     | 261/585 [02:03<01:35,  3.41it/s] 45%|████▍     | 262/585 [02:03<01:34,  3.42it/s] 45%|████▍     | 263/585 [02:04<01:34,  3.42it/s] 45%|████▌     | 264/585 [02:04<01:33,  3.43it/s] 45%|████▌     | 265/585 [02:04<01:33,  3.43it/s] 45%|████▌     | 266/585 [02:05<01:33,  3.43it/s] 46%|████▌     | 267/585 [02:05<01:32,  3.43it/s] 46%|████▌     | 268/585 [02:05<01:32,  3.43it/s] 46%|████▌     | 269/585 [02:05<01:33,  3.40it/s] 46%|████▌     | 270/585 [02:06<01:32,  3.41it/s] 46%|████▋     | 271/585 [02:06<01:31,  3.42it/s] 46%|████▋     | 272/585 [02:06<01:31,  3.42it/s] 47%|████▋     | 273/585 [02:07<01:30,  3.43it/s] 47%|████▋     | 274/585 [02:07<01:30,  3.43it/s] 47%|████▋     | 275/585 [02:07<01:30,  3.43it/s] 47%|████▋     | 276/585 [02:08<01:30,  3.43it/s] 47%|████▋     | 277/585 [02:08<01:29,  3.43it/s] 48%|████▊     | 278/585 [02:08<01:29,  3.43it/s] 48%|████▊     | 279/585 [02:08<01:29,  3.44it/s] 48%|████▊     | 280/585 [02:09<01:30,  3.37it/s] 48%|████▊     | 281/585 [02:09<01:29,  3.39it/s] 48%|████▊     | 282/585 [02:09<01:29,  3.40it/s] 48%|████▊     | 283/585 [02:10<01:28,  3.41it/s] 49%|████▊     | 284/585 [02:10<01:28,  3.42it/s] 49%|████▊     | 285/585 [02:10<01:27,  3.42it/s] 49%|████▉     | 286/585 [02:10<01:27,  3.41it/s] 49%|████▉     | 287/585 [02:11<01:27,  3.40it/s] 49%|████▉     | 288/585 [02:11<01:27,  3.40it/s] 49%|████▉     | 289/585 [02:11<01:27,  3.39it/s] 50%|████▉     | 290/585 [02:12<01:26,  3.39it/s] 50%|████▉     | 291/585 [02:12<01:27,  3.37it/s] 50%|████▉     | 292/585 [02:12<01:26,  3.38it/s] 50%|█████     | 293/585 [02:13<01:26,  3.38it/s] 50%|█████     | 294/585 [02:13<01:25,  3.39it/s] 50%|█████     | 295/585 [02:13<01:25,  3.41it/s] 51%|█████     | 296/585 [02:13<01:24,  3.41it/s] 51%|█████     | 297/585 [02:14<01:24,  3.42it/s] 51%|█████     | 298/585 [02:14<01:23,  3.42it/s] 51%|█████     | 299/585 [02:14<01:23,  3.43it/s] 51%|█████▏    | 300/585 [02:15<01:23,  3.43it/s] 51%|█████▏    | 301/585 [02:15<01:22,  3.43it/s] 52%|█████▏    | 302/585 [02:15<01:23,  3.40it/s] 52%|█████▏    | 303/585 [02:15<01:22,  3.41it/s] 52%|█████▏    | 304/585 [02:16<01:22,  3.42it/s] 52%|█████▏    | 305/585 [02:16<01:21,  3.42it/s] 52%|█████▏    | 306/585 [02:16<01:21,  3.42it/s] 52%|█████▏    | 307/585 [02:17<01:21,  3.43it/s] 53%|█████▎    | 308/585 [02:17<01:20,  3.43it/s] 53%|█████▎    | 309/585 [02:17<01:20,  3.43it/s] 53%|█████▎    | 310/585 [02:17<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:18<01:19,  3.43it/s] 53%|█████▎    | 312/585 [02:18<01:19,  3.43it/s] 54%|█████▎    | 313/585 [02:18<01:19,  3.42it/s] 54%|█████▎    | 314/585 [02:19<01:19,  3.42it/s] 54%|█████▍    | 315/585 [02:19<01:18,  3.43it/s] 54%|█████▍    | 316/585 [02:19<01:18,  3.43it/s] 54%|█████▍    | 317/585 [02:20<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:20<01:17,  3.43it/s] 55%|█████▍    | 319/585 [02:20<01:17,  3.43it/s] 55%|█████▍    | 320/585 [02:20<01:17,  3.43it/s] 55%|█████▍    | 321/585 [02:21<01:16,  3.43it/s] 55%|█████▌    | 322/585 [02:21<01:16,  3.43it/s] 55%|█████▌    | 323/585 [02:21<01:16,  3.43it/s] 55%|█████▌    | 324/585 [02:22<01:16,  3.41it/s] 56%|█████▌    | 325/585 [02:22<01:16,  3.42it/s] 56%|█████▌    | 326/585 [02:22<01:15,  3.42it/s] 56%|█████▌    | 327/585 [02:22<01:15,  3.42it/s] 56%|█████▌    | 328/585 [02:23<01:15,  3.42it/s] 56%|█████▌    | 329/585 [02:23<01:14,  3.43it/s] 56%|█████▋    | 330/585 [02:23<01:14,  3.43it/s] 57%|█████▋    | 331/585 [02:24<01:14,  3.43it/s] 57%|█████▋    | 332/585 [02:24<01:13,  3.43it/s] 57%|█████▋    | 333/585 [02:24<01:13,  3.43it/s] 57%|█████▋    | 334/585 [02:24<01:13,  3.43it/s] 57%|█████▋    | 335/585 [02:25<01:12,  3.43it/s] 57%|█████▋    | 336/585 [02:25<01:12,  3.44it/s] 58%|█████▊    | 337/585 [02:25<01:12,  3.43it/s] 58%|█████▊    | 338/585 [02:26<01:11,  3.43it/s] 58%|█████▊    | 339/585 [02:26<01:11,  3.42it/s] 58%|█████▊    | 340/585 [02:26<01:11,  3.42it/s] 58%|█████▊    | 341/585 [02:27<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:27<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:27<01:10,  3.43it/s] 59%|█████▉    | 344/585 [02:27<01:10,  3.43it/s] 59%|█████▉    | 345/585 [02:28<01:09,  3.43it/s] 59%|█████▉    | 346/585 [02:28<01:09,  3.43it/s] 59%|█████▉    | 347/585 [02:28<01:09,  3.43it/s] 59%|█████▉    | 348/585 [02:29<01:09,  3.43it/s] 60%|█████▉    | 349/585 [02:29<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:29<01:10,  3.34it/s] 60%|██████    | 351/585 [02:29<01:09,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 22:36:59,577 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:36:59,578 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:36:59,578 >>   Batch size = 8
{'eval_loss': 1.0202925205230713, 'eval_runtime': 12.3459, 'eval_samples_per_second': 351.696, 'eval_steps_per_second': 43.982, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.17it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.90it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.21it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.05it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.69it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.48it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.38it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.34it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.30it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.39it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.24it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.02it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.12it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.10it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.11it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.14it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.19it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.33it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.23it/s][A
 19%|█▉        | 102/543 [00:02<00:10, 44.10it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.15it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.03it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.07it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.09it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.16it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.26it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.31it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.20it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.10it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.06it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.06it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.96it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.04it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.23it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.31it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.37it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.13it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.18it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.12it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 43.92it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.01it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.94it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.23it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.36it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.20it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.28it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.17it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.05it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.03it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.86it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.04it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.21it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.22it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.32it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.23it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.19it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.12it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 43.99it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.99it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.03it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.00it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.25it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.25it/s][A
 59%|█████▉    | 322/543 [00:07<00:04, 44.26it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.13it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.09it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.98it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.03it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.11it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.20it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.28it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.14it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.23it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.10it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.01it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.00it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.98it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.05it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.12it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.22it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.22it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.21it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.21it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.15it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.11it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.98it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.06it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.24it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.24it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.09it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.04it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.07it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 43.99it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.11it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.00it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.97it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.15it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.23it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.23it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.19it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.01it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.04it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.02it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.11it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.02it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.19it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.20it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.19it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.19it/s][A 60%|██████    | 351/585 [02:42<01:09,  3.37it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:37:11,910 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 22:37:11,942 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:37:14,621 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:37:14,638 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:37:14,648 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:51<26:20,  6.78s/it] 60%|██████    | 353/585 [02:52<18:42,  4.84s/it] 61%|██████    | 354/585 [02:52<13:22,  3.48s/it] 61%|██████    | 355/585 [02:52<09:39,  2.52s/it] 61%|██████    | 356/585 [02:53<07:04,  1.85s/it] 61%|██████    | 357/585 [02:53<05:16,  1.39s/it] 61%|██████    | 358/585 [02:53<04:00,  1.06s/it] 61%|██████▏   | 359/585 [02:53<03:07,  1.21it/s] 62%|██████▏   | 360/585 [02:54<02:30,  1.49it/s] 62%|██████▏   | 361/585 [02:54<02:04,  1.79it/s] 62%|██████▏   | 362/585 [02:54<01:46,  2.09it/s] 62%|██████▏   | 363/585 [02:55<01:34,  2.36it/s] 62%|██████▏   | 364/585 [02:55<01:24,  2.61it/s] 62%|██████▏   | 365/585 [02:55<01:18,  2.80it/s] 63%|██████▎   | 366/585 [02:56<01:13,  2.96it/s] 63%|██████▎   | 367/585 [02:56<01:10,  3.09it/s] 63%|██████▎   | 368/585 [02:56<01:08,  3.18it/s] 63%|██████▎   | 369/585 [02:56<01:06,  3.26it/s] 63%|██████▎   | 370/585 [02:57<01:05,  3.28it/s] 63%|██████▎   | 371/585 [02:57<01:04,  3.33it/s] 64%|██████▎   | 372/585 [02:57<01:03,  3.36it/s] 64%|██████▍   | 373/585 [02:58<01:02,  3.38it/s] 64%|██████▍   | 374/585 [02:58<01:02,  3.40it/s] 64%|██████▍   | 375/585 [02:58<01:01,  3.41it/s] 64%|██████▍   | 376/585 [02:58<01:01,  3.41it/s] 64%|██████▍   | 377/585 [02:59<01:00,  3.42it/s] 65%|██████▍   | 378/585 [02:59<01:00,  3.42it/s] 65%|██████▍   | 379/585 [02:59<01:00,  3.43it/s] 65%|██████▍   | 380/585 [03:00<01:03,  3.25it/s] 65%|██████▌   | 381/585 [03:00<01:02,  3.29it/s] 65%|██████▌   | 382/585 [03:00<01:01,  3.33it/s] 65%|██████▌   | 383/585 [03:01<01:00,  3.36it/s] 66%|██████▌   | 384/585 [03:01<00:59,  3.38it/s] 66%|██████▌   | 385/585 [03:01<00:58,  3.39it/s] 66%|██████▌   | 386/585 [03:01<00:58,  3.41it/s] 66%|██████▌   | 387/585 [03:02<00:57,  3.42it/s] 66%|██████▋   | 388/585 [03:02<00:57,  3.42it/s] 66%|██████▋   | 389/585 [03:02<00:57,  3.42it/s] 67%|██████▋   | 390/585 [03:03<00:57,  3.41it/s] 67%|██████▋   | 391/585 [03:03<00:56,  3.41it/s] 67%|██████▋   | 392/585 [03:03<00:57,  3.38it/s] 67%|██████▋   | 393/585 [03:03<00:56,  3.38it/s] 67%|██████▋   | 394/585 [03:04<00:56,  3.38it/s] 68%|██████▊   | 395/585 [03:04<00:56,  3.38it/s] 68%|██████▊   | 396/585 [03:04<00:55,  3.39it/s] 68%|██████▊   | 397/585 [03:05<00:55,  3.39it/s] 68%|██████▊   | 398/585 [03:05<00:55,  3.38it/s] 68%|██████▊   | 399/585 [03:05<00:54,  3.38it/s] 68%|██████▊   | 400/585 [03:06<00:54,  3.38it/s] 69%|██████▊   | 401/585 [03:06<00:54,  3.38it/s] 69%|██████▊   | 402/585 [03:06<00:54,  3.38it/s] 69%|██████▉   | 403/585 [03:06<00:55,  3.26it/s] 69%|██████▉   | 404/585 [03:07<00:55,  3.29it/s] 69%|██████▉   | 405/585 [03:07<00:54,  3.32it/s] 69%|██████▉   | 406/585 [03:07<00:53,  3.34it/s] 70%|██████▉   | 407/585 [03:08<00:53,  3.35it/s] 70%|██████▉   | 408/585 [03:08<00:52,  3.36it/s] 70%|██████▉   | 409/585 [03:08<00:52,  3.36it/s] 70%|███████   | 410/585 [03:09<00:51,  3.37it/s] 70%|███████   | 411/585 [03:09<00:51,  3.37it/s] 70%|███████   | 412/585 [03:09<00:51,  3.38it/s] 71%|███████   | 413/585 [03:09<00:50,  3.38it/s] 71%|███████   | 414/585 [03:10<00:51,  3.33it/s] 71%|███████   | 415/585 [03:10<00:50,  3.35it/s] 71%|███████   | 416/585 [03:10<00:50,  3.36it/s] 71%|███████▏  | 417/585 [03:11<00:49,  3.36it/s] 71%|███████▏  | 418/585 [03:11<00:49,  3.37it/s] 72%|███████▏  | 419/585 [03:11<00:49,  3.37it/s] 72%|███████▏  | 420/585 [03:12<00:48,  3.38it/s] 72%|███████▏  | 421/585 [03:12<00:48,  3.37it/s] 72%|███████▏  | 422/585 [03:12<00:48,  3.37it/s] 72%|███████▏  | 423/585 [03:12<00:47,  3.38it/s] 72%|███████▏  | 424/585 [03:13<00:47,  3.38it/s] 73%|███████▎  | 425/585 [03:13<00:47,  3.37it/s] 73%|███████▎  | 426/585 [03:13<00:47,  3.37it/s] 73%|███████▎  | 427/585 [03:14<00:46,  3.37it/s] 73%|███████▎  | 428/585 [03:14<00:46,  3.38it/s] 73%|███████▎  | 429/585 [03:14<00:46,  3.38it/s] 74%|███████▎  | 430/585 [03:14<00:45,  3.38it/s] 74%|███████▎  | 431/585 [03:15<00:45,  3.38it/s] 74%|███████▍  | 432/585 [03:15<00:45,  3.38it/s] 74%|███████▍  | 433/585 [03:15<00:44,  3.38it/s] 74%|███████▍  | 434/585 [03:16<00:44,  3.38it/s] 74%|███████▍  | 435/585 [03:16<00:44,  3.38it/s] 75%|███████▍  | 436/585 [03:16<00:44,  3.33it/s] 75%|███████▍  | 437/585 [03:17<00:44,  3.35it/s] 75%|███████▍  | 438/585 [03:17<00:43,  3.38it/s] 75%|███████▌  | 439/585 [03:17<00:43,  3.39it/s] 75%|███████▌  | 440/585 [03:17<00:42,  3.40it/s] 75%|███████▌  | 441/585 [03:18<00:42,  3.41it/s] 76%|███████▌  | 442/585 [03:18<00:41,  3.41it/s] 76%|███████▌  | 443/585 [03:18<00:41,  3.42it/s] 76%|███████▌  | 444/585 [03:19<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:19<00:40,  3.42it/s] 76%|███████▌  | 446/585 [03:19<00:40,  3.42it/s] 76%|███████▋  | 447/585 [03:19<00:40,  3.40it/s] 77%|███████▋  | 448/585 [03:20<00:40,  3.41it/s] 77%|███████▋  | 449/585 [03:20<00:39,  3.42it/s] 77%|███████▋  | 450/585 [03:20<00:39,  3.42it/s] 77%|███████▋  | 451/585 [03:21<00:39,  3.42it/s] 77%|███████▋  | 452/585 [03:21<00:38,  3.42it/s] 77%|███████▋  | 453/585 [03:21<00:38,  3.42it/s] 78%|███████▊  | 454/585 [03:22<00:38,  3.43it/s] 78%|███████▊  | 455/585 [03:22<00:37,  3.43it/s] 78%|███████▊  | 456/585 [03:22<00:37,  3.43it/s] 78%|███████▊  | 457/585 [03:22<00:37,  3.43it/s] 78%|███████▊  | 458/585 [03:23<00:37,  3.38it/s] 78%|███████▊  | 459/585 [03:23<00:37,  3.39it/s] 79%|███████▊  | 460/585 [03:23<00:36,  3.41it/s] 79%|███████▉  | 461/585 [03:24<00:36,  3.41it/s] 79%|███████▉  | 462/585 [03:24<00:36,  3.41it/s] 79%|███████▉  | 463/585 [03:24<00:35,  3.42it/s] 79%|███████▉  | 464/585 [03:24<00:35,  3.42it/s] 79%|███████▉  | 465/585 [03:25<00:35,  3.42it/s] 80%|███████▉  | 466/585 [03:25<00:34,  3.42it/s] 80%|███████▉  | 467/585 [03:25<00:34,  3.42it/s] 80%|████████  | 468/585 [03:26<00:34,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 22:37:55,728 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:37:55,728 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:37:55,728 >>   Batch size = 8
{'eval_loss': 1.033337116241455, 'eval_runtime': 12.3179, 'eval_samples_per_second': 352.496, 'eval_steps_per_second': 44.082, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.81it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.62it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.93it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.21it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.84it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.45it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.37it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.35it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.36it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.42it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.27it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.17it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.04it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.14it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.12it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.07it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.12it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.27it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.12it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.10it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.09it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.10it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.07it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.99it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.05it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.15it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.21it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.04it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.05it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.04it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.09it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.06it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.22it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.14it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.25it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.28it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.12it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.04it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.00it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.01it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.04it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.17it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.21it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.23it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.12it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.07it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.03it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.98it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.03it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.05it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.04it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.14it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.20it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.08it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.08it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.04it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.06it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.99it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.98it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.15it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.03it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.21it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 43.99it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.10it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.89it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 43.97it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.99it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.04it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.11it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.07it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.11it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.05it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.10it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.01it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.07it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.96it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.05it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.01it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.11it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.13it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.14it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.07it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.10it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.08it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.05it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.03it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.07it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.14it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.21it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.14it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.10it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.10it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.04it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.97it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.03it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 43.96it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.18it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.15it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 43.98it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 43.99it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.12it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.10it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.06it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.10it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.04it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.10it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.23it/s][A 80%|████████  | 468/585 [03:38<00:34,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:38:08,104 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 22:38:08,140 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:38:12,185 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:38:12,213 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:38:12,245 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:49<13:43,  7.10s/it] 80%|████████  | 470/585 [03:49<09:41,  5.06s/it] 81%|████████  | 471/585 [03:49<06:53,  3.63s/it] 81%|████████  | 472/585 [03:49<04:56,  2.63s/it] 81%|████████  | 473/585 [03:50<03:35,  1.93s/it] 81%|████████  | 474/585 [03:50<02:39,  1.44s/it] 81%|████████  | 475/585 [03:50<02:00,  1.10s/it] 81%|████████▏ | 476/585 [03:51<01:33,  1.17it/s] 82%|████████▏ | 477/585 [03:51<01:14,  1.45it/s] 82%|████████▏ | 478/585 [03:51<01:00,  1.75it/s] 82%|████████▏ | 479/585 [03:52<00:51,  2.05it/s] 82%|████████▏ | 480/585 [03:52<00:45,  2.33it/s] 82%|████████▏ | 481/585 [03:52<00:40,  2.56it/s] 82%|████████▏ | 482/585 [03:52<00:37,  2.76it/s] 83%|████████▎ | 483/585 [03:53<00:34,  2.92it/s] 83%|████████▎ | 484/585 [03:53<00:33,  3.05it/s] 83%|████████▎ | 485/585 [03:53<00:31,  3.14it/s] 83%|████████▎ | 486/585 [03:54<00:30,  3.22it/s] 83%|████████▎ | 487/585 [03:54<00:29,  3.27it/s] 83%|████████▎ | 488/585 [03:54<00:29,  3.30it/s] 84%|████████▎ | 489/585 [03:55<00:28,  3.33it/s] 84%|████████▍ | 490/585 [03:55<00:28,  3.35it/s] 84%|████████▍ | 491/585 [03:55<00:27,  3.36it/s] 84%|████████▍ | 492/585 [03:55<00:27,  3.36it/s] 84%|████████▍ | 493/585 [03:56<00:27,  3.37it/s] 84%|████████▍ | 494/585 [03:56<00:26,  3.37it/s] 85%|████████▍ | 495/585 [03:56<00:26,  3.38it/s] 85%|████████▍ | 496/585 [03:57<00:26,  3.38it/s] 85%|████████▍ | 497/585 [03:57<00:26,  3.38it/s] 85%|████████▌ | 498/585 [03:57<00:25,  3.38it/s] 85%|████████▌ | 499/585 [03:57<00:25,  3.39it/s] 85%|████████▌ | 500/585 [03:58<00:25,  3.39it/s]                                                  85%|████████▌ | 500/585 [03:58<00:25,  3.39it/s] 86%|████████▌ | 501/585 [03:58<00:24,  3.37it/s] 86%|████████▌ | 502/585 [03:58<00:24,  3.38it/s] 86%|████████▌ | 503/585 [03:59<00:24,  3.38it/s] 86%|████████▌ | 504/585 [03:59<00:23,  3.38it/s] 86%|████████▋ | 505/585 [03:59<00:23,  3.38it/s] 86%|████████▋ | 506/585 [04:00<00:23,  3.38it/s] 87%|████████▋ | 507/585 [04:00<00:23,  3.29it/s] 87%|████████▋ | 508/585 [04:00<00:23,  3.32it/s] 87%|████████▋ | 509/585 [04:00<00:22,  3.34it/s] 87%|████████▋ | 510/585 [04:01<00:22,  3.36it/s] 87%|████████▋ | 511/585 [04:01<00:21,  3.37it/s] 88%|████████▊ | 512/585 [04:01<00:21,  3.35it/s] 88%|████████▊ | 513/585 [04:02<00:21,  3.36it/s] 88%|████████▊ | 514/585 [04:02<00:21,  3.37it/s] 88%|████████▊ | 515/585 [04:02<00:20,  3.38it/s] 88%|████████▊ | 516/585 [04:03<00:20,  3.38it/s] 88%|████████▊ | 517/585 [04:03<00:20,  3.39it/s] 89%|████████▊ | 518/585 [04:03<00:19,  3.39it/s] 89%|████████▊ | 519/585 [04:03<00:19,  3.39it/s] 89%|████████▉ | 520/585 [04:04<00:19,  3.39it/s] 89%|████████▉ | 521/585 [04:04<00:18,  3.39it/s] 89%|████████▉ | 522/585 [04:04<00:18,  3.38it/s] 89%|████████▉ | 523/585 [04:05<00:18,  3.30it/s] 90%|████████▉ | 524/585 [04:05<00:18,  3.33it/s] 90%|████████▉ | 525/585 [04:05<00:17,  3.34it/s] 90%|████████▉ | 526/585 [04:05<00:17,  3.36it/s] 90%|█████████ | 527/585 [04:06<00:17,  3.35it/s] 90%|█████████ | 528/585 [04:06<00:16,  3.36it/s] 90%|█████████ | 529/585 [04:06<00:16,  3.37it/s] 91%|█████████ | 530/585 [04:07<00:16,  3.37it/s] 91%|█████████ | 531/585 [04:07<00:15,  3.38it/s] 91%|█████████ | 532/585 [04:07<00:15,  3.38it/s] 91%|█████████ | 533/585 [04:08<00:15,  3.38it/s] 91%|█████████▏| 534/585 [04:08<00:15,  3.37it/s] 91%|█████████▏| 535/585 [04:08<00:14,  3.37it/s] 92%|█████████▏| 536/585 [04:08<00:14,  3.37it/s] 92%|█████████▏| 537/585 [04:09<00:14,  3.38it/s] 92%|█████████▏| 538/585 [04:09<00:13,  3.38it/s] 92%|█████████▏| 539/585 [04:09<00:13,  3.38it/s] 92%|█████████▏| 540/585 [04:10<00:13,  3.38it/s] 92%|█████████▏| 541/585 [04:10<00:13,  3.38it/s] 93%|█████████▎| 542/585 [04:10<00:12,  3.38it/s] 93%|█████████▎| 543/585 [04:11<00:12,  3.39it/s] 93%|█████████▎| 544/585 [04:11<00:12,  3.37it/s] 93%|█████████▎| 545/585 [04:11<00:11,  3.37it/s] 93%|█████████▎| 546/585 [04:11<00:11,  3.37it/s] 94%|█████████▎| 547/585 [04:12<00:11,  3.38it/s] 94%|█████████▎| 548/585 [04:12<00:10,  3.38it/s] 94%|█████████▍| 549/585 [04:12<00:10,  3.38it/s] 94%|█████████▍| 550/585 [04:13<00:10,  3.38it/s] 94%|█████████▍| 551/585 [04:13<00:10,  3.39it/s] 94%|█████████▍| 552/585 [04:13<00:09,  3.40it/s] 95%|█████████▍| 553/585 [04:13<00:09,  3.41it/s] 95%|█████████▍| 554/585 [04:14<00:09,  3.42it/s] 95%|█████████▍| 555/585 [04:14<00:08,  3.42it/s] 95%|█████████▌| 556/585 [04:14<00:08,  3.42it/s] 95%|█████████▌| 557/585 [04:15<00:08,  3.43it/s] 95%|█████████▌| 558/585 [04:15<00:07,  3.43it/s] 96%|█████████▌| 559/585 [04:15<00:07,  3.43it/s] 96%|█████████▌| 560/585 [04:16<00:07,  3.43it/s] 96%|█████████▌| 561/585 [04:16<00:06,  3.43it/s] 96%|█████████▌| 562/585 [04:16<00:06,  3.43it/s] 96%|█████████▌| 563/585 [04:16<00:06,  3.43it/s] 96%|█████████▋| 564/585 [04:17<00:06,  3.43it/s] 97%|█████████▋| 565/585 [04:17<00:05,  3.43it/s] 97%|█████████▋| 566/585 [04:17<00:05,  3.43it/s] 97%|█████████▋| 567/585 [04:18<00:05,  3.42it/s] 97%|█████████▋| 568/585 [04:18<00:04,  3.42it/s] 97%|█████████▋| 569/585 [04:18<00:04,  3.42it/s] 97%|█████████▋| 570/585 [04:18<00:04,  3.42it/s] 98%|█████████▊| 571/585 [04:19<00:04,  3.43it/s] 98%|█████████▊| 572/585 [04:19<00:03,  3.43it/s] 98%|█████████▊| 573/585 [04:19<00:03,  3.43it/s] 98%|█████████▊| 574/585 [04:20<00:03,  3.43it/s] 98%|█████████▊| 575/585 [04:20<00:02,  3.43it/s] 98%|█████████▊| 576/585 [04:20<00:02,  3.43it/s] 99%|█████████▊| 577/585 [04:20<00:02,  3.43it/s] 99%|█████████▉| 578/585 [04:21<00:02,  3.42it/s] 99%|█████████▉| 579/585 [04:21<00:01,  3.43it/s] 99%|█████████▉| 580/585 [04:21<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:22<00:01,  3.43it/s] 99%|█████████▉| 582/585 [04:22<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:22<00:00,  3.43it/s]100%|█████████▉| 584/585 [04:23<00:00,  3.43it/s]100%|██████████| 585/585 [04:23<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 22:38:52,888 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:38:52,888 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:38:52,888 >>   Batch size = 8
{'eval_loss': 1.0392671823501587, 'eval_runtime': 12.3273, 'eval_samples_per_second': 352.227, 'eval_steps_per_second': 44.049, 'epoch': 4.0}
{'loss': 0.5181, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.92it/s][A
  2%|▏         | 12/543 [00:00<00:11, 48.23it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.35it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.40it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.99it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.52it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.20it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.24it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.27it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.48it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.33it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.38it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.31it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.11it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.95it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.87it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 43.99it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.21it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.35it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.29it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.26it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.18it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.13it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 43.89it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 43.96it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 43.93it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.17it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.32it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.00it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.14it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.07it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.04it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.89it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 43.90it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.05it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.03it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.20it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 43.98it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.14it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.07it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.00it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.92it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.96it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.06it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.20it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.09it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.23it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.22it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.07it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.01it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 43.69it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 43.90it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.06it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.00it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.14it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.18it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.26it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.16it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.95it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 43.96it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.11it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.15it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.14it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.19it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.15it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.29it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.11it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 43.99it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.99it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.05it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.19it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.23it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.25it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.08it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.20it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.13it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.55it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.11it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.13it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.21it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.29it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.28it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.23it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.22it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.09it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.00it/s][A
 80%|████████  | 437/543 [00:09<00:02, 43.97it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 43.98it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.20it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.25it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.26it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.15it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.23it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.14it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 43.97it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 43.97it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.01it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.27it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.21it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.27it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.24it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.23it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.04it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 43.92it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.00it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.05it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.15it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.23it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.23it/s][A100%|██████████| 585/585 [04:35<00:00,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 22:39:05,242 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 22:39:05,286 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:39:08,715 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:39:08,735 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:39:08,743 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 22:39:17,514 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 22:39:17,517 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117 (score: 1.012073040008545).
                                                 100%|██████████| 585/585 [04:52<00:00,  3.42it/s]100%|██████████| 585/585 [04:52<00:00,  2.00it/s]
[INFO|trainer.py:1894] 2023-08-28 22:39:21,688 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 22:39:21,755 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 22:39:26,000 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 22:39:26,023 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 22:39:26,030 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:39:26,252 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:26,252 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:26,252 >>   train_loss               =     0.5148
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:26,252 >>   train_runtime            = 0:04:52.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:26,252 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:26,252 >>   train_samples_per_second =    128.393
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:26,252 >>   train_steps_per_second   =      2.003
{'eval_loss': 1.0481154918670654, 'eval_runtime': 12.3059, 'eval_samples_per_second': 352.838, 'eval_steps_per_second': 44.125, 'epoch': 5.0}
{'train_runtime': 292.0336, 'train_samples_per_second': 128.393, 'train_steps_per_second': 2.003, 'train_loss': 0.5148411547016893, 'epoch': 5.0}
08/28/2023 22:39:26 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 22:39:26,349 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 22:39:26,349 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-28 22:39:26,349 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.89it/s]  2%|▏         | 12/543 [00:00<00:10, 49.13it/s]  3%|▎         | 17/543 [00:00<00:11, 47.48it/s]  4%|▍         | 22/543 [00:00<00:11, 46.56it/s]  5%|▍         | 27/543 [00:00<00:11, 46.04it/s]  6%|▌         | 32/543 [00:00<00:11, 45.78it/s]  7%|▋         | 37/543 [00:00<00:11, 45.41it/s]  8%|▊         | 42/543 [00:00<00:11, 44.92it/s]  9%|▊         | 47/543 [00:01<00:11, 44.30it/s] 10%|▉         | 52/543 [00:01<00:11, 44.04it/s] 10%|█         | 57/543 [00:01<00:10, 44.22it/s] 11%|█▏        | 62/543 [00:01<00:10, 44.43it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.60it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.83it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.75it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.80it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.42it/s] 17%|█▋        | 92/543 [00:02<00:10, 44.07it/s] 18%|█▊        | 97/543 [00:02<00:10, 44.00it/s] 19%|█▉        | 102/543 [00:02<00:09, 44.16it/s] 20%|█▉        | 107/543 [00:02<00:09, 44.27it/s] 21%|██        | 112/543 [00:02<00:09, 44.46it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.41it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.58it/s] 23%|██▎       | 127/543 [00:02<00:09, 43.74it/s] 24%|██▍       | 132/543 [00:02<00:09, 43.82it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.70it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.80it/s] 27%|██▋       | 147/543 [00:03<00:09, 43.96it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.03it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.30it/s] 30%|██▉       | 162/543 [00:03<00:08, 43.25it/s] 31%|███       | 167/543 [00:03<00:08, 43.66it/s] 32%|███▏      | 172/543 [00:03<00:08, 43.92it/s] 33%|███▎      | 177/543 [00:03<00:08, 43.83it/s] 34%|███▎      | 182/543 [00:04<00:08, 43.93it/s] 34%|███▍      | 187/543 [00:04<00:08, 43.91it/s] 35%|███▌      | 192/543 [00:04<00:07, 44.06it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.12it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.20it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.27it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.48it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.35it/s] 41%|████      | 222/543 [00:04<00:07, 44.23it/s] 42%|████▏     | 227/543 [00:05<00:07, 44.21it/s] 43%|████▎     | 232/543 [00:05<00:07, 44.24it/s] 44%|████▎     | 237/543 [00:05<00:06, 44.23it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.23it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.20it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.31it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.35it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.38it/s] 49%|████▉     | 267/543 [00:06<00:06, 44.25it/s] 50%|█████     | 272/543 [00:06<00:06, 44.18it/s] 51%|█████     | 277/543 [00:06<00:06, 44.15it/s] 52%|█████▏    | 282/543 [00:06<00:05, 44.24it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.19it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.16it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.36it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.36it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.27it/s] 57%|█████▋    | 312/543 [00:07<00:05, 44.28it/s] 58%|█████▊    | 317/543 [00:07<00:05, 44.14it/s] 59%|█████▉    | 322/543 [00:07<00:05, 44.17it/s] 60%|██████    | 327/543 [00:07<00:04, 44.20it/s] 61%|██████    | 332/543 [00:07<00:04, 44.18it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.12it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.30it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.39it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.16it/s] 66%|██████▌   | 357/543 [00:08<00:04, 44.20it/s] 67%|██████▋   | 362/543 [00:08<00:04, 44.16it/s] 68%|██████▊   | 367/543 [00:08<00:03, 44.18it/s] 69%|██████▊   | 372/543 [00:08<00:03, 44.19it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.05it/s] 70%|███████   | 382/543 [00:08<00:03, 44.27it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.29it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.36it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.26it/s] 74%|███████▍  | 402/543 [00:09<00:03, 44.13it/s] 75%|███████▍  | 407/543 [00:09<00:03, 44.19it/s] 76%|███████▌  | 412/543 [00:09<00:02, 44.25it/s] 77%|███████▋  | 417/543 [00:09<00:02, 44.10it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.10it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.31it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.37it/s] 80%|████████  | 437/543 [00:09<00:02, 44.32it/s] 81%|████████▏ | 442/543 [00:09<00:02, 44.24it/s] 82%|████████▏ | 447/543 [00:10<00:02, 44.11it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.05it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.12it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.12it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.00it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.27it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.20it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.41it/s] 90%|████████▉ | 487/543 [00:10<00:01, 44.23it/s] 91%|█████████ | 492/543 [00:11<00:01, 44.22it/s] 92%|█████████▏| 497/543 [00:11<00:01, 44.16it/s] 92%|█████████▏| 502/543 [00:11<00:00, 44.15it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.04it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.22it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.15it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.16it/s] 97%|█████████▋| 527/543 [00:11<00:00, 44.31it/s] 98%|█████████▊| 532/543 [00:11<00:00, 44.25it/s] 99%|█████████▉| 537/543 [00:12<00:00, 44.17it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.14it/s]100%|██████████| 543/543 [00:12<00:00, 44.30it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 22:39:38,624 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:38,624 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:38,624 >>   eval_loss               =     1.0121
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:38,624 >>   eval_runtime            = 0:00:12.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:38,624 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:38,624 >>   eval_samples_per_second =    353.722
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:38,625 >>   eval_steps_per_second   =     44.236
[INFO|trainer_pt_utils.py:913] 2023-08-28 22:39:38,625 >>   perplexity              =     2.7513
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,247 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:45,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:39:45,564 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:39:45,564 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:39:45,839 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:39:46,942 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:39:46,942 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:49,070 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:49,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:49,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:49,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:39:49,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:39:49,435 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:39:49,436 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:39:49,702 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:39:49,857 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:39:49,858 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.78it/s]Extractor Predicting: 2it [00:01,  1.75it/s]Extractor Predicting: 3it [00:01,  1.73it/s]Extractor Predicting: 4it [00:02,  1.78it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.69it/s]Extractor Predicting: 7it [00:04,  1.71it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.79it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.63it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:09,  1.62it/s]Extractor Predicting: 18it [00:10,  1.60it/s]Extractor Predicting: 19it [00:11,  1.54it/s]Extractor Predicting: 20it [00:11,  1.52it/s]Extractor Predicting: 21it [00:12,  1.55it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:13,  1.59it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:15,  1.57it/s]Extractor Predicting: 27it [00:16,  1.56it/s]Extractor Predicting: 28it [00:16,  1.59it/s]Extractor Predicting: 29it [00:17,  1.56it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:18,  1.58it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:20,  1.57it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.52it/s]Extractor Predicting: 37it [00:22,  1.53it/s]Extractor Predicting: 38it [00:23,  1.52it/s]Extractor Predicting: 39it [00:24,  1.56it/s]Extractor Predicting: 40it [00:24,  1.57it/s]Extractor Predicting: 41it [00:25,  1.55it/s]Extractor Predicting: 42it [00:26,  1.54it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.62it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:29,  1.59it/s]Extractor Predicting: 49it [00:30,  1.58it/s]Extractor Predicting: 50it [00:30,  1.60it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:32,  1.55it/s]Extractor Predicting: 54it [00:33,  1.49it/s]Extractor Predicting: 55it [00:34,  1.55it/s]Extractor Predicting: 56it [00:34,  1.52it/s]Extractor Predicting: 57it [00:35,  1.52it/s]Extractor Predicting: 58it [00:36,  1.57it/s]Extractor Predicting: 59it [00:36,  1.54it/s]Extractor Predicting: 60it [00:37,  1.56it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:38,  1.59it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:39,  1.59it/s]Extractor Predicting: 65it [00:40,  1.63it/s]Extractor Predicting: 66it [00:41,  1.62it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.59it/s]Extractor Predicting: 69it [00:43,  1.60it/s]Extractor Predicting: 70it [00:43,  1.60it/s]Extractor Predicting: 71it [00:44,  1.59it/s]Extractor Predicting: 72it [00:44,  1.59it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:46,  1.56it/s]Extractor Predicting: 75it [00:46,  1.53it/s]Extractor Predicting: 76it [00:47,  1.55it/s]Extractor Predicting: 77it [00:48,  1.59it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:49,  1.55it/s]Extractor Predicting: 80it [00:50,  1.58it/s]Extractor Predicting: 81it [00:50,  1.58it/s]Extractor Predicting: 82it [00:51,  1.58it/s]Extractor Predicting: 83it [00:51,  1.60it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:53,  1.62it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:55,  1.62it/s]Extractor Predicting: 89it [00:55,  1.62it/s]Extractor Predicting: 90it [00:56,  1.60it/s]Extractor Predicting: 91it [00:56,  1.57it/s]Extractor Predicting: 92it [00:57,  1.56it/s]Extractor Predicting: 93it [00:58,  1.61it/s]Extractor Predicting: 94it [00:58,  1.62it/s]Extractor Predicting: 95it [00:59,  1.61it/s]Extractor Predicting: 96it [01:00,  1.62it/s]Extractor Predicting: 97it [01:00,  1.64it/s]Extractor Predicting: 98it [01:01,  1.62it/s]Extractor Predicting: 99it [01:01,  1.60it/s]Extractor Predicting: 100it [01:02,  1.60it/s]Extractor Predicting: 101it [01:03,  1.61it/s]Extractor Predicting: 102it [01:03,  1.60it/s]Extractor Predicting: 103it [01:04,  1.61it/s]Extractor Predicting: 104it [01:05,  1.62it/s]Extractor Predicting: 105it [01:05,  1.59it/s]Extractor Predicting: 106it [01:06,  1.62it/s]Extractor Predicting: 107it [01:06,  1.61it/s]Extractor Predicting: 108it [01:07,  1.61it/s]Extractor Predicting: 109it [01:08,  1.60it/s]Extractor Predicting: 110it [01:08,  1.59it/s]Extractor Predicting: 111it [01:09,  1.60it/s]Extractor Predicting: 112it [01:09,  1.64it/s]Extractor Predicting: 113it [01:10,  1.64it/s]Extractor Predicting: 114it [01:11,  1.62it/s]Extractor Predicting: 115it [01:11,  1.61it/s]Extractor Predicting: 116it [01:12,  1.60it/s]Extractor Predicting: 117it [01:13,  1.47it/s]Extractor Predicting: 118it [01:13,  1.47it/s]Extractor Predicting: 119it [01:14,  1.50it/s]Extractor Predicting: 120it [01:15,  1.51it/s]Extractor Predicting: 121it [01:15,  1.51it/s]Extractor Predicting: 122it [01:16,  1.51it/s]Extractor Predicting: 123it [01:17,  1.56it/s]Extractor Predicting: 124it [01:17,  1.58it/s]Extractor Predicting: 125it [01:18,  1.60it/s]Extractor Predicting: 126it [01:19,  1.56it/s]Extractor Predicting: 127it [01:19,  1.55it/s]Extractor Predicting: 128it [01:20,  1.57it/s]Extractor Predicting: 129it [01:20,  1.59it/s]Extractor Predicting: 130it [01:21,  1.64it/s]Extractor Predicting: 131it [01:22,  1.59it/s]Extractor Predicting: 132it [01:22,  1.59it/s]Extractor Predicting: 133it [01:23,  1.55it/s]Extractor Predicting: 134it [01:24,  1.57it/s]Extractor Predicting: 135it [01:24,  1.59it/s]Extractor Predicting: 136it [01:25,  1.59it/s]Extractor Predicting: 137it [01:25,  1.61it/s]Extractor Predicting: 138it [01:26,  1.57it/s]Extractor Predicting: 139it [01:27,  1.56it/s]Extractor Predicting: 140it [01:27,  1.60it/s]Extractor Predicting: 141it [01:28,  1.61it/s]Extractor Predicting: 142it [01:29,  1.64it/s]Extractor Predicting: 143it [01:29,  1.63it/s]Extractor Predicting: 144it [01:30,  1.63it/s]Extractor Predicting: 145it [01:30,  1.61it/s]Extractor Predicting: 146it [01:31,  1.62it/s]Extractor Predicting: 147it [01:32,  1.62it/s]Extractor Predicting: 148it [01:32,  1.62it/s]Extractor Predicting: 149it [01:33,  1.63it/s]Extractor Predicting: 150it [01:34,  1.59it/s]Extractor Predicting: 151it [01:34,  1.59it/s]Extractor Predicting: 152it [01:35,  1.60it/s]Extractor Predicting: 153it [01:35,  1.57it/s]Extractor Predicting: 154it [01:36,  1.59it/s]Extractor Predicting: 155it [01:37,  1.56it/s]Extractor Predicting: 156it [01:37,  1.56it/s]Extractor Predicting: 157it [01:38,  1.56it/s]Extractor Predicting: 158it [01:39,  1.56it/s]Extractor Predicting: 159it [01:39,  1.59it/s]Extractor Predicting: 160it [01:40,  1.63it/s]Extractor Predicting: 161it [01:40,  1.64it/s]Extractor Predicting: 162it [01:41,  1.64it/s]Extractor Predicting: 163it [01:42,  1.52it/s]Extractor Predicting: 163it [01:42,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:46,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:46,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:46,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:46,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:46,054 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:41:46,930 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:41:46,931 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:41:48,124 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:41:49,162 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:41:49,164 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:53,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:53,921 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:53,921 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:53,921 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:41:53,921 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:41:55,172 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:41:55,173 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:41:55,889 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:41:56,119 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:41:56,119 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.19375,
  "recall": 0.02855826807922616,
  "score": 0.049779205138498595,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.73it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.66it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.66it/s]Extractor Predicting: 16it [00:09,  1.52it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:10,  1.57it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.67it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.67it/s]Extractor Predicting: 33it [00:20,  1.66it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.68it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.63it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:24,  1.59it/s]Extractor Predicting: 42it [00:25,  1.59it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:26,  1.58it/s]Extractor Predicting: 45it [00:27,  1.56it/s]Extractor Predicting: 46it [00:28,  1.54it/s]Extractor Predicting: 47it [00:28,  1.50it/s]Extractor Predicting: 48it [00:29,  1.53it/s]Extractor Predicting: 49it [00:30,  1.52it/s]Extractor Predicting: 50it [00:30,  1.54it/s]Extractor Predicting: 51it [00:31,  1.55it/s]Extractor Predicting: 52it [00:32,  1.54it/s]Extractor Predicting: 53it [00:32,  1.53it/s]Extractor Predicting: 54it [00:33,  1.49it/s]Extractor Predicting: 55it [00:34,  1.53it/s]Extractor Predicting: 56it [00:34,  1.58it/s]Extractor Predicting: 57it [00:35,  1.56it/s]Extractor Predicting: 58it [00:35,  1.56it/s]Extractor Predicting: 59it [00:36,  1.53it/s]Extractor Predicting: 60it [00:37,  1.53it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.55it/s]Extractor Predicting: 63it [00:39,  1.51it/s]Extractor Predicting: 64it [00:39,  1.52it/s]Extractor Predicting: 65it [00:40,  1.53it/s]Extractor Predicting: 66it [00:41,  1.51it/s]Extractor Predicting: 67it [00:41,  1.52it/s]Extractor Predicting: 68it [00:42,  1.51it/s]Extractor Predicting: 69it [00:43,  1.52it/s]Extractor Predicting: 70it [00:43,  1.54it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:45,  1.55it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:47,  1.58it/s]Extractor Predicting: 77it [00:48,  1.61it/s]Extractor Predicting: 78it [00:48,  1.59it/s]Extractor Predicting: 79it [00:49,  1.55it/s]Extractor Predicting: 80it [00:50,  1.59it/s]Extractor Predicting: 81it [00:50,  1.47it/s]Extractor Predicting: 82it [00:51,  1.52it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:52,  1.52it/s]Extractor Predicting: 85it [00:53,  1.54it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:55,  1.57it/s]Extractor Predicting: 89it [00:55,  1.61it/s]Extractor Predicting: 90it [00:56,  1.62it/s]Extractor Predicting: 91it [00:57,  1.64it/s]Extractor Predicting: 92it [00:57,  1.63it/s]Extractor Predicting: 93it [00:58,  1.62it/s]Extractor Predicting: 94it [00:59,  1.61it/s]Extractor Predicting: 95it [00:59,  1.61it/s]Extractor Predicting: 96it [01:00,  1.64it/s]Extractor Predicting: 97it [01:00,  1.64it/s]Extractor Predicting: 98it [01:01,  1.58it/s]Extractor Predicting: 99it [01:02,  1.58it/s]Extractor Predicting: 100it [01:02,  1.58it/s]Extractor Predicting: 101it [01:03,  1.57it/s]Extractor Predicting: 102it [01:04,  1.59it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:05,  1.60it/s]Extractor Predicting: 105it [01:06,  1.54it/s]Extractor Predicting: 106it [01:06,  1.52it/s]Extractor Predicting: 107it [01:07,  1.51it/s]Extractor Predicting: 108it [01:07,  1.55it/s]Extractor Predicting: 109it [01:08,  1.50it/s]Extractor Predicting: 110it [01:09,  1.51it/s]Extractor Predicting: 111it [01:10,  1.51it/s]Extractor Predicting: 112it [01:10,  1.53it/s]Extractor Predicting: 113it [01:11,  1.38it/s]Extractor Predicting: 114it [01:12,  1.43it/s]Extractor Predicting: 115it [01:12,  1.45it/s]Extractor Predicting: 116it [01:13,  1.45it/s]Extractor Predicting: 117it [01:14,  1.48it/s]Extractor Predicting: 118it [01:14,  1.54it/s]Extractor Predicting: 119it [01:15,  1.54it/s]Extractor Predicting: 120it [01:15,  1.59it/s]Extractor Predicting: 121it [01:16,  1.58it/s]Extractor Predicting: 122it [01:17,  1.55it/s]Extractor Predicting: 123it [01:17,  1.59it/s]Extractor Predicting: 124it [01:18,  1.63it/s]Extractor Predicting: 125it [01:19,  1.62it/s]Extractor Predicting: 126it [01:19,  1.59it/s]Extractor Predicting: 127it [01:20,  1.58it/s]Extractor Predicting: 128it [01:21,  1.57it/s]Extractor Predicting: 129it [01:21,  1.58it/s]Extractor Predicting: 130it [01:22,  1.57it/s]Extractor Predicting: 131it [01:22,  1.64it/s]Extractor Predicting: 132it [01:23,  1.61it/s]Extractor Predicting: 133it [01:24,  1.61it/s]Extractor Predicting: 134it [01:24,  1.64it/s]Extractor Predicting: 135it [01:25,  1.64it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:26,  1.68it/s]Extractor Predicting: 138it [01:27,  1.66it/s]Extractor Predicting: 139it [01:27,  1.66it/s]Extractor Predicting: 140it [01:28,  1.65it/s]Extractor Predicting: 141it [01:28,  1.65it/s]Extractor Predicting: 142it [01:29,  1.63it/s]Extractor Predicting: 143it [01:30,  1.60it/s]Extractor Predicting: 144it [01:30,  1.61it/s]Extractor Predicting: 145it [01:31,  1.56it/s]Extractor Predicting: 146it [01:32,  1.52it/s]Extractor Predicting: 147it [01:32,  1.55it/s]Extractor Predicting: 148it [01:33,  1.54it/s]Extractor Predicting: 149it [01:34,  1.57it/s]Extractor Predicting: 150it [01:34,  1.58it/s]Extractor Predicting: 151it [01:35,  1.59it/s]Extractor Predicting: 152it [01:35,  1.58it/s]Extractor Predicting: 153it [01:36,  1.59it/s]Extractor Predicting: 154it [01:37,  1.61it/s]Extractor Predicting: 155it [01:37,  1.64it/s]Extractor Predicting: 156it [01:38,  1.60it/s]Extractor Predicting: 157it [01:39,  1.61it/s]Extractor Predicting: 158it [01:39,  1.61it/s]Extractor Predicting: 159it [01:40,  1.61it/s]Extractor Predicting: 160it [01:40,  1.64it/s]Extractor Predicting: 161it [01:41,  1.57it/s]Extractor Predicting: 162it [01:42,  1.62it/s]Extractor Predicting: 163it [01:42,  1.63it/s]Extractor Predicting: 164it [01:43,  1.61it/s]Extractor Predicting: 165it [01:43,  1.67it/s]Extractor Predicting: 166it [01:44,  1.68it/s]Extractor Predicting: 167it [01:45,  1.67it/s]Extractor Predicting: 168it [01:45,  1.72it/s]Extractor Predicting: 169it [01:46,  1.73it/s]Extractor Predicting: 170it [01:46,  1.68it/s]Extractor Predicting: 171it [01:47,  1.67it/s]Extractor Predicting: 172it [01:48,  1.67it/s]Extractor Predicting: 173it [01:48,  1.64it/s]Extractor Predicting: 174it [01:49,  1.68it/s]Extractor Predicting: 175it [01:49,  1.67it/s]Extractor Predicting: 176it [01:50,  1.65it/s]Extractor Predicting: 177it [01:51,  1.63it/s]Extractor Predicting: 178it [01:51,  1.62it/s]Extractor Predicting: 179it [01:52,  1.70it/s]Extractor Predicting: 180it [01:52,  1.67it/s]Extractor Predicting: 181it [01:53,  1.67it/s]Extractor Predicting: 182it [01:54,  1.64it/s]Extractor Predicting: 183it [01:54,  1.64it/s]Extractor Predicting: 184it [01:55,  1.62it/s]Extractor Predicting: 185it [01:55,  1.66it/s]Extractor Predicting: 186it [01:56,  1.63it/s]Extractor Predicting: 187it [01:57,  1.63it/s]Extractor Predicting: 188it [01:57,  1.61it/s]Extractor Predicting: 189it [01:58,  1.59it/s]Extractor Predicting: 190it [01:59,  1.57it/s]Extractor Predicting: 191it [01:59,  1.57it/s]Extractor Predicting: 192it [02:00,  1.59it/s]Extractor Predicting: 193it [02:00,  1.63it/s]Extractor Predicting: 194it [02:01,  1.63it/s]Extractor Predicting: 195it [02:02,  1.61it/s]Extractor Predicting: 196it [02:02,  1.62it/s]Extractor Predicting: 197it [02:03,  1.62it/s]Extractor Predicting: 198it [02:04,  1.63it/s]Extractor Predicting: 199it [02:04,  1.62it/s]Extractor Predicting: 200it [02:05,  1.61it/s]Extractor Predicting: 201it [02:05,  1.62it/s]Extractor Predicting: 202it [02:06,  1.62it/s]Extractor Predicting: 203it [02:07,  1.62it/s]Extractor Predicting: 204it [02:07,  1.62it/s]Extractor Predicting: 205it [02:08,  1.62it/s]Extractor Predicting: 206it [02:09,  1.60it/s]Extractor Predicting: 207it [02:09,  1.63it/s]Extractor Predicting: 208it [02:10,  1.44it/s]Extractor Predicting: 209it [02:11,  1.49it/s]Extractor Predicting: 210it [02:11,  1.51it/s]Extractor Predicting: 211it [02:12,  1.53it/s]Extractor Predicting: 212it [02:13,  1.56it/s]Extractor Predicting: 213it [02:13,  1.57it/s]Extractor Predicting: 214it [02:14,  1.57it/s]Extractor Predicting: 215it [02:14,  1.54it/s]Extractor Predicting: 216it [02:15,  1.57it/s]Extractor Predicting: 217it [02:16,  1.60it/s]Extractor Predicting: 218it [02:16,  1.60it/s]Extractor Predicting: 219it [02:17,  1.58it/s]Extractor Predicting: 220it [02:18,  1.60it/s]Extractor Predicting: 221it [02:18,  1.57it/s]Extractor Predicting: 222it [02:19,  1.58it/s]Extractor Predicting: 223it [02:20,  1.52it/s]Extractor Predicting: 224it [02:20,  1.54it/s]Extractor Predicting: 225it [02:21,  1.52it/s]Extractor Predicting: 226it [02:22,  1.53it/s]Extractor Predicting: 227it [02:22,  1.50it/s]Extractor Predicting: 228it [02:23,  1.45it/s]Extractor Predicting: 229it [02:24,  1.47it/s]Extractor Predicting: 230it [02:24,  1.49it/s]Extractor Predicting: 231it [02:25,  1.49it/s]Extractor Predicting: 232it [02:26,  1.50it/s]Extractor Predicting: 233it [02:26,  1.51it/s]Extractor Predicting: 234it [02:27,  1.53it/s]Extractor Predicting: 235it [02:28,  1.54it/s]Extractor Predicting: 236it [02:28,  1.55it/s]Extractor Predicting: 237it [02:29,  1.54it/s]Extractor Predicting: 238it [02:29,  1.61it/s]Extractor Predicting: 239it [02:30,  1.63it/s]Extractor Predicting: 240it [02:30,  1.69it/s]Extractor Predicting: 241it [02:31,  1.67it/s]Extractor Predicting: 242it [02:32,  1.66it/s]Extractor Predicting: 243it [02:32,  1.62it/s]Extractor Predicting: 244it [02:33,  1.63it/s]Extractor Predicting: 245it [02:34,  1.66it/s]Extractor Predicting: 246it [02:34,  1.64it/s]Extractor Predicting: 247it [02:35,  1.65it/s]Extractor Predicting: 248it [02:35,  1.66it/s]Extractor Predicting: 249it [02:36,  1.72it/s]Extractor Predicting: 250it [02:36,  1.72it/s]Extractor Predicting: 251it [02:37,  1.78it/s]Extractor Predicting: 252it [02:38,  1.79it/s]Extractor Predicting: 253it [02:38,  1.75it/s]Extractor Predicting: 254it [02:39,  1.77it/s]Extractor Predicting: 255it [02:39,  1.71it/s]Extractor Predicting: 256it [02:40,  1.76it/s]Extractor Predicting: 257it [02:40,  1.71it/s]Extractor Predicting: 258it [02:41,  1.67it/s]Extractor Predicting: 259it [02:42,  1.62it/s]Extractor Predicting: 260it [02:42,  1.68it/s]Extractor Predicting: 261it [02:43,  1.65it/s]Extractor Predicting: 262it [02:44,  1.65it/s]Extractor Predicting: 263it [02:44,  1.70it/s]Extractor Predicting: 264it [02:45,  1.71it/s]Extractor Predicting: 265it [02:45,  1.70it/s]Extractor Predicting: 266it [02:46,  1.73it/s]Extractor Predicting: 267it [02:46,  1.76it/s]Extractor Predicting: 268it [02:47,  1.75it/s]Extractor Predicting: 269it [02:48,  1.71it/s]Extractor Predicting: 270it [02:48,  1.73it/s]Extractor Predicting: 271it [02:49,  1.72it/s]Extractor Predicting: 272it [02:49,  1.68it/s]Extractor Predicting: 273it [02:50,  1.74it/s]Extractor Predicting: 274it [02:50,  1.75it/s]Extractor Predicting: 275it [02:51,  1.73it/s]Extractor Predicting: 276it [02:52,  1.71it/s]Extractor Predicting: 277it [02:52,  1.71it/s]Extractor Predicting: 278it [02:53,  1.73it/s]Extractor Predicting: 279it [02:53,  1.67it/s]Extractor Predicting: 280it [02:54,  1.73it/s]Extractor Predicting: 281it [02:55,  1.74it/s]Extractor Predicting: 282it [02:55,  1.76it/s]Extractor Predicting: 283it [02:56,  1.76it/s]Extractor Predicting: 284it [02:56,  1.75it/s]Extractor Predicting: 285it [02:57,  1.76it/s]Extractor Predicting: 286it [02:57,  1.75it/s]Extractor Predicting: 287it [02:58,  1.75it/s]Extractor Predicting: 288it [02:59,  1.71it/s]Extractor Predicting: 289it [02:59,  1.71it/s]Extractor Predicting: 290it [03:00,  1.63it/s]Extractor Predicting: 291it [03:00,  1.66it/s]Extractor Predicting: 292it [03:01,  1.70it/s]Extractor Predicting: 293it [03:02,  1.71it/s]Extractor Predicting: 294it [03:02,  1.72it/s]Extractor Predicting: 295it [03:03,  1.73it/s]Extractor Predicting: 296it [03:03,  1.64it/s]Extractor Predicting: 297it [03:04,  1.63it/s]Extractor Predicting: 298it [03:05,  1.57it/s]Extractor Predicting: 299it [03:05,  1.58it/s]Extractor Predicting: 300it [03:06,  1.60it/s]Extractor Predicting: 301it [03:07,  1.59it/s]Extractor Predicting: 302it [03:07,  1.61it/s]Extractor Predicting: 303it [03:08,  1.57it/s]Extractor Predicting: 304it [03:08,  1.57it/s]Extractor Predicting: 305it [03:09,  1.61it/s]Extractor Predicting: 306it [03:10,  1.56it/s]Extractor Predicting: 307it [03:10,  1.53it/s]Extractor Predicting: 308it [03:11,  1.54it/s]Extractor Predicting: 309it [03:12,  1.57it/s]Extractor Predicting: 310it [03:12,  1.60it/s]Extractor Predicting: 311it [03:13,  1.60it/s]Extractor Predicting: 312it [03:14,  1.58it/s]Extractor Predicting: 313it [03:14,  1.54it/s]Extractor Predicting: 314it [03:15,  1.52it/s]Extractor Predicting: 315it [03:16,  1.53it/s]Extractor Predicting: 316it [03:16,  1.53it/s]Extractor Predicting: 317it [03:17,  1.54it/s]Extractor Predicting: 318it [03:17,  1.56it/s]Extractor Predicting: 319it [03:18,  1.57it/s]Extractor Predicting: 320it [03:19,  1.57it/s]Extractor Predicting: 321it [03:19,  1.63it/s]Extractor Predicting: 322it [03:20,  1.62it/s]Extractor Predicting: 323it [03:21,  1.60it/s]Extractor Predicting: 324it [03:21,  1.41it/s]Extractor Predicting: 325it [03:22,  1.45it/s]Extractor Predicting: 326it [03:23,  1.47it/s]Extractor Predicting: 327it [03:23,  1.46it/s]Extractor Predicting: 328it [03:24,  1.48it/s]Extractor Predicting: 329it [03:25,  1.54it/s]Extractor Predicting: 330it [03:25,  1.58it/s]Extractor Predicting: 331it [03:26,  1.75it/s]Extractor Predicting: 331it [03:26,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:32,087 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:32,098 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:32,099 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:32,099 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:32,099 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:45:32,885 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:45:32,886 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:45:33,146 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:45:34,244 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:45:34,244 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:36,067 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:36,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:36,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:36,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:45:36,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:45:36,460 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:45:36,461 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:45:36,742 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:45:36,910 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:45:36,910 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4588159588159588,
  "recall": 0.08990039087126465,
  "score": 0.15034264628360566,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.51it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:15,  1.47it/s]Extractor Predicting: 25it [00:16,  1.47it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:23,  1.57it/s]Extractor Predicting: 37it [00:24,  1.57it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:28,  1.56it/s]Extractor Predicting: 44it [00:28,  1.56it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:30,  1.57it/s]Extractor Predicting: 47it [00:30,  1.55it/s]Extractor Predicting: 48it [00:31,  1.59it/s]Extractor Predicting: 49it [00:31,  1.60it/s]Extractor Predicting: 50it [00:32,  1.62it/s]Extractor Predicting: 51it [00:33,  1.63it/s]Extractor Predicting: 52it [00:33,  1.61it/s]Extractor Predicting: 53it [00:34,  1.53it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:35,  1.58it/s]Extractor Predicting: 56it [00:36,  1.65it/s]Extractor Predicting: 57it [00:36,  1.69it/s]Extractor Predicting: 58it [00:37,  1.75it/s]Extractor Predicting: 59it [00:37,  1.83it/s]Extractor Predicting: 60it [00:38,  1.90it/s]Extractor Predicting: 61it [00:38,  1.94it/s]Extractor Predicting: 62it [00:39,  1.93it/s]Extractor Predicting: 63it [00:39,  1.96it/s]Extractor Predicting: 64it [00:40,  1.94it/s]Extractor Predicting: 65it [00:40,  1.93it/s]Extractor Predicting: 66it [00:41,  1.90it/s]Extractor Predicting: 67it [00:41,  1.89it/s]Extractor Predicting: 68it [00:42,  1.92it/s]Extractor Predicting: 69it [00:42,  1.95it/s]Extractor Predicting: 70it [00:43,  1.91it/s]Extractor Predicting: 71it [00:44,  1.91it/s]Extractor Predicting: 72it [00:44,  1.92it/s]Extractor Predicting: 73it [00:45,  1.97it/s]Extractor Predicting: 74it [00:45,  1.77it/s]Extractor Predicting: 75it [00:46,  1.82it/s]Extractor Predicting: 76it [00:46,  1.83it/s]Extractor Predicting: 77it [00:47,  1.91it/s]Extractor Predicting: 78it [00:47,  1.87it/s]Extractor Predicting: 79it [00:48,  1.88it/s]Extractor Predicting: 80it [00:48,  1.88it/s]Extractor Predicting: 81it [00:49,  1.87it/s]Extractor Predicting: 82it [00:49,  1.91it/s]Extractor Predicting: 83it [00:50,  1.91it/s]Extractor Predicting: 84it [00:50,  1.91it/s]Extractor Predicting: 85it [00:51,  1.92it/s]Extractor Predicting: 86it [00:52,  1.79it/s]Extractor Predicting: 87it [00:52,  1.74it/s]Extractor Predicting: 88it [00:53,  1.70it/s]Extractor Predicting: 89it [00:53,  1.69it/s]Extractor Predicting: 90it [00:54,  1.69it/s]Extractor Predicting: 91it [00:55,  1.66it/s]Extractor Predicting: 92it [00:55,  1.65it/s]Extractor Predicting: 93it [00:56,  1.66it/s]Extractor Predicting: 94it [00:56,  1.64it/s]Extractor Predicting: 95it [00:57,  1.67it/s]Extractor Predicting: 96it [00:58,  1.67it/s]Extractor Predicting: 97it [00:58,  1.68it/s]Extractor Predicting: 98it [00:59,  1.68it/s]Extractor Predicting: 99it [00:59,  1.65it/s]Extractor Predicting: 100it [01:00,  1.60it/s]Extractor Predicting: 101it [01:01,  1.62it/s]Extractor Predicting: 102it [01:01,  1.62it/s]Extractor Predicting: 103it [01:02,  1.57it/s]Extractor Predicting: 104it [01:03,  1.56it/s]Extractor Predicting: 105it [01:03,  1.55it/s]Extractor Predicting: 106it [01:04,  1.53it/s]Extractor Predicting: 107it [01:05,  1.52it/s]Extractor Predicting: 108it [01:05,  1.47it/s]Extractor Predicting: 108it [01:05,  1.64it/s]
[INFO|configuration_utils.py:515] 2023-08-28 22:46:44,699 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:46:44,700 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 22:46:44,713 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:46:44,714 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 22:46:44,728 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 22:46:49,292 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 22:46:49,292 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 22:46:49,334 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 22:46:49,335 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 22:46:49,356 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:46:49,366 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:46:49,366 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:46:49,366 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:46:49,366 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:46:49,366 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 22:46:49,366 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.7704447632711621,
  "recall": 0.08601633829889477,
  "score": 0.1547550432276657,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 22:46:49,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:50,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:50,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:51,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:52,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:52,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:53,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:54,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:54,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:55,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:56,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:56,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:57,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:57,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:58,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:58,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:59,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:46:59,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:00,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:01,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:12<02:53, 12.41s/it][WARNING|generation_utils.py:914] 2023-08-28 22:47:02,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:02,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:03,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:04,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:04,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:05,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:05,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:06,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:06,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:07,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:08,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:08,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:09,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:10,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:10,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:11,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:11,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:12,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:13,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:13,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:14,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:25<02:44, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-28 22:47:15,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:15,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:16,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:16,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:17,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:18,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:18,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:19,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:19,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:20,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:20,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:21,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:22,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:22,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:23,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:24,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:24,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:25,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:25,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:26,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:37<02:28, 12.37s/it][WARNING|generation_utils.py:914] 2023-08-28 22:47:27,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:27,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:28,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:28,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:29,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:30,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:30,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:31,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:31,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:32,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:33,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:33,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:34,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:34,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:35,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:35,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:36,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:37,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:37,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:38,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:38,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:49<02:16, 12.39s/it][WARNING|generation_utils.py:914] 2023-08-28 22:47:39,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:40,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:40,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:41,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:41,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:42,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:42,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:43,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:44,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:44,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:45,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:46,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:46,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:47,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:47,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:48,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:48,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:49,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:50,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:50,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:51,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:02<02:04, 12.41s/it][WARNING|generation_utils.py:914] 2023-08-28 22:47:51,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:52,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:53,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:53,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:54,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:54,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:55,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:56,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:56,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:57,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:57,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:58,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:58,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:47:59,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:00,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:00,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:01,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:01,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:02,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:03,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:13<01:49, 12.16s/it][WARNING|generation_utils.py:914] 2023-08-28 22:48:03,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:04,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:04,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:05,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:05,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:06,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:06,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:07,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:08,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:08,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:09,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:09,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:10,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:11,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:11,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:12,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:12,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:13,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:14,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:14,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:15,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:26<01:37, 12.19s/it][WARNING|generation_utils.py:914] 2023-08-28 22:48:15,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:16,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:17,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:17,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:18,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:18,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:19,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:19,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:20,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:20,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:21,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:22,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:22,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:23,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:23,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:24,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:24,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:25,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:26,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:26,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:27,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:38<01:24, 12.14s/it][WARNING|generation_utils.py:914] 2023-08-28 22:48:27,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:28,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:28,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:29,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:29,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:30,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:30,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:31,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:31,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:32,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:33,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:33,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:34,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:34,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:35,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:35,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:36,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:36,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:37,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:37,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:38,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:48<01:10, 11.73s/it][WARNING|generation_utils.py:914] 2023-08-28 22:48:38,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:39,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:39,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:40,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:40,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:41,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:41,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:42,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:42,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:43,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:43,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:44,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:44,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:45,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:45,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:46,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:46,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:47,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:48,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:48,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:49,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [01:59<00:57, 11.46s/it][WARNING|generation_utils.py:914] 2023-08-28 22:48:49,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:50,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:50,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:51,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:51,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:52,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:52,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:53,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:54,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:54,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:55,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:55,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:56,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:57,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:57,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:58,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:58,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:59,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:48:59,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:00,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:00,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:11<00:46, 11.54s/it][WARNING|generation_utils.py:914] 2023-08-28 22:49:01,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:01,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:02,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:03,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:03,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:04,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:04,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:05,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:06,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:07,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:07,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:08,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:09,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:09,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:10,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:10,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:11,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:12,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:12,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:13,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:13,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:14,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:25<00:36, 12.23s/it][WARNING|generation_utils.py:914] 2023-08-28 22:49:15,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:15,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:16,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:17,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:17,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:18,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:18,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:19,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:20,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:20,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:21,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:21,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:22,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:23,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:23,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:24,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:24,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:25,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:26,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:26,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:27,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:27,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:38<00:25, 12.60s/it][WARNING|generation_utils.py:914] 2023-08-28 22:49:28,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:28,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:29,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:30,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:30,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:31,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:31,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:32,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:32,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:33,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:34,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:34,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:35,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:35,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:36,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:36,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:37,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:37,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:38,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:38,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:49<00:12, 12.11s/it][WARNING|generation_utils.py:914] 2023-08-28 22:49:39,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:40,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:40,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:41,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:41,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:42,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:42,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:43,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:44,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:44,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:45,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:45,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:46,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:47,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:47,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:48,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:49,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:49,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:50,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:51,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 22:49:51,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:02<00:00, 12.39s/it]Generating: 100%|██████████| 15/15 [03:02<00:00, 12.19s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:58,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:58,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:58,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:58,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:49:58,702 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:49:59,113 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:49:59,114 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:49:59,375 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:50:00,476 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:50:00,476 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:01,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:01,817 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:01,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:01,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:50:01,818 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:50:02,641 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:50:02,642 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:50:03,028 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:50:03,229 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:50:03,229 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.9546875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.9241071428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9484375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 630, 'raw': 672}
{'prompt': 'Relation : made from material .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 630, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8948863636363636, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 8539
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8639, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.56it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.57it/s]Extractor Estimating: 4it [00:02,  1.57it/s]Extractor Estimating: 5it [00:03,  1.62it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.60it/s]Extractor Estimating: 9it [00:05,  1.62it/s]Extractor Estimating: 10it [00:06,  1.62it/s]Extractor Estimating: 11it [00:06,  1.65it/s]Extractor Estimating: 12it [00:07,  1.57it/s]Extractor Estimating: 13it [00:08,  1.59it/s]Extractor Estimating: 14it [00:08,  1.65it/s]Extractor Estimating: 15it [00:09,  1.70it/s]Extractor Estimating: 16it [00:09,  1.69it/s]Extractor Estimating: 17it [00:10,  1.70it/s]Extractor Estimating: 18it [00:11,  1.68it/s]Extractor Estimating: 19it [00:11,  1.73it/s]Extractor Estimating: 20it [00:12,  1.71it/s]Extractor Estimating: 21it [00:12,  1.69it/s]Extractor Estimating: 22it [00:13,  1.71it/s]Extractor Estimating: 23it [00:13,  1.72it/s]Extractor Estimating: 24it [00:14,  1.71it/s]Extractor Estimating: 25it [00:15,  1.66it/s]Extractor Estimating: 26it [00:15,  1.62it/s]Extractor Estimating: 27it [00:16,  1.60it/s]Extractor Estimating: 28it [00:17,  1.58it/s]Extractor Estimating: 29it [00:17,  1.56it/s]Extractor Estimating: 30it [00:18,  1.58it/s]Extractor Estimating: 31it [00:19,  1.61it/s]Extractor Estimating: 32it [00:19,  1.63it/s]Extractor Estimating: 33it [00:20,  1.65it/s]Extractor Estimating: 34it [00:20,  1.55it/s]Extractor Estimating: 35it [00:21,  1.60it/s]Extractor Estimating: 36it [00:22,  1.63it/s]Extractor Estimating: 37it [00:22,  1.63it/s]Extractor Estimating: 38it [00:23,  1.63it/s]Extractor Estimating: 39it [00:24,  1.42it/s]Extractor Estimating: 40it [00:24,  1.39it/s]Extractor Estimating: 41it [00:25,  1.45it/s]Extractor Estimating: 42it [00:26,  1.52it/s]Extractor Estimating: 43it [00:26,  1.58it/s]Extractor Estimating: 44it [00:27,  1.61it/s]Extractor Estimating: 45it [00:28,  1.58it/s]Extractor Estimating: 46it [00:28,  1.56it/s]Extractor Estimating: 47it [00:29,  1.55it/s]Extractor Estimating: 48it [00:29,  1.57it/s]Extractor Estimating: 49it [00:30,  1.60it/s]Extractor Estimating: 50it [00:31,  1.62it/s]Extractor Estimating: 51it [00:31,  1.66it/s]Extractor Estimating: 52it [00:32,  1.60it/s]Extractor Estimating: 53it [00:32,  1.68it/s]Extractor Estimating: 54it [00:33,  1.66it/s]Extractor Estimating: 55it [00:34,  1.69it/s]Extractor Estimating: 56it [00:34,  1.69it/s]Extractor Estimating: 57it [00:35,  1.64it/s]Extractor Estimating: 58it [00:35,  1.70it/s]Extractor Estimating: 59it [00:36,  1.76it/s]Extractor Estimating: 60it [00:36,  1.75it/s]Extractor Estimating: 61it [00:37,  1.80it/s]Extractor Estimating: 62it [00:38,  1.80it/s]Extractor Estimating: 63it [00:38,  1.83it/s]Extractor Estimating: 64it [00:39,  1.89it/s]Extractor Estimating: 65it [00:39,  1.79it/s]Extractor Estimating: 66it [00:40,  1.80it/s]Extractor Estimating: 67it [00:40,  1.75it/s]Extractor Estimating: 68it [00:41,  1.65it/s]Extractor Estimating: 69it [00:42,  1.69it/s]Extractor Estimating: 70it [00:42,  1.72it/s]Extractor Estimating: 71it [00:43,  1.77it/s]Extractor Estimating: 72it [00:43,  1.78it/s]Extractor Estimating: 73it [00:44,  1.75it/s]Extractor Estimating: 74it [00:44,  1.70it/s]Extractor Estimating: 75it [00:45,  1.73it/s]Extractor Estimating: 76it [00:46,  1.70it/s]Extractor Estimating: 77it [00:46,  1.67it/s]Extractor Estimating: 78it [00:47,  1.71it/s]Extractor Estimating: 79it [00:47,  1.70it/s]Extractor Estimating: 80it [00:48,  1.63it/s]Extractor Estimating: 81it [00:49,  1.63it/s]Extractor Estimating: 82it [00:49,  1.68it/s]Extractor Estimating: 83it [00:50,  1.63it/s]Extractor Estimating: 84it [00:51,  1.58it/s]Extractor Estimating: 85it [00:51,  1.59it/s]Extractor Estimating: 86it [00:52,  1.61it/s]Extractor Estimating: 87it [00:52,  1.57it/s]Extractor Estimating: 88it [00:53,  1.59it/s]Extractor Estimating: 89it [00:54,  1.63it/s]Extractor Estimating: 90it [00:54,  1.60it/s]Extractor Estimating: 91it [00:55,  1.67it/s]Extractor Estimating: 92it [00:55,  1.69it/s]Extractor Estimating: 93it [00:56,  1.68it/s]Extractor Estimating: 94it [00:57,  1.67it/s]Extractor Estimating: 95it [00:57,  1.68it/s]Extractor Estimating: 96it [00:58,  1.67it/s]Extractor Estimating: 97it [00:58,  1.74it/s]Extractor Estimating: 98it [00:59,  1.77it/s]Extractor Estimating: 99it [00:59,  1.77it/s]Extractor Estimating: 100it [01:00,  1.73it/s]Extractor Estimating: 101it [01:01,  1.74it/s]Extractor Estimating: 102it [01:01,  1.66it/s]Extractor Estimating: 103it [01:02,  1.63it/s]Extractor Estimating: 104it [01:03,  1.66it/s]Extractor Estimating: 105it [01:03,  1.66it/s]Extractor Estimating: 106it [01:04,  1.66it/s]Extractor Estimating: 107it [01:04,  1.55it/s]Extractor Estimating: 108it [01:05,  1.61it/s]Extractor Estimating: 109it [01:06,  1.63it/s]Extractor Estimating: 110it [01:06,  1.66it/s]Extractor Estimating: 111it [01:07,  1.66it/s]Extractor Estimating: 112it [01:07,  1.62it/s]Extractor Estimating: 113it [01:08,  1.42it/s]Extractor Estimating: 114it [01:09,  1.48it/s]Extractor Estimating: 115it [01:10,  1.52it/s]Extractor Estimating: 116it [01:10,  1.51it/s]Extractor Estimating: 117it [01:11,  1.42it/s]Extractor Estimating: 118it [01:12,  1.44it/s]Extractor Estimating: 119it [01:12,  1.48it/s]Extractor Estimating: 120it [01:13,  1.47it/s]Extractor Estimating: 121it [01:14,  1.50it/s]Extractor Estimating: 122it [01:14,  1.43it/s]Extractor Estimating: 123it [01:15,  1.48it/s]Extractor Estimating: 124it [01:16,  1.50it/s]Extractor Estimating: 125it [01:16,  1.52it/s]Extractor Estimating: 126it [01:17,  1.54it/s]Extractor Estimating: 127it [01:18,  1.52it/s]Extractor Estimating: 128it [01:18,  1.56it/s]Extractor Estimating: 129it [01:19,  1.64it/s]Extractor Estimating: 130it [01:19,  1.63it/s]Extractor Estimating: 131it [01:20,  1.64it/s]Extractor Estimating: 132it [01:21,  1.62it/s]Extractor Estimating: 133it [01:21,  1.63it/s]Extractor Estimating: 134it [01:22,  1.59it/s]Extractor Estimating: 135it [01:23,  1.62it/s]Extractor Estimating: 136it [01:23,  1.64it/s]Extractor Estimating: 137it [01:24,  1.64it/s]Extractor Estimating: 138it [01:24,  1.67it/s]Extractor Estimating: 139it [01:25,  1.66it/s]Extractor Estimating: 140it [01:26,  1.62it/s]Extractor Estimating: 141it [01:26,  1.64it/s]Extractor Estimating: 142it [01:27,  1.58it/s]Extractor Estimating: 143it [01:27,  1.61it/s]Extractor Estimating: 144it [01:28,  1.58it/s]Extractor Estimating: 145it [01:29,  1.60it/s]Extractor Estimating: 146it [01:29,  1.63it/s]Extractor Estimating: 147it [01:30,  1.66it/s]Extractor Estimating: 148it [01:30,  1.65it/s]Extractor Estimating: 149it [01:31,  1.67it/s]Extractor Estimating: 150it [01:32,  1.69it/s]Extractor Estimating: 151it [01:32,  1.72it/s]Extractor Estimating: 152it [01:33,  1.72it/s]Extractor Estimating: 153it [01:33,  1.71it/s]Extractor Estimating: 154it [01:34,  1.71it/s]Extractor Estimating: 155it [01:35,  1.72it/s]Extractor Estimating: 156it [01:35,  1.71it/s]Extractor Estimating: 157it [01:36,  1.73it/s]Extractor Estimating: 158it [01:36,  1.70it/s]Extractor Estimating: 159it [01:37,  1.74it/s]Extractor Estimating: 160it [01:37,  1.68it/s]Extractor Estimating: 161it [01:38,  1.61it/s]Extractor Estimating: 162it [01:39,  1.65it/s]Extractor Estimating: 163it [01:39,  1.67it/s]Extractor Estimating: 164it [01:40,  1.63it/s]Extractor Estimating: 165it [01:41,  1.69it/s]Extractor Estimating: 166it [01:41,  1.71it/s]Extractor Estimating: 167it [01:42,  1.78it/s]Extractor Estimating: 168it [01:42,  1.74it/s]Extractor Estimating: 169it [01:43,  1.71it/s]Extractor Estimating: 170it [01:43,  1.69it/s]Extractor Estimating: 171it [01:44,  1.61it/s]Extractor Estimating: 172it [01:45,  1.53it/s]Extractor Estimating: 173it [01:45,  1.54it/s]Extractor Estimating: 174it [01:46,  1.60it/s]Extractor Estimating: 175it [01:47,  1.64it/s]Extractor Estimating: 176it [01:47,  1.60it/s]Extractor Estimating: 177it [01:48,  1.58it/s]Extractor Estimating: 178it [01:49,  1.58it/s]Extractor Estimating: 179it [01:49,  1.61it/s]Extractor Estimating: 180it [01:50,  1.57it/s]Extractor Estimating: 181it [01:50,  1.57it/s]Extractor Estimating: 182it [01:51,  1.58it/s]Extractor Estimating: 183it [01:52,  1.60it/s]Extractor Estimating: 184it [01:52,  1.60it/s]Extractor Estimating: 185it [01:53,  1.57it/s]Extractor Estimating: 186it [01:54,  1.56it/s]Extractor Estimating: 187it [01:54,  1.59it/s]Extractor Estimating: 188it [01:55,  1.59it/s]Extractor Estimating: 189it [01:56,  1.56it/s]Extractor Estimating: 190it [01:56,  1.44it/s]Extractor Estimating: 191it [01:57,  1.52it/s]Extractor Estimating: 192it [01:58,  1.48it/s]Extractor Estimating: 193it [01:58,  1.54it/s]Extractor Estimating: 194it [01:59,  1.48it/s]Extractor Estimating: 195it [02:00,  1.49it/s]Extractor Estimating: 196it [02:00,  1.51it/s]Extractor Estimating: 197it [02:01,  1.50it/s]Extractor Estimating: 198it [02:02,  1.49it/s]Extractor Estimating: 199it [02:02,  1.50it/s]Extractor Estimating: 200it [02:03,  1.57it/s]Extractor Estimating: 201it [02:03,  1.66it/s]Extractor Estimating: 202it [02:04,  1.75it/s]Extractor Estimating: 203it [02:04,  1.86it/s]Extractor Estimating: 204it [02:05,  1.83it/s]Extractor Estimating: 205it [02:05,  1.87it/s]Extractor Estimating: 206it [02:06,  1.87it/s]Extractor Estimating: 207it [02:06,  1.92it/s]Extractor Estimating: 208it [02:07,  1.95it/s]Extractor Estimating: 209it [02:07,  1.97it/s]Extractor Estimating: 210it [02:08,  1.88it/s]Extractor Estimating: 211it [02:08,  1.91it/s]Extractor Estimating: 212it [02:09,  2.00it/s]Extractor Estimating: 213it [02:09,  1.96it/s]Extractor Estimating: 214it [02:10,  1.92it/s]Extractor Estimating: 215it [02:11,  1.94it/s]Extractor Estimating: 216it [02:11,  1.94it/s]Extractor Estimating: 217it [02:12,  1.93it/s]Extractor Estimating: 218it [02:12,  1.91it/s]Extractor Estimating: 219it [02:13,  1.94it/s]Extractor Estimating: 220it [02:13,  1.86it/s]Extractor Estimating: 221it [02:14,  1.92it/s]Extractor Estimating: 222it [02:14,  1.95it/s]Extractor Estimating: 223it [02:15,  1.92it/s]Extractor Estimating: 224it [02:15,  1.95it/s]Extractor Estimating: 225it [02:16,  1.94it/s]Extractor Estimating: 226it [02:16,  1.93it/s]Extractor Estimating: 227it [02:17,  1.89it/s]Extractor Estimating: 228it [02:17,  1.86it/s]Extractor Estimating: 229it [02:18,  1.93it/s]Extractor Estimating: 230it [02:18,  1.96it/s]Extractor Estimating: 231it [02:19,  1.96it/s]Extractor Estimating: 232it [02:19,  1.96it/s]Extractor Estimating: 233it [02:20,  1.99it/s]Extractor Estimating: 234it [02:20,  2.01it/s]Extractor Estimating: 235it [02:21,  1.95it/s]Extractor Estimating: 236it [02:21,  1.92it/s]Extractor Estimating: 237it [02:22,  1.99it/s]Extractor Estimating: 238it [02:22,  1.93it/s]Extractor Estimating: 239it [02:23,  1.94it/s]Extractor Estimating: 240it [02:23,  1.98it/s]Extractor Estimating: 241it [02:24,  1.83it/s]Extractor Estimating: 242it [02:25,  1.90it/s]Extractor Estimating: 243it [02:25,  1.88it/s]Extractor Estimating: 244it [02:26,  1.91it/s]Extractor Estimating: 245it [02:26,  1.96it/s]Extractor Estimating: 246it [02:27,  1.95it/s]Extractor Estimating: 247it [02:27,  1.91it/s]Extractor Estimating: 248it [02:28,  1.96it/s]Extractor Estimating: 249it [02:28,  1.91it/s]Extractor Estimating: 250it [02:29,  1.88it/s]Extractor Estimating: 251it [02:29,  1.82it/s]Extractor Estimating: 252it [02:30,  1.78it/s]Extractor Estimating: 253it [02:31,  1.73it/s]Extractor Estimating: 254it [02:31,  1.76it/s]Extractor Estimating: 255it [02:32,  1.74it/s]Extractor Estimating: 256it [02:32,  1.70it/s]Extractor Estimating: 257it [02:33,  1.73it/s]Extractor Estimating: 258it [02:33,  1.72it/s]Extractor Estimating: 259it [02:34,  1.72it/s]Extractor Estimating: 260it [02:35,  1.69it/s]Extractor Estimating: 261it [02:35,  1.69it/s]Extractor Estimating: 262it [02:36,  1.68it/s]Extractor Estimating: 263it [02:36,  1.68it/s]Extractor Estimating: 264it [02:37,  1.67it/s]Extractor Estimating: 265it [02:38,  1.71it/s]Extractor Estimating: 266it [02:38,  1.70it/s]Extractor Estimating: 267it [02:39,  1.71it/s]Extractor Estimating: 268it [02:39,  1.70it/s]Extractor Estimating: 269it [02:40,  1.70it/s]Extractor Estimating: 270it [02:40,  1.72it/s]Extractor Estimating: 271it [02:41,  1.70it/s]Extractor Estimating: 272it [02:42,  1.71it/s]Extractor Estimating: 273it [02:42,  1.70it/s]Extractor Estimating: 274it [02:43,  1.61it/s]Extractor Estimating: 275it [02:44,  1.58it/s]Extractor Estimating: 276it [02:44,  1.57it/s]Extractor Estimating: 277it [02:45,  1.55it/s]Extractor Estimating: 278it [02:45,  1.65it/s]Extractor Estimating: 279it [02:46,  1.65it/s]Extractor Estimating: 280it [02:47,  1.59it/s]Extractor Estimating: 281it [02:47,  1.61it/s]Extractor Estimating: 282it [02:48,  1.60it/s]Extractor Estimating: 283it [02:49,  1.55it/s]Extractor Estimating: 284it [02:49,  1.48it/s]Extractor Estimating: 285it [02:50,  1.45it/s]Extractor Estimating: 286it [02:51,  1.45it/s]Extractor Estimating: 287it [02:51,  1.47it/s]Extractor Estimating: 288it [02:52,  1.52it/s]Extractor Estimating: 289it [02:53,  1.54it/s]Extractor Estimating: 290it [02:53,  1.55it/s]Extractor Estimating: 291it [02:54,  1.55it/s]Extractor Estimating: 292it [02:55,  1.55it/s]Extractor Estimating: 293it [02:55,  1.57it/s]Extractor Estimating: 294it [02:56,  1.60it/s]Extractor Estimating: 295it [02:57,  1.56it/s]Extractor Estimating: 296it [02:57,  1.61it/s]Extractor Estimating: 297it [02:58,  1.60it/s]Extractor Estimating: 298it [02:58,  1.61it/s]Extractor Estimating: 299it [02:59,  1.59it/s]Extractor Estimating: 300it [03:00,  1.62it/s]Extractor Estimating: 301it [03:00,  1.60it/s]Extractor Estimating: 302it [03:01,  1.65it/s]Extractor Estimating: 303it [03:01,  1.68it/s]Extractor Estimating: 304it [03:02,  1.72it/s]Extractor Estimating: 305it [03:02,  1.77it/s]Extractor Estimating: 306it [03:03,  1.73it/s]Extractor Estimating: 307it [03:04,  1.67it/s]Extractor Estimating: 308it [03:04,  1.67it/s]Extractor Estimating: 309it [03:05,  1.65it/s]Extractor Estimating: 310it [03:05,  1.67it/s]Extractor Estimating: 311it [03:06,  1.64it/s]Extractor Estimating: 312it [03:07,  1.62it/s]Extractor Estimating: 313it [03:07,  1.62it/s]Extractor Estimating: 314it [03:08,  1.68it/s]Extractor Estimating: 315it [03:08,  1.70it/s]Extractor Estimating: 316it [03:09,  1.68it/s]Extractor Estimating: 317it [03:10,  1.67it/s]Extractor Estimating: 318it [03:10,  1.64it/s]Extractor Estimating: 319it [03:11,  1.64it/s]Extractor Estimating: 320it [03:12,  1.63it/s]Extractor Estimating: 321it [03:12,  1.65it/s]Extractor Estimating: 322it [03:13,  1.69it/s]Extractor Estimating: 323it [03:13,  1.63it/s]Extractor Estimating: 324it [03:14,  1.64it/s]Extractor Estimating: 325it [03:14,  1.78it/s]Extractor Estimating: 326it [03:15,  1.90it/s]Extractor Estimating: 327it [03:15,  2.05it/s]Extractor Estimating: 328it [03:16,  2.11it/s]Extractor Estimating: 329it [03:16,  2.11it/s]Extractor Estimating: 330it [03:17,  2.19it/s]Extractor Estimating: 331it [03:17,  2.15it/s]Extractor Estimating: 332it [03:18,  2.22it/s]Extractor Estimating: 333it [03:18,  2.16it/s]Extractor Estimating: 334it [03:18,  2.21it/s]Extractor Estimating: 335it [03:19,  2.21it/s]Extractor Estimating: 336it [03:19,  2.05it/s]Extractor Estimating: 337it [03:20,  2.14it/s]Extractor Estimating: 338it [03:20,  2.23it/s]Extractor Estimating: 339it [03:21,  2.18it/s]Extractor Estimating: 340it [03:21,  2.27it/s]Extractor Estimating: 341it [03:22,  2.27it/s]Extractor Estimating: 342it [03:22,  2.18it/s]Extractor Estimating: 343it [03:22,  2.33it/s]Extractor Estimating: 344it [03:23,  2.32it/s]Extractor Estimating: 345it [03:23,  2.31it/s]Extractor Estimating: 346it [03:24,  2.29it/s]Extractor Estimating: 347it [03:24,  2.25it/s]Extractor Estimating: 348it [03:25,  2.29it/s]Extractor Estimating: 349it [03:25,  2.28it/s]Extractor Estimating: 350it [03:26,  2.13it/s]Extractor Estimating: 351it [03:26,  2.01it/s]Extractor Estimating: 352it [03:27,  1.94it/s]Extractor Estimating: 353it [03:27,  1.85it/s]Extractor Estimating: 354it [03:28,  1.82it/s]Extractor Estimating: 355it [03:29,  1.74it/s]Extractor Estimating: 356it [03:29,  1.78it/s]Extractor Estimating: 357it [03:30,  1.76it/s]Extractor Estimating: 358it [03:30,  1.83it/s]Extractor Estimating: 359it [03:31,  1.82it/s]Extractor Estimating: 360it [03:31,  1.80it/s]Extractor Estimating: 361it [03:32,  1.78it/s]Extractor Estimating: 362it [03:32,  1.80it/s]Extractor Estimating: 363it [03:33,  1.79it/s]Extractor Estimating: 364it [03:34,  1.73it/s]Extractor Estimating: 365it [03:34,  1.67it/s]Extractor Estimating: 366it [03:35,  1.71it/s]Extractor Estimating: 367it [03:36,  1.59it/s]Extractor Estimating: 368it [03:36,  1.69it/s]Extractor Estimating: 369it [03:37,  1.68it/s]Extractor Estimating: 370it [03:37,  1.63it/s]Extractor Estimating: 371it [03:38,  1.63it/s]Extractor Estimating: 372it [03:38,  1.67it/s]Extractor Estimating: 373it [03:39,  1.64it/s]Extractor Estimating: 374it [03:40,  1.64it/s]Extractor Estimating: 375it [03:40,  1.98it/s]Extractor Estimating: 375it [03:40,  1.70it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:59,650 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:59,655 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:59,656 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:59,656 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:53:59,656 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 22:54:00,274 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 22:54:00,275 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:54:01,433 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 22:54:02,462 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:54:02,463 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:06,345 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:06,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:06,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:06,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 22:54:06,353 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 22:54:07,039 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 22:54:07,040 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 22:54:08,293 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 22:54:08,448 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 22:54:08,448 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:07:33,493 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:07:33,531 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7488 mean pseudo reward: 0.9204306808911049
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl'}
train vocab size: 16649
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16749, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16749, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.016, loss:636.4132
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.006, loss:562.3859
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.000, loss:531.2070
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.994, loss:512.9494
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.014, loss:521.7674
>> valid entity prec:0.4908, rec:0.5527, f1:0.5199
>> valid relation prec:0.0973, rec:0.0150, f1:0.0260
>> valid relation with NER prec:0.0973, rec:0.0150, f1:0.0260
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.405, loss:532.3438
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.016, loss:489.1107
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.006, loss:490.1467
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.006, loss:527.6633
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.009, loss:483.3933
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4898, rec:0.3962, f1:0.4381
>> valid relation prec:0.1417, rec:0.0274, f1:0.0460
>> valid relation with NER prec:0.1417, rec:0.0274, f1:0.0460
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.409, loss:499.0070
g_step 1200, step 264, avg_time 0.999, loss:486.4324
g_step 1300, step 52, avg_time 1.018, loss:489.1721
g_step 1400, step 152, avg_time 1.024, loss:473.5954
g_step 1500, step 252, avg_time 1.013, loss:471.2338
>> valid entity prec:0.4763, rec:0.3758, f1:0.4201
>> valid relation prec:0.0887, rec:0.0143, f1:0.0246
>> valid relation with NER prec:0.0887, rec:0.0143, f1:0.0246
g_step 1600, step 40, avg_time 2.411, loss:468.1959
g_step 1700, step 140, avg_time 1.015, loss:445.9290
g_step 1800, step 240, avg_time 1.018, loss:458.7208
g_step 1900, step 28, avg_time 1.019, loss:467.4306
g_step 2000, step 128, avg_time 1.005, loss:435.5195
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5064, rec:0.3889, f1:0.4399
>> valid relation prec:0.0734, rec:0.0120, f1:0.0206
>> valid relation with NER prec:0.0734, rec:0.0120, f1:0.0206
g_step 2100, step 228, avg_time 2.424, loss:461.3949
g_step 2200, step 16, avg_time 1.014, loss:444.4178
g_step 2300, step 116, avg_time 1.004, loss:415.5417
g_step 2400, step 216, avg_time 1.032, loss:435.1562
g_step 2500, step 4, avg_time 1.021, loss:436.3814
>> valid entity prec:0.4983, rec:0.4337, f1:0.4637
>> valid relation prec:0.1297, rec:0.0277, f1:0.0456
>> valid relation with NER prec:0.1297, rec:0.0277, f1:0.0456
g_step 2600, step 104, avg_time 2.421, loss:424.3989
g_step 2700, step 204, avg_time 1.010, loss:419.4607
g_step 2800, step 304, avg_time 1.018, loss:423.3933
g_step 2900, step 92, avg_time 1.013, loss:403.7728
g_step 3000, step 192, avg_time 1.005, loss:404.2911
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4875, rec:0.4587, f1:0.4727
>> valid relation prec:0.0925, rec:0.0198, f1:0.0327
>> valid relation with NER prec:0.0925, rec:0.0198, f1:0.0327
g_step 3100, step 292, avg_time 2.434, loss:409.3425
g_step 3200, step 80, avg_time 1.000, loss:386.2154
g_step 3300, step 180, avg_time 1.022, loss:392.3994
g_step 3400, step 280, avg_time 1.028, loss:412.4324
g_step 3500, step 68, avg_time 1.014, loss:384.1053
>> valid entity prec:0.4637, rec:0.4182, f1:0.4397
>> valid relation prec:0.0851, rec:0.0187, f1:0.0306
>> valid relation with NER prec:0.0851, rec:0.0187, f1:0.0306
g_step 3600, step 168, avg_time 2.425, loss:389.5375
g_step 3700, step 268, avg_time 1.016, loss:390.3341
g_step 3800, step 56, avg_time 1.011, loss:389.0605
g_step 3900, step 156, avg_time 1.011, loss:363.1921
g_step 4000, step 256, avg_time 1.019, loss:384.4917
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4811, rec:0.4289, f1:0.4535
>> valid relation prec:0.0829, rec:0.0187, f1:0.0305
>> valid relation with NER prec:0.0829, rec:0.0187, f1:0.0305
g_step 4100, step 44, avg_time 2.420, loss:368.4142
g_step 4200, step 144, avg_time 1.009, loss:377.5616
g_step 4300, step 244, avg_time 1.003, loss:372.5675
g_step 4400, step 32, avg_time 1.027, loss:358.8802
g_step 4500, step 132, avg_time 1.021, loss:342.4757
>> valid entity prec:0.4793, rec:0.3885, f1:0.4292
>> valid relation prec:0.0970, rec:0.0231, f1:0.0373
>> valid relation with NER prec:0.0970, rec:0.0231, f1:0.0373
g_step 4600, step 232, avg_time 2.427, loss:369.6662
g_step 4700, step 20, avg_time 1.014, loss:359.0984
g_step 4800, step 120, avg_time 1.005, loss:336.6430
g_step 4900, step 220, avg_time 1.003, loss:349.9822
g_step 5000, step 8, avg_time 1.025, loss:349.9076
learning rate was adjusted to 0.0008
>> valid entity prec:0.4648, rec:0.4045, f1:0.4325
>> valid relation prec:0.0943, rec:0.0277, f1:0.0428
>> valid relation with NER prec:0.0943, rec:0.0277, f1:0.0428
g_step 5100, step 108, avg_time 2.422, loss:321.5272
g_step 5200, step 208, avg_time 1.005, loss:343.0197
g_step 5300, step 308, avg_time 1.009, loss:353.6721
g_step 5400, step 96, avg_time 1.016, loss:318.5050
g_step 5500, step 196, avg_time 1.000, loss:330.1474
>> valid entity prec:0.4620, rec:0.3487, f1:0.3974
>> valid relation prec:0.0560, rec:0.0150, f1:0.0236
>> valid relation with NER prec:0.0560, rec:0.0150, f1:0.0236
g_step 5600, step 296, avg_time 2.407, loss:345.4348
g_step 5700, step 84, avg_time 1.014, loss:325.9197
g_step 5800, step 184, avg_time 1.004, loss:327.7078
g_step 5900, step 284, avg_time 1.014, loss:341.9755
g_step 6000, step 72, avg_time 1.015, loss:323.1509
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5056, rec:0.3776, f1:0.4323
>> valid relation prec:0.1247, rec:0.0311, f1:0.0498
>> valid relation with NER prec:0.1247, rec:0.0311, f1:0.0498
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 172, avg_time 2.392, loss:310.0657
g_step 6200, step 272, avg_time 1.018, loss:329.0152
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:07:33 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:07:33 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-07-33_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:07:34 - WARNING - datasets.builder -   Using custom data configuration default-f337f4d6cf5e77d2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f337f4d6cf5e77d2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  4.67 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:07:35,125 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:07:35,126 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:07:35,127 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:07:35,128 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:07:35,140 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:07:35,146 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:07:35,146 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:07:35,146 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:07:35,146 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:07:35,146 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:07:35,146 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:07:35,327 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:07:38,386 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:07:38,388 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f337f4d6cf5e77d2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.05ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.07ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.73ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.13ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.41ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.60ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.73ba/s]100%|██████████| 8/8 [00:01<00:00,  4.57ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.32ba/s] 40%|████      | 2/5 [00:00<00:00,  4.48ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.56ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.57ba/s]100%|██████████| 5/5 [00:00<00:00,  5.20ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.29ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.52ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.69ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.92ba/s]100%|██████████| 8/8 [00:00<00:00, 11.49ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  9.52ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.52ba/s]100%|██████████| 5/5 [00:00<00:00, 12.95ba/s]100%|██████████| 5/5 [00:00<00:00, 12.22ba/s]
[INFO|trainer.py:414] 2023-08-29 01:07:42,584 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:07:42,595 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:07:42,595 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 01:07:42,595 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:07:42,595 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:07:42,595 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:07:42,595 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:07:42,595 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<03:07,  3.12it/s]  0%|          | 2/585 [00:00<02:56,  3.31it/s]  1%|          | 3/585 [00:00<02:52,  3.37it/s]  1%|          | 4/585 [00:01<02:50,  3.40it/s]  1%|          | 5/585 [00:01<02:49,  3.42it/s]  1%|          | 6/585 [00:01<02:48,  3.43it/s]  1%|          | 7/585 [00:02<02:49,  3.42it/s]  1%|▏         | 8/585 [00:02<02:49,  3.40it/s]  2%|▏         | 9/585 [00:02<02:48,  3.41it/s]  2%|▏         | 10/585 [00:02<02:48,  3.42it/s]  2%|▏         | 11/585 [00:03<02:47,  3.43it/s]  2%|▏         | 12/585 [00:03<02:46,  3.44it/s]  2%|▏         | 13/585 [00:03<02:46,  3.44it/s]  2%|▏         | 14/585 [00:04<02:45,  3.44it/s]  3%|▎         | 15/585 [00:04<02:45,  3.44it/s]  3%|▎         | 16/585 [00:04<02:45,  3.45it/s]  3%|▎         | 17/585 [00:04<02:44,  3.44it/s]  3%|▎         | 18/585 [00:05<02:44,  3.45it/s]  3%|▎         | 19/585 [00:05<02:45,  3.43it/s]  3%|▎         | 20/585 [00:05<02:44,  3.43it/s]  4%|▎         | 21/585 [00:06<02:44,  3.44it/s]  4%|▍         | 22/585 [00:06<02:43,  3.44it/s]  4%|▍         | 23/585 [00:06<02:43,  3.44it/s]  4%|▍         | 24/585 [00:07<02:43,  3.44it/s]  4%|▍         | 25/585 [00:07<02:42,  3.44it/s]  4%|▍         | 26/585 [00:07<02:42,  3.44it/s]  5%|▍         | 27/585 [00:07<02:42,  3.44it/s]  5%|▍         | 28/585 [00:08<02:41,  3.44it/s]  5%|▍         | 29/585 [00:08<02:41,  3.44it/s]  5%|▌         | 30/585 [00:08<02:41,  3.44it/s]  5%|▌         | 31/585 [00:09<02:41,  3.44it/s]  5%|▌         | 32/585 [00:09<02:40,  3.44it/s]  6%|▌         | 33/585 [00:09<02:40,  3.44it/s]  6%|▌         | 34/585 [00:09<02:40,  3.44it/s]  6%|▌         | 35/585 [00:10<02:39,  3.44it/s]  6%|▌         | 36/585 [00:10<02:39,  3.44it/s]  6%|▋         | 37/585 [00:10<02:39,  3.44it/s]  6%|▋         | 38/585 [00:11<02:38,  3.44it/s]  7%|▋         | 39/585 [00:11<02:38,  3.44it/s]  7%|▋         | 40/585 [00:11<02:38,  3.44it/s]  7%|▋         | 41/585 [00:11<02:38,  3.43it/s]  7%|▋         | 42/585 [00:12<02:38,  3.43it/s]  7%|▋         | 43/585 [00:12<02:37,  3.43it/s]  8%|▊         | 44/585 [00:12<02:37,  3.44it/s]  8%|▊         | 45/585 [00:13<02:36,  3.44it/s]  8%|▊         | 46/585 [00:13<02:36,  3.44it/s]  8%|▊         | 47/585 [00:13<02:36,  3.44it/s]  8%|▊         | 48/585 [00:13<02:36,  3.44it/s]  8%|▊         | 49/585 [00:14<02:35,  3.44it/s]  9%|▊         | 50/585 [00:14<02:35,  3.44it/s]  9%|▊         | 51/585 [00:14<02:35,  3.44it/s]  9%|▉         | 52/585 [00:15<02:35,  3.43it/s]  9%|▉         | 53/585 [00:15<02:34,  3.44it/s]  9%|▉         | 54/585 [00:15<02:34,  3.43it/s]  9%|▉         | 55/585 [00:16<02:34,  3.44it/s] 10%|▉         | 56/585 [00:16<02:33,  3.44it/s] 10%|▉         | 57/585 [00:16<02:33,  3.44it/s] 10%|▉         | 58/585 [00:16<02:33,  3.44it/s] 10%|█         | 59/585 [00:17<02:32,  3.44it/s] 10%|█         | 60/585 [00:17<02:32,  3.44it/s] 10%|█         | 61/585 [00:17<02:32,  3.44it/s] 11%|█         | 62/585 [00:18<02:31,  3.44it/s] 11%|█         | 63/585 [00:18<02:32,  3.41it/s] 11%|█         | 64/585 [00:18<02:32,  3.42it/s] 11%|█         | 65/585 [00:18<02:31,  3.43it/s] 11%|█▏        | 66/585 [00:19<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.43it/s] 12%|█▏        | 68/585 [00:19<02:30,  3.44it/s] 12%|█▏        | 69/585 [00:20<02:30,  3.44it/s] 12%|█▏        | 70/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 71/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 72/585 [00:20<02:29,  3.44it/s] 12%|█▏        | 73/585 [00:21<02:28,  3.44it/s] 13%|█▎        | 74/585 [00:21<02:29,  3.42it/s] 13%|█▎        | 75/585 [00:21<02:29,  3.41it/s] 13%|█▎        | 76/585 [00:22<02:28,  3.42it/s] 13%|█▎        | 77/585 [00:22<02:28,  3.43it/s] 13%|█▎        | 78/585 [00:22<02:28,  3.41it/s] 14%|█▎        | 79/585 [00:23<02:28,  3.40it/s] 14%|█▎        | 80/585 [00:23<02:28,  3.40it/s] 14%|█▍        | 81/585 [00:23<02:28,  3.40it/s] 14%|█▍        | 82/585 [00:23<02:28,  3.39it/s] 14%|█▍        | 83/585 [00:24<02:27,  3.39it/s] 14%|█▍        | 84/585 [00:24<02:27,  3.39it/s] 15%|█▍        | 85/585 [00:24<02:27,  3.39it/s] 15%|█▍        | 86/585 [00:25<02:27,  3.39it/s] 15%|█▍        | 87/585 [00:25<02:27,  3.38it/s] 15%|█▌        | 88/585 [00:25<02:26,  3.38it/s] 15%|█▌        | 89/585 [00:25<02:26,  3.38it/s] 15%|█▌        | 90/585 [00:26<02:26,  3.39it/s] 16%|█▌        | 91/585 [00:26<02:25,  3.39it/s] 16%|█▌        | 92/585 [00:26<02:25,  3.39it/s] 16%|█▌        | 93/585 [00:27<02:25,  3.39it/s] 16%|█▌        | 94/585 [00:27<02:24,  3.39it/s] 16%|█▌        | 95/585 [00:27<02:24,  3.39it/s] 16%|█▋        | 96/585 [00:28<02:24,  3.39it/s] 17%|█▋        | 97/585 [00:28<02:24,  3.38it/s] 17%|█▋        | 98/585 [00:28<02:24,  3.38it/s] 17%|█▋        | 99/585 [00:28<02:23,  3.38it/s] 17%|█▋        | 100/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 101/585 [00:29<02:23,  3.38it/s] 17%|█▋        | 102/585 [00:29<02:22,  3.39it/s] 18%|█▊        | 103/585 [00:30<02:22,  3.39it/s] 18%|█▊        | 104/585 [00:30<02:21,  3.39it/s] 18%|█▊        | 105/585 [00:30<02:21,  3.39it/s] 18%|█▊        | 106/585 [00:31<02:21,  3.39it/s] 18%|█▊        | 107/585 [00:31<02:20,  3.40it/s] 18%|█▊        | 108/585 [00:31<02:20,  3.40it/s] 19%|█▊        | 109/585 [00:31<02:19,  3.41it/s] 19%|█▉        | 110/585 [00:32<02:18,  3.42it/s] 19%|█▉        | 111/585 [00:32<02:18,  3.42it/s] 19%|█▉        | 112/585 [00:32<02:17,  3.43it/s] 19%|█▉        | 113/585 [00:33<02:17,  3.43it/s] 19%|█▉        | 114/585 [00:33<02:17,  3.43it/s] 20%|█▉        | 115/585 [00:33<02:16,  3.43it/s] 20%|█▉        | 116/585 [00:33<02:16,  3.43it/s] 20%|██        | 117/585 [00:34<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:08:16,839 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:08:16,839 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 01:08:16,839 >>   Batch size = 8

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 54.85it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.46it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.72it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.22it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.72it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.66it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.51it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.31it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.39it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.36it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.35it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.25it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.08it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.11it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.24it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.19it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.11it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.29it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.22it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.20it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.20it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.17it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.07it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.11it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.13it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.21it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.27it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.21it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.07it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.10it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.11it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.05it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.10it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.17it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.29it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.22it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.20it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.18it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.10it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.11it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 43.93it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.01it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.15it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.21it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.18it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.26it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.12it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.18it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.17it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.15it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.13it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.23it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.16it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.21it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.12it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.07it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.19it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.17it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.12it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.17it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.24it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.13it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.17it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.13it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.05it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.26it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.25it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.15it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.15it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.19it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.19it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.07it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.13it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.17it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.21it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.14it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.15it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.13it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.19it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.12it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.06it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.16it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.18it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.19it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.08it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.11it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.20it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.18it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.18it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.11it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.15it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.19it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.18it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.97it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.11it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.20it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.18it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.21it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.14it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.19it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.20it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.16it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.05it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.11it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.17it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.24it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.12it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.20it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.20it/s][A 20%|██        | 117/585 [00:46<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:08:29,172 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 01:08:29,219 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:08:34,524 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:08:34,563 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:08:34,576 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [00:56<52:46,  6.78s/it] 20%|██        | 119/585 [00:56<37:33,  4.84s/it] 21%|██        | 120/585 [00:56<26:54,  3.47s/it] 21%|██        | 121/585 [00:57<19:28,  2.52s/it] 21%|██        | 122/585 [00:57<14:17,  1.85s/it] 21%|██        | 123/585 [00:57<10:39,  1.38s/it] 21%|██        | 124/585 [00:57<08:07,  1.06s/it] 21%|██▏       | 125/585 [00:58<06:21,  1.21it/s] 22%|██▏       | 126/585 [00:58<05:06,  1.50it/s] 22%|██▏       | 127/585 [00:58<04:14,  1.80it/s] 22%|██▏       | 128/585 [00:59<03:38,  2.09it/s] 22%|██▏       | 129/585 [00:59<03:12,  2.37it/s] 22%|██▏       | 130/585 [00:59<02:55,  2.59it/s] 22%|██▏       | 131/585 [00:59<02:42,  2.79it/s] 23%|██▎       | 132/585 [01:00<02:33,  2.95it/s] 23%|██▎       | 133/585 [01:00<02:27,  3.07it/s] 23%|██▎       | 134/585 [01:00<02:22,  3.16it/s] 23%|██▎       | 135/585 [01:01<02:19,  3.23it/s] 23%|██▎       | 136/585 [01:01<02:17,  3.28it/s] 23%|██▎       | 137/585 [01:01<02:15,  3.31it/s] 24%|██▎       | 138/585 [01:02<02:13,  3.34it/s] 24%|██▍       | 139/585 [01:02<02:13,  3.35it/s] 24%|██▍       | 140/585 [01:02<02:12,  3.36it/s] 24%|██▍       | 141/585 [01:02<02:12,  3.35it/s] 24%|██▍       | 142/585 [01:03<02:11,  3.37it/s] 24%|██▍       | 143/585 [01:03<02:10,  3.38it/s] 25%|██▍       | 144/585 [01:03<02:10,  3.38it/s] 25%|██▍       | 145/585 [01:04<02:09,  3.39it/s] 25%|██▍       | 146/585 [01:04<02:09,  3.39it/s] 25%|██▌       | 147/585 [01:04<02:09,  3.39it/s] 25%|██▌       | 148/585 [01:04<02:08,  3.39it/s] 25%|██▌       | 149/585 [01:05<02:08,  3.39it/s] 26%|██▌       | 150/585 [01:05<02:08,  3.40it/s] 26%|██▌       | 151/585 [01:05<02:08,  3.38it/s] 26%|██▌       | 152/585 [01:06<02:08,  3.37it/s] 26%|██▌       | 153/585 [01:06<02:07,  3.38it/s] 26%|██▋       | 154/585 [01:06<02:07,  3.38it/s] 26%|██▋       | 155/585 [01:07<02:07,  3.38it/s] 27%|██▋       | 156/585 [01:07<02:06,  3.38it/s] 27%|██▋       | 157/585 [01:07<02:06,  3.38it/s] 27%|██▋       | 158/585 [01:07<02:06,  3.39it/s] 27%|██▋       | 159/585 [01:08<02:05,  3.39it/s] 27%|██▋       | 160/585 [01:08<02:05,  3.39it/s] 28%|██▊       | 161/585 [01:08<02:05,  3.39it/s] 28%|██▊       | 162/585 [01:09<02:04,  3.39it/s] 28%|██▊       | 163/585 [01:09<02:04,  3.38it/s] 28%|██▊       | 164/585 [01:09<02:04,  3.38it/s] 28%|██▊       | 165/585 [01:10<02:04,  3.38it/s] 28%|██▊       | 166/585 [01:10<02:03,  3.39it/s] 29%|██▊       | 167/585 [01:10<02:03,  3.39it/s] 29%|██▊       | 168/585 [01:10<02:03,  3.39it/s] 29%|██▉       | 169/585 [01:11<02:02,  3.39it/s] 29%|██▉       | 170/585 [01:11<02:02,  3.39it/s] 29%|██▉       | 171/585 [01:11<02:02,  3.39it/s] 29%|██▉       | 172/585 [01:12<02:01,  3.39it/s] 30%|██▉       | 173/585 [01:12<02:01,  3.39it/s] 30%|██▉       | 174/585 [01:12<02:01,  3.37it/s] 30%|██▉       | 175/585 [01:12<02:01,  3.38it/s] 30%|███       | 176/585 [01:13<02:01,  3.38it/s] 30%|███       | 177/585 [01:13<02:00,  3.38it/s] 30%|███       | 178/585 [01:13<02:00,  3.38it/s] 31%|███       | 179/585 [01:14<01:59,  3.39it/s] 31%|███       | 180/585 [01:14<01:59,  3.39it/s] 31%|███       | 181/585 [01:14<01:59,  3.39it/s] 31%|███       | 182/585 [01:15<01:58,  3.39it/s] 31%|███▏      | 183/585 [01:15<01:58,  3.39it/s] 31%|███▏      | 184/585 [01:15<01:58,  3.39it/s] 32%|███▏      | 185/585 [01:15<01:59,  3.36it/s] 32%|███▏      | 186/585 [01:16<01:58,  3.37it/s] 32%|███▏      | 187/585 [01:16<01:57,  3.38it/s] 32%|███▏      | 188/585 [01:16<01:57,  3.38it/s] 32%|███▏      | 189/585 [01:17<01:57,  3.38it/s] 32%|███▏      | 190/585 [01:17<01:56,  3.39it/s] 33%|███▎      | 191/585 [01:17<01:56,  3.39it/s] 33%|███▎      | 192/585 [01:17<01:55,  3.39it/s] 33%|███▎      | 193/585 [01:18<01:55,  3.39it/s] 33%|███▎      | 194/585 [01:18<01:55,  3.39it/s] 33%|███▎      | 195/585 [01:18<01:55,  3.39it/s] 34%|███▎      | 196/585 [01:19<01:54,  3.39it/s] 34%|███▎      | 197/585 [01:19<01:54,  3.39it/s] 34%|███▍      | 198/585 [01:19<01:54,  3.39it/s] 34%|███▍      | 199/585 [01:20<01:53,  3.39it/s] 34%|███▍      | 200/585 [01:20<01:53,  3.39it/s] 34%|███▍      | 201/585 [01:20<01:53,  3.39it/s] 35%|███▍      | 202/585 [01:20<01:53,  3.39it/s] 35%|███▍      | 203/585 [01:21<01:52,  3.38it/s] 35%|███▍      | 204/585 [01:21<01:52,  3.38it/s] 35%|███▌      | 205/585 [01:21<01:52,  3.38it/s] 35%|███▌      | 206/585 [01:22<01:51,  3.39it/s] 35%|███▌      | 207/585 [01:22<01:51,  3.39it/s] 36%|███▌      | 208/585 [01:22<01:51,  3.39it/s] 36%|███▌      | 209/585 [01:22<01:50,  3.39it/s] 36%|███▌      | 210/585 [01:23<01:50,  3.39it/s] 36%|███▌      | 211/585 [01:23<01:50,  3.39it/s] 36%|███▌      | 212/585 [01:23<01:50,  3.39it/s] 36%|███▋      | 213/585 [01:24<01:49,  3.39it/s] 37%|███▋      | 214/585 [01:24<01:49,  3.38it/s] 37%|███▋      | 215/585 [01:24<01:49,  3.38it/s] 37%|███▋      | 216/585 [01:25<01:49,  3.38it/s] 37%|███▋      | 217/585 [01:25<01:48,  3.39it/s] 37%|███▋      | 218/585 [01:25<01:48,  3.39it/s] 37%|███▋      | 219/585 [01:25<01:48,  3.39it/s] 38%|███▊      | 220/585 [01:26<01:47,  3.38it/s] 38%|███▊      | 221/585 [01:26<01:47,  3.39it/s] 38%|███▊      | 222/585 [01:26<01:47,  3.39it/s] 38%|███▊      | 223/585 [01:27<01:46,  3.39it/s] 38%|███▊      | 224/585 [01:27<01:46,  3.38it/s] 38%|███▊      | 225/585 [01:27<01:46,  3.38it/s] 39%|███▊      | 226/585 [01:28<01:46,  3.38it/s] 39%|███▉      | 227/585 [01:28<01:45,  3.38it/s] 39%|███▉      | 228/585 [01:28<01:45,  3.38it/s] 39%|███▉      | 229/585 [01:28<01:45,  3.38it/s] 39%|███▉      | 230/585 [01:29<01:44,  3.38it/s] 39%|███▉      | 231/585 [01:29<01:44,  3.39it/s] 40%|███▉      | 232/585 [01:29<01:44,  3.39it/s] 40%|███▉      | 233/585 [01:30<01:43,  3.39it/s] 40%|████      | 234/585 [01:30<01:43,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 01:09:13,019 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:09:13,019 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 01:09:13,019 >>   Batch size = 8
{'eval_loss': 1.0607802867889404, 'eval_runtime': 12.314, 'eval_samples_per_second': 352.606, 'eval_steps_per_second': 44.096, 'epoch': 1.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 57.92it/s][A
  2%|▏         | 12/543 [00:00<00:10, 48.30it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.31it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.39it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.73it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.50it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.43it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.36it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.52it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.52it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.41it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.34it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.20it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.03it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.11it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.07it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.09it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.24it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.35it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.28it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.20it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.08it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.06it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.04it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.02it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.16it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.23it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.30it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.32it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.20it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.15it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.14it/s][A
 31%|███       | 167/543 [00:03<00:08, 43.98it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.06it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.14it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.18it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.33it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.23it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.08it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.16it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.14it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.10it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.13it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.03it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.25it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.27it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.20it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.12it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.10it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.14it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.18it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.13it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.15it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.22it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.19it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.12it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.20it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.13it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.15it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.07it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.06it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.21it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.17it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.14it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.20it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.17it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.14it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.05it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 43.96it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.08it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.20it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.23it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.16it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.19it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.09it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.07it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.09it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.02it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.11it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.24it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.15it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.22it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.20it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.12it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.16it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.04it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.10it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.16it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.00it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.04it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.19it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.22it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.19it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.07it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.07it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.16it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.10it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.07it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.14it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.17it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.23it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.18it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.16it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.19it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.14it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.11it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 43.99it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.17it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.17it/s][A 40%|████      | 234/585 [01:42<01:43,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:09:25,351 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 01:09:25,372 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:09:27,460 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:09:27,476 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:09:27,485 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [01:50<36:46,  6.30s/it] 40%|████      | 236/585 [01:50<26:10,  4.50s/it] 41%|████      | 237/585 [01:51<18:46,  3.24s/it] 41%|████      | 238/585 [01:51<13:36,  2.35s/it] 41%|████      | 239/585 [01:51<10:00,  1.73s/it] 41%|████      | 240/585 [01:52<07:29,  1.30s/it] 41%|████      | 241/585 [01:52<05:46,  1.01s/it] 41%|████▏     | 242/585 [01:52<04:32,  1.26it/s] 42%|████▏     | 243/585 [01:53<03:39,  1.56it/s] 42%|████▏     | 244/585 [01:53<03:03,  1.86it/s] 42%|████▏     | 245/585 [01:53<02:37,  2.16it/s] 42%|████▏     | 246/585 [01:53<02:19,  2.43it/s] 42%|████▏     | 247/585 [01:54<02:07,  2.66it/s] 42%|████▏     | 248/585 [01:54<01:58,  2.86it/s] 43%|████▎     | 249/585 [01:54<01:51,  3.01it/s] 43%|████▎     | 250/585 [01:55<01:47,  3.13it/s] 43%|████▎     | 251/585 [01:55<01:44,  3.19it/s] 43%|████▎     | 252/585 [01:55<01:42,  3.26it/s] 43%|████▎     | 253/585 [01:55<01:40,  3.32it/s] 43%|████▎     | 254/585 [01:56<01:38,  3.35it/s] 44%|████▎     | 255/585 [01:56<01:37,  3.38it/s] 44%|████▍     | 256/585 [01:56<01:36,  3.40it/s] 44%|████▍     | 257/585 [01:57<01:36,  3.41it/s] 44%|████▍     | 258/585 [01:57<01:35,  3.42it/s] 44%|████▍     | 259/585 [01:57<01:35,  3.43it/s] 44%|████▍     | 260/585 [01:58<01:34,  3.43it/s] 45%|████▍     | 261/585 [01:58<01:34,  3.43it/s] 45%|████▍     | 262/585 [01:58<01:34,  3.43it/s] 45%|████▍     | 263/585 [01:58<01:33,  3.43it/s] 45%|████▌     | 264/585 [01:59<01:33,  3.43it/s] 45%|████▌     | 265/585 [01:59<01:33,  3.44it/s] 45%|████▌     | 266/585 [01:59<01:32,  3.44it/s] 46%|████▌     | 267/585 [02:00<01:32,  3.44it/s] 46%|████▌     | 268/585 [02:00<01:32,  3.44it/s] 46%|████▌     | 269/585 [02:00<01:31,  3.44it/s] 46%|████▌     | 270/585 [02:00<01:31,  3.44it/s] 46%|████▋     | 271/585 [02:01<01:31,  3.44it/s] 46%|████▋     | 272/585 [02:01<01:31,  3.44it/s] 47%|████▋     | 273/585 [02:01<01:30,  3.43it/s] 47%|████▋     | 274/585 [02:02<01:30,  3.43it/s] 47%|████▋     | 275/585 [02:02<01:30,  3.43it/s] 47%|████▋     | 276/585 [02:02<01:30,  3.43it/s] 47%|████▋     | 277/585 [02:02<01:29,  3.43it/s] 48%|████▊     | 278/585 [02:03<01:29,  3.43it/s] 48%|████▊     | 279/585 [02:03<01:29,  3.44it/s] 48%|████▊     | 280/585 [02:03<01:28,  3.44it/s] 48%|████▊     | 281/585 [02:04<01:28,  3.44it/s] 48%|████▊     | 282/585 [02:04<01:28,  3.44it/s] 48%|████▊     | 283/585 [02:04<01:27,  3.44it/s] 49%|████▊     | 284/585 [02:05<01:27,  3.43it/s] 49%|████▊     | 285/585 [02:05<01:27,  3.43it/s] 49%|████▉     | 286/585 [02:05<01:27,  3.43it/s] 49%|████▉     | 287/585 [02:05<01:26,  3.43it/s] 49%|████▉     | 288/585 [02:06<01:26,  3.43it/s] 49%|████▉     | 289/585 [02:06<01:26,  3.44it/s] 50%|████▉     | 290/585 [02:06<01:25,  3.43it/s] 50%|████▉     | 291/585 [02:07<01:25,  3.43it/s] 50%|████▉     | 292/585 [02:07<01:25,  3.43it/s] 50%|█████     | 293/585 [02:07<01:24,  3.44it/s] 50%|█████     | 294/585 [02:07<01:24,  3.43it/s] 50%|█████     | 295/585 [02:08<01:24,  3.43it/s] 51%|█████     | 296/585 [02:08<01:24,  3.43it/s] 51%|█████     | 297/585 [02:08<01:23,  3.43it/s] 51%|█████     | 298/585 [02:09<01:23,  3.43it/s] 51%|█████     | 299/585 [02:09<01:23,  3.43it/s] 51%|█████▏    | 300/585 [02:09<01:23,  3.43it/s] 51%|█████▏    | 301/585 [02:09<01:22,  3.44it/s] 52%|█████▏    | 302/585 [02:10<01:22,  3.43it/s] 52%|█████▏    | 303/585 [02:10<01:22,  3.44it/s] 52%|█████▏    | 304/585 [02:10<01:21,  3.44it/s] 52%|█████▏    | 305/585 [02:11<01:21,  3.44it/s] 52%|█████▏    | 306/585 [02:11<01:21,  3.43it/s] 52%|█████▏    | 307/585 [02:11<01:21,  3.43it/s] 53%|█████▎    | 308/585 [02:11<01:20,  3.43it/s] 53%|█████▎    | 309/585 [02:12<01:20,  3.43it/s] 53%|█████▎    | 310/585 [02:12<01:20,  3.43it/s] 53%|█████▎    | 311/585 [02:12<01:19,  3.43it/s] 53%|█████▎    | 312/585 [02:13<01:19,  3.43it/s] 54%|█████▎    | 313/585 [02:13<01:19,  3.43it/s] 54%|█████▎    | 314/585 [02:13<01:18,  3.43it/s] 54%|█████▍    | 315/585 [02:14<01:18,  3.43it/s] 54%|█████▍    | 316/585 [02:14<01:18,  3.44it/s] 54%|█████▍    | 317/585 [02:14<01:18,  3.43it/s] 54%|█████▍    | 318/585 [02:14<01:17,  3.43it/s] 55%|█████▍    | 319/585 [02:15<01:17,  3.43it/s] 55%|█████▍    | 320/585 [02:15<01:17,  3.44it/s] 55%|█████▍    | 321/585 [02:15<01:16,  3.44it/s] 55%|█████▌    | 322/585 [02:16<01:16,  3.44it/s] 55%|█████▌    | 323/585 [02:16<01:16,  3.44it/s] 55%|█████▌    | 324/585 [02:16<01:15,  3.44it/s] 56%|█████▌    | 325/585 [02:16<01:15,  3.44it/s] 56%|█████▌    | 326/585 [02:17<01:15,  3.44it/s] 56%|█████▌    | 327/585 [02:17<01:15,  3.44it/s] 56%|█████▌    | 328/585 [02:17<01:14,  3.43it/s] 56%|█████▌    | 329/585 [02:18<01:14,  3.43it/s] 56%|█████▋    | 330/585 [02:18<01:14,  3.43it/s] 57%|█████▋    | 331/585 [02:18<01:14,  3.43it/s] 57%|█████▋    | 332/585 [02:18<01:13,  3.43it/s] 57%|█████▋    | 333/585 [02:19<01:13,  3.43it/s] 57%|█████▋    | 334/585 [02:19<01:13,  3.43it/s] 57%|█████▋    | 335/585 [02:19<01:12,  3.44it/s] 57%|█████▋    | 336/585 [02:20<01:12,  3.43it/s] 58%|█████▊    | 337/585 [02:20<01:12,  3.43it/s] 58%|█████▊    | 338/585 [02:20<01:12,  3.43it/s] 58%|█████▊    | 339/585 [02:21<01:11,  3.43it/s] 58%|█████▊    | 340/585 [02:21<01:11,  3.43it/s] 58%|█████▊    | 341/585 [02:21<01:11,  3.43it/s] 58%|█████▊    | 342/585 [02:21<01:10,  3.43it/s] 59%|█████▊    | 343/585 [02:22<01:10,  3.43it/s] 59%|█████▉    | 344/585 [02:22<01:10,  3.43it/s] 59%|█████▉    | 345/585 [02:22<01:10,  3.42it/s] 59%|█████▉    | 346/585 [02:23<01:09,  3.43it/s] 59%|█████▉    | 347/585 [02:23<01:09,  3.42it/s] 59%|█████▉    | 348/585 [02:23<01:09,  3.43it/s] 60%|█████▉    | 349/585 [02:23<01:08,  3.43it/s] 60%|█████▉    | 350/585 [02:24<01:08,  3.43it/s] 60%|██████    | 351/585 [02:24<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:10:07,166 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:10:07,166 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 01:10:07,166 >>   Batch size = 8
{'eval_loss': 1.077687382698059, 'eval_runtime': 12.3148, 'eval_samples_per_second': 352.583, 'eval_steps_per_second': 44.093, 'epoch': 2.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.39it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.87it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.98it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.43it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.68it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.36it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.34it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.27it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.31it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.39it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.39it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.29it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.26it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.06it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 43.96it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.01it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.10it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.21it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.31it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.26it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.11it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.15it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.02it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.01it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.03it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.08it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.27it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.25it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.21it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.11it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.02it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 43.97it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.02it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.03it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.13it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.19it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 43.94it/s][A
 35%|███▌      | 192/543 [00:04<00:08, 43.52it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.21it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.03it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.00it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.90it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 44.02it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.00it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.09it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.15it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.15it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 43.99it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 43.80it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 43.81it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.01it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.05it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.09it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.21it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.17it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.22it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.19it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.13it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 43.98it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.01it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 43.94it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.12it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.15it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.16it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.22it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.16it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.11it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.00it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.05it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.11it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.15it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.21it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.19it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.26it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.20it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.11it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 43.96it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.00it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.07it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.17it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.08it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.09it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.16it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.17it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.15it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 43.97it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.05it/s][A
 81%|████████▏ | 442/543 [00:10<00:02, 44.12it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.27it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.22it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.17it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.28it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.17it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.02it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.00it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.02it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.11it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.19it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.22it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.16it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.19it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.15it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 43.99it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.05it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.04it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.02it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.17it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.22it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.22it/s][A 60%|██████    | 351/585 [02:36<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:10:19,518 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 01:10:19,543 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:10:21,280 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:10:21,298 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:10:21,313 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [02:42<21:53,  5.64s/it] 60%|██████    | 353/585 [02:42<15:36,  4.04s/it] 61%|██████    | 354/585 [02:43<11:13,  2.91s/it] 61%|██████    | 355/585 [02:43<08:09,  2.13s/it] 61%|██████    | 356/585 [02:43<06:01,  1.58s/it] 61%|██████    | 357/585 [02:44<04:32,  1.19s/it] 61%|██████    | 358/585 [02:44<03:29,  1.08it/s] 61%|██████▏   | 359/585 [02:44<02:46,  1.36it/s] 62%|██████▏   | 360/585 [02:45<02:15,  1.66it/s] 62%|██████▏   | 361/585 [02:45<01:54,  1.96it/s] 62%|██████▏   | 362/585 [02:45<01:39,  2.24it/s] 62%|██████▏   | 363/585 [02:45<01:29,  2.49it/s] 62%|██████▏   | 364/585 [02:46<01:21,  2.70it/s] 62%|██████▏   | 365/585 [02:46<01:16,  2.88it/s] 63%|██████▎   | 366/585 [02:46<01:12,  3.01it/s] 63%|██████▎   | 367/585 [02:47<01:09,  3.11it/s] 63%|██████▎   | 368/585 [02:47<01:07,  3.19it/s] 63%|██████▎   | 369/585 [02:47<01:06,  3.25it/s] 63%|██████▎   | 370/585 [02:47<01:05,  3.29it/s] 63%|██████▎   | 371/585 [02:48<01:04,  3.32it/s] 64%|██████▎   | 372/585 [02:48<01:03,  3.34it/s] 64%|██████▍   | 373/585 [02:48<01:03,  3.35it/s] 64%|██████▍   | 374/585 [02:49<01:02,  3.36it/s] 64%|██████▍   | 375/585 [02:49<01:02,  3.36it/s] 64%|██████▍   | 376/585 [02:49<01:02,  3.37it/s] 64%|██████▍   | 377/585 [02:50<01:01,  3.37it/s] 65%|██████▍   | 378/585 [02:50<01:01,  3.36it/s] 65%|██████▍   | 379/585 [02:50<01:01,  3.37it/s] 65%|██████▍   | 380/585 [02:50<01:00,  3.37it/s] 65%|██████▌   | 381/585 [02:51<01:00,  3.38it/s] 65%|██████▌   | 382/585 [02:51<01:00,  3.38it/s] 65%|██████▌   | 383/585 [02:51<00:59,  3.38it/s] 66%|██████▌   | 384/585 [02:52<00:59,  3.38it/s] 66%|██████▌   | 385/585 [02:52<00:59,  3.37it/s] 66%|██████▌   | 386/585 [02:52<01:00,  3.30it/s] 66%|██████▌   | 387/585 [02:53<00:59,  3.32it/s] 66%|██████▋   | 388/585 [02:53<00:58,  3.34it/s] 66%|██████▋   | 389/585 [02:53<00:58,  3.35it/s] 67%|██████▋   | 390/585 [02:53<00:58,  3.36it/s] 67%|██████▋   | 391/585 [02:54<00:57,  3.37it/s] 67%|██████▋   | 392/585 [02:54<00:57,  3.37it/s] 67%|██████▋   | 393/585 [02:54<00:56,  3.37it/s] 67%|██████▋   | 394/585 [02:55<00:56,  3.38it/s] 68%|██████▊   | 395/585 [02:55<00:56,  3.38it/s] 68%|██████▊   | 396/585 [02:55<00:55,  3.38it/s] 68%|██████▊   | 397/585 [02:55<00:55,  3.39it/s] 68%|██████▊   | 398/585 [02:56<00:55,  3.39it/s] 68%|██████▊   | 399/585 [02:56<00:54,  3.39it/s] 68%|██████▊   | 400/585 [02:56<00:54,  3.38it/s] 69%|██████▊   | 401/585 [02:57<00:54,  3.38it/s] 69%|██████▊   | 402/585 [02:57<00:54,  3.38it/s] 69%|██████▉   | 403/585 [02:57<00:53,  3.38it/s] 69%|██████▉   | 404/585 [02:58<00:53,  3.38it/s] 69%|██████▉   | 405/585 [02:58<00:53,  3.38it/s] 69%|██████▉   | 406/585 [02:58<00:52,  3.38it/s] 70%|██████▉   | 407/585 [02:58<00:52,  3.38it/s] 70%|██████▉   | 408/585 [02:59<00:52,  3.39it/s] 70%|██████▉   | 409/585 [02:59<00:52,  3.38it/s] 70%|███████   | 410/585 [02:59<00:51,  3.38it/s] 70%|███████   | 411/585 [03:00<00:51,  3.36it/s] 70%|███████   | 412/585 [03:00<00:51,  3.37it/s] 71%|███████   | 413/585 [03:00<00:50,  3.38it/s] 71%|███████   | 414/585 [03:01<00:50,  3.38it/s] 71%|███████   | 415/585 [03:01<00:50,  3.38it/s] 71%|███████   | 416/585 [03:01<00:49,  3.39it/s] 71%|███████▏  | 417/585 [03:01<00:49,  3.39it/s] 71%|███████▏  | 418/585 [03:02<00:49,  3.41it/s] 72%|███████▏  | 419/585 [03:02<00:48,  3.42it/s] 72%|███████▏  | 420/585 [03:02<00:48,  3.42it/s] 72%|███████▏  | 421/585 [03:03<00:47,  3.42it/s] 72%|███████▏  | 422/585 [03:03<00:47,  3.41it/s] 72%|███████▏  | 423/585 [03:03<00:47,  3.42it/s] 72%|███████▏  | 424/585 [03:03<00:47,  3.42it/s] 73%|███████▎  | 425/585 [03:04<00:46,  3.43it/s] 73%|███████▎  | 426/585 [03:04<00:46,  3.43it/s] 73%|███████▎  | 427/585 [03:04<00:46,  3.43it/s] 73%|███████▎  | 428/585 [03:05<00:45,  3.43it/s] 73%|███████▎  | 429/585 [03:05<00:45,  3.43it/s] 74%|███████▎  | 430/585 [03:05<00:45,  3.43it/s] 74%|███████▎  | 431/585 [03:05<00:44,  3.43it/s] 74%|███████▍  | 432/585 [03:06<00:44,  3.43it/s] 74%|███████▍  | 433/585 [03:06<00:44,  3.42it/s] 74%|███████▍  | 434/585 [03:06<00:44,  3.42it/s] 74%|███████▍  | 435/585 [03:07<00:43,  3.42it/s] 75%|███████▍  | 436/585 [03:07<00:43,  3.43it/s] 75%|███████▍  | 437/585 [03:07<00:43,  3.43it/s] 75%|███████▍  | 438/585 [03:08<00:42,  3.43it/s] 75%|███████▌  | 439/585 [03:08<00:42,  3.43it/s] 75%|███████▌  | 440/585 [03:08<00:42,  3.43it/s] 75%|███████▌  | 441/585 [03:08<00:41,  3.43it/s] 76%|███████▌  | 442/585 [03:09<00:41,  3.43it/s] 76%|███████▌  | 443/585 [03:09<00:41,  3.43it/s] 76%|███████▌  | 444/585 [03:09<00:41,  3.42it/s] 76%|███████▌  | 445/585 [03:10<00:40,  3.42it/s] 76%|███████▌  | 446/585 [03:10<00:40,  3.42it/s] 76%|███████▋  | 447/585 [03:10<00:40,  3.43it/s] 77%|███████▋  | 448/585 [03:10<00:39,  3.43it/s] 77%|███████▋  | 449/585 [03:11<00:39,  3.43it/s] 77%|███████▋  | 450/585 [03:11<00:39,  3.43it/s] 77%|███████▋  | 451/585 [03:11<00:39,  3.43it/s] 77%|███████▋  | 452/585 [03:12<00:38,  3.43it/s] 77%|███████▋  | 453/585 [03:12<00:38,  3.43it/s] 78%|███████▊  | 454/585 [03:12<00:38,  3.43it/s] 78%|███████▊  | 455/585 [03:12<00:38,  3.41it/s] 78%|███████▊  | 456/585 [03:13<00:37,  3.42it/s] 78%|███████▊  | 457/585 [03:13<00:37,  3.42it/s] 78%|███████▊  | 458/585 [03:13<00:37,  3.43it/s] 78%|███████▊  | 459/585 [03:14<00:36,  3.42it/s] 79%|███████▊  | 460/585 [03:14<00:36,  3.43it/s] 79%|███████▉  | 461/585 [03:14<00:36,  3.43it/s] 79%|███████▉  | 462/585 [03:15<00:35,  3.43it/s] 79%|███████▉  | 463/585 [03:15<00:35,  3.43it/s] 79%|███████▉  | 464/585 [03:15<00:35,  3.43it/s] 79%|███████▉  | 465/585 [03:15<00:34,  3.43it/s] 80%|███████▉  | 466/585 [03:16<00:34,  3.42it/s] 80%|███████▉  | 467/585 [03:16<00:34,  3.42it/s] 80%|████████  | 468/585 [03:16<00:34,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 01:10:59,410 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:10:59,410 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 01:10:59,410 >>   Batch size = 8
{'eval_loss': 1.0794620513916016, 'eval_runtime': 12.3262, 'eval_samples_per_second': 352.258, 'eval_steps_per_second': 44.052, 'epoch': 3.0}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.25it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.84it/s][A
  3%|▎         | 17/543 [00:00<00:11, 45.91it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.37it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.85it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.53it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.41it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.21it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.27it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.44it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.29it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.27it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.16it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.07it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.11it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 44.05it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.11it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.10it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.20it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.20it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.18it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.14it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.13it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.03it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.06it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.00it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.06it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.25it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.22it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.08it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.09it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.09it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.01it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.08it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 43.97it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.10it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.21it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.24it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.13it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.07it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.00it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 44.00it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.97it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.14it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.22it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.26it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.18it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.12it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.05it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.06it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.03it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.08it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.15it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.20it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.09it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.13it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.11it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.13it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.08it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.06it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.18it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.20it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.10it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.17it/s][A
 60%|██████    | 327/543 [00:07<00:04, 43.99it/s][A
 61%|██████    | 332/543 [00:07<00:04, 43.95it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.12it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.13it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.09it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.19it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.22it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.09it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.15it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.07it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.04it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.14it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.02it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.14it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.20it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.17it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.15it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.07it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.07it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.08it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.12it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.06it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.10it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.14it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.18it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.09it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.04it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.08it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.11it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 44.01it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.09it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.06it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.10it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.16it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.11it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.12it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.12it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.15it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.19it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.12it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.06it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.10it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.13it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.14it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.14it/s][A 80%|████████  | 468/585 [03:29<00:34,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:11:11,749 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 01:11:11,766 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:11:13,488 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:11:13,509 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:11:13,518 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [03:35<11:06,  5.74s/it] 80%|████████  | 470/585 [03:35<07:52,  4.11s/it] 81%|████████  | 471/585 [03:35<05:38,  2.97s/it] 81%|████████  | 472/585 [03:36<04:04,  2.16s/it] 81%|████████  | 473/585 [03:36<02:59,  1.60s/it] 81%|████████  | 474/585 [03:36<02:14,  1.21s/it] 81%|████████  | 475/585 [03:37<01:43,  1.07it/s] 81%|████████▏ | 476/585 [03:37<01:21,  1.34it/s] 82%|████████▏ | 477/585 [03:37<01:05,  1.64it/s] 82%|████████▏ | 478/585 [03:37<00:55,  1.94it/s] 82%|████████▏ | 479/585 [03:38<00:47,  2.23it/s] 82%|████████▏ | 480/585 [03:38<00:42,  2.48it/s] 82%|████████▏ | 481/585 [03:38<00:38,  2.70it/s] 82%|████████▏ | 482/585 [03:39<00:35,  2.89it/s] 83%|████████▎ | 483/585 [03:39<00:33,  3.03it/s] 83%|████████▎ | 484/585 [03:39<00:32,  3.15it/s] 83%|████████▎ | 485/585 [03:39<00:30,  3.23it/s] 83%|████████▎ | 486/585 [03:40<00:30,  3.29it/s] 83%|████████▎ | 487/585 [03:40<00:29,  3.33it/s] 83%|████████▎ | 488/585 [03:40<00:28,  3.36it/s] 84%|████████▎ | 489/585 [03:41<00:28,  3.39it/s] 84%|████████▍ | 490/585 [03:41<00:27,  3.40it/s] 84%|████████▍ | 491/585 [03:41<00:27,  3.41it/s] 84%|████████▍ | 492/585 [03:41<00:27,  3.40it/s] 84%|████████▍ | 493/585 [03:42<00:26,  3.41it/s] 84%|████████▍ | 494/585 [03:42<00:26,  3.42it/s] 85%|████████▍ | 495/585 [03:42<00:26,  3.42it/s] 85%|████████▍ | 496/585 [03:43<00:25,  3.43it/s] 85%|████████▍ | 497/585 [03:43<00:25,  3.43it/s] 85%|████████▌ | 498/585 [03:43<00:25,  3.43it/s] 85%|████████▌ | 499/585 [03:44<00:25,  3.43it/s] 85%|████████▌ | 500/585 [03:44<00:24,  3.43it/s]                                                  85%|████████▌ | 500/585 [03:44<00:24,  3.43it/s] 86%|████████▌ | 501/585 [03:44<00:24,  3.44it/s] 86%|████████▌ | 502/585 [03:44<00:24,  3.43it/s] 86%|████████▌ | 503/585 [03:45<00:23,  3.42it/s] 86%|████████▌ | 504/585 [03:45<00:23,  3.43it/s] 86%|████████▋ | 505/585 [03:45<00:23,  3.43it/s] 86%|████████▋ | 506/585 [03:46<00:23,  3.43it/s] 87%|████████▋ | 507/585 [03:46<00:22,  3.43it/s] 87%|████████▋ | 508/585 [03:46<00:22,  3.44it/s] 87%|████████▋ | 509/585 [03:46<00:22,  3.44it/s] 87%|████████▋ | 510/585 [03:47<00:21,  3.44it/s] 87%|████████▋ | 511/585 [03:47<00:21,  3.44it/s] 88%|████████▊ | 512/585 [03:47<00:21,  3.44it/s] 88%|████████▊ | 513/585 [03:48<00:20,  3.44it/s] 88%|████████▊ | 514/585 [03:48<00:20,  3.42it/s] 88%|████████▊ | 515/585 [03:48<00:20,  3.42it/s] 88%|████████▊ | 516/585 [03:48<00:20,  3.43it/s] 88%|████████▊ | 517/585 [03:49<00:19,  3.43it/s] 89%|████████▊ | 518/585 [03:49<00:19,  3.43it/s] 89%|████████▊ | 519/585 [03:49<00:19,  3.43it/s] 89%|████████▉ | 520/585 [03:50<00:18,  3.43it/s] 89%|████████▉ | 521/585 [03:50<00:18,  3.44it/s] 89%|████████▉ | 522/585 [03:50<00:18,  3.44it/s] 89%|████████▉ | 523/585 [03:51<00:18,  3.44it/s] 90%|████████▉ | 524/585 [03:51<00:17,  3.44it/s] 90%|████████▉ | 525/585 [03:51<00:17,  3.41it/s] 90%|████████▉ | 526/585 [03:51<00:17,  3.42it/s] 90%|█████████ | 527/585 [03:52<00:16,  3.42it/s] 90%|█████████ | 528/585 [03:52<00:16,  3.42it/s] 90%|█████████ | 529/585 [03:52<00:17,  3.25it/s] 91%|█████████ | 530/585 [03:53<00:16,  3.31it/s] 91%|█████████ | 531/585 [03:53<00:16,  3.35it/s] 91%|█████████ | 532/585 [03:53<00:15,  3.38it/s] 91%|█████████ | 533/585 [03:53<00:15,  3.39it/s] 91%|█████████▏| 534/585 [03:54<00:14,  3.41it/s] 91%|█████████▏| 535/585 [03:54<00:14,  3.42it/s] 92%|█████████▏| 536/585 [03:54<00:14,  3.41it/s] 92%|█████████▏| 537/585 [03:55<00:14,  3.42it/s] 92%|█████████▏| 538/585 [03:55<00:13,  3.43it/s] 92%|█████████▏| 539/585 [03:55<00:13,  3.43it/s] 92%|█████████▏| 540/585 [03:56<00:13,  3.43it/s] 92%|█████████▏| 541/585 [03:56<00:12,  3.44it/s] 93%|█████████▎| 542/585 [03:56<00:12,  3.43it/s] 93%|█████████▎| 543/585 [03:56<00:12,  3.44it/s] 93%|█████████▎| 544/585 [03:57<00:11,  3.44it/s] 93%|█████████▎| 545/585 [03:57<00:11,  3.44it/s] 93%|█████████▎| 546/585 [03:57<00:11,  3.44it/s] 94%|█████████▎| 547/585 [03:58<00:11,  3.42it/s] 94%|█████████▎| 548/585 [03:58<00:10,  3.42it/s] 94%|█████████▍| 549/585 [03:58<00:10,  3.43it/s] 94%|█████████▍| 550/585 [03:58<00:10,  3.43it/s] 94%|█████████▍| 551/585 [03:59<00:09,  3.43it/s] 94%|█████████▍| 552/585 [03:59<00:09,  3.43it/s] 95%|█████████▍| 553/585 [03:59<00:09,  3.43it/s] 95%|█████████▍| 554/585 [04:00<00:09,  3.44it/s] 95%|█████████▍| 555/585 [04:00<00:08,  3.43it/s] 95%|█████████▌| 556/585 [04:00<00:08,  3.43it/s] 95%|█████████▌| 557/585 [04:00<00:08,  3.43it/s] 95%|█████████▌| 558/585 [04:01<00:07,  3.43it/s] 96%|█████████▌| 559/585 [04:01<00:07,  3.44it/s] 96%|█████████▌| 560/585 [04:01<00:07,  3.43it/s] 96%|█████████▌| 561/585 [04:02<00:06,  3.44it/s] 96%|█████████▌| 562/585 [04:02<00:06,  3.44it/s] 96%|█████████▌| 563/585 [04:02<00:06,  3.44it/s] 96%|█████████▋| 564/585 [04:03<00:06,  3.44it/s] 97%|█████████▋| 565/585 [04:03<00:05,  3.44it/s] 97%|█████████▋| 566/585 [04:03<00:05,  3.43it/s] 97%|█████████▋| 567/585 [04:03<00:05,  3.44it/s] 97%|█████████▋| 568/585 [04:04<00:04,  3.43it/s] 97%|█████████▋| 569/585 [04:04<00:04,  3.42it/s] 97%|█████████▋| 570/585 [04:04<00:04,  3.42it/s] 98%|█████████▊| 571/585 [04:05<00:04,  3.42it/s] 98%|█████████▊| 572/585 [04:05<00:03,  3.43it/s] 98%|█████████▊| 573/585 [04:05<00:03,  3.43it/s] 98%|█████████▊| 574/585 [04:05<00:03,  3.43it/s] 98%|█████████▊| 575/585 [04:06<00:02,  3.43it/s] 98%|█████████▊| 576/585 [04:06<00:02,  3.43it/s] 99%|█████████▊| 577/585 [04:06<00:02,  3.44it/s] 99%|█████████▉| 578/585 [04:07<00:02,  3.44it/s] 99%|█████████▉| 579/585 [04:07<00:01,  3.44it/s] 99%|█████████▉| 580/585 [04:07<00:01,  3.43it/s] 99%|█████████▉| 581/585 [04:07<00:01,  3.43it/s] 99%|█████████▉| 582/585 [04:08<00:00,  3.43it/s]100%|█████████▉| 583/585 [04:08<00:00,  3.43it/s]100%|█████████▉| 584/585 [04:08<00:00,  3.43it/s]100%|██████████| 585/585 [04:09<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 01:11:51,735 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:11:51,735 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 01:11:51,735 >>   Batch size = 8
{'eval_loss': 1.0982648134231567, 'eval_runtime': 12.3247, 'eval_samples_per_second': 352.3, 'eval_steps_per_second': 44.058, 'epoch': 4.0}
{'loss': 0.4023, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/543 [00:00<?, ?it/s][A
  1%|          | 6/543 [00:00<00:09, 55.65it/s][A
  2%|▏         | 12/543 [00:00<00:11, 47.67it/s][A
  3%|▎         | 17/543 [00:00<00:11, 46.11it/s][A
  4%|▍         | 22/543 [00:00<00:11, 45.21it/s][A
  5%|▍         | 27/543 [00:00<00:11, 44.94it/s][A
  6%|▌         | 32/543 [00:00<00:11, 44.56it/s][A
  7%|▋         | 37/543 [00:00<00:11, 44.50it/s][A
  8%|▊         | 42/543 [00:00<00:11, 44.25it/s][A
  9%|▊         | 47/543 [00:01<00:11, 44.37it/s][A
 10%|▉         | 52/543 [00:01<00:11, 44.45it/s][A
 10%|█         | 57/543 [00:01<00:10, 44.37it/s][A
 11%|█▏        | 62/543 [00:01<00:10, 44.22it/s][A
 12%|█▏        | 67/543 [00:01<00:10, 44.16it/s][A
 13%|█▎        | 72/543 [00:01<00:10, 44.11it/s][A
 14%|█▍        | 77/543 [00:01<00:10, 44.08it/s][A
 15%|█▌        | 82/543 [00:01<00:10, 43.97it/s][A
 16%|█▌        | 87/543 [00:01<00:10, 44.15it/s][A
 17%|█▋        | 92/543 [00:02<00:10, 44.23it/s][A
 18%|█▊        | 97/543 [00:02<00:10, 44.30it/s][A
 19%|█▉        | 102/543 [00:02<00:09, 44.29it/s][A
 20%|█▉        | 107/543 [00:02<00:09, 44.11it/s][A
 21%|██        | 112/543 [00:02<00:09, 44.20it/s][A
 22%|██▏       | 117/543 [00:02<00:09, 44.11it/s][A
 22%|██▏       | 122/543 [00:02<00:09, 44.01it/s][A
 23%|██▎       | 127/543 [00:02<00:09, 44.07it/s][A
 24%|██▍       | 132/543 [00:02<00:09, 44.16it/s][A
 25%|██▌       | 137/543 [00:03<00:09, 44.22it/s][A
 26%|██▌       | 142/543 [00:03<00:09, 44.28it/s][A
 27%|██▋       | 147/543 [00:03<00:08, 44.10it/s][A
 28%|██▊       | 152/543 [00:03<00:08, 44.09it/s][A
 29%|██▉       | 157/543 [00:03<00:08, 44.19it/s][A
 30%|██▉       | 162/543 [00:03<00:08, 44.14it/s][A
 31%|███       | 167/543 [00:03<00:08, 44.02it/s][A
 32%|███▏      | 172/543 [00:03<00:08, 44.00it/s][A
 33%|███▎      | 177/543 [00:03<00:08, 44.12it/s][A
 34%|███▎      | 182/543 [00:04<00:08, 44.15it/s][A
 34%|███▍      | 187/543 [00:04<00:08, 44.19it/s][A
 35%|███▌      | 192/543 [00:04<00:07, 44.20it/s][A
 36%|███▋      | 197/543 [00:04<00:07, 44.13it/s][A
 37%|███▋      | 202/543 [00:04<00:07, 44.15it/s][A
 38%|███▊      | 207/543 [00:04<00:07, 44.11it/s][A
 39%|███▉      | 212/543 [00:04<00:07, 43.96it/s][A
 40%|███▉      | 217/543 [00:04<00:07, 43.97it/s][A
 41%|████      | 222/543 [00:05<00:07, 44.13it/s][A
 42%|████▏     | 227/543 [00:05<00:07, 44.17it/s][A
 43%|████▎     | 232/543 [00:05<00:07, 44.20it/s][A
 44%|████▎     | 237/543 [00:05<00:06, 44.23it/s][A
 45%|████▍     | 242/543 [00:05<00:06, 44.17it/s][A
 45%|████▌     | 247/543 [00:05<00:06, 44.23it/s][A
 46%|████▋     | 252/543 [00:05<00:06, 44.15it/s][A
 47%|████▋     | 257/543 [00:05<00:06, 44.04it/s][A
 48%|████▊     | 262/543 [00:05<00:06, 44.11it/s][A
 49%|████▉     | 267/543 [00:06<00:06, 44.11it/s][A
 50%|█████     | 272/543 [00:06<00:06, 44.13it/s][A
 51%|█████     | 277/543 [00:06<00:06, 44.20it/s][A
 52%|█████▏    | 282/543 [00:06<00:05, 44.19it/s][A
 53%|█████▎    | 287/543 [00:06<00:05, 44.19it/s][A
 54%|█████▍    | 292/543 [00:06<00:05, 44.11it/s][A
 55%|█████▍    | 297/543 [00:06<00:05, 44.17it/s][A
 56%|█████▌    | 302/543 [00:06<00:05, 44.01it/s][A
 57%|█████▋    | 307/543 [00:06<00:05, 44.14it/s][A
 57%|█████▋    | 312/543 [00:07<00:05, 44.18it/s][A
 58%|█████▊    | 317/543 [00:07<00:05, 44.10it/s][A
 59%|█████▉    | 322/543 [00:07<00:05, 44.16it/s][A
 60%|██████    | 327/543 [00:07<00:04, 44.18it/s][A
 61%|██████    | 332/543 [00:07<00:04, 44.12it/s][A
 62%|██████▏   | 337/543 [00:07<00:04, 44.18it/s][A
 63%|██████▎   | 342/543 [00:07<00:04, 44.05it/s][A
 64%|██████▍   | 347/543 [00:07<00:04, 44.09it/s][A
 65%|██████▍   | 352/543 [00:07<00:04, 44.23it/s][A
 66%|██████▌   | 357/543 [00:08<00:04, 44.26it/s][A
 67%|██████▋   | 362/543 [00:08<00:04, 44.20it/s][A
 68%|██████▊   | 367/543 [00:08<00:03, 44.15it/s][A
 69%|██████▊   | 372/543 [00:08<00:03, 44.19it/s][A
 69%|██████▉   | 377/543 [00:08<00:03, 44.17it/s][A
 70%|███████   | 382/543 [00:08<00:03, 44.20it/s][A
 71%|███████▏  | 387/543 [00:08<00:03, 44.10it/s][A
 72%|███████▏  | 392/543 [00:08<00:03, 44.06it/s][A
 73%|███████▎  | 397/543 [00:08<00:03, 44.20it/s][A
 74%|███████▍  | 402/543 [00:09<00:03, 44.19it/s][A
 75%|███████▍  | 407/543 [00:09<00:03, 44.11it/s][A
 76%|███████▌  | 412/543 [00:09<00:02, 44.19it/s][A
 77%|███████▋  | 417/543 [00:09<00:02, 44.16it/s][A
 78%|███████▊  | 422/543 [00:09<00:02, 44.19it/s][A
 79%|███████▊  | 427/543 [00:09<00:02, 44.06it/s][A
 80%|███████▉  | 432/543 [00:09<00:02, 44.09it/s][A
 80%|████████  | 437/543 [00:09<00:02, 44.06it/s][A
 81%|████████▏ | 442/543 [00:09<00:02, 44.22it/s][A
 82%|████████▏ | 447/543 [00:10<00:02, 44.23it/s][A
 83%|████████▎ | 452/543 [00:10<00:02, 44.14it/s][A
 84%|████████▍ | 457/543 [00:10<00:01, 44.26it/s][A
 85%|████████▌ | 462/543 [00:10<00:01, 44.18it/s][A
 86%|████████▌ | 467/543 [00:10<00:01, 44.13it/s][A
 87%|████████▋ | 472/543 [00:10<00:01, 43.99it/s][A
 88%|████████▊ | 477/543 [00:10<00:01, 44.07it/s][A
 89%|████████▉ | 482/543 [00:10<00:01, 44.04it/s][A
 90%|████████▉ | 487/543 [00:11<00:01, 44.23it/s][A
 91%|█████████ | 492/543 [00:11<00:01, 44.13it/s][A
 92%|█████████▏| 497/543 [00:11<00:01, 44.21it/s][A
 92%|█████████▏| 502/543 [00:11<00:00, 44.21it/s][A
 93%|█████████▎| 507/543 [00:11<00:00, 44.15it/s][A
 94%|█████████▍| 512/543 [00:11<00:00, 44.09it/s][A
 95%|█████████▌| 517/543 [00:11<00:00, 44.08it/s][A
 96%|█████████▌| 522/543 [00:11<00:00, 44.03it/s][A
 97%|█████████▋| 527/543 [00:11<00:00, 44.10it/s][A
 98%|█████████▊| 532/543 [00:12<00:00, 44.12it/s][A
 99%|█████████▉| 537/543 [00:12<00:00, 44.28it/s][A
100%|█████████▉| 542/543 [00:12<00:00, 44.29it/s][A
                                                 [A                                                 
100%|██████████| 543/543 [00:12<00:00, 44.29it/s][A100%|██████████| 585/585 [04:21<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:12:04,048 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 01:12:04,068 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:12:06,090 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:12:06,107 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:12:06,117 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:12:10,016 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:12:10,020 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117 (score: 1.0607802867889404).
                                                 100%|██████████| 585/585 [04:29<00:00,  3.43it/s]100%|██████████| 585/585 [04:29<00:00,  2.17it/s]
[INFO|trainer.py:1894] 2023-08-29 01:12:11,826 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 01:12:11,840 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:12:14,163 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:12:14,179 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:12:14,189 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:12:14,415 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:14,415 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:14,415 >>   train_loss               =     0.3995
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:14,415 >>   train_runtime            = 0:04:29.22
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:14,415 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:14,415 >>   train_samples_per_second =    139.289
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:14,415 >>   train_steps_per_second   =      2.173
{'eval_loss': 1.1033875942230225, 'eval_runtime': 12.2968, 'eval_samples_per_second': 353.1, 'eval_steps_per_second': 44.158, 'epoch': 5.0}
{'train_runtime': 269.2249, 'train_samples_per_second': 139.289, 'train_steps_per_second': 2.173, 'train_loss': 0.39946958753797746, 'epoch': 5.0}
08/29/2023 01:12:14 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:12:14,457 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:12:14,457 >>   Num examples = 4342
[INFO|trainer.py:2145] 2023-08-29 01:12:14,457 >>   Batch size = 8
  0%|          | 0/543 [00:00<?, ?it/s]  1%|          | 6/543 [00:00<00:09, 55.29it/s]  2%|▏         | 12/543 [00:00<00:10, 48.82it/s]  3%|▎         | 17/543 [00:00<00:11, 47.26it/s]  4%|▍         | 22/543 [00:00<00:11, 46.43it/s]  5%|▍         | 27/543 [00:00<00:11, 45.77it/s]  6%|▌         | 32/543 [00:00<00:11, 45.55it/s]  7%|▋         | 37/543 [00:00<00:11, 45.37it/s]  8%|▊         | 42/543 [00:00<00:11, 44.78it/s]  9%|▊         | 47/543 [00:01<00:11, 44.12it/s] 10%|▉         | 52/543 [00:01<00:11, 43.88it/s] 10%|█         | 57/543 [00:01<00:11, 44.09it/s] 11%|█▏        | 62/543 [00:01<00:10, 44.25it/s] 12%|█▏        | 67/543 [00:01<00:10, 44.45it/s] 13%|█▎        | 72/543 [00:01<00:10, 44.57it/s] 14%|█▍        | 77/543 [00:01<00:10, 44.73it/s] 15%|█▌        | 82/543 [00:01<00:10, 44.62it/s] 16%|█▌        | 87/543 [00:01<00:10, 44.31it/s] 17%|█▋        | 92/543 [00:02<00:10, 43.93it/s] 18%|█▊        | 97/543 [00:02<00:10, 43.85it/s] 19%|█▉        | 102/543 [00:02<00:10, 43.99it/s] 20%|█▉        | 107/543 [00:02<00:09, 44.16it/s] 21%|██        | 112/543 [00:02<00:09, 44.33it/s] 22%|██▏       | 117/543 [00:02<00:09, 44.52it/s] 22%|██▏       | 122/543 [00:02<00:09, 44.66it/s] 23%|██▎       | 127/543 [00:02<00:09, 44.48it/s] 24%|██▍       | 132/543 [00:02<00:09, 44.20it/s] 25%|██▌       | 137/543 [00:03<00:09, 43.82it/s] 26%|██▌       | 142/543 [00:03<00:09, 43.86it/s] 27%|██▋       | 147/543 [00:03<00:08, 44.04it/s] 28%|██▊       | 152/543 [00:03<00:08, 44.26it/s] 29%|██▉       | 157/543 [00:03<00:08, 44.32it/s] 30%|██▉       | 162/543 [00:03<00:08, 44.35it/s] 31%|███       | 167/543 [00:03<00:08, 44.63it/s] 32%|███▏      | 172/543 [00:03<00:08, 44.54it/s] 33%|███▎      | 177/543 [00:03<00:08, 44.21it/s] 34%|███▎      | 182/543 [00:04<00:08, 44.05it/s] 34%|███▍      | 187/543 [00:04<00:08, 44.03it/s] 35%|███▌      | 192/543 [00:04<00:07, 44.18it/s] 36%|███▋      | 197/543 [00:04<00:07, 44.30it/s] 37%|███▋      | 202/543 [00:04<00:07, 44.55it/s] 38%|███▊      | 207/543 [00:04<00:07, 44.58it/s] 39%|███▉      | 212/543 [00:04<00:07, 44.68it/s] 40%|███▉      | 217/543 [00:04<00:07, 44.52it/s] 41%|████      | 222/543 [00:04<00:07, 44.27it/s] 42%|████▏     | 227/543 [00:05<00:07, 44.13it/s] 43%|████▎     | 232/543 [00:05<00:07, 44.02it/s] 44%|████▎     | 237/543 [00:05<00:06, 44.07it/s] 45%|████▍     | 242/543 [00:05<00:06, 44.30it/s] 45%|████▌     | 247/543 [00:05<00:06, 44.46it/s] 46%|████▋     | 252/543 [00:05<00:06, 44.65it/s] 47%|████▋     | 257/543 [00:05<00:06, 44.58it/s] 48%|████▊     | 262/543 [00:05<00:06, 44.30it/s] 49%|████▉     | 267/543 [00:05<00:06, 44.22it/s] 50%|█████     | 272/543 [00:06<00:06, 44.09it/s] 51%|█████     | 277/543 [00:06<00:06, 44.11it/s] 52%|█████▏    | 282/543 [00:06<00:05, 44.12it/s] 53%|█████▎    | 287/543 [00:06<00:05, 44.32it/s] 54%|█████▍    | 292/543 [00:06<00:05, 44.47it/s] 55%|█████▍    | 297/543 [00:06<00:05, 44.59it/s] 56%|█████▌    | 302/543 [00:06<00:05, 44.52it/s] 57%|█████▋    | 307/543 [00:06<00:05, 44.31it/s] 57%|█████▋    | 312/543 [00:07<00:05, 44.24it/s] 58%|█████▊    | 317/543 [00:07<00:05, 44.13it/s] 59%|█████▉    | 322/543 [00:07<00:05, 44.04it/s] 60%|██████    | 327/543 [00:07<00:04, 44.21it/s] 61%|██████    | 332/543 [00:07<00:04, 44.23it/s] 62%|██████▏   | 337/543 [00:07<00:04, 44.48it/s] 63%|██████▎   | 342/543 [00:07<00:04, 44.60it/s] 64%|██████▍   | 347/543 [00:07<00:04, 44.52it/s] 65%|██████▍   | 352/543 [00:07<00:04, 44.36it/s] 66%|██████▌   | 357/543 [00:08<00:04, 44.31it/s] 67%|██████▋   | 362/543 [00:08<00:04, 44.21it/s] 68%|██████▊   | 367/543 [00:08<00:03, 44.24it/s] 69%|██████▊   | 372/543 [00:08<00:03, 44.22it/s] 69%|██████▉   | 377/543 [00:08<00:03, 44.29it/s] 70%|███████   | 382/543 [00:08<00:03, 44.42it/s] 71%|███████▏  | 387/543 [00:08<00:03, 44.52it/s] 72%|███████▏  | 392/543 [00:08<00:03, 44.55it/s] 73%|███████▎  | 397/543 [00:08<00:03, 44.24it/s] 74%|███████▍  | 402/543 [00:09<00:03, 44.26it/s] 75%|███████▍  | 407/543 [00:09<00:03, 44.24it/s] 76%|███████▌  | 412/543 [00:09<00:02, 44.24it/s] 77%|███████▋  | 417/543 [00:09<00:02, 44.30it/s] 78%|███████▊  | 422/543 [00:09<00:02, 44.32it/s] 79%|███████▊  | 427/543 [00:09<00:02, 44.39it/s] 80%|███████▉  | 432/543 [00:09<00:02, 44.51it/s] 80%|████████  | 437/543 [00:09<00:02, 44.45it/s] 81%|████████▏ | 442/543 [00:09<00:02, 44.21it/s] 82%|████████▏ | 447/543 [00:10<00:02, 44.12it/s] 83%|████████▎ | 452/543 [00:10<00:02, 44.27it/s] 84%|████████▍ | 457/543 [00:10<00:01, 44.32it/s] 85%|████████▌ | 462/543 [00:10<00:01, 44.33it/s] 86%|████████▌ | 467/543 [00:10<00:01, 44.38it/s] 87%|████████▋ | 472/543 [00:10<00:01, 44.43it/s] 88%|████████▊ | 477/543 [00:10<00:01, 44.47it/s] 89%|████████▉ | 482/543 [00:10<00:01, 44.35it/s] 90%|████████▉ | 487/543 [00:10<00:01, 44.21it/s] 91%|█████████ | 492/543 [00:11<00:01, 44.16it/s] 92%|█████████▏| 497/543 [00:11<00:01, 44.25it/s] 92%|█████████▏| 502/543 [00:11<00:00, 44.33it/s] 93%|█████████▎| 507/543 [00:11<00:00, 44.30it/s] 94%|█████████▍| 512/543 [00:11<00:00, 44.36it/s] 95%|█████████▌| 517/543 [00:11<00:00, 44.45it/s] 96%|█████████▌| 522/543 [00:11<00:00, 44.45it/s] 97%|█████████▋| 527/543 [00:11<00:00, 44.33it/s] 98%|█████████▊| 532/543 [00:11<00:00, 44.03it/s] 99%|█████████▉| 537/543 [00:12<00:00, 44.23it/s]100%|█████████▉| 542/543 [00:12<00:00, 44.31it/s]100%|██████████| 543/543 [00:12<00:00, 44.39it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:12:26,706 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:26,706 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:26,706 >>   eval_loss               =     1.0608
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:26,706 >>   eval_runtime            = 0:00:12.24
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:26,706 >>   eval_samples            =       4342
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:26,706 >>   eval_samples_per_second =    354.479
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:26,706 >>   eval_steps_per_second   =      44.33
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:12:26,706 >>   perplexity              =     2.8886
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:33,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:33,508 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:33,508 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:33,508 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:33,508 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:12:34,132 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:12:34,133 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:12:34,704 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:12:35,776 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:12:35,776 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:38,779 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:38,784 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:38,784 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:38,784 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:12:38,784 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:12:39,652 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:12:39,653 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:12:40,243 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:12:40,635 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:12:40,636 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'located on terrain feature', 'made from material', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12955
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13055, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.74it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.73it/s]Extractor Predicting: 4it [00:02,  1.79it/s]Extractor Predicting: 5it [00:02,  1.73it/s]Extractor Predicting: 6it [00:03,  1.68it/s]Extractor Predicting: 7it [00:04,  1.70it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.78it/s]Extractor Predicting: 10it [00:05,  1.79it/s]Extractor Predicting: 11it [00:06,  1.80it/s]Extractor Predicting: 12it [00:06,  1.81it/s]Extractor Predicting: 13it [00:07,  1.72it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:08,  1.61it/s]Extractor Predicting: 16it [00:09,  1.62it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:10,  1.59it/s]Extractor Predicting: 19it [00:11,  1.54it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:12,  1.55it/s]Extractor Predicting: 22it [00:13,  1.58it/s]Extractor Predicting: 23it [00:13,  1.59it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:15,  1.57it/s]Extractor Predicting: 27it [00:16,  1.56it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:17,  1.56it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:18,  1.58it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:20,  1.58it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.54it/s]Extractor Predicting: 37it [00:22,  1.45it/s]Extractor Predicting: 38it [00:23,  1.47it/s]Extractor Predicting: 39it [00:24,  1.51it/s]Extractor Predicting: 40it [00:24,  1.53it/s]Extractor Predicting: 41it [00:25,  1.52it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:26,  1.56it/s]Extractor Predicting: 44it [00:27,  1.58it/s]Extractor Predicting: 45it [00:28,  1.58it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:29,  1.59it/s]Extractor Predicting: 48it [00:29,  1.58it/s]Extractor Predicting: 49it [00:30,  1.57it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:31,  1.59it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:33,  1.55it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.62it/s]Extractor Predicting: 56it [00:35,  1.59it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:36,  1.61it/s]Extractor Predicting: 59it [00:36,  1.57it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.60it/s]Extractor Predicting: 62it [00:38,  1.60it/s]Extractor Predicting: 63it [00:39,  1.60it/s]Extractor Predicting: 64it [00:40,  1.60it/s]Extractor Predicting: 65it [00:40,  1.64it/s]Extractor Predicting: 66it [00:41,  1.64it/s]Extractor Predicting: 67it [00:41,  1.63it/s]Extractor Predicting: 68it [00:42,  1.60it/s]Extractor Predicting: 69it [00:43,  1.61it/s]Extractor Predicting: 70it [00:43,  1.60it/s]Extractor Predicting: 71it [00:44,  1.60it/s]Extractor Predicting: 72it [00:44,  1.60it/s]Extractor Predicting: 73it [00:45,  1.59it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:46,  1.54it/s]Extractor Predicting: 76it [00:47,  1.55it/s]Extractor Predicting: 77it [00:48,  1.58it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:50,  1.58it/s]Extractor Predicting: 81it [00:50,  1.58it/s]Extractor Predicting: 82it [00:51,  1.59it/s]Extractor Predicting: 83it [00:51,  1.60it/s]Extractor Predicting: 84it [00:52,  1.61it/s]Extractor Predicting: 85it [00:53,  1.62it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:54,  1.59it/s]Extractor Predicting: 88it [00:55,  1.61it/s]Extractor Predicting: 89it [00:55,  1.63it/s]Extractor Predicting: 90it [00:56,  1.59it/s]Extractor Predicting: 91it [00:56,  1.56it/s]Extractor Predicting: 92it [00:57,  1.55it/s]Extractor Predicting: 93it [00:58,  1.60it/s]Extractor Predicting: 94it [00:58,  1.61it/s]Extractor Predicting: 95it [00:59,  1.60it/s]Extractor Predicting: 96it [01:00,  1.62it/s]Extractor Predicting: 97it [01:00,  1.50it/s]Extractor Predicting: 98it [01:01,  1.52it/s]Extractor Predicting: 99it [01:02,  1.52it/s]Extractor Predicting: 100it [01:02,  1.54it/s]Extractor Predicting: 101it [01:03,  1.56it/s]Extractor Predicting: 102it [01:04,  1.56it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:05,  1.58it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:06,  1.58it/s]Extractor Predicting: 107it [01:07,  1.58it/s]Extractor Predicting: 108it [01:07,  1.58it/s]Extractor Predicting: 109it [01:08,  1.58it/s]Extractor Predicting: 110it [01:09,  1.57it/s]Extractor Predicting: 111it [01:09,  1.58it/s]Extractor Predicting: 112it [01:10,  1.61it/s]Extractor Predicting: 113it [01:10,  1.62it/s]Extractor Predicting: 114it [01:11,  1.61it/s]Extractor Predicting: 115it [01:12,  1.60it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:14,  1.57it/s]Extractor Predicting: 119it [01:14,  1.57it/s]Extractor Predicting: 120it [01:15,  1.56it/s]Extractor Predicting: 121it [01:16,  1.55it/s]Extractor Predicting: 122it [01:16,  1.54it/s]Extractor Predicting: 123it [01:17,  1.58it/s]Extractor Predicting: 124it [01:17,  1.60it/s]Extractor Predicting: 125it [01:18,  1.61it/s]Extractor Predicting: 126it [01:19,  1.56it/s]Extractor Predicting: 127it [01:19,  1.56it/s]Extractor Predicting: 128it [01:20,  1.58it/s]Extractor Predicting: 129it [01:21,  1.59it/s]Extractor Predicting: 130it [01:21,  1.64it/s]Extractor Predicting: 131it [01:22,  1.60it/s]Extractor Predicting: 132it [01:22,  1.59it/s]Extractor Predicting: 133it [01:23,  1.60it/s]Extractor Predicting: 134it [01:24,  1.60it/s]Extractor Predicting: 135it [01:24,  1.62it/s]Extractor Predicting: 136it [01:25,  1.61it/s]Extractor Predicting: 137it [01:26,  1.63it/s]Extractor Predicting: 138it [01:26,  1.59it/s]Extractor Predicting: 139it [01:27,  1.58it/s]Extractor Predicting: 140it [01:27,  1.61it/s]Extractor Predicting: 141it [01:28,  1.62it/s]Extractor Predicting: 142it [01:29,  1.65it/s]Extractor Predicting: 143it [01:29,  1.64it/s]Extractor Predicting: 144it [01:30,  1.64it/s]Extractor Predicting: 145it [01:30,  1.62it/s]Extractor Predicting: 146it [01:31,  1.63it/s]Extractor Predicting: 147it [01:32,  1.63it/s]Extractor Predicting: 148it [01:32,  1.64it/s]Extractor Predicting: 149it [01:33,  1.65it/s]Extractor Predicting: 150it [01:34,  1.60it/s]Extractor Predicting: 151it [01:34,  1.60it/s]Extractor Predicting: 152it [01:35,  1.60it/s]Extractor Predicting: 153it [01:35,  1.58it/s]Extractor Predicting: 154it [01:36,  1.59it/s]Extractor Predicting: 155it [01:37,  1.57it/s]Extractor Predicting: 156it [01:37,  1.56it/s]Extractor Predicting: 157it [01:38,  1.56it/s]Extractor Predicting: 158it [01:39,  1.57it/s]Extractor Predicting: 159it [01:39,  1.60it/s]Extractor Predicting: 160it [01:40,  1.63it/s]Extractor Predicting: 161it [01:40,  1.64it/s]Extractor Predicting: 162it [01:41,  1.64it/s]Extractor Predicting: 163it [01:42,  1.52it/s]Extractor Predicting: 163it [01:42,  1.59it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:31,317 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:31,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:31,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:31,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:31,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:14:31,634 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:14:31,634 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:14:32,292 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:14:33,318 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:14:33,318 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:34,972 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:34,980 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:34,980 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:34,980 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:14:34,980 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:14:35,301 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:14:35,302 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:14:35,563 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:14:35,701 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:14:35,701 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.11916264090177134,
  "recall": 0.01704283740211884,
  "score": 0.02982067298005239,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 21583
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21683, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.72it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:02,  1.66it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.69it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:08,  1.66it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.66it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.64it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.67it/s]Extractor Predicting: 28it [00:16,  1.66it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.67it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.68it/s]Extractor Predicting: 33it [00:19,  1.67it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.68it/s]Extractor Predicting: 36it [00:21,  1.66it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:22,  1.63it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.66it/s]Extractor Predicting: 41it [00:24,  1.58it/s]Extractor Predicting: 42it [00:25,  1.59it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:26,  1.58it/s]Extractor Predicting: 45it [00:27,  1.56it/s]Extractor Predicting: 46it [00:27,  1.54it/s]Extractor Predicting: 47it [00:28,  1.52it/s]Extractor Predicting: 48it [00:29,  1.55it/s]Extractor Predicting: 49it [00:29,  1.54it/s]Extractor Predicting: 50it [00:30,  1.56it/s]Extractor Predicting: 51it [00:31,  1.56it/s]Extractor Predicting: 52it [00:31,  1.55it/s]Extractor Predicting: 53it [00:32,  1.54it/s]Extractor Predicting: 54it [00:33,  1.50it/s]Extractor Predicting: 55it [00:33,  1.54it/s]Extractor Predicting: 56it [00:34,  1.58it/s]Extractor Predicting: 57it [00:35,  1.56it/s]Extractor Predicting: 58it [00:35,  1.55it/s]Extractor Predicting: 59it [00:36,  1.52it/s]Extractor Predicting: 60it [00:37,  1.52it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.56it/s]Extractor Predicting: 63it [00:39,  1.51it/s]Extractor Predicting: 64it [00:39,  1.52it/s]Extractor Predicting: 65it [00:40,  1.53it/s]Extractor Predicting: 66it [00:41,  1.51it/s]Extractor Predicting: 67it [00:41,  1.53it/s]Extractor Predicting: 68it [00:42,  1.51it/s]Extractor Predicting: 69it [00:42,  1.52it/s]Extractor Predicting: 70it [00:43,  1.53it/s]Extractor Predicting: 71it [00:44,  1.55it/s]Extractor Predicting: 72it [00:44,  1.54it/s]Extractor Predicting: 73it [00:45,  1.53it/s]Extractor Predicting: 74it [00:46,  1.55it/s]Extractor Predicting: 75it [00:46,  1.55it/s]Extractor Predicting: 76it [00:47,  1.57it/s]Extractor Predicting: 77it [00:48,  1.60it/s]Extractor Predicting: 78it [00:48,  1.59it/s]Extractor Predicting: 79it [00:49,  1.55it/s]Extractor Predicting: 80it [00:49,  1.59it/s]Extractor Predicting: 81it [00:50,  1.46it/s]Extractor Predicting: 82it [00:51,  1.50it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:52,  1.51it/s]Extractor Predicting: 85it [00:53,  1.53it/s]Extractor Predicting: 86it [00:53,  1.55it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:55,  1.60it/s]Extractor Predicting: 90it [00:56,  1.61it/s]Extractor Predicting: 91it [00:57,  1.63it/s]Extractor Predicting: 92it [00:57,  1.63it/s]Extractor Predicting: 93it [00:58,  1.62it/s]Extractor Predicting: 94it [00:58,  1.61it/s]Extractor Predicting: 95it [00:59,  1.62it/s]Extractor Predicting: 96it [01:00,  1.65it/s]Extractor Predicting: 97it [01:00,  1.47it/s]Extractor Predicting: 98it [01:01,  1.46it/s]Extractor Predicting: 99it [01:02,  1.49it/s]Extractor Predicting: 100it [01:02,  1.51it/s]Extractor Predicting: 101it [01:03,  1.51it/s]Extractor Predicting: 102it [01:04,  1.54it/s]Extractor Predicting: 103it [01:04,  1.52it/s]Extractor Predicting: 104it [01:05,  1.56it/s]Extractor Predicting: 105it [01:06,  1.55it/s]Extractor Predicting: 106it [01:06,  1.51it/s]Extractor Predicting: 107it [01:07,  1.50it/s]Extractor Predicting: 108it [01:08,  1.53it/s]Extractor Predicting: 109it [01:08,  1.48it/s]Extractor Predicting: 110it [01:09,  1.51it/s]Extractor Predicting: 111it [01:10,  1.50it/s]Extractor Predicting: 112it [01:10,  1.52it/s]Extractor Predicting: 113it [01:11,  1.52it/s]Extractor Predicting: 114it [01:12,  1.53it/s]Extractor Predicting: 115it [01:12,  1.52it/s]Extractor Predicting: 116it [01:13,  1.50it/s]Extractor Predicting: 117it [01:14,  1.51it/s]Extractor Predicting: 118it [01:14,  1.56it/s]Extractor Predicting: 119it [01:15,  1.56it/s]Extractor Predicting: 120it [01:15,  1.60it/s]Extractor Predicting: 121it [01:16,  1.58it/s]Extractor Predicting: 122it [01:17,  1.61it/s]Extractor Predicting: 123it [01:17,  1.63it/s]Extractor Predicting: 124it [01:18,  1.65it/s]Extractor Predicting: 125it [01:18,  1.63it/s]Extractor Predicting: 126it [01:19,  1.60it/s]Extractor Predicting: 127it [01:20,  1.59it/s]Extractor Predicting: 128it [01:20,  1.58it/s]Extractor Predicting: 129it [01:21,  1.58it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:22,  1.64it/s]Extractor Predicting: 132it [01:23,  1.62it/s]Extractor Predicting: 133it [01:24,  1.62it/s]Extractor Predicting: 134it [01:24,  1.64it/s]Extractor Predicting: 135it [01:25,  1.64it/s]Extractor Predicting: 136it [01:25,  1.64it/s]Extractor Predicting: 137it [01:26,  1.68it/s]Extractor Predicting: 138it [01:26,  1.66it/s]Extractor Predicting: 139it [01:27,  1.66it/s]Extractor Predicting: 140it [01:28,  1.64it/s]Extractor Predicting: 141it [01:28,  1.65it/s]Extractor Predicting: 142it [01:29,  1.62it/s]Extractor Predicting: 143it [01:30,  1.60it/s]Extractor Predicting: 144it [01:30,  1.61it/s]Extractor Predicting: 145it [01:31,  1.56it/s]Extractor Predicting: 146it [01:32,  1.51it/s]Extractor Predicting: 147it [01:32,  1.54it/s]Extractor Predicting: 148it [01:33,  1.54it/s]Extractor Predicting: 149it [01:33,  1.57it/s]Extractor Predicting: 150it [01:34,  1.57it/s]Extractor Predicting: 151it [01:35,  1.57it/s]Extractor Predicting: 152it [01:35,  1.57it/s]Extractor Predicting: 153it [01:36,  1.60it/s]Extractor Predicting: 154it [01:37,  1.62it/s]Extractor Predicting: 155it [01:37,  1.64it/s]Extractor Predicting: 156it [01:38,  1.60it/s]Extractor Predicting: 157it [01:38,  1.60it/s]Extractor Predicting: 158it [01:39,  1.61it/s]Extractor Predicting: 159it [01:40,  1.61it/s]Extractor Predicting: 160it [01:40,  1.63it/s]Extractor Predicting: 161it [01:41,  1.55it/s]Extractor Predicting: 162it [01:42,  1.60it/s]Extractor Predicting: 163it [01:42,  1.63it/s]Extractor Predicting: 164it [01:43,  1.61it/s]Extractor Predicting: 165it [01:43,  1.66it/s]Extractor Predicting: 166it [01:44,  1.66it/s]Extractor Predicting: 167it [01:45,  1.65it/s]Extractor Predicting: 168it [01:45,  1.71it/s]Extractor Predicting: 169it [01:46,  1.72it/s]Extractor Predicting: 170it [01:46,  1.67it/s]Extractor Predicting: 171it [01:47,  1.66it/s]Extractor Predicting: 172it [01:48,  1.66it/s]Extractor Predicting: 173it [01:48,  1.63it/s]Extractor Predicting: 174it [01:49,  1.66it/s]Extractor Predicting: 175it [01:49,  1.65it/s]Extractor Predicting: 176it [01:50,  1.63it/s]Extractor Predicting: 177it [01:51,  1.61it/s]Extractor Predicting: 178it [01:51,  1.61it/s]Extractor Predicting: 179it [01:52,  1.69it/s]Extractor Predicting: 180it [01:52,  1.66it/s]Extractor Predicting: 181it [01:53,  1.66it/s]Extractor Predicting: 182it [01:54,  1.63it/s]Extractor Predicting: 183it [01:54,  1.63it/s]Extractor Predicting: 184it [01:55,  1.62it/s]Extractor Predicting: 185it [01:55,  1.66it/s]Extractor Predicting: 186it [01:56,  1.62it/s]Extractor Predicting: 187it [01:57,  1.62it/s]Extractor Predicting: 188it [01:57,  1.60it/s]Extractor Predicting: 189it [01:58,  1.59it/s]Extractor Predicting: 190it [01:59,  1.55it/s]Extractor Predicting: 191it [02:00,  1.39it/s]Extractor Predicting: 192it [02:00,  1.45it/s]Extractor Predicting: 193it [02:01,  1.52it/s]Extractor Predicting: 194it [02:01,  1.55it/s]Extractor Predicting: 195it [02:02,  1.53it/s]Extractor Predicting: 196it [02:03,  1.56it/s]Extractor Predicting: 197it [02:03,  1.57it/s]Extractor Predicting: 198it [02:04,  1.59it/s]Extractor Predicting: 199it [02:05,  1.58it/s]Extractor Predicting: 200it [02:05,  1.58it/s]Extractor Predicting: 201it [02:06,  1.59it/s]Extractor Predicting: 202it [02:06,  1.59it/s]Extractor Predicting: 203it [02:07,  1.59it/s]Extractor Predicting: 204it [02:08,  1.60it/s]Extractor Predicting: 205it [02:08,  1.59it/s]Extractor Predicting: 206it [02:09,  1.58it/s]Extractor Predicting: 207it [02:10,  1.60it/s]Extractor Predicting: 208it [02:10,  1.60it/s]Extractor Predicting: 209it [02:11,  1.61it/s]Extractor Predicting: 210it [02:11,  1.60it/s]Extractor Predicting: 211it [02:12,  1.60it/s]Extractor Predicting: 212it [02:13,  1.61it/s]Extractor Predicting: 213it [02:13,  1.60it/s]Extractor Predicting: 214it [02:14,  1.60it/s]Extractor Predicting: 215it [02:15,  1.56it/s]Extractor Predicting: 216it [02:15,  1.59it/s]Extractor Predicting: 217it [02:16,  1.62it/s]Extractor Predicting: 218it [02:16,  1.62it/s]Extractor Predicting: 219it [02:17,  1.60it/s]Extractor Predicting: 220it [02:18,  1.61it/s]Extractor Predicting: 221it [02:18,  1.59it/s]Extractor Predicting: 222it [02:19,  1.60it/s]Extractor Predicting: 223it [02:20,  1.54it/s]Extractor Predicting: 224it [02:20,  1.55it/s]Extractor Predicting: 225it [02:21,  1.53it/s]Extractor Predicting: 226it [02:22,  1.54it/s]Extractor Predicting: 227it [02:22,  1.50it/s]Extractor Predicting: 228it [02:23,  1.46it/s]Extractor Predicting: 229it [02:24,  1.48it/s]Extractor Predicting: 230it [02:24,  1.50it/s]Extractor Predicting: 231it [02:25,  1.50it/s]Extractor Predicting: 232it [02:26,  1.51it/s]Extractor Predicting: 233it [02:26,  1.52it/s]Extractor Predicting: 234it [02:27,  1.54it/s]Extractor Predicting: 235it [02:28,  1.54it/s]Extractor Predicting: 236it [02:28,  1.56it/s]Extractor Predicting: 237it [02:29,  1.55it/s]Extractor Predicting: 238it [02:29,  1.61it/s]Extractor Predicting: 239it [02:30,  1.63it/s]Extractor Predicting: 240it [02:31,  1.68it/s]Extractor Predicting: 241it [02:31,  1.67it/s]Extractor Predicting: 242it [02:32,  1.66it/s]Extractor Predicting: 243it [02:32,  1.63it/s]Extractor Predicting: 244it [02:33,  1.63it/s]Extractor Predicting: 245it [02:34,  1.66it/s]Extractor Predicting: 246it [02:34,  1.63it/s]Extractor Predicting: 247it [02:35,  1.65it/s]Extractor Predicting: 248it [02:35,  1.66it/s]Extractor Predicting: 249it [02:36,  1.71it/s]Extractor Predicting: 250it [02:37,  1.72it/s]Extractor Predicting: 251it [02:37,  1.77it/s]Extractor Predicting: 252it [02:38,  1.78it/s]Extractor Predicting: 253it [02:38,  1.75it/s]Extractor Predicting: 254it [02:39,  1.77it/s]Extractor Predicting: 255it [02:39,  1.72it/s]Extractor Predicting: 256it [02:40,  1.76it/s]Extractor Predicting: 257it [02:41,  1.72it/s]Extractor Predicting: 258it [02:41,  1.67it/s]Extractor Predicting: 259it [02:42,  1.62it/s]Extractor Predicting: 260it [02:42,  1.68it/s]Extractor Predicting: 261it [02:43,  1.65it/s]Extractor Predicting: 262it [02:44,  1.65it/s]Extractor Predicting: 263it [02:44,  1.68it/s]Extractor Predicting: 264it [02:45,  1.70it/s]Extractor Predicting: 265it [02:45,  1.69it/s]Extractor Predicting: 266it [02:46,  1.72it/s]Extractor Predicting: 267it [02:47,  1.75it/s]Extractor Predicting: 268it [02:47,  1.73it/s]Extractor Predicting: 269it [02:48,  1.70it/s]Extractor Predicting: 270it [02:48,  1.72it/s]Extractor Predicting: 271it [02:49,  1.70it/s]Extractor Predicting: 272it [02:50,  1.68it/s]Extractor Predicting: 273it [02:50,  1.72it/s]Extractor Predicting: 274it [02:51,  1.73it/s]Extractor Predicting: 275it [02:51,  1.71it/s]Extractor Predicting: 276it [02:52,  1.70it/s]Extractor Predicting: 277it [02:52,  1.69it/s]Extractor Predicting: 278it [02:53,  1.72it/s]Extractor Predicting: 279it [02:54,  1.65it/s]Extractor Predicting: 280it [02:54,  1.71it/s]Extractor Predicting: 281it [02:55,  1.73it/s]Extractor Predicting: 282it [02:55,  1.75it/s]Extractor Predicting: 283it [02:56,  1.76it/s]Extractor Predicting: 284it [02:56,  1.75it/s]Extractor Predicting: 285it [02:57,  1.76it/s]Extractor Predicting: 286it [02:58,  1.75it/s]Extractor Predicting: 287it [02:58,  1.74it/s]Extractor Predicting: 288it [02:59,  1.70it/s]Extractor Predicting: 289it [03:00,  1.49it/s]Extractor Predicting: 290it [03:00,  1.48it/s]Extractor Predicting: 291it [03:01,  1.53it/s]Extractor Predicting: 292it [03:02,  1.59it/s]Extractor Predicting: 293it [03:02,  1.62it/s]Extractor Predicting: 294it [03:03,  1.64it/s]Extractor Predicting: 295it [03:03,  1.66it/s]Extractor Predicting: 296it [03:04,  1.59it/s]Extractor Predicting: 297it [03:05,  1.57it/s]Extractor Predicting: 298it [03:05,  1.53it/s]Extractor Predicting: 299it [03:06,  1.53it/s]Extractor Predicting: 300it [03:07,  1.56it/s]Extractor Predicting: 301it [03:07,  1.57it/s]Extractor Predicting: 302it [03:08,  1.58it/s]Extractor Predicting: 303it [03:09,  1.54it/s]Extractor Predicting: 304it [03:09,  1.55it/s]Extractor Predicting: 305it [03:10,  1.58it/s]Extractor Predicting: 306it [03:10,  1.55it/s]Extractor Predicting: 307it [03:11,  1.52it/s]Extractor Predicting: 308it [03:12,  1.52it/s]Extractor Predicting: 309it [03:12,  1.55it/s]Extractor Predicting: 310it [03:13,  1.59it/s]Extractor Predicting: 311it [03:14,  1.60it/s]Extractor Predicting: 312it [03:14,  1.58it/s]Extractor Predicting: 313it [03:15,  1.55it/s]Extractor Predicting: 314it [03:16,  1.53it/s]Extractor Predicting: 315it [03:16,  1.53it/s]Extractor Predicting: 316it [03:17,  1.53it/s]Extractor Predicting: 317it [03:18,  1.53it/s]Extractor Predicting: 318it [03:18,  1.55it/s]Extractor Predicting: 319it [03:19,  1.56it/s]Extractor Predicting: 320it [03:19,  1.57it/s]Extractor Predicting: 321it [03:20,  1.62it/s]Extractor Predicting: 322it [03:21,  1.62it/s]Extractor Predicting: 323it [03:21,  1.60it/s]Extractor Predicting: 324it [03:22,  1.58it/s]Extractor Predicting: 325it [03:23,  1.57it/s]Extractor Predicting: 326it [03:23,  1.56it/s]Extractor Predicting: 327it [03:24,  1.54it/s]Extractor Predicting: 328it [03:25,  1.54it/s]Extractor Predicting: 329it [03:25,  1.59it/s]Extractor Predicting: 330it [03:26,  1.62it/s]Extractor Predicting: 331it [03:26,  1.78it/s]Extractor Predicting: 331it [03:26,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:11,815 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:11,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:11,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:11,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:11,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:18:12,462 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:18:12,463 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:18:13,036 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:18:14,078 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:18:14,078 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:16,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:16,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:16,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:16,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:18:16,940 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:18:17,590 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:18:17,591 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:18:18,156 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:18:18,329 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:18:18,329 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5179153094462541,
  "recall": 0.10023956625898374,
  "score": 0.1679695753222058,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 9022
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9122, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.54it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.50it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.49it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:13,  1.46it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.44it/s]Extractor Predicting: 24it [00:16,  1.45it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.50it/s]Extractor Predicting: 27it [00:17,  1.53it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:19,  1.51it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:20,  1.44it/s]Extractor Predicting: 32it [00:21,  1.48it/s]Extractor Predicting: 33it [00:22,  1.51it/s]Extractor Predicting: 34it [00:22,  1.51it/s]Extractor Predicting: 35it [00:23,  1.53it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.55it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.51it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.54it/s]Extractor Predicting: 44it [00:29,  1.54it/s]Extractor Predicting: 45it [00:29,  1.54it/s]Extractor Predicting: 46it [00:30,  1.55it/s]Extractor Predicting: 47it [00:31,  1.54it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:32,  1.58it/s]Extractor Predicting: 50it [00:32,  1.60it/s]Extractor Predicting: 51it [00:33,  1.60it/s]Extractor Predicting: 52it [00:34,  1.60it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:35,  1.56it/s]Extractor Predicting: 55it [00:36,  1.57it/s]Extractor Predicting: 56it [00:36,  1.64it/s]Extractor Predicting: 57it [00:37,  1.69it/s]Extractor Predicting: 58it [00:37,  1.74it/s]Extractor Predicting: 59it [00:38,  1.82it/s]Extractor Predicting: 60it [00:38,  1.89it/s]Extractor Predicting: 61it [00:39,  1.93it/s]Extractor Predicting: 62it [00:39,  1.93it/s]Extractor Predicting: 63it [00:40,  1.95it/s]Extractor Predicting: 64it [00:40,  1.93it/s]Extractor Predicting: 65it [00:41,  1.93it/s]Extractor Predicting: 66it [00:41,  1.92it/s]Extractor Predicting: 67it [00:42,  1.90it/s]Extractor Predicting: 68it [00:42,  1.92it/s]Extractor Predicting: 69it [00:43,  1.95it/s]Extractor Predicting: 70it [00:43,  1.92it/s]Extractor Predicting: 71it [00:44,  1.92it/s]Extractor Predicting: 72it [00:44,  1.94it/s]Extractor Predicting: 73it [00:45,  1.97it/s]Extractor Predicting: 74it [00:45,  1.99it/s]Extractor Predicting: 75it [00:46,  1.97it/s]Extractor Predicting: 76it [00:47,  1.95it/s]Extractor Predicting: 77it [00:47,  2.01it/s]Extractor Predicting: 78it [00:48,  1.94it/s]Extractor Predicting: 79it [00:48,  1.93it/s]Extractor Predicting: 80it [00:49,  1.92it/s]Extractor Predicting: 81it [00:49,  1.91it/s]Extractor Predicting: 82it [00:50,  1.94it/s]Extractor Predicting: 83it [00:50,  1.94it/s]Extractor Predicting: 84it [00:51,  1.95it/s]Extractor Predicting: 85it [00:51,  1.96it/s]Extractor Predicting: 86it [00:52,  1.82it/s]Extractor Predicting: 87it [00:52,  1.76it/s]Extractor Predicting: 88it [00:53,  1.72it/s]Extractor Predicting: 89it [00:54,  1.70it/s]Extractor Predicting: 90it [00:54,  1.70it/s]Extractor Predicting: 91it [00:55,  1.67it/s]Extractor Predicting: 92it [00:55,  1.66it/s]Extractor Predicting: 93it [00:56,  1.67it/s]Extractor Predicting: 94it [00:57,  1.66it/s]Extractor Predicting: 95it [00:57,  1.68it/s]Extractor Predicting: 96it [00:58,  1.68it/s]Extractor Predicting: 97it [00:58,  1.69it/s]Extractor Predicting: 98it [00:59,  1.69it/s]Extractor Predicting: 99it [01:00,  1.67it/s]Extractor Predicting: 100it [01:00,  1.61it/s]Extractor Predicting: 101it [01:01,  1.63it/s]Extractor Predicting: 102it [01:01,  1.62it/s]Extractor Predicting: 103it [01:02,  1.57it/s]Extractor Predicting: 104it [01:03,  1.55it/s]Extractor Predicting: 105it [01:03,  1.55it/s]Extractor Predicting: 106it [01:04,  1.54it/s]Extractor Predicting: 107it [01:05,  1.52it/s]Extractor Predicting: 108it [01:06,  1.46it/s]Extractor Predicting: 108it [01:06,  1.64it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8559077809798271,
  "recall": 0.09514656415185008,
  "score": 0.17125558598817933,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/', 'labels': ['cast member', 'follows', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of political party', 'mother', 'residence', 'shares border with', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
