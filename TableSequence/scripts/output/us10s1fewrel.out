/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_1', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7490 mean pseudo reward: 0.951858053246523
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 27599
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27699, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27699, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.688, loss:3094.2812
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.265, loss:2125.4076
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.257, loss:1806.9061
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.268, loss:1689.0366
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.266, loss:1602.1446
>> valid entity prec:0.4754, rec:0.4286, f1:0.4508
>> valid relation prec:0.2248, rec:0.0193, f1:0.0355
>> valid relation with NER prec:0.2248, rec:0.0193, f1:0.0355
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 3.075, loss:1481.4364
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.270, loss:1322.7238
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.275, loss:1293.6024
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.260, loss:1238.6008
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.273, loss:1137.5541
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4229, rec:0.4536, f1:0.4377
>> valid relation prec:0.0729, rec:0.0052, f1:0.0097
>> valid relation with NER prec:0.0729, rec:0.0052, f1:0.0097
g_step 1100, step 161, avg_time 3.020, loss:1081.9005
g_step 1200, step 261, avg_time 1.271, loss:1109.1541
g_step 1300, step 48, avg_time 1.248, loss:1038.8031
g_step 1400, step 148, avg_time 1.267, loss:993.0765
g_step 1500, step 248, avg_time 1.275, loss:996.6207
>> valid entity prec:0.5071, rec:0.4399, f1:0.4711
>> valid relation prec:0.0931, rec:0.0101, f1:0.0182
>> valid relation with NER prec:0.0931, rec:0.0101, f1:0.0182
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 3.012, loss:968.6615
g_step 1700, step 135, avg_time 1.275, loss:942.6569
g_step 1800, step 235, avg_time 1.252, loss:921.7485
g_step 1900, step 22, avg_time 1.256, loss:917.1017
g_step 2000, step 122, avg_time 1.274, loss:843.0417
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4665, rec:0.4088, f1:0.4358
>> valid relation prec:0.0216, rec:0.0029, f1:0.0051
>> valid relation with NER prec:0.0216, rec:0.0029, f1:0.0051
g_step 2100, step 222, avg_time 3.016, loss:899.8591
g_step 2200, step 9, avg_time 1.270, loss:853.2123
g_step 2300, step 109, avg_time 1.275, loss:815.3724
g_step 2400, step 209, avg_time 1.268, loss:828.1803
g_step 2500, step 309, avg_time 1.261, loss:856.9453
>> valid entity prec:0.4823, rec:0.4556, f1:0.4686
>> valid relation prec:0.0614, rec:0.0089, f1:0.0156
>> valid relation with NER prec:0.0614, rec:0.0089, f1:0.0156
g_step 2600, step 96, avg_time 3.019, loss:771.4314
g_step 2700, step 196, avg_time 1.272, loss:796.2394
g_step 2800, step 296, avg_time 1.260, loss:797.5958
g_step 2900, step 83, avg_time 1.268, loss:746.0358
g_step 3000, step 183, avg_time 1.271, loss:752.3780
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4509, rec:0.4249, f1:0.4375
>> valid relation prec:0.0272, rec:0.0043, f1:0.0074
>> valid relation with NER prec:0.0272, rec:0.0043, f1:0.0074
g_step 3100, step 283, avg_time 3.025, loss:761.8111
g_step 3200, step 70, avg_time 1.253, loss:716.7768
g_step 3300, step 170, avg_time 1.267, loss:700.9118
g_step 3400, step 270, avg_time 1.276, loss:731.8846
g_step 3500, step 57, avg_time 1.266, loss:698.0030
>> valid entity prec:0.4475, rec:0.4205, f1:0.4336
>> valid relation prec:0.0305, rec:0.0072, f1:0.0116
>> valid relation with NER prec:0.0305, rec:0.0072, f1:0.0116
g_step 3600, step 157, avg_time 3.024, loss:667.9429
g_step 3700, step 257, avg_time 1.262, loss:680.8000
g_step 3800, step 44, avg_time 1.258, loss:667.7802
g_step 3900, step 144, avg_time 1.262, loss:629.0618
g_step 4000, step 244, avg_time 1.286, loss:665.3437
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4505, rec:0.4275, f1:0.4387
>> valid relation prec:0.0411, rec:0.0089, f1:0.0146
>> valid relation with NER prec:0.0411, rec:0.0089, f1:0.0146
g_step 4100, step 31, avg_time 3.012, loss:644.1922
g_step 4200, step 131, avg_time 1.274, loss:624.0804
g_step 4300, step 231, avg_time 1.268, loss:630.8667
g_step 4400, step 18, avg_time 1.254, loss:625.9825
g_step 4500, step 118, avg_time 1.270, loss:582.7767
>> valid entity prec:0.4610, rec:0.4712, f1:0.4661
>> valid relation prec:0.0205, rec:0.0057, f1:0.0090
>> valid relation with NER prec:0.0205, rec:0.0057, f1:0.0090
g_step 4600, step 218, avg_time 3.017, loss:603.9680
g_step 4700, step 5, avg_time 1.255, loss:611.6479
g_step 4800, step 105, avg_time 1.259, loss:550.1247
g_step 4900, step 205, avg_time 1.279, loss:581.2835
g_step 5000, step 305, avg_time 1.269, loss:592.8789
learning rate was adjusted to 0.0008
>> valid entity prec:0.4616, rec:0.4305, f1:0.4455
>> valid relation prec:0.0258, rec:0.0069, f1:0.0109
>> valid relation with NER prec:0.0258, rec:0.0069, f1:0.0109
g_step 5100, step 92, avg_time 3.017, loss:543.2774
g_step 5200, step 192, avg_time 1.263, loss:547.6001
g_step 5300, step 292, avg_time 1.265, loss:562.7960
g_step 5400, step 79, avg_time 1.267, loss:517.3804
g_step 5500, step 179, avg_time 1.272, loss:541.5745
>> valid entity prec:0.4765, rec:0.3803, f1:0.4230
>> valid relation prec:0.0179, rec:0.0046, f1:0.0073
>> valid relation with NER prec:0.0179, rec:0.0046, f1:0.0073
g_step 5600, step 279, avg_time 3.019, loss:529.5512
g_step 5700, step 66, avg_time 1.262, loss:521.3035
g_step 5800, step 166, avg_time 1.257, loss:505.1203
g_step 5900, step 266, avg_time 1.251, loss:523.7622
g_step 6000, step 53, avg_time 1.254, loss:496.8380
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4525, rec:0.3717, f1:0.4081
>> valid relation prec:0.0330, rec:0.0078, f1:0.0126
>> valid relation with NER prec:0.0330, rec:0.0078, f1:0.0126
g_step 6100, step 153, avg_time 2.979, loss:478.2603
g_step 6200, step 253, avg_time 1.260, loss:494.4326
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 23:39:33 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 23:39:33 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_23-39-33_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 23:39:34 - WARNING - datasets.builder -   Using custom data configuration default-77a3e4e3e5546d4e
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-77a3e4e3e5546d4e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  2.21 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 23:39:34,905 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:39:34,906 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 23:39:34,908 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:39:34,909 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 23:39:34,919 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:39:34,924 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:39:34,925 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:39:34,925 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:39:34,925 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:39:34,925 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:39:34,925 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 23:39:35,094 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 23:39:40,238 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 23:39:40,242 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-77a3e4e3e5546d4e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 23:39:40 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14850dd3fb90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  2.50ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.45ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  3.90ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:00,  4.18ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.33ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.42ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.49ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.32ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.43ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  4.12ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.38ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.50ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.69ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.12ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  8.41ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00, 10.43ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00, 10.87ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00, 10.97ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.47ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  9.01ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 10.74ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 12.15ba/s]
[INFO|trainer.py:414] 2023-08-27 23:39:44,261 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 23:39:44,287 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 23:39:44,288 >>   Num examples = 7513
[INFO|trainer.py:1149] 2023-08-27 23:39:44,288 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 23:39:44,288 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 23:39:44,288 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 23:39:44,288 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 23:39:44,288 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<04:36,  2.11it/s]  0%|          | 2/585 [00:00<03:31,  2.75it/s]  1%|          | 3/585 [00:01<03:10,  3.05it/s]  1%|          | 4/585 [00:01<03:00,  3.21it/s]  1%|          | 5/585 [00:01<02:55,  3.31it/s]  1%|          | 6/585 [00:01<02:51,  3.38it/s]  1%|          | 7/585 [00:02<02:49,  3.42it/s]  1%|â–         | 8/585 [00:02<02:47,  3.44it/s]  2%|â–         | 9/585 [00:02<02:46,  3.46it/s]  2%|â–         | 10/585 [00:03<02:45,  3.48it/s]  2%|â–         | 11/585 [00:03<02:44,  3.48it/s]  2%|â–         | 12/585 [00:03<02:44,  3.49it/s]  2%|â–         | 13/585 [00:03<02:43,  3.50it/s]  2%|â–         | 14/585 [00:04<02:43,  3.50it/s]  3%|â–Ž         | 15/585 [00:04<02:42,  3.50it/s]  3%|â–Ž         | 16/585 [00:04<02:42,  3.50it/s]  3%|â–Ž         | 17/585 [00:05<02:41,  3.51it/s]  3%|â–Ž         | 18/585 [00:05<02:41,  3.51it/s]  3%|â–Ž         | 19/585 [00:05<02:41,  3.51it/s]  3%|â–Ž         | 20/585 [00:05<02:41,  3.51it/s]  4%|â–Ž         | 21/585 [00:06<02:40,  3.51it/s]  4%|â–         | 22/585 [00:06<02:40,  3.51it/s]  4%|â–         | 23/585 [00:06<02:40,  3.51it/s]  4%|â–         | 24/585 [00:07<02:39,  3.51it/s]  4%|â–         | 25/585 [00:07<02:39,  3.51it/s]  4%|â–         | 26/585 [00:07<02:39,  3.51it/s]  5%|â–         | 27/585 [00:07<02:39,  3.51it/s]  5%|â–         | 28/585 [00:08<02:38,  3.51it/s]  5%|â–         | 29/585 [00:08<02:38,  3.51it/s]  5%|â–Œ         | 30/585 [00:08<02:38,  3.51it/s]  5%|â–Œ         | 31/585 [00:09<02:37,  3.51it/s]  5%|â–Œ         | 32/585 [00:09<02:37,  3.51it/s]  6%|â–Œ         | 33/585 [00:09<02:37,  3.51it/s]  6%|â–Œ         | 34/585 [00:09<02:37,  3.51it/s]  6%|â–Œ         | 35/585 [00:10<02:36,  3.51it/s]  6%|â–Œ         | 36/585 [00:10<02:36,  3.51it/s]  6%|â–‹         | 37/585 [00:10<02:36,  3.51it/s]  6%|â–‹         | 38/585 [00:11<02:35,  3.51it/s]  7%|â–‹         | 39/585 [00:11<02:35,  3.51it/s]  7%|â–‹         | 40/585 [00:11<02:35,  3.51it/s]  7%|â–‹         | 41/585 [00:11<02:35,  3.51it/s]  7%|â–‹         | 42/585 [00:12<02:34,  3.51it/s]  7%|â–‹         | 43/585 [00:12<02:34,  3.51it/s]  8%|â–Š         | 44/585 [00:12<02:34,  3.51it/s]  8%|â–Š         | 45/585 [00:13<02:33,  3.51it/s]  8%|â–Š         | 46/585 [00:13<02:33,  3.51it/s]  8%|â–Š         | 47/585 [00:13<02:33,  3.51it/s]  8%|â–Š         | 48/585 [00:13<02:33,  3.51it/s]  8%|â–Š         | 49/585 [00:14<02:32,  3.51it/s]  9%|â–Š         | 50/585 [00:14<02:32,  3.50it/s]  9%|â–Š         | 51/585 [00:14<02:32,  3.49it/s]  9%|â–‰         | 52/585 [00:15<02:32,  3.50it/s]  9%|â–‰         | 53/585 [00:15<02:32,  3.50it/s]  9%|â–‰         | 54/585 [00:15<02:31,  3.50it/s]  9%|â–‰         | 55/585 [00:15<02:31,  3.50it/s] 10%|â–‰         | 56/585 [00:16<02:30,  3.50it/s] 10%|â–‰         | 57/585 [00:16<02:30,  3.50it/s] 10%|â–‰         | 58/585 [00:16<02:30,  3.50it/s] 10%|â–ˆ         | 59/585 [00:17<02:30,  3.51it/s] 10%|â–ˆ         | 60/585 [00:17<02:29,  3.51it/s] 10%|â–ˆ         | 61/585 [00:17<02:29,  3.50it/s] 11%|â–ˆ         | 62/585 [00:17<02:29,  3.50it/s] 11%|â–ˆ         | 63/585 [00:18<02:29,  3.50it/s] 11%|â–ˆ         | 64/585 [00:18<02:28,  3.50it/s] 11%|â–ˆ         | 65/585 [00:18<02:28,  3.50it/s] 11%|â–ˆâ–        | 66/585 [00:19<02:28,  3.50it/s] 11%|â–ˆâ–        | 67/585 [00:19<02:28,  3.50it/s] 12%|â–ˆâ–        | 68/585 [00:19<02:27,  3.50it/s] 12%|â–ˆâ–        | 69/585 [00:19<02:27,  3.50it/s] 12%|â–ˆâ–        | 70/585 [00:20<02:27,  3.50it/s] 12%|â–ˆâ–        | 71/585 [00:20<02:26,  3.50it/s] 12%|â–ˆâ–        | 72/585 [00:20<02:26,  3.50it/s] 12%|â–ˆâ–        | 73/585 [00:21<02:26,  3.50it/s] 13%|â–ˆâ–Ž        | 74/585 [00:21<02:26,  3.49it/s] 13%|â–ˆâ–Ž        | 75/585 [00:21<02:26,  3.49it/s] 13%|â–ˆâ–Ž        | 76/585 [00:21<02:25,  3.49it/s] 13%|â–ˆâ–Ž        | 77/585 [00:22<02:25,  3.49it/s] 13%|â–ˆâ–Ž        | 78/585 [00:22<02:25,  3.49it/s] 14%|â–ˆâ–Ž        | 79/585 [00:22<02:24,  3.49it/s] 14%|â–ˆâ–Ž        | 80/585 [00:23<02:24,  3.49it/s] 14%|â–ˆâ–        | 81/585 [00:23<02:24,  3.50it/s] 14%|â–ˆâ–        | 82/585 [00:23<02:23,  3.50it/s] 14%|â–ˆâ–        | 83/585 [00:23<02:23,  3.50it/s] 14%|â–ˆâ–        | 84/585 [00:24<02:23,  3.50it/s] 15%|â–ˆâ–        | 85/585 [00:24<02:22,  3.50it/s] 15%|â–ˆâ–        | 86/585 [00:24<02:22,  3.50it/s] 15%|â–ˆâ–        | 87/585 [00:25<02:22,  3.50it/s] 15%|â–ˆâ–Œ        | 88/585 [00:25<02:22,  3.50it/s] 15%|â–ˆâ–Œ        | 89/585 [00:25<02:21,  3.50it/s] 15%|â–ˆâ–Œ        | 90/585 [00:25<02:21,  3.49it/s] 16%|â–ˆâ–Œ        | 91/585 [00:26<02:21,  3.49it/s] 16%|â–ˆâ–Œ        | 92/585 [00:26<02:21,  3.50it/s] 16%|â–ˆâ–Œ        | 93/585 [00:26<02:20,  3.49it/s] 16%|â–ˆâ–Œ        | 94/585 [00:27<02:20,  3.49it/s] 16%|â–ˆâ–Œ        | 95/585 [00:27<02:20,  3.49it/s] 16%|â–ˆâ–‹        | 96/585 [00:27<02:19,  3.49it/s] 17%|â–ˆâ–‹        | 97/585 [00:27<02:19,  3.49it/s] 17%|â–ˆâ–‹        | 98/585 [00:28<02:19,  3.50it/s] 17%|â–ˆâ–‹        | 99/585 [00:28<02:19,  3.50it/s] 17%|â–ˆâ–‹        | 100/585 [00:28<02:18,  3.50it/s] 17%|â–ˆâ–‹        | 101/585 [00:29<02:18,  3.50it/s] 17%|â–ˆâ–‹        | 102/585 [00:29<02:18,  3.49it/s] 18%|â–ˆâ–Š        | 103/585 [00:29<02:18,  3.49it/s] 18%|â–ˆâ–Š        | 104/585 [00:29<02:17,  3.49it/s] 18%|â–ˆâ–Š        | 105/585 [00:30<02:17,  3.49it/s] 18%|â–ˆâ–Š        | 106/585 [00:30<02:17,  3.49it/s] 18%|â–ˆâ–Š        | 107/585 [00:30<02:16,  3.49it/s] 18%|â–ˆâ–Š        | 108/585 [00:31<02:16,  3.50it/s] 19%|â–ˆâ–Š        | 109/585 [00:31<02:16,  3.49it/s] 19%|â–ˆâ–‰        | 110/585 [00:31<02:15,  3.50it/s] 19%|â–ˆâ–‰        | 111/585 [00:31<02:15,  3.50it/s] 19%|â–ˆâ–‰        | 112/585 [00:32<02:15,  3.49it/s] 19%|â–ˆâ–‰        | 113/585 [00:32<02:15,  3.49it/s] 19%|â–ˆâ–‰        | 114/585 [00:32<02:14,  3.49it/s] 20%|â–ˆâ–‰        | 115/585 [00:33<02:14,  3.50it/s] 20%|â–ˆâ–‰        | 116/585 [00:33<02:14,  3.49it/s] 20%|â–ˆâ–ˆ        | 117/585 [00:33<02:13,  3.49it/s][INFO|trainer.py:2140] 2023-08-27 23:40:18,011 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:40:18,011 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-27 23:40:18,011 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 59.58it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 51.43it/s][A
  4%|â–         | 18/435 [00:00<00:08, 49.25it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.40it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.90it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.47it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.16it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.84it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.75it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.72it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.80it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.78it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.78it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.79it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.77it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.78it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.68it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.65it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.63it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.63it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.68it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.73it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.71it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.74it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.76it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.71it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.74it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.69it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.63it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.69it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.70it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.69it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.73it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.70it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.72it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.75it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.68it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.72it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.69it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.64it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.68it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.72it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.68it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.45it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.71it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.69it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.73it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.68it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.72it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.74it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.66it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.72it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.76it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.68it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.71it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.75it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.67it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.64it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.62it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.67it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.71it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.71it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.72it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.75it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.71it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.69it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.66it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.62it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.62it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.61it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.64it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.42it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.75it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.75it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.75it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.66it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.70it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.67it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.62it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.67it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.72it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.68it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.72it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.74it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.69it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.68it/s][A                                                 
                                                 [A 20%|â–ˆâ–ˆ        | 117/585 [00:43<02:13,  3.49it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.68it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:40:27,350 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-27 23:40:27,367 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:40:29,471 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:40:29,491 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:40:29,500 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 118/585 [00:50<40:41,  5.23s/it] 20%|â–ˆâ–ˆ        | 119/585 [00:50<29:05,  3.75s/it] 21%|â–ˆâ–ˆ        | 120/585 [00:50<20:58,  2.71s/it] 21%|â–ˆâ–ˆ        | 121/585 [00:51<15:19,  1.98s/it] 21%|â–ˆâ–ˆ        | 122/585 [00:51<11:21,  1.47s/it] 21%|â–ˆâ–ˆ        | 123/585 [00:51<08:35,  1.12s/it] 21%|â–ˆâ–ˆ        | 124/585 [00:52<06:39,  1.15it/s] 21%|â–ˆâ–ˆâ–       | 125/585 [00:52<05:18,  1.44it/s] 22%|â–ˆâ–ˆâ–       | 126/585 [00:52<04:21,  1.75it/s] 22%|â–ˆâ–ˆâ–       | 127/585 [00:52<03:42,  2.06it/s] 22%|â–ˆâ–ˆâ–       | 128/585 [00:53<03:14,  2.35it/s] 22%|â–ˆâ–ˆâ–       | 129/585 [00:53<02:54,  2.61it/s] 22%|â–ˆâ–ˆâ–       | 130/585 [00:53<02:41,  2.82it/s] 22%|â–ˆâ–ˆâ–       | 131/585 [00:54<02:31,  2.99it/s] 23%|â–ˆâ–ˆâ–Ž       | 132/585 [00:54<02:24,  3.13it/s] 23%|â–ˆâ–ˆâ–Ž       | 133/585 [00:54<02:19,  3.23it/s] 23%|â–ˆâ–ˆâ–Ž       | 134/585 [00:54<02:16,  3.31it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/585 [00:55<02:13,  3.37it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/585 [00:55<02:11,  3.41it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/585 [00:55<02:10,  3.43it/s] 24%|â–ˆâ–ˆâ–Ž       | 138/585 [00:56<02:09,  3.46it/s] 24%|â–ˆâ–ˆâ–       | 139/585 [00:56<02:08,  3.47it/s] 24%|â–ˆâ–ˆâ–       | 140/585 [00:56<02:08,  3.47it/s] 24%|â–ˆâ–ˆâ–       | 141/585 [00:56<02:07,  3.48it/s] 24%|â–ˆâ–ˆâ–       | 142/585 [00:57<02:06,  3.49it/s] 24%|â–ˆâ–ˆâ–       | 143/585 [00:57<02:06,  3.49it/s] 25%|â–ˆâ–ˆâ–       | 144/585 [00:57<02:06,  3.50it/s] 25%|â–ˆâ–ˆâ–       | 145/585 [00:58<02:05,  3.50it/s] 25%|â–ˆâ–ˆâ–       | 146/585 [00:58<02:05,  3.50it/s] 25%|â–ˆâ–ˆâ–Œ       | 147/585 [00:58<02:05,  3.50it/s] 25%|â–ˆâ–ˆâ–Œ       | 148/585 [00:58<02:04,  3.50it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/585 [00:59<02:04,  3.50it/s] 26%|â–ˆâ–ˆâ–Œ       | 150/585 [00:59<02:04,  3.50it/s] 26%|â–ˆâ–ˆâ–Œ       | 151/585 [00:59<02:04,  3.50it/s] 26%|â–ˆâ–ˆâ–Œ       | 152/585 [01:00<02:03,  3.50it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/585 [01:00<02:03,  3.50it/s] 26%|â–ˆâ–ˆâ–‹       | 154/585 [01:00<02:03,  3.50it/s] 26%|â–ˆâ–ˆâ–‹       | 155/585 [01:00<02:02,  3.50it/s] 27%|â–ˆâ–ˆâ–‹       | 156/585 [01:01<02:02,  3.50it/s] 27%|â–ˆâ–ˆâ–‹       | 157/585 [01:01<02:02,  3.50it/s] 27%|â–ˆâ–ˆâ–‹       | 158/585 [01:01<02:02,  3.50it/s] 27%|â–ˆâ–ˆâ–‹       | 159/585 [01:02<02:01,  3.50it/s] 27%|â–ˆâ–ˆâ–‹       | 160/585 [01:02<02:01,  3.50it/s] 28%|â–ˆâ–ˆâ–Š       | 161/585 [01:02<02:01,  3.50it/s] 28%|â–ˆâ–ˆâ–Š       | 162/585 [01:02<02:01,  3.49it/s] 28%|â–ˆâ–ˆâ–Š       | 163/585 [01:03<02:00,  3.49it/s] 28%|â–ˆâ–ˆâ–Š       | 164/585 [01:03<02:00,  3.49it/s] 28%|â–ˆâ–ˆâ–Š       | 165/585 [01:03<02:00,  3.49it/s] 28%|â–ˆâ–ˆâ–Š       | 166/585 [01:04<01:59,  3.50it/s] 29%|â–ˆâ–ˆâ–Š       | 167/585 [01:04<01:59,  3.50it/s] 29%|â–ˆâ–ˆâ–Š       | 168/585 [01:04<01:59,  3.50it/s] 29%|â–ˆâ–ˆâ–‰       | 169/585 [01:04<01:58,  3.50it/s] 29%|â–ˆâ–ˆâ–‰       | 170/585 [01:05<01:58,  3.50it/s] 29%|â–ˆâ–ˆâ–‰       | 171/585 [01:05<01:58,  3.50it/s] 29%|â–ˆâ–ˆâ–‰       | 172/585 [01:05<01:58,  3.50it/s] 30%|â–ˆâ–ˆâ–‰       | 173/585 [01:06<01:59,  3.45it/s] 30%|â–ˆâ–ˆâ–‰       | 174/585 [01:06<01:58,  3.47it/s] 30%|â–ˆâ–ˆâ–‰       | 175/585 [01:06<01:57,  3.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 176/585 [01:06<01:57,  3.48it/s] 30%|â–ˆâ–ˆâ–ˆ       | 177/585 [01:07<01:56,  3.49it/s] 30%|â–ˆâ–ˆâ–ˆ       | 178/585 [01:07<01:56,  3.49it/s] 31%|â–ˆâ–ˆâ–ˆ       | 179/585 [01:07<01:56,  3.49it/s] 31%|â–ˆâ–ˆâ–ˆ       | 180/585 [01:08<01:55,  3.49it/s] 31%|â–ˆâ–ˆâ–ˆ       | 181/585 [01:08<01:55,  3.49it/s] 31%|â–ˆâ–ˆâ–ˆ       | 182/585 [01:08<01:55,  3.49it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 183/585 [01:08<01:55,  3.50it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 184/585 [01:09<01:55,  3.48it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 185/585 [01:09<01:54,  3.49it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 186/585 [01:09<01:54,  3.49it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 187/585 [01:10<01:53,  3.49it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 188/585 [01:10<01:53,  3.49it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/585 [01:10<01:53,  3.49it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/585 [01:10<01:53,  3.50it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 191/585 [01:11<01:52,  3.49it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 192/585 [01:11<01:52,  3.49it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 193/585 [01:11<01:52,  3.49it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/585 [01:12<01:51,  3.49it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/585 [01:12<01:51,  3.48it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/585 [01:12<01:51,  3.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/585 [01:12<01:51,  3.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 198/585 [01:13<01:50,  3.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 199/585 [01:13<01:50,  3.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 200/585 [01:13<01:50,  3.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/585 [01:14<01:49,  3.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 202/585 [01:14<01:49,  3.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 203/585 [01:14<01:49,  3.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 204/585 [01:14<01:49,  3.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 205/585 [01:15<01:48,  3.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 206/585 [01:15<01:48,  3.49it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 207/585 [01:15<01:48,  3.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 208/585 [01:16<01:47,  3.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/585 [01:16<01:47,  3.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/585 [01:16<01:47,  3.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/585 [01:16<01:47,  3.50it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/585 [01:17<01:46,  3.49it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 213/585 [01:17<01:46,  3.49it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 214/585 [01:17<01:46,  3.50it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 215/585 [01:18<01:45,  3.50it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 216/585 [01:18<01:45,  3.49it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 217/585 [01:18<01:45,  3.48it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 218/585 [01:18<01:45,  3.49it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/585 [01:19<01:44,  3.49it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 220/585 [01:19<01:44,  3.49it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 221/585 [01:19<01:44,  3.49it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 222/585 [01:20<01:43,  3.49it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 223/585 [01:20<01:43,  3.49it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 224/585 [01:20<01:43,  3.49it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/585 [01:20<01:43,  3.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 226/585 [01:21<01:42,  3.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 227/585 [01:21<01:42,  3.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 228/585 [01:21<01:42,  3.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 229/585 [01:22<01:41,  3.49it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 230/585 [01:22<01:41,  3.50it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/585 [01:22<01:41,  3.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 232/585 [01:22<01:41,  3.49it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 233/585 [01:23<01:40,  3.50it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:23<01:40,  3.50it/s][INFO|trainer.py:2140] 2023-08-27 23:41:07,964 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:41:07,964 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-27 23:41:07,964 >>   Batch size = 8
{'eval_loss': 0.9710797071456909, 'eval_runtime': 9.3258, 'eval_samples_per_second': 373.05, 'eval_steps_per_second': 46.645, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.42it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.59it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.78it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.02it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.57it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.35it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.05it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.77it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.77it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.75it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.76it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.75it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.73it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.75it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.80it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.75it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.66it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.67it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.64it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.66it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.67it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.53it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.74it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.72it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.72it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.72it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.71it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.67it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.68it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.68it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.65it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.66it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.67it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.69it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.70it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.68it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.70it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.70it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.65it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.67it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.69it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.68it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.68it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.68it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.70it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.70it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.68it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.68it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.68it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.65it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.57it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.61it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.60it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.64it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.66it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.66it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.67it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.66it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.66it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.67it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.66it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.67it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.68it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.65it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.67it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.67it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.65it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.66it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.67it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.65it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.66it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.64it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.68it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.68it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.63it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.65it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.66it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.64it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.66it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.67it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.65it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.66it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.66it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.67it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.67it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.65it/s][A                                                 
                                                 [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:32<01:40,  3.50it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:41:17,298 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-27 23:41:17,314 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:41:19,178 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:41:19,186 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:41:19,197 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 235/585 [01:39<28:34,  4.90s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 236/585 [01:39<20:27,  3.52s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 237/585 [01:39<14:46,  2.55s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/585 [01:40<10:48,  1.87s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/585 [01:40<08:02,  1.39s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 240/585 [01:40<06:06,  1.06s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/585 [01:40<04:45,  1.21it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242/585 [01:41<03:48,  1.50it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243/585 [01:41<03:08,  1.81it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/585 [01:41<02:40,  2.12it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/585 [01:42<02:21,  2.40it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/585 [01:42<02:07,  2.65it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/585 [01:42<01:58,  2.86it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/585 [01:42<01:51,  3.03it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 249/585 [01:43<01:46,  3.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 250/585 [01:43<01:43,  3.25it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 251/585 [01:43<01:40,  3.32it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 252/585 [01:44<01:38,  3.38it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/585 [01:44<01:37,  3.41it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 254/585 [01:44<01:36,  3.44it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/585 [01:44<01:35,  3.46it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/585 [01:45<01:34,  3.47it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/585 [01:45<01:34,  3.48it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/585 [01:45<01:34,  3.48it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/585 [01:46<01:33,  3.48it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/585 [01:46<01:33,  3.49it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/585 [01:46<01:32,  3.49it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 262/585 [01:46<01:32,  3.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/585 [01:47<01:32,  3.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 264/585 [01:47<01:31,  3.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 265/585 [01:47<01:31,  3.50it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 266/585 [01:48<01:31,  3.50it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 267/585 [01:48<01:30,  3.50it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 268/585 [01:48<01:30,  3.50it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/585 [01:48<01:30,  3.49it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 270/585 [01:49<01:30,  3.49it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 271/585 [01:49<01:29,  3.49it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 272/585 [01:49<01:29,  3.49it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 273/585 [01:50<01:32,  3.38it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 274/585 [01:50<01:31,  3.42it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 275/585 [01:50<01:30,  3.44it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 276/585 [01:50<01:29,  3.45it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/585 [01:51<01:28,  3.46it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 278/585 [01:51<01:28,  3.47it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 279/585 [01:51<01:28,  3.48it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 280/585 [01:52<01:27,  3.48it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 281/585 [01:52<01:27,  3.48it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 282/585 [01:52<01:26,  3.48it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/585 [01:52<01:26,  3.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 284/585 [01:53<01:26,  3.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/585 [01:53<01:26,  3.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 286/585 [01:53<01:25,  3.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 287/585 [01:54<01:25,  3.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 288/585 [01:54<01:25,  3.49it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 289/585 [01:54<01:24,  3.49it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 290/585 [01:55<01:25,  3.46it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/585 [01:55<01:24,  3.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 292/585 [01:55<01:24,  3.47it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 293/585 [01:55<01:23,  3.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 294/585 [01:56<01:23,  3.48it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 295/585 [01:56<01:23,  3.48it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 296/585 [01:56<01:22,  3.48it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 297/585 [01:57<01:22,  3.49it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 298/585 [01:57<01:22,  3.48it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/585 [01:57<01:22,  3.49it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/585 [01:57<01:21,  3.49it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/585 [01:58<01:21,  3.48it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/585 [01:58<01:21,  3.48it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/585 [01:58<01:20,  3.48it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/585 [01:59<01:20,  3.48it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/585 [01:59<01:20,  3.48it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306/585 [01:59<01:20,  3.48it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/585 [01:59<01:19,  3.48it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 308/585 [02:00<01:19,  3.49it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 309/585 [02:00<01:19,  3.49it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 310/585 [02:00<01:18,  3.49it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 311/585 [02:01<01:18,  3.49it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 312/585 [02:01<01:18,  3.47it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/585 [02:01<01:18,  3.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 314/585 [02:01<01:18,  3.47it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 315/585 [02:02<01:17,  3.47it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/585 [02:02<01:17,  3.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/585 [02:02<01:17,  3.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/585 [02:03<01:16,  3.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/585 [02:03<01:16,  3.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/585 [02:03<01:16,  3.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/585 [02:03<01:15,  3.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 322/585 [02:04<01:15,  3.49it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 323/585 [02:04<01:15,  3.48it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 324/585 [02:04<01:14,  3.48it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 325/585 [02:05<01:14,  3.48it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 326/585 [02:05<01:14,  3.48it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 327/585 [02:05<01:13,  3.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 328/585 [02:05<01:13,  3.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/585 [02:06<01:13,  3.49it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 330/585 [02:06<01:13,  3.49it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 331/585 [02:06<01:12,  3.49it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 332/585 [02:07<01:12,  3.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 333/585 [02:07<01:12,  3.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 334/585 [02:07<01:12,  3.47it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/585 [02:07<01:11,  3.48it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 336/585 [02:08<01:11,  3.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 337/585 [02:08<01:11,  3.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 338/585 [02:08<01:10,  3.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 339/585 [02:09<01:10,  3.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 340/585 [02:09<01:10,  3.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 341/585 [02:09<01:09,  3.49it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 342/585 [02:09<01:09,  3.49it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/585 [02:10<01:09,  3.49it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 344/585 [02:10<01:09,  3.49it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 345/585 [02:10<01:08,  3.48it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 346/585 [02:11<01:08,  3.48it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 347/585 [02:11<01:08,  3.48it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 348/585 [02:11<01:08,  3.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 349/585 [02:11<01:07,  3.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 350/585 [02:12<01:07,  3.48it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:12<01:07,  3.48it/s][INFO|trainer.py:2140] 2023-08-27 23:41:56,915 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:41:56,915 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-27 23:41:56,915 >>   Batch size = 8
{'eval_loss': 0.9675213098526001, 'eval_runtime': 9.3204, 'eval_samples_per_second': 373.269, 'eval_steps_per_second': 46.672, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.92it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.29it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.54it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.81it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.41it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.15it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.96it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.86it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.78it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.71it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.68it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.64it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.64it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.63it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.60it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.61it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.62it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.59it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.61it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.61it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.59it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.61it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.59it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.61it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.62it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.60it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.61it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.62it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.48it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.53it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.53it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.56it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.59it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.57it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.59it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.60it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.58it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.60it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.60it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.59it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.61it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.57it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.58it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.60it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.59it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.60it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.62it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.60it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.61it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.60it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.60it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.61it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.59it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.61it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.62it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.60it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.61it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.61it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.59it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.61it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.60it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.61it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.62it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.59it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.60it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.61it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.59it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.57it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.59it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.59it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.60it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.58it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.59it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.61it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.58it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.60it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.62it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.59it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.61it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.59it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.59it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.60it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.60it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.62it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.46it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.50it/s][A                                                 
                                                 [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:21<01:07,  3.48it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:42:06,270 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-27 23:42:06,292 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:42:08,290 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:42:08,314 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:42:08,322 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 352/585 [02:28<19:50,  5.11s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 353/585 [02:29<14:09,  3.66s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 354/585 [02:29<10:12,  2.65s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 355/585 [02:29<07:26,  1.94s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 356/585 [02:30<05:30,  1.44s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/585 [02:30<04:09,  1.10s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/585 [02:30<03:13,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/585 [02:30<02:34,  1.46it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/585 [02:31<02:06,  1.77it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/585 [02:31<01:47,  2.08it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362/585 [02:31<01:34,  2.37it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 363/585 [02:32<01:24,  2.63it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 364/585 [02:32<01:18,  2.83it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 365/585 [02:32<01:13,  3.00it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 366/585 [02:32<01:09,  3.14it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 367/585 [02:33<01:07,  3.24it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 368/585 [02:33<01:05,  3.31it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 369/585 [02:33<01:04,  3.37it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 370/585 [02:34<01:03,  3.41it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 371/585 [02:34<01:02,  3.44it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/585 [02:34<01:01,  3.46it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 373/585 [02:34<01:01,  3.47it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/585 [02:35<01:00,  3.48it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375/585 [02:35<01:00,  3.46it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376/585 [02:35<01:00,  3.48it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/585 [02:36<00:59,  3.48it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/585 [02:36<00:59,  3.49it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/585 [02:36<00:58,  3.49it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/585 [02:36<00:58,  3.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 381/585 [02:37<00:58,  3.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 382/585 [02:37<00:57,  3.50it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 383/585 [02:37<00:57,  3.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 384/585 [02:38<00:57,  3.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 385/585 [02:38<00:57,  3.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 386/585 [02:38<00:56,  3.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 387/585 [02:38<00:56,  3.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 388/585 [02:39<00:56,  3.50it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 389/585 [02:39<00:56,  3.50it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 390/585 [02:39<00:55,  3.50it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 391/585 [02:40<00:55,  3.49it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 392/585 [02:40<00:55,  3.49it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 393/585 [02:40<00:54,  3.50it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 394/585 [02:40<00:54,  3.50it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 395/585 [02:41<00:54,  3.50it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 396/585 [02:41<00:54,  3.50it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 397/585 [02:41<00:53,  3.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 398/585 [02:42<00:53,  3.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 399/585 [02:42<00:53,  3.49it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/585 [02:42<00:52,  3.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 401/585 [02:42<00:52,  3.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/585 [02:43<00:52,  3.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 403/585 [02:43<00:52,  3.49it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 404/585 [02:43<00:51,  3.50it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 405/585 [02:44<00:51,  3.50it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 406/585 [02:44<00:51,  3.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 407/585 [02:44<00:50,  3.50it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 408/585 [02:44<00:50,  3.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 409/585 [02:45<00:50,  3.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 410/585 [02:45<00:50,  3.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 411/585 [02:45<00:49,  3.49it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 412/585 [02:46<00:49,  3.49it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 413/585 [02:46<00:49,  3.49it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 414/585 [02:46<00:48,  3.49it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 415/585 [02:46<00:48,  3.50it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 416/585 [02:47<00:48,  3.49it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417/585 [02:47<00:48,  3.49it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418/585 [02:47<00:47,  3.50it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 419/585 [02:48<00:47,  3.48it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 420/585 [02:48<00:47,  3.49it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 421/585 [02:48<00:47,  3.49it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 422/585 [02:48<00:46,  3.49it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 423/585 [02:49<00:46,  3.49it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/585 [02:49<00:46,  3.49it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 425/585 [02:49<00:45,  3.49it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 426/585 [02:50<00:45,  3.49it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 427/585 [02:50<00:47,  3.34it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 428/585 [02:50<00:46,  3.38it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 429/585 [02:50<00:45,  3.41it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 430/585 [02:51<00:45,  3.41it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 431/585 [02:51<00:44,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 432/585 [02:51<00:44,  3.45it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 433/585 [02:52<00:43,  3.46it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 434/585 [02:52<00:43,  3.47it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 435/585 [02:52<00:43,  3.47it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 436/585 [02:52<00:42,  3.48it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 437/585 [02:53<00:42,  3.48it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 438/585 [02:53<00:42,  3.48it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 439/585 [02:53<00:41,  3.48it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 440/585 [02:54<00:41,  3.48it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 441/585 [02:54<00:41,  3.48it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 442/585 [02:54<00:41,  3.49it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 443/585 [02:54<00:40,  3.49it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 444/585 [02:55<00:40,  3.48it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 445/585 [02:55<00:40,  3.49it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 446/585 [02:55<00:39,  3.49it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 447/585 [02:56<00:39,  3.49it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 448/585 [02:56<00:39,  3.48it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 449/585 [02:56<00:39,  3.48it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 450/585 [02:56<00:38,  3.48it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 451/585 [02:57<00:38,  3.48it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 452/585 [02:57<00:38,  3.48it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 453/585 [02:57<00:37,  3.48it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 454/585 [02:58<00:37,  3.49it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 455/585 [02:58<00:37,  3.49it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 456/585 [02:58<00:37,  3.49it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 457/585 [02:58<00:36,  3.49it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 458/585 [02:59<00:36,  3.49it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 459/585 [02:59<00:36,  3.48it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 460/585 [02:59<00:35,  3.48it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 461/585 [03:00<00:35,  3.48it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 462/585 [03:00<00:35,  3.49it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 463/585 [03:00<00:35,  3.49it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 464/585 [03:00<00:34,  3.49it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 465/585 [03:01<00:34,  3.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 466/585 [03:01<00:34,  3.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 467/585 [03:01<00:33,  3.49it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:02<00:33,  3.49it/s][INFO|trainer.py:2140] 2023-08-27 23:42:46,543 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:42:46,543 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-27 23:42:46,543 >>   Batch size = 8
{'eval_loss': 0.9707444310188293, 'eval_runtime': 9.337, 'eval_samples_per_second': 372.602, 'eval_steps_per_second': 46.589, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.67it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.28it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.55it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.83it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.43it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.15it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.99it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.86it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.73it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.70it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.68it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.64it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.64it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.63it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.62it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.62it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.60it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.61it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.62it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.60it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.61it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.62it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.60it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.61it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.61it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.60it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.61it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.60it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.61it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.62it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.60it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.61it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.62it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.59it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.60it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.60it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.61it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.62it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.60it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.61it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.62it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.59it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.61it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.61it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.59it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.60it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.59it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.60it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.61it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.60it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.62it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.63it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.57it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.60it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.60it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.60it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.61it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.59it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.61it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.62it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.57it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.60it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.57it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.57it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.59it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.59it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.59it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.60it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.58it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.60it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.61it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.58it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.60it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.60it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.57it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.59it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.54it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.51it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.48it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.47it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.52it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.55it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.55it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.58it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.57it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.59it/s][A                                                 
                                                 [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:11<00:33,  3.49it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.59it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:42:55,896 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-27 23:42:55,917 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:42:58,163 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:42:58,183 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:42:58,194 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 469/585 [03:18<09:47,  5.06s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 470/585 [03:18<06:57,  3.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 471/585 [03:18<04:59,  2.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 472/585 [03:19<03:37,  1.92s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 473/585 [03:19<02:40,  1.43s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 474/585 [03:19<02:00,  1.09s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 475/585 [03:20<01:33,  1.18it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 476/585 [03:20<01:14,  1.47it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 477/585 [03:20<01:00,  1.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 478/585 [03:20<00:51,  2.09it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 479/585 [03:21<00:44,  2.38it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 480/585 [03:21<00:39,  2.63it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 481/585 [03:21<00:36,  2.83it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 482/585 [03:22<00:34,  3.00it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 483/585 [03:22<00:32,  3.14it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 484/585 [03:22<00:31,  3.24it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 485/585 [03:22<00:30,  3.31it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 486/585 [03:23<00:29,  3.37it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 487/585 [03:23<00:28,  3.41it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 488/585 [03:23<00:28,  3.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 489/585 [03:24<00:27,  3.45it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 490/585 [03:24<00:27,  3.46it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 491/585 [03:24<00:27,  3.47it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 492/585 [03:24<00:26,  3.48it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 493/585 [03:25<00:26,  3.49it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 494/585 [03:25<00:26,  3.49it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 495/585 [03:25<00:25,  3.50it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 496/585 [03:26<00:25,  3.50it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 497/585 [03:26<00:25,  3.50it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 498/585 [03:26<00:24,  3.50it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 499/585 [03:26<00:24,  3.50it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [03:27<00:24,  3.49it/s]                                                  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [03:27<00:24,  3.49it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 501/585 [03:27<00:24,  3.47it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 502/585 [03:27<00:23,  3.48it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 503/585 [03:28<00:23,  3.48it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 504/585 [03:28<00:23,  3.48it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 505/585 [03:28<00:22,  3.49it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 506/585 [03:28<00:22,  3.49it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 507/585 [03:29<00:22,  3.49it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 508/585 [03:29<00:22,  3.49it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 509/585 [03:29<00:21,  3.49it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 510/585 [03:30<00:21,  3.49it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 511/585 [03:30<00:21,  3.49it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 512/585 [03:30<00:20,  3.49it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 513/585 [03:30<00:20,  3.49it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 514/585 [03:31<00:20,  3.49it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 515/585 [03:31<00:20,  3.50it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 516/585 [03:31<00:19,  3.50it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 517/585 [03:32<00:19,  3.49it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 518/585 [03:32<00:19,  3.50it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 519/585 [03:32<00:18,  3.49it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 520/585 [03:32<00:18,  3.49it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 521/585 [03:33<00:18,  3.49it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 522/585 [03:33<00:18,  3.48it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 523/585 [03:33<00:17,  3.49it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 524/585 [03:34<00:17,  3.49it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 525/585 [03:34<00:17,  3.49it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 526/585 [03:34<00:16,  3.49it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 527/585 [03:34<00:16,  3.49it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 528/585 [03:35<00:16,  3.49it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 529/585 [03:35<00:16,  3.49it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 530/585 [03:35<00:15,  3.49it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 531/585 [03:36<00:15,  3.49it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 532/585 [03:36<00:15,  3.49it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 533/585 [03:36<00:14,  3.48it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 534/585 [03:36<00:14,  3.48it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 535/585 [03:37<00:14,  3.49it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 536/585 [03:37<00:14,  3.49it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 537/585 [03:37<00:13,  3.49it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 538/585 [03:38<00:13,  3.49it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 539/585 [03:38<00:13,  3.49it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 540/585 [03:38<00:12,  3.49it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 541/585 [03:38<00:12,  3.49it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 542/585 [03:39<00:12,  3.49it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 543/585 [03:39<00:12,  3.49it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 544/585 [03:39<00:11,  3.49it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 545/585 [03:40<00:11,  3.49it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 546/585 [03:40<00:11,  3.49it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 547/585 [03:40<00:10,  3.49it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 548/585 [03:40<00:10,  3.49it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/585 [03:41<00:10,  3.49it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 550/585 [03:41<00:10,  3.49it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 551/585 [03:41<00:09,  3.49it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 552/585 [03:42<00:09,  3.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 553/585 [03:42<00:09,  3.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 554/585 [03:42<00:08,  3.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 555/585 [03:42<00:08,  3.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 556/585 [03:43<00:08,  3.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 557/585 [03:43<00:08,  3.49it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 558/585 [03:43<00:07,  3.49it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 559/585 [03:44<00:07,  3.49it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 560/585 [03:44<00:07,  3.49it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 561/585 [03:44<00:06,  3.49it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 562/585 [03:44<00:06,  3.49it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 563/585 [03:45<00:06,  3.49it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 564/585 [03:45<00:06,  3.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 565/585 [03:45<00:05,  3.50it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 566/585 [03:46<00:05,  3.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 567/585 [03:46<00:05,  3.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 568/585 [03:46<00:04,  3.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 569/585 [03:46<00:04,  3.49it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 570/585 [03:47<00:04,  3.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 571/585 [03:47<00:04,  3.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 572/585 [03:47<00:03,  3.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 573/585 [03:48<00:03,  3.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 574/585 [03:48<00:03,  3.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 575/585 [03:48<00:02,  3.49it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 576/585 [03:48<00:02,  3.49it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 577/585 [03:49<00:02,  3.49it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 578/585 [03:49<00:02,  3.49it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 579/585 [03:49<00:01,  3.49it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 580/585 [03:50<00:01,  3.49it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 581/585 [03:50<00:01,  3.41it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 582/585 [03:50<00:00,  3.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 583/585 [03:51<00:00,  3.45it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 584/585 [03:51<00:00,  3.46it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [03:51<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-27 23:43:35,886 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:43:35,887 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-27 23:43:35,887 >>   Batch size = 8
{'eval_loss': 0.972622275352478, 'eval_runtime': 9.3365, 'eval_samples_per_second': 372.624, 'eval_steps_per_second': 46.591, 'epoch': 4.0}
{'loss': 0.8193, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.80it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.32it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.57it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.83it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.43it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.16it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.98it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.87it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.77it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.73it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.70it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.66it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.65it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.64it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.61it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.62it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.62it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.61it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.62it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.60it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.61it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.62it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.60it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.60it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.63it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.60it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.61it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.60it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.60it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.61it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.59it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.61it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.61it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.59it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.61it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.63it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.60it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.61it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.60it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.61it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.61it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.59it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.61it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.62it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.58it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.60it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.60it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.60it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.61it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.59it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.60it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.61it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.60it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.62it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.62it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.60it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.62it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.61it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.61it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.61it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.58it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.60it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.61it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.59it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.60it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.61it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.59it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.61it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.60it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.56it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.58it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.56it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.59it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.60it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.58it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.59it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.59it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.58it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.60it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.58it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.60it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.61it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.59it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.61it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.61it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.60it/s][A                                                 
                                                 [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:00<00:00,  3.47it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:43:45,244 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-27 23:43:45,265 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:43:47,595 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:43:47,616 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:43:47,624 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 23:43:52,108 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 23:43:52,111 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234 (score: 0.9675213098526001).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:09<00:00,  3.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:09<00:00,  2.35it/s]
[INFO|trainer.py:1894] 2023-08-27 23:43:53,729 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 23:43:53,743 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:43:55,994 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:43:56,013 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:43:56,019 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 23:43:56,185 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:43:56,186 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:43:56,186 >>   train_loss               =     0.8135
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:43:56,186 >>   train_runtime            = 0:04:09.43
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:43:56,186 >>   train_samples            =       7513
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:43:56,186 >>   train_samples_per_second =    150.599
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:43:56,186 >>   train_steps_per_second   =      2.345
{'eval_loss': 0.9752117395401001, 'eval_runtime': 9.3348, 'eval_samples_per_second': 372.693, 'eval_steps_per_second': 46.6, 'epoch': 5.0}
{'train_runtime': 249.4379, 'train_samples_per_second': 150.599, 'train_steps_per_second': 2.345, 'train_loss': 0.8135299160949185, 'epoch': 5.0}
08/27/2023 23:43:56 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 23:43:56,220 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:43:56,220 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-27 23:43:56,220 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 58.04it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 51.13it/s]  4%|â–         | 18/435 [00:00<00:08, 49.16it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.45it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.97it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.64it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.48it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.13it/s] 11%|â–ˆ         | 48/435 [00:00<00:08, 47.07it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 47.07it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 47.05it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 47.01it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 47.05it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 47.05it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 47.02it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 47.05it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 47.01it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.93it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.90it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.88it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.87it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.95it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.95it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.96it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 47.01it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 47.01it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.93it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.90it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.86it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.88it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.94it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.98it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.98it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.97it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.92it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.87it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.82it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.88it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.90it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.92it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.95it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.96it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.90it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.88it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.84it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 46.88it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.91it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.93it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.95it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.94it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.93it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.88it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.85it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.86it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.90it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.88it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.91it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.94it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.91it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.91it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.89it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.85it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.86it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.87it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.83it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.87it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.90it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.91it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.92it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.90it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.85it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.87it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.91it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.90it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.87it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.90it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.90it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.92it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.92it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:08<00:00, 46.88it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.89it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 47.01it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 23:44:05,500 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:44:05,500 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:44:05,501 >>   eval_loss               =     0.9675
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:44:05,501 >>   eval_runtime            = 0:00:09.28
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:44:05,501 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:44:05,501 >>   eval_samples_per_second =    374.885
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:44:05,501 >>   eval_steps_per_second   =     46.874
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:44:05,501 >>   perplexity              =     2.6314
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_1', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:35<08:12, 35.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:59<06:15, 28.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:25<05:28, 27.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:48<04:44, 25.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [02:11<04:08, 24.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:35<03:39, 24.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:57<03:09, 23.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [03:21<02:45, 23.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:44<02:21, 23.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [04:05<01:54, 22.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [04:29<01:32, 23.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:51<01:08, 22.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [05:14<00:45, 22.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [05:39<00:23, 23.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:03<00:00, 23.56s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:03<00:00, 24.21s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n', 'Relation : country . Context : In 1994 , , he was named as the winner of the Eurovision Song Contest 2000 . Head Entity : Eurovision Song Contest 2000 , Tail Entity : France .\n']
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 137, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 174, 'raw': 288}
{'target': 600, 'success': 190, 'raw': 320}
{'target': 600, 'success': 211, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 252, 'raw': 416}
{'target': 600, 'success': 274, 'raw': 448}
{'target': 600, 'success': 294, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 347, 'raw': 576}
{'target': 600, 'success': 361, 'raw': 608}
{'target': 600, 'success': 388, 'raw': 640}
{'target': 600, 'success': 408, 'raw': 672}
{'target': 600, 'success': 428, 'raw': 704}
{'target': 600, 'success': 443, 'raw': 736}
{'target': 600, 'success': 462, 'raw': 768}
{'target': 600, 'success': 483, 'raw': 800}
{'target': 600, 'success': 501, 'raw': 832}
{'target': 600, 'success': 519, 'raw': 864}
{'target': 600, 'success': 541, 'raw': 896}
{'target': 600, 'success': 562, 'raw': 928}
{'target': 600, 'success': 585, 'raw': 960}
{'target': 600, 'success': 608, 'raw': 992}
{'prompt': 'Relation : country .', 'success_rate': 0.6129032258064516, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 485, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year , the band formed New River Band with two of their members at the end of 2010 , Mikey McLeod ( the lyricist ) and Tim McCarroll ( bass ) . Head Entity : Mikey McNamara , Tail Entity : New River Band .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : member of .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : subsidiary . Context : The CIBN ( CBNI ) , also called CBE ( CBW , CBQ , CBQR , CJAX , CJD , CJEC , CJF , CJFY ) , is a United States National Research Council scientific satellite constellation . Head Entity : CBI , Tail Entity : United States National Research Council .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', "('A', 'subsidiary', '', 'On August 2017 , the company began shipping a new product for children in Europe : A .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Battle of Brackewald', 'field of work', '', 'In 1849 he became a volunteer for the British Army at the Battle of Brackewald in Normandy .')", 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : Later in the year ( 1141â€“1231 ) he met Ferdinand I of Spain and the Duke of Prussia , whom he bore in the name of Christophe , together with Robert I of Belgium . Head Entity : Christophe , Tail Entity : John I of Belgium .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.859375, 'errors': {''}}
["Relation : occupation . Context : On 31 March 2014 , the Romanian Army , under a new President of Romania , Luiz InÃ¡cio Lutcic , announced the departure of Luiz 's second - generation Air Force commander , former Admiral of Romania , Sigmund Kaveliu . Head Entity : GeniÃ§iu Lutcic , Tail Entity : Romanian Army .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 14467
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14567, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.28s/it]Extractor Estimating: 2it [00:19,  8.60s/it]Extractor Estimating: 3it [00:21,  5.31s/it]Extractor Estimating: 4it [00:22,  3.55s/it]Extractor Estimating: 5it [00:22,  2.53s/it]Extractor Estimating: 6it [00:23,  1.93s/it]Extractor Estimating: 7it [00:24,  1.54s/it]Extractor Estimating: 8it [00:25,  1.31s/it]Extractor Estimating: 9it [00:25,  1.14s/it]Extractor Estimating: 10it [00:26,  1.03s/it]Extractor Estimating: 11it [00:27,  1.05it/s]Extractor Estimating: 12it [00:28,  1.07it/s]Extractor Estimating: 13it [00:29,  1.10it/s]Extractor Estimating: 14it [00:29,  1.15it/s]Extractor Estimating: 15it [00:30,  1.17it/s]Extractor Estimating: 16it [00:32,  1.02s/it]Extractor Estimating: 17it [00:32,  1.05it/s]Extractor Estimating: 18it [00:34,  1.27s/it]Extractor Estimating: 19it [00:35,  1.12s/it]Extractor Estimating: 20it [00:36,  1.02s/it]Extractor Estimating: 21it [00:37,  1.05it/s]Extractor Estimating: 22it [00:38,  1.06it/s]Extractor Estimating: 23it [00:39,  1.12it/s]Extractor Estimating: 24it [00:39,  1.15it/s]Extractor Estimating: 25it [00:40,  1.19it/s]Extractor Estimating: 26it [00:41,  1.21it/s]Extractor Estimating: 27it [00:42,  1.13it/s]Extractor Estimating: 28it [00:43,  1.17it/s]Extractor Estimating: 29it [00:44,  1.18it/s]Extractor Estimating: 30it [00:44,  1.16it/s]Extractor Estimating: 31it [00:45,  1.23it/s]Extractor Estimating: 32it [00:46,  1.26it/s]Extractor Estimating: 33it [00:47,  1.22it/s]Extractor Estimating: 34it [00:48,  1.23it/s]Extractor Estimating: 35it [00:48,  1.23it/s]Extractor Estimating: 36it [00:49,  1.23it/s]Extractor Estimating: 37it [00:50,  1.22it/s]Extractor Estimating: 38it [00:51,  1.23it/s]Extractor Estimating: 39it [00:52,  1.22it/s]Extractor Estimating: 40it [00:52,  1.27it/s]Extractor Estimating: 41it [00:53,  1.24it/s]Extractor Estimating: 42it [00:54,  1.20it/s]Extractor Estimating: 43it [00:55,  1.16it/s]Extractor Estimating: 44it [00:56,  1.15it/s]Extractor Estimating: 45it [00:57,  1.19it/s]Extractor Estimating: 46it [00:58,  1.21it/s]Extractor Estimating: 47it [00:58,  1.19it/s]Extractor Estimating: 48it [00:59,  1.24it/s]Extractor Estimating: 49it [01:01,  1.18s/it]Extractor Estimating: 50it [01:02,  1.07s/it]Extractor Estimating: 51it [01:03,  1.01it/s]Extractor Estimating: 52it [01:04,  1.05it/s]Extractor Estimating: 53it [01:04,  1.12it/s]Extractor Estimating: 54it [01:05,  1.14it/s]Extractor Estimating: 55it [01:06,  1.18it/s]Extractor Estimating: 56it [01:07,  1.20it/s]Extractor Estimating: 57it [01:08,  1.20it/s]Extractor Estimating: 58it [01:08,  1.21it/s]Extractor Estimating: 59it [01:09,  1.24it/s]Extractor Estimating: 60it [01:10,  1.22it/s]Extractor Estimating: 61it [01:11,  1.20it/s]Extractor Estimating: 62it [01:12,  1.18it/s]Extractor Estimating: 63it [01:13,  1.22it/s]Extractor Estimating: 64it [01:13,  1.21it/s]Extractor Estimating: 65it [01:14,  1.23it/s]Extractor Estimating: 66it [01:15,  1.25it/s]Extractor Estimating: 67it [01:16,  1.25it/s]Extractor Estimating: 68it [01:17,  1.23it/s]Extractor Estimating: 69it [01:17,  1.20it/s]Extractor Estimating: 70it [01:18,  1.21it/s]Extractor Estimating: 71it [01:19,  1.22it/s]Extractor Estimating: 72it [01:20,  1.23it/s]Extractor Estimating: 73it [01:21,  1.20it/s]Extractor Estimating: 74it [01:22,  1.23it/s]Extractor Estimating: 75it [01:22,  1.22it/s]Extractor Estimating: 76it [01:23,  1.25it/s]Extractor Estimating: 77it [01:24,  1.25it/s]Extractor Estimating: 78it [01:25,  1.24it/s]Extractor Estimating: 79it [01:26,  1.23it/s]Extractor Estimating: 80it [01:26,  1.26it/s]Extractor Estimating: 81it [01:27,  1.26it/s]Extractor Estimating: 82it [01:28,  1.23it/s]Extractor Estimating: 83it [01:29,  1.26it/s]Extractor Estimating: 84it [01:30,  1.26it/s]Extractor Estimating: 85it [01:30,  1.25it/s]Extractor Estimating: 86it [01:31,  1.24it/s]Extractor Estimating: 87it [01:32,  1.23it/s]Extractor Estimating: 88it [01:33,  1.23it/s]Extractor Estimating: 89it [01:34,  1.24it/s]Extractor Estimating: 90it [01:34,  1.23it/s]Extractor Estimating: 91it [01:35,  1.25it/s]Extractor Estimating: 92it [01:36,  1.25it/s]Extractor Estimating: 93it [01:37,  1.27it/s]Extractor Estimating: 94it [01:38,  1.24it/s]Extractor Estimating: 95it [01:38,  1.26it/s]Extractor Estimating: 96it [01:39,  1.26it/s]Extractor Estimating: 97it [01:40,  1.25it/s]Extractor Estimating: 98it [01:41,  1.24it/s]Extractor Estimating: 99it [01:42,  1.26it/s]Extractor Estimating: 100it [01:42,  1.25it/s]Extractor Estimating: 101it [01:43,  1.19it/s]Extractor Estimating: 102it [01:44,  1.25it/s]Extractor Estimating: 103it [01:45,  1.28it/s]Extractor Estimating: 104it [01:45,  1.29it/s]Extractor Estimating: 105it [01:46,  1.23it/s]Extractor Estimating: 106it [01:47,  1.23it/s]Extractor Estimating: 107it [01:48,  1.21it/s]Extractor Estimating: 108it [01:49,  1.24it/s]Extractor Estimating: 109it [01:50,  1.24it/s]Extractor Estimating: 110it [01:51,  1.21it/s]Extractor Estimating: 111it [01:51,  1.24it/s]Extractor Estimating: 112it [01:52,  1.18it/s]Extractor Estimating: 113it [01:53,  1.17it/s]Extractor Estimating: 114it [01:54,  1.18it/s]Extractor Estimating: 115it [01:55,  1.21it/s]Extractor Estimating: 116it [01:55,  1.24it/s]Extractor Estimating: 117it [01:56,  1.16it/s]Extractor Estimating: 118it [01:57,  1.21it/s]Extractor Estimating: 119it [01:58,  1.21it/s]Extractor Estimating: 120it [01:59,  1.22it/s]Extractor Estimating: 121it [02:00,  1.27it/s]Extractor Estimating: 122it [02:00,  1.22it/s]Extractor Estimating: 123it [02:01,  1.27it/s]Extractor Estimating: 124it [02:02,  1.24it/s]Extractor Estimating: 125it [02:03,  1.22it/s]Extractor Estimating: 126it [02:04,  1.28it/s]Extractor Estimating: 127it [02:04,  1.26it/s]Extractor Estimating: 128it [02:05,  1.28it/s]Extractor Estimating: 129it [02:06,  1.28it/s]Extractor Estimating: 130it [02:07,  1.35it/s]Extractor Estimating: 131it [02:07,  1.32it/s]Extractor Estimating: 132it [02:08,  1.32it/s]Extractor Estimating: 133it [02:09,  1.32it/s]Extractor Estimating: 134it [02:10,  1.30it/s]Extractor Estimating: 135it [02:10,  1.32it/s]Extractor Estimating: 136it [02:11,  1.33it/s]Extractor Estimating: 137it [02:12,  1.24it/s]Extractor Estimating: 138it [02:13,  1.25it/s]Extractor Estimating: 139it [02:14,  1.30it/s]Extractor Estimating: 140it [02:14,  1.31it/s]Extractor Estimating: 141it [02:15,  1.17it/s]Extractor Estimating: 142it [02:16,  1.20it/s]Extractor Estimating: 143it [02:17,  1.25it/s]Extractor Estimating: 144it [02:18,  1.26it/s]Extractor Estimating: 145it [02:18,  1.24it/s]Extractor Estimating: 146it [02:19,  1.29it/s]Extractor Estimating: 147it [02:20,  1.28it/s]Extractor Estimating: 148it [02:21,  1.26it/s]Extractor Estimating: 149it [02:22,  1.23it/s]Extractor Estimating: 150it [02:22,  1.22it/s]Extractor Estimating: 151it [02:23,  1.27it/s]Extractor Estimating: 152it [02:24,  1.26it/s]Extractor Estimating: 153it [02:25,  1.27it/s]Extractor Estimating: 154it [02:26,  1.27it/s]Extractor Estimating: 155it [02:26,  1.27it/s]Extractor Estimating: 156it [02:27,  1.27it/s]Extractor Estimating: 157it [02:28,  1.23it/s]Extractor Estimating: 158it [02:29,  1.24it/s]Extractor Estimating: 159it [02:30,  1.23it/s]Extractor Estimating: 160it [02:30,  1.25it/s]Extractor Estimating: 161it [02:31,  1.25it/s]Extractor Estimating: 162it [02:32,  1.25it/s]Extractor Estimating: 163it [02:33,  1.25it/s]Extractor Estimating: 164it [02:34,  1.28it/s]Extractor Estimating: 165it [02:34,  1.30it/s]Extractor Estimating: 166it [02:35,  1.28it/s]Extractor Estimating: 167it [02:36,  1.25it/s]Extractor Estimating: 168it [02:37,  1.26it/s]Extractor Estimating: 169it [02:37,  1.27it/s]Extractor Estimating: 170it [02:38,  1.33it/s]Extractor Estimating: 171it [02:39,  1.31it/s]Extractor Estimating: 172it [02:40,  1.25it/s]Extractor Estimating: 173it [02:41,  1.22it/s]Extractor Estimating: 174it [02:41,  1.22it/s]Extractor Estimating: 175it [02:42,  1.23it/s]Extractor Estimating: 176it [02:43,  1.21it/s]Extractor Estimating: 177it [02:44,  1.22it/s]Extractor Estimating: 178it [02:45,  1.24it/s]Extractor Estimating: 179it [02:45,  1.25it/s]Extractor Estimating: 180it [02:46,  1.25it/s]Extractor Estimating: 181it [02:47,  1.28it/s]Extractor Estimating: 182it [02:48,  1.27it/s]Extractor Estimating: 183it [02:49,  1.26it/s]Extractor Estimating: 184it [02:49,  1.26it/s]Extractor Estimating: 185it [02:50,  1.27it/s]Extractor Estimating: 186it [02:51,  1.27it/s]Extractor Estimating: 187it [02:52,  1.24it/s]Extractor Estimating: 188it [02:53,  1.25it/s]Extractor Estimating: 189it [02:55,  1.21s/it]Extractor Estimating: 190it [02:56,  1.07s/it]Extractor Estimating: 191it [02:56,  1.03it/s]Extractor Estimating: 192it [02:57,  1.07it/s]Extractor Estimating: 193it [02:58,  1.12it/s]Extractor Estimating: 194it [02:59,  1.14it/s]Extractor Estimating: 195it [03:00,  1.18it/s]Extractor Estimating: 196it [03:00,  1.22it/s]Extractor Estimating: 197it [03:01,  1.20it/s]Extractor Estimating: 198it [03:02,  1.22it/s]Extractor Estimating: 199it [03:03,  1.26it/s]Extractor Estimating: 200it [03:04,  1.24it/s]Extractor Estimating: 201it [03:05,  1.14it/s]Extractor Estimating: 202it [03:05,  1.17it/s]Extractor Estimating: 203it [03:06,  1.18it/s]Extractor Estimating: 204it [03:07,  1.19it/s]Extractor Estimating: 205it [03:08,  1.22it/s]Extractor Estimating: 206it [03:09,  1.21it/s]Extractor Estimating: 207it [03:09,  1.25it/s]Extractor Estimating: 208it [03:10,  1.18it/s]Extractor Estimating: 209it [03:11,  1.17it/s]Extractor Estimating: 210it [03:12,  1.20it/s]Extractor Estimating: 211it [03:13,  1.19it/s]Extractor Estimating: 212it [03:14,  1.18it/s]Extractor Estimating: 213it [03:15,  1.16it/s]Extractor Estimating: 214it [03:15,  1.17it/s]Extractor Estimating: 215it [03:16,  1.20it/s]Extractor Estimating: 216it [03:17,  1.15it/s]Extractor Estimating: 217it [03:18,  1.15it/s]Extractor Estimating: 218it [03:19,  1.19it/s]Extractor Estimating: 219it [03:20,  1.17it/s]Extractor Estimating: 220it [03:21,  1.20it/s]Extractor Estimating: 221it [03:21,  1.15it/s]Extractor Estimating: 222it [03:22,  1.15it/s]Extractor Estimating: 223it [03:23,  1.16it/s]Extractor Estimating: 224it [03:24,  1.17it/s]Extractor Estimating: 225it [03:25,  1.19it/s]Extractor Estimating: 226it [03:26,  1.17it/s]Extractor Estimating: 227it [03:26,  1.23it/s]Extractor Estimating: 228it [03:27,  1.27it/s]Extractor Estimating: 229it [03:28,  1.29it/s]Extractor Estimating: 230it [03:29,  1.29it/s]Extractor Estimating: 231it [03:29,  1.32it/s]Extractor Estimating: 232it [03:30,  1.36it/s]Extractor Estimating: 233it [03:31,  1.39it/s]Extractor Estimating: 234it [03:31,  1.38it/s]Extractor Estimating: 235it [03:32,  1.35it/s]Extractor Estimating: 236it [03:33,  1.34it/s]Extractor Estimating: 237it [03:34,  1.34it/s]Extractor Estimating: 238it [03:35,  1.32it/s]Extractor Estimating: 239it [03:35,  1.31it/s]Extractor Estimating: 240it [03:36,  1.32it/s]Extractor Estimating: 241it [03:37,  1.34it/s]Extractor Estimating: 242it [03:38,  1.33it/s]Extractor Estimating: 243it [03:38,  1.29it/s]Extractor Estimating: 244it [03:39,  1.28it/s]Extractor Estimating: 245it [03:40,  1.30it/s]Extractor Estimating: 246it [03:41,  1.33it/s]Extractor Estimating: 247it [03:41,  1.32it/s]Extractor Estimating: 248it [03:42,  1.29it/s]Extractor Estimating: 249it [03:43,  1.30it/s]Extractor Estimating: 250it [03:44,  1.32it/s]Extractor Estimating: 251it [03:45,  1.23it/s]Extractor Estimating: 252it [03:45,  1.27it/s]Extractor Estimating: 253it [03:46,  1.28it/s]Extractor Estimating: 254it [03:47,  1.21it/s]Extractor Estimating: 255it [03:48,  1.25it/s]Extractor Estimating: 256it [03:48,  1.33it/s]Extractor Estimating: 257it [03:49,  1.31it/s]Extractor Estimating: 258it [03:50,  1.22it/s]Extractor Estimating: 259it [03:51,  1.26it/s]Extractor Estimating: 260it [03:52,  1.28it/s]Extractor Estimating: 261it [03:53,  1.24it/s]Extractor Estimating: 262it [03:53,  1.22it/s]Extractor Estimating: 263it [03:54,  1.23it/s]Extractor Estimating: 264it [03:55,  1.25it/s]Extractor Estimating: 265it [03:56,  1.21it/s]Extractor Estimating: 266it [03:57,  1.23it/s]Extractor Estimating: 267it [03:58,  1.21it/s]Extractor Estimating: 268it [03:58,  1.25it/s]Extractor Estimating: 269it [03:59,  1.19it/s]Extractor Estimating: 270it [04:00,  1.18it/s]Extractor Estimating: 271it [04:01,  1.19it/s]Extractor Estimating: 272it [04:02,  1.19it/s]Extractor Estimating: 273it [04:02,  1.21it/s]Extractor Estimating: 274it [04:03,  1.20it/s]Extractor Estimating: 275it [04:04,  1.23it/s]Extractor Estimating: 276it [04:05,  1.28it/s]Extractor Estimating: 277it [04:06,  1.27it/s]Extractor Estimating: 278it [04:06,  1.33it/s]Extractor Estimating: 279it [04:07,  1.31it/s]Extractor Estimating: 280it [04:08,  1.31it/s]Extractor Estimating: 281it [04:09,  1.32it/s]Extractor Estimating: 282it [04:09,  1.35it/s]Extractor Estimating: 283it [04:10,  1.33it/s]Extractor Estimating: 284it [04:11,  1.31it/s]Extractor Estimating: 285it [04:12,  1.33it/s]Extractor Estimating: 286it [04:12,  1.33it/s]Extractor Estimating: 287it [04:13,  1.32it/s]Extractor Estimating: 288it [04:14,  1.29it/s]Extractor Estimating: 289it [04:15,  1.31it/s]Extractor Estimating: 290it [04:15,  1.31it/s]Extractor Estimating: 291it [04:16,  1.35it/s]Extractor Estimating: 292it [04:17,  1.31it/s]Extractor Estimating: 293it [04:18,  1.33it/s]Extractor Estimating: 294it [04:18,  1.31it/s]Extractor Estimating: 295it [04:19,  1.30it/s]Extractor Estimating: 296it [04:20,  1.34it/s]Extractor Estimating: 297it [04:21,  1.35it/s]Extractor Estimating: 298it [04:21,  1.36it/s]Extractor Estimating: 299it [04:22,  1.39it/s]Extractor Estimating: 300it [04:23,  1.40it/s]Extractor Estimating: 301it [04:24,  1.33it/s]Extractor Estimating: 302it [04:24,  1.27it/s]Extractor Estimating: 303it [04:25,  1.27it/s]Extractor Estimating: 304it [04:26,  1.32it/s]Extractor Estimating: 305it [04:27,  1.31it/s]Extractor Estimating: 306it [04:27,  1.34it/s]Extractor Estimating: 307it [04:28,  1.35it/s]Extractor Estimating: 308it [04:29,  1.41it/s]Extractor Estimating: 309it [04:29,  1.40it/s]Extractor Estimating: 310it [04:30,  1.35it/s]Extractor Estimating: 311it [04:31,  1.28it/s]Extractor Estimating: 312it [04:32,  1.33it/s]Extractor Estimating: 313it [04:33,  1.30it/s]Extractor Estimating: 314it [04:33,  1.28it/s]Extractor Estimating: 315it [04:34,  1.17it/s]Extractor Estimating: 316it [04:35,  1.21it/s]Extractor Estimating: 317it [04:36,  1.25it/s]Extractor Estimating: 318it [04:37,  1.24it/s]Extractor Estimating: 319it [04:38,  1.27it/s]Extractor Estimating: 320it [04:38,  1.23it/s]Extractor Estimating: 321it [04:39,  1.28it/s]Extractor Estimating: 322it [04:40,  1.27it/s]Extractor Estimating: 323it [04:41,  1.29it/s]Extractor Estimating: 324it [04:41,  1.30it/s]Extractor Estimating: 325it [04:42,  1.31it/s]Extractor Estimating: 326it [04:43,  1.19it/s]Extractor Estimating: 327it [04:44,  1.22it/s]Extractor Estimating: 328it [04:45,  1.21it/s]Extractor Estimating: 329it [04:46,  1.24it/s]Extractor Estimating: 330it [04:46,  1.24it/s]Extractor Estimating: 331it [04:47,  1.26it/s]Extractor Estimating: 332it [04:48,  1.25it/s]Extractor Estimating: 333it [04:49,  1.24it/s]Extractor Estimating: 334it [04:50,  1.26it/s]Extractor Estimating: 335it [04:50,  1.23it/s]Extractor Estimating: 336it [04:51,  1.25it/s]Extractor Estimating: 337it [04:52,  1.27it/s]Extractor Estimating: 338it [04:53,  1.25it/s]Extractor Estimating: 339it [04:54,  1.26it/s]Extractor Estimating: 340it [04:54,  1.28it/s]Extractor Estimating: 341it [04:55,  1.22it/s]Extractor Estimating: 342it [04:56,  1.24it/s]Extractor Estimating: 343it [04:57,  1.15it/s]Extractor Estimating: 344it [04:58,  1.17it/s]Extractor Estimating: 345it [04:59,  1.21it/s]Extractor Estimating: 346it [04:59,  1.17it/s]Extractor Estimating: 347it [05:00,  1.13it/s]Extractor Estimating: 348it [05:01,  1.12it/s]Extractor Estimating: 349it [05:02,  1.13it/s]Extractor Estimating: 350it [05:03,  1.17it/s]Extractor Estimating: 351it [05:04,  1.19it/s]Extractor Estimating: 352it [05:05,  1.23it/s]Extractor Estimating: 353it [05:05,  1.26it/s]Extractor Estimating: 354it [05:06,  1.30it/s]Extractor Estimating: 355it [05:07,  1.32it/s]Extractor Estimating: 356it [05:08,  1.28it/s]Extractor Estimating: 357it [05:08,  1.29it/s]Extractor Estimating: 358it [05:09,  1.28it/s]Extractor Estimating: 359it [05:10,  1.34it/s]Extractor Estimating: 360it [05:11,  1.34it/s]Extractor Estimating: 361it [05:11,  1.33it/s]Extractor Estimating: 362it [05:12,  1.30it/s]Extractor Estimating: 363it [05:13,  1.34it/s]Extractor Estimating: 364it [05:14,  1.29it/s]Extractor Estimating: 365it [05:15,  1.24it/s]Extractor Estimating: 366it [05:15,  1.25it/s]Extractor Estimating: 367it [05:16,  1.26it/s]Extractor Estimating: 368it [05:17,  1.24it/s]Extractor Estimating: 369it [05:18,  1.25it/s]Extractor Estimating: 370it [05:18,  1.27it/s]Extractor Estimating: 371it [05:19,  1.26it/s]Extractor Estimating: 372it [05:20,  1.29it/s]Extractor Estimating: 373it [05:21,  1.26it/s]Extractor Estimating: 374it [05:22,  1.24it/s]Extractor Estimating: 375it [05:22,  1.32it/s]Extractor Estimating: 375it [05:22,  1.16it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7697 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 27856
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27956, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_10_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27956, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.657, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.380, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.316, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 79, avg_time 1.334, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 179, avg_time 1.319, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 279, avg_time 2.743, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 58, avg_time 1.327, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 158, avg_time 1.323, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 258, avg_time 1.328, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 37, avg_time 1.326, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 137, avg_time 2.742, loss:nan
g_step 1200, step 237, avg_time 1.319, loss:nan
g_step 1300, step 16, avg_time 1.330, loss:nan
g_step 1400, step 116, avg_time 1.319, loss:nan
g_step 1500, step 216, avg_time 1.347, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 316, avg_time 2.731, loss:nan
g_step 1700, step 95, avg_time 1.318, loss:nan
g_step 1800, step 195, avg_time 1.327, loss:nan
g_step 1900, step 295, avg_time 1.340, loss:nan
g_step 2000, step 74, avg_time 1.342, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 174, avg_time 2.729, loss:nan
g_step 2200, step 274, avg_time 1.328, loss:nan
g_step 2300, step 53, avg_time 1.336, loss:nan
g_step 2400, step 153, avg_time 1.330, loss:nan
g_step 2500, step 253, avg_time 1.336, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.724, loss:nan
g_step 2700, step 132, avg_time 1.336, loss:nan
g_step 2800, step 232, avg_time 1.337, loss:nan
g_step 2900, step 11, avg_time 1.341, loss:nan
g_step 3000, step 111, avg_time 1.314, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 211, avg_time 2.748, loss:nan
g_step 3200, step 311, avg_time 1.335, loss:nan
g_step 3300, step 90, avg_time 1.334, loss:nan
g_step 3400, step 190, avg_time 1.327, loss:nan
g_step 3500, step 290, avg_time 1.339, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 69, avg_time 2.712, loss:nan
g_step 3700, step 169, avg_time 1.338, loss:nan
g_step 3800, step 269, avg_time 1.321, loss:nan
g_step 3900, step 48, avg_time 1.336, loss:nan
g_step 4000, step 148, avg_time 1.346, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 248, avg_time 2.726, loss:nan
g_step 4200, step 27, avg_time 1.328, loss:nan
g_step 4300, step 127, avg_time 1.320, loss:nan
g_step 4400, step 227, avg_time 1.338, loss:nan
g_step 4500, step 6, avg_time 1.348, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 106, avg_time 2.718, loss:nan
g_step 4700, step 206, avg_time 1.342, loss:nan
g_step 4800, step 306, avg_time 1.320, loss:nan
g_step 4900, step 85, avg_time 1.343, loss:nan
g_step 5000, step 185, avg_time 1.340, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 285, avg_time 2.726, loss:nan
g_step 5200, step 64, avg_time 1.313, loss:nan
g_step 5300, step 164, avg_time 1.338, loss:nan
g_step 5400, step 264, avg_time 1.326, loss:nan
g_step 5500, step 43, avg_time 1.324, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 143, avg_time 2.745, loss:nan
g_step 5700, step 243, avg_time 1.339, loss:nan
g_step 5800, step 22, avg_time 1.332, loss:nan
g_step 5900, step 122, avg_time 1.347, loss:nan
g_step 6000, step 222, avg_time 1.333, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 1, avg_time 2.729, loss:nan
g_step 6200, step 101, avg_time 1.332, loss:nan
g_step 6300, step 201, avg_time 1.335, loss:nan
g_step 6400, step 301, avg_time 1.335, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 02:48:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 02:48:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_02-48-18_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 02:48:19 - WARNING - datasets.builder -   Using custom data configuration default-fd38aa59ef93ac4a
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-fd38aa59ef93ac4a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 02:48:19,400 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:48:19,401 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:48:19,401 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:48:19,402 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:48:19,410 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:48:19,413 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:48:19,413 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:48:19,413 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:48:19,413 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:48:19,413 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:48:19,413 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 02:48:19,520 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:48:22,580 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 02:48:22,588 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-fd38aa59ef93ac4a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 02:48:22 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x147517ba1290> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  2.95ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.78ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.11ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  4.31ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.40ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  3.81ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.04ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.55ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.18ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  4.05ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.35ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.49ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.66ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.09ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:01,  6.91ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:00,  7.62ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  7.87ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  8.06ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  8.31ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:00<00:00,  8.41ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00,  8.42ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.49ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.86ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  7.80ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  8.01ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  9.13ba/s]
[INFO|trainer.py:414] 2023-08-28 02:48:27,108 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 02:48:27,126 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 02:48:27,127 >>   Num examples = 7740
[INFO|trainer.py:1149] 2023-08-28 02:48:27,127 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 02:48:27,127 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 02:48:27,127 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 02:48:27,127 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 02:48:27,127 >>   Total optimization steps = 605
  0%|          | 0/605 [00:00<?, ?it/s]  0%|          | 1/605 [00:00<02:53,  3.48it/s]  0%|          | 2/605 [00:00<02:48,  3.57it/s]  0%|          | 3/605 [00:00<02:46,  3.61it/s]  1%|          | 4/605 [00:01<02:45,  3.62it/s]  1%|          | 5/605 [00:01<02:44,  3.64it/s]  1%|          | 6/605 [00:01<02:44,  3.65it/s]  1%|          | 7/605 [00:01<02:45,  3.61it/s]  1%|â–         | 8/605 [00:02<02:44,  3.62it/s]  1%|â–         | 9/605 [00:02<02:44,  3.63it/s]  2%|â–         | 10/605 [00:02<02:43,  3.64it/s]  2%|â–         | 11/605 [00:03<02:42,  3.65it/s]  2%|â–         | 12/605 [00:03<02:42,  3.65it/s]  2%|â–         | 13/605 [00:03<02:41,  3.66it/s]  2%|â–         | 14/605 [00:03<02:41,  3.66it/s]  2%|â–         | 15/605 [00:04<02:41,  3.66it/s]  3%|â–Ž         | 16/605 [00:04<02:40,  3.66it/s]  3%|â–Ž         | 17/605 [00:04<02:40,  3.66it/s]  3%|â–Ž         | 18/605 [00:05<03:10,  3.08it/s]  3%|â–Ž         | 19/605 [00:05<03:01,  3.24it/s]  3%|â–Ž         | 20/605 [00:05<02:54,  3.35it/s]  3%|â–Ž         | 21/605 [00:05<02:49,  3.44it/s]  4%|â–Ž         | 22/605 [00:06<02:46,  3.50it/s]  4%|â–         | 23/605 [00:06<02:43,  3.55it/s]  4%|â–         | 24/605 [00:06<02:42,  3.58it/s]  4%|â–         | 25/605 [00:07<02:40,  3.61it/s]  4%|â–         | 26/605 [00:07<02:39,  3.62it/s]  4%|â–         | 27/605 [00:07<02:39,  3.63it/s]  5%|â–         | 28/605 [00:07<02:38,  3.64it/s]  5%|â–         | 29/605 [00:08<02:38,  3.64it/s]  5%|â–         | 30/605 [00:08<02:37,  3.65it/s]  5%|â–Œ         | 31/605 [00:08<02:37,  3.65it/s]  5%|â–Œ         | 32/605 [00:08<02:36,  3.65it/s]  5%|â–Œ         | 33/605 [00:09<02:36,  3.66it/s]  6%|â–Œ         | 34/605 [00:09<02:36,  3.66it/s]  6%|â–Œ         | 35/605 [00:09<02:35,  3.66it/s]  6%|â–Œ         | 36/605 [00:10<02:35,  3.66it/s]  6%|â–Œ         | 37/605 [00:10<02:35,  3.66it/s]  6%|â–‹         | 38/605 [00:10<02:34,  3.66it/s]  6%|â–‹         | 39/605 [00:10<02:34,  3.66it/s]  7%|â–‹         | 40/605 [00:11<02:34,  3.65it/s]  7%|â–‹         | 41/605 [00:11<02:34,  3.65it/s]  7%|â–‹         | 42/605 [00:11<02:34,  3.65it/s]  7%|â–‹         | 43/605 [00:11<02:33,  3.65it/s]  7%|â–‹         | 44/605 [00:12<02:33,  3.66it/s]  7%|â–‹         | 45/605 [00:12<02:33,  3.66it/s]  8%|â–Š         | 46/605 [00:12<02:32,  3.66it/s]  8%|â–Š         | 47/605 [00:13<02:32,  3.66it/s]  8%|â–Š         | 48/605 [00:13<02:32,  3.66it/s]  8%|â–Š         | 49/605 [00:13<02:32,  3.65it/s]  8%|â–Š         | 50/605 [00:13<02:31,  3.66it/s]  8%|â–Š         | 51/605 [00:14<02:31,  3.65it/s]  9%|â–Š         | 52/605 [00:14<02:31,  3.65it/s]  9%|â–‰         | 53/605 [00:14<02:31,  3.65it/s]  9%|â–‰         | 54/605 [00:14<02:30,  3.65it/s]  9%|â–‰         | 55/605 [00:15<02:30,  3.65it/s]  9%|â–‰         | 56/605 [00:15<02:30,  3.66it/s]  9%|â–‰         | 57/605 [00:15<02:29,  3.66it/s] 10%|â–‰         | 58/605 [00:16<02:29,  3.66it/s] 10%|â–‰         | 59/605 [00:16<02:29,  3.66it/s] 10%|â–‰         | 60/605 [00:16<02:29,  3.66it/s] 10%|â–ˆ         | 61/605 [00:16<02:28,  3.66it/s] 10%|â–ˆ         | 62/605 [00:17<02:30,  3.61it/s] 10%|â–ˆ         | 63/605 [00:17<02:29,  3.63it/s] 11%|â–ˆ         | 64/605 [00:17<02:28,  3.64it/s] 11%|â–ˆ         | 65/605 [00:17<02:28,  3.64it/s] 11%|â–ˆ         | 66/605 [00:18<02:27,  3.65it/s] 11%|â–ˆ         | 67/605 [00:18<02:27,  3.65it/s] 11%|â–ˆ         | 68/605 [00:18<02:27,  3.65it/s] 11%|â–ˆâ–        | 69/605 [00:19<02:26,  3.65it/s] 12%|â–ˆâ–        | 70/605 [00:19<02:26,  3.65it/s] 12%|â–ˆâ–        | 71/605 [00:19<02:26,  3.65it/s] 12%|â–ˆâ–        | 72/605 [00:19<02:26,  3.65it/s] 12%|â–ˆâ–        | 73/605 [00:20<02:26,  3.63it/s] 12%|â–ˆâ–        | 74/605 [00:20<02:26,  3.63it/s] 12%|â–ˆâ–        | 75/605 [00:20<02:25,  3.64it/s] 13%|â–ˆâ–Ž        | 76/605 [00:20<02:25,  3.64it/s] 13%|â–ˆâ–Ž        | 77/605 [00:21<02:25,  3.64it/s] 13%|â–ˆâ–Ž        | 78/605 [00:21<02:24,  3.64it/s] 13%|â–ˆâ–Ž        | 79/605 [00:21<02:24,  3.64it/s] 13%|â–ˆâ–Ž        | 80/605 [00:22<02:24,  3.64it/s] 13%|â–ˆâ–Ž        | 81/605 [00:22<02:23,  3.64it/s] 14%|â–ˆâ–Ž        | 82/605 [00:22<02:23,  3.64it/s] 14%|â–ˆâ–Ž        | 83/605 [00:22<02:23,  3.64it/s] 14%|â–ˆâ–        | 84/605 [00:23<02:23,  3.64it/s] 14%|â–ˆâ–        | 85/605 [00:23<02:22,  3.64it/s] 14%|â–ˆâ–        | 86/605 [00:23<02:22,  3.64it/s] 14%|â–ˆâ–        | 87/605 [00:24<02:22,  3.64it/s] 15%|â–ˆâ–        | 88/605 [00:24<02:21,  3.64it/s] 15%|â–ˆâ–        | 89/605 [00:24<02:21,  3.64it/s] 15%|â–ˆâ–        | 90/605 [00:24<02:21,  3.63it/s] 15%|â–ˆâ–Œ        | 91/605 [00:25<02:21,  3.64it/s] 15%|â–ˆâ–Œ        | 92/605 [00:25<02:21,  3.64it/s] 15%|â–ˆâ–Œ        | 93/605 [00:25<02:20,  3.64it/s] 16%|â–ˆâ–Œ        | 94/605 [00:25<02:20,  3.64it/s] 16%|â–ˆâ–Œ        | 95/605 [00:26<02:20,  3.64it/s] 16%|â–ˆâ–Œ        | 96/605 [00:26<02:19,  3.64it/s] 16%|â–ˆâ–Œ        | 97/605 [00:26<02:19,  3.64it/s] 16%|â–ˆâ–Œ        | 98/605 [00:27<02:19,  3.64it/s] 16%|â–ˆâ–‹        | 99/605 [00:27<02:18,  3.64it/s] 17%|â–ˆâ–‹        | 100/605 [00:27<02:18,  3.64it/s] 17%|â–ˆâ–‹        | 101/605 [00:27<02:18,  3.63it/s] 17%|â–ˆâ–‹        | 102/605 [00:28<02:18,  3.63it/s] 17%|â–ˆâ–‹        | 103/605 [00:28<02:18,  3.64it/s] 17%|â–ˆâ–‹        | 104/605 [00:28<02:17,  3.64it/s] 17%|â–ˆâ–‹        | 105/605 [00:28<02:17,  3.63it/s] 18%|â–ˆâ–Š        | 106/605 [00:29<02:17,  3.64it/s] 18%|â–ˆâ–Š        | 107/605 [00:29<02:16,  3.64it/s] 18%|â–ˆâ–Š        | 108/605 [00:29<02:16,  3.64it/s] 18%|â–ˆâ–Š        | 109/605 [00:30<02:16,  3.64it/s] 18%|â–ˆâ–Š        | 110/605 [00:30<02:15,  3.64it/s] 18%|â–ˆâ–Š        | 111/605 [00:30<02:15,  3.64it/s] 19%|â–ˆâ–Š        | 112/605 [00:30<02:15,  3.63it/s] 19%|â–ˆâ–Š        | 113/605 [00:31<02:15,  3.63it/s] 19%|â–ˆâ–‰        | 114/605 [00:31<02:15,  3.63it/s] 19%|â–ˆâ–‰        | 115/605 [00:31<02:14,  3.64it/s] 19%|â–ˆâ–‰        | 116/605 [00:31<02:14,  3.64it/s] 19%|â–ˆâ–‰        | 117/605 [00:32<02:14,  3.64it/s] 20%|â–ˆâ–‰        | 118/605 [00:32<02:13,  3.64it/s] 20%|â–ˆâ–‰        | 119/605 [00:32<02:13,  3.64it/s] 20%|â–ˆâ–‰        | 120/605 [00:33<02:13,  3.64it/s] 20%|â–ˆâ–ˆ        | 121/605 [00:33<02:12,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 02:49:00,492 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:49:00,492 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 02:49:00,492 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.32it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.41it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.78it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.15it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.75it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.51it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.29it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 47.00it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.91it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.88it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.79it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.81it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.80it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.83it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.89it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.93it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.80it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.80it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.80it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.75it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.77it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.78it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.76it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.79it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.79it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.81it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.82it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.79it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.79it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.79it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.76it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.77it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.78it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.76it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.80it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.79it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.77it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.81it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.81it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.76it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.79it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.79it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.74it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.76it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.77it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.74it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.75it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.77it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 46.79it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.79it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.79it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.74it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.77it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.76it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.76it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.78it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.48it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.88it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.86it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.81it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.81it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.81it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.77it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.79it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.79it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.75it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.79it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.79it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.74it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.76it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.77it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.76it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.77it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.76it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.73it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.75it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.73it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.74it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.76it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.71it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.77it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.77it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.73it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.74it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.69it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.70it/s][A                                                 
                                                 [A 20%|â–ˆâ–ˆ        | 121/605 [00:42<02:12,  3.65it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.70it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:49:09,820 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121
[INFO|configuration_utils.py:351] 2023-08-28 02:49:09,839 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:49:12,219 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:49:12,243 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:49:12,256 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 122/605 [00:45<31:15,  3.88s/it] 20%|â–ˆâ–ˆ        | 123/605 [00:45<22:29,  2.80s/it] 20%|â–ˆâ–ˆ        | 124/605 [00:46<16:22,  2.04s/it] 21%|â–ˆâ–ˆ        | 125/605 [00:46<12:05,  1.51s/it] 21%|â–ˆâ–ˆ        | 126/605 [00:46<09:06,  1.14s/it] 21%|â–ˆâ–ˆ        | 127/605 [00:47<07:01,  1.14it/s] 21%|â–ˆâ–ˆ        | 128/605 [00:47<05:33,  1.43it/s] 21%|â–ˆâ–ˆâ–       | 129/605 [00:47<04:31,  1.75it/s] 21%|â–ˆâ–ˆâ–       | 130/605 [00:47<03:49,  2.07it/s] 22%|â–ˆâ–ˆâ–       | 131/605 [00:48<03:19,  2.38it/s] 22%|â–ˆâ–ˆâ–       | 132/605 [00:48<02:58,  2.65it/s] 22%|â–ˆâ–ˆâ–       | 133/605 [00:48<02:43,  2.88it/s] 22%|â–ˆâ–ˆâ–       | 134/605 [00:48<02:33,  3.08it/s] 22%|â–ˆâ–ˆâ–       | 135/605 [00:49<02:25,  3.23it/s] 22%|â–ˆâ–ˆâ–       | 136/605 [00:49<02:20,  3.34it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/605 [00:49<02:17,  3.41it/s] 23%|â–ˆâ–ˆâ–Ž       | 138/605 [00:50<02:14,  3.48it/s] 23%|â–ˆâ–ˆâ–Ž       | 139/605 [00:50<02:12,  3.53it/s] 23%|â–ˆâ–ˆâ–Ž       | 140/605 [00:50<02:10,  3.56it/s] 23%|â–ˆâ–ˆâ–Ž       | 141/605 [00:50<02:09,  3.59it/s] 23%|â–ˆâ–ˆâ–Ž       | 142/605 [00:51<02:13,  3.48it/s] 24%|â–ˆâ–ˆâ–Ž       | 143/605 [00:51<02:13,  3.47it/s] 24%|â–ˆâ–ˆâ–       | 144/605 [00:51<02:11,  3.52it/s] 24%|â–ˆâ–ˆâ–       | 145/605 [00:52<02:09,  3.55it/s] 24%|â–ˆâ–ˆâ–       | 146/605 [00:52<02:08,  3.58it/s] 24%|â–ˆâ–ˆâ–       | 147/605 [00:52<02:07,  3.60it/s] 24%|â–ˆâ–ˆâ–       | 148/605 [00:52<02:06,  3.61it/s] 25%|â–ˆâ–ˆâ–       | 149/605 [00:53<02:06,  3.62it/s] 25%|â–ˆâ–ˆâ–       | 150/605 [00:53<02:05,  3.62it/s] 25%|â–ˆâ–ˆâ–       | 151/605 [00:53<02:05,  3.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 152/605 [00:53<02:04,  3.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 153/605 [00:54<02:04,  3.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 154/605 [00:54<02:04,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 155/605 [00:54<02:03,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 156/605 [00:55<02:03,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 157/605 [00:55<02:03,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 158/605 [00:55<02:03,  3.63it/s] 26%|â–ˆâ–ˆâ–‹       | 159/605 [00:55<02:02,  3.63it/s] 26%|â–ˆâ–ˆâ–‹       | 160/605 [00:56<02:02,  3.63it/s] 27%|â–ˆâ–ˆâ–‹       | 161/605 [00:56<02:02,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 162/605 [00:56<02:01,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 163/605 [00:56<02:01,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 164/605 [00:57<02:01,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 165/605 [00:57<02:00,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 166/605 [00:57<02:00,  3.64it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|â–ˆâ–ˆâ–Š       | 167/605 [00:58<02:03,  3.54it/s] 28%|â–ˆâ–ˆâ–Š       | 168/605 [00:58<02:03,  3.55it/s] 28%|â–ˆâ–ˆâ–Š       | 169/605 [00:58<02:02,  3.57it/s] 28%|â–ˆâ–ˆâ–Š       | 170/605 [00:58<02:01,  3.59it/s] 28%|â–ˆâ–ˆâ–Š       | 171/605 [00:59<02:00,  3.60it/s] 28%|â–ˆâ–ˆâ–Š       | 172/605 [00:59<01:59,  3.61it/s] 29%|â–ˆâ–ˆâ–Š       | 173/605 [00:59<01:59,  3.62it/s] 29%|â–ˆâ–ˆâ–‰       | 174/605 [01:00<01:58,  3.63it/s] 29%|â–ˆâ–ˆâ–‰       | 175/605 [01:00<01:58,  3.63it/s] 29%|â–ˆâ–ˆâ–‰       | 176/605 [01:00<01:58,  3.63it/s] 29%|â–ˆâ–ˆâ–‰       | 177/605 [01:00<01:57,  3.63it/s] 29%|â–ˆâ–ˆâ–‰       | 178/605 [01:01<01:57,  3.64it/s] 30%|â–ˆâ–ˆâ–‰       | 179/605 [01:01<01:57,  3.64it/s] 30%|â–ˆâ–ˆâ–‰       | 180/605 [01:01<01:57,  3.62it/s] 30%|â–ˆâ–ˆâ–‰       | 181/605 [01:01<01:57,  3.62it/s] 30%|â–ˆâ–ˆâ–ˆ       | 182/605 [01:02<01:56,  3.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 183/605 [01:02<01:56,  3.63it/s] 30%|â–ˆâ–ˆâ–ˆ       | 184/605 [01:02<01:55,  3.63it/s] 31%|â–ˆâ–ˆâ–ˆ       | 185/605 [01:03<01:55,  3.63it/s] 31%|â–ˆâ–ˆâ–ˆ       | 186/605 [01:03<01:55,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 187/605 [01:03<01:54,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 188/605 [01:03<01:54,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 189/605 [01:04<01:54,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 190/605 [01:04<01:54,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 191/605 [01:04<01:54,  3.62it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 192/605 [01:04<01:53,  3.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 193/605 [01:05<01:53,  3.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 194/605 [01:05<01:53,  3.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 195/605 [01:05<01:52,  3.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 196/605 [01:06<01:52,  3.63it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/605 [01:06<01:52,  3.63it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 198/605 [01:06<01:51,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 199/605 [01:06<01:51,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 200/605 [01:07<01:51,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 201/605 [01:07<01:51,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 202/605 [01:07<01:51,  3.61it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 203/605 [01:08<01:51,  3.62it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 204/605 [01:08<01:50,  3.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 205/605 [01:08<01:50,  3.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 206/605 [01:08<01:49,  3.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 207/605 [01:09<01:49,  3.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 208/605 [01:09<01:49,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 209/605 [01:09<01:48,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 210/605 [01:09<01:48,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 211/605 [01:10<01:48,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/605 [01:10<01:48,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 213/605 [01:10<01:48,  3.62it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 214/605 [01:11<01:47,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 215/605 [01:11<01:47,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 216/605 [01:11<01:47,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 217/605 [01:11<01:46,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 218/605 [01:12<01:46,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 219/605 [01:12<01:46,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 220/605 [01:12<01:45,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 221/605 [01:12<01:45,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 222/605 [01:13<01:45,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 223/605 [01:13<01:45,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 224/605 [01:13<01:45,  3.62it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 225/605 [01:14<01:44,  3.63it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 226/605 [01:14<01:44,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 227/605 [01:14<01:44,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 228/605 [01:14<01:43,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 229/605 [01:15<01:43,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 230/605 [01:15<01:43,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 231/605 [01:15<01:42,  3.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 232/605 [01:16<01:42,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 233/605 [01:16<01:42,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 234/605 [01:16<01:42,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 235/605 [01:16<01:41,  3.63it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 236/605 [01:17<01:41,  3.63it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 237/605 [01:17<01:41,  3.63it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 238/605 [01:17<01:40,  3.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 239/605 [01:17<01:40,  3.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 240/605 [01:18<01:40,  3.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 241/605 [01:18<01:40,  3.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 242/605 [01:18<01:38,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 02:49:45,878 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:49:45,878 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 02:49:45,878 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3003, 'eval_samples_per_second': 374.073, 'eval_steps_per_second': 46.773, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.07it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.24it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.57it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.91it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.47it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.23it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.07it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.93it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.86it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.81it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.76it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.74it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.71it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.72it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.72it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.55it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.63it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.66it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.65it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.68it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.69it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.67it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.69it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.69it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.69it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.70it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.67it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.69it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.70it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.68it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.69it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.70it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.66it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.68it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.68it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.66it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.68it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.68it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.68it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.70it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.65it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.59it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.55it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.54it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.59it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.63it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.62it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.65it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.64it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.65it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.68it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.64it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.65it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.67it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.65it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.67it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.69it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.67it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.69it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.69it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.68it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.69it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.66it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.66it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.68it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.66it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.67it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.69it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.66it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.68it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.70it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.68it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.69it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.69it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.66it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.68it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.66it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.68it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.69it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.67it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.69it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.69it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.67it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.68it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.70it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.68it/s][A                                                 
                                                 [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 242/605 [01:28<01:38,  3.69it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.68it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:49:55,228 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242
[INFO|configuration_utils.py:351] 2023-08-28 02:49:55,256 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:49:57,333 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:49:57,351 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:49:57,367 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 243/605 [01:30<22:54,  3.80s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 244/605 [01:31<16:29,  2.74s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 245/605 [01:31<12:00,  2.00s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 246/605 [01:31<08:52,  1.48s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 247/605 [01:31<06:41,  1.12s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 248/605 [01:32<05:09,  1.15it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 249/605 [01:32<04:05,  1.45it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 250/605 [01:32<03:20,  1.77it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/605 [01:32<02:49,  2.09it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 252/605 [01:33<02:27,  2.40it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 253/605 [01:33<02:11,  2.67it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 254/605 [01:33<02:00,  2.90it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 255/605 [01:34<01:53,  3.09it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/605 [01:34<01:47,  3.24it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/605 [01:34<01:43,  3.35it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 258/605 [01:34<01:41,  3.43it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 259/605 [01:35<01:39,  3.49it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 260/605 [01:35<01:37,  3.54it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 261/605 [01:35<01:36,  3.57it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 262/605 [01:35<01:35,  3.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 263/605 [01:36<01:34,  3.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 264/605 [01:36<01:34,  3.60it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 265/605 [01:36<01:34,  3.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 266/605 [01:37<01:33,  3.62it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 267/605 [01:37<01:33,  3.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 268/605 [01:37<01:32,  3.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 269/605 [01:37<01:32,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 270/605 [01:38<01:32,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 271/605 [01:38<01:31,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 272/605 [01:38<01:31,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 273/605 [01:39<01:31,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 274/605 [01:39<01:30,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 275/605 [01:39<01:30,  3.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 276/605 [01:39<01:30,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 277/605 [01:40<01:30,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 278/605 [01:40<01:29,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 279/605 [01:40<01:29,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 280/605 [01:40<01:29,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 281/605 [01:41<01:28,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 282/605 [01:41<01:28,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 283/605 [01:41<01:28,  3.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 284/605 [01:42<01:28,  3.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 285/605 [01:42<01:27,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 286/605 [01:42<01:28,  3.61it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 287/605 [01:42<01:27,  3.62it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 288/605 [01:43<01:27,  3.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 289/605 [01:43<01:27,  3.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 290/605 [01:43<01:26,  3.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 291/605 [01:43<01:26,  3.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 292/605 [01:44<01:26,  3.63it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 293/605 [01:44<01:25,  3.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 294/605 [01:44<01:25,  3.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 295/605 [01:45<01:25,  3.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 296/605 [01:45<01:25,  3.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 297/605 [01:45<01:25,  3.62it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 298/605 [01:45<01:24,  3.62it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 299/605 [01:46<01:24,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 300/605 [01:46<01:23,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 301/605 [01:46<01:23,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 302/605 [01:46<01:23,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 303/605 [01:47<01:23,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 304/605 [01:47<01:22,  3.64it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 305/605 [01:47<01:22,  3.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 306/605 [01:48<01:22,  3.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 307/605 [01:48<01:21,  3.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 308/605 [01:48<01:21,  3.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 309/605 [01:48<01:21,  3.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 310/605 [01:49<01:21,  3.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 311/605 [01:49<01:20,  3.64it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 312/605 [01:49<01:20,  3.64it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 313/605 [01:50<01:20,  3.63it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 314/605 [01:50<01:20,  3.63it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 315/605 [01:50<01:19,  3.63it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/605 [01:50<01:19,  3.63it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/605 [01:51<01:19,  3.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 318/605 [01:51<01:20,  3.55it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 319/605 [01:51<01:20,  3.55it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 320/605 [01:51<01:19,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 321/605 [01:52<01:19,  3.59it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 322/605 [01:52<01:18,  3.61it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 323/605 [01:52<01:18,  3.62it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 324/605 [01:53<01:17,  3.62it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 325/605 [01:53<01:17,  3.62it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 326/605 [01:53<01:16,  3.63it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 327/605 [01:53<01:16,  3.63it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 328/605 [01:54<01:16,  3.63it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 329/605 [01:54<01:15,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 330/605 [01:54<01:15,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 331/605 [01:55<01:15,  3.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 332/605 [01:55<01:15,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 333/605 [01:55<01:14,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 334/605 [01:55<01:14,  3.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 335/605 [01:56<01:14,  3.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 336/605 [01:56<01:14,  3.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 337/605 [01:56<01:13,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 338/605 [01:56<01:13,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 339/605 [01:57<01:13,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 340/605 [01:57<01:12,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 341/605 [01:57<01:12,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 342/605 [01:58<01:12,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 343/605 [01:58<01:12,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 344/605 [01:58<01:11,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 345/605 [01:58<01:11,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 346/605 [01:59<01:11,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 347/605 [01:59<01:11,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 348/605 [01:59<01:10,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 349/605 [01:59<01:10,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 350/605 [02:00<01:10,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 351/605 [02:00<01:09,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 352/605 [02:00<01:09,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 353/605 [02:01<01:09,  3.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 354/605 [02:01<01:09,  3.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 355/605 [02:01<01:08,  3.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 356/605 [02:01<01:08,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 357/605 [02:02<01:08,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 358/605 [02:02<01:07,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 359/605 [02:02<01:07,  3.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 360/605 [02:02<01:07,  3.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 361/605 [02:03<01:07,  3.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 362/605 [02:03<01:06,  3.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 363/605 [02:03<01:05,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 02:50:30,933 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:50:30,933 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 02:50:30,933 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3217, 'eval_samples_per_second': 373.214, 'eval_steps_per_second': 46.665, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.73it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.19it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.42it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.75it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.40it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.14it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.01it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.92it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.83it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.81it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.78it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.72it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.72it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.71it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.68it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.70it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.69it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.68it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.70it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.67it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.68it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.69it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.66it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.67it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.69it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.66it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.43it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.48it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.56it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.61it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.62it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.65it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.66it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.65it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.66it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.68it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.66it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.68it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.68it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.67it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.68it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.66it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.68it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.69it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.67it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.70it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.70it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.68it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.69it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.70it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.68it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.69it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.69it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.68it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.69it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.67it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.69it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.70it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.67it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.69it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.70it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.69it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.70it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.70it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.66it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.67it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.67it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.68it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.69it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.67it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.69it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.70it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.67it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.68it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.69it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.67it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.69it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.70it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.67it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.68it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.69it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.68it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.68it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.68it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.68it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.69it/s][A                                                 
                                                 [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 363/605 [02:13<01:05,  3.68it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.69it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:50:40,267 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363
[INFO|configuration_utils.py:351] 2023-08-28 02:50:40,289 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:50:42,424 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:50:42,438 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:50:42,445 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 364/605 [02:15<15:15,  3.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 365/605 [02:16<10:58,  2.74s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 366/605 [02:16<07:58,  2.00s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 367/605 [02:16<05:53,  1.48s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 368/605 [02:16<04:25,  1.12s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 369/605 [02:17<03:24,  1.15it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 370/605 [02:17<02:41,  1.45it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 371/605 [02:17<02:12,  1.77it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 372/605 [02:18<01:51,  2.09it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 373/605 [02:18<01:36,  2.40it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/605 [02:18<01:26,  2.67it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375/605 [02:18<01:19,  2.90it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376/605 [02:19<01:14,  3.09it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/605 [02:19<01:10,  3.23it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/605 [02:19<01:07,  3.34it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 379/605 [02:19<01:05,  3.43it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 380/605 [02:20<01:04,  3.49it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 381/605 [02:20<01:03,  3.53it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 382/605 [02:20<01:02,  3.56it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 383/605 [02:21<01:01,  3.59it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 384/605 [02:21<01:01,  3.60it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 385/605 [02:21<01:01,  3.60it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 386/605 [02:21<01:00,  3.61it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 387/605 [02:22<01:00,  3.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 388/605 [02:22<00:59,  3.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 389/605 [02:22<00:59,  3.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 390/605 [02:22<00:59,  3.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 391/605 [02:23<00:58,  3.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 392/605 [02:23<00:58,  3.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 393/605 [02:23<00:58,  3.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 394/605 [02:24<00:58,  3.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 395/605 [02:24<00:57,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 396/605 [02:24<00:57,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 397/605 [02:24<00:57,  3.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 398/605 [02:25<00:56,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 399/605 [02:25<00:56,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 400/605 [02:25<00:56,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 401/605 [02:26<00:56,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 402/605 [02:26<00:55,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 403/605 [02:26<00:55,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 404/605 [02:26<00:55,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 405/605 [02:27<00:55,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 406/605 [02:27<00:54,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 407/605 [02:27<00:54,  3.62it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 408/605 [02:27<00:54,  3.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 409/605 [02:28<00:54,  3.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 410/605 [02:28<00:53,  3.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 411/605 [02:28<00:53,  3.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 412/605 [02:29<00:53,  3.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 413/605 [02:29<00:52,  3.63it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 414/605 [02:29<00:52,  3.63it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 415/605 [02:29<00:52,  3.63it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 416/605 [02:30<00:52,  3.63it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 417/605 [02:30<00:51,  3.63it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 418/605 [02:30<00:52,  3.60it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 419/605 [02:30<00:51,  3.61it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 420/605 [02:31<00:51,  3.62it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 421/605 [02:31<00:50,  3.62it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 422/605 [02:31<00:50,  3.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 423/605 [02:32<00:50,  3.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 424/605 [02:32<00:49,  3.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 425/605 [02:32<00:49,  3.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 426/605 [02:32<00:49,  3.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 427/605 [02:33<00:48,  3.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 428/605 [02:33<00:48,  3.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 429/605 [02:33<00:48,  3.62it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 430/605 [02:34<00:48,  3.62it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 431/605 [02:34<00:47,  3.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 432/605 [02:34<00:47,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 433/605 [02:34<00:47,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 434/605 [02:35<00:47,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 435/605 [02:35<00:46,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 436/605 [02:35<00:46,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 437/605 [02:35<00:46,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 438/605 [02:36<00:45,  3.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 439/605 [02:36<00:45,  3.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 440/605 [02:36<00:45,  3.61it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 441/605 [02:37<00:45,  3.62it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 442/605 [02:37<00:44,  3.62it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 443/605 [02:37<00:44,  3.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 444/605 [02:37<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 445/605 [02:38<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 446/605 [02:38<00:43,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 447/605 [02:38<00:43,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 448/605 [02:38<00:43,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 449/605 [02:39<00:42,  3.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 450/605 [02:39<00:42,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 451/605 [02:39<00:42,  3.62it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 452/605 [02:40<00:42,  3.62it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 453/605 [02:40<00:41,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 454/605 [02:40<00:41,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 455/605 [02:40<00:41,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 456/605 [02:41<00:41,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 457/605 [02:41<00:40,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 458/605 [02:41<00:40,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 459/605 [02:41<00:40,  3.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 460/605 [02:42<00:39,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 461/605 [02:42<00:39,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 462/605 [02:42<00:39,  3.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 463/605 [02:43<00:39,  3.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 464/605 [02:43<00:38,  3.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 465/605 [02:43<00:38,  3.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 466/605 [02:43<00:38,  3.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 467/605 [02:44<00:38,  3.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 468/605 [02:44<00:37,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 469/605 [02:44<00:37,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 470/605 [02:45<00:37,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 471/605 [02:45<00:36,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 472/605 [02:45<00:36,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 473/605 [02:45<00:36,  3.60it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 474/605 [02:46<00:36,  3.61it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 475/605 [02:46<00:35,  3.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 476/605 [02:46<00:35,  3.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 477/605 [02:46<00:35,  3.63it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 478/605 [02:47<00:34,  3.63it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 479/605 [02:47<00:34,  3.63it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 480/605 [02:47<00:34,  3.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 481/605 [02:48<00:34,  3.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 482/605 [02:48<00:33,  3.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 483/605 [02:48<00:33,  3.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 484/605 [02:48<00:33,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 02:51:16,007 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:51:16,007 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 02:51:16,007 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3224, 'eval_samples_per_second': 373.189, 'eval_steps_per_second': 46.662, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.29it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.39it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.61it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.93it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.49it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.24it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.06it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.90it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.76it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.67it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.40it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 46.42it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.45it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.53it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.58it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.59it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.63it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.66it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.66it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.68it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.68it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.68it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:07, 42.46it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:07, 43.66it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 44.48it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 45.12it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 45.59it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 45.89it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.13it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.29it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.41it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.45it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.43it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.45it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.45it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.45it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.45it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.45it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.43it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.44it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.43it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.46it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.46it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.43it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.44it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.41it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.43it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.44it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.47it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.54it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.59it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.54it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.52it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.48it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.55it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.60it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.56it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.53it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.49it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.54it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.59it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.60it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.45it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.53it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.54it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.60it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.55it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.49it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.49it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.46it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.47it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.47it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.43it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.44it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.44it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.46it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.46it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.43it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.46it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.45it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.43it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.43it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.43it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.46it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.46it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.44it/s][A                                                 
                                                 [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 484/605 [02:58<00:33,  3.65it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.44it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:51:25,634 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484
[INFO|configuration_utils.py:351] 2023-08-28 02:51:25,678 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:51:28,129 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:51:28,141 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:51:28,151 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 485/605 [03:01<08:11,  4.10s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 486/605 [03:02<05:51,  2.95s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 487/605 [03:02<04:13,  2.15s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 488/605 [03:02<03:05,  1.59s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 489/605 [03:02<02:18,  1.19s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 490/605 [03:03<01:45,  1.09it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 491/605 [03:03<01:22,  1.38it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 492/605 [03:03<01:06,  1.70it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 493/605 [03:04<00:55,  2.02it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 494/605 [03:04<00:47,  2.33it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 495/605 [03:04<00:42,  2.61it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 496/605 [03:04<00:38,  2.86it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 497/605 [03:05<00:35,  3.04it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 498/605 [03:05<00:33,  3.20it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 499/605 [03:05<00:31,  3.32it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 500/605 [03:06<00:30,  3.41it/s]                                                  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 500/605 [03:06<00:30,  3.41it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 501/605 [03:06<00:29,  3.48it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 502/605 [03:06<00:29,  3.52it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 503/605 [03:06<00:28,  3.56it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 504/605 [03:07<00:28,  3.58it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 505/605 [03:07<00:27,  3.60it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 506/605 [03:07<00:27,  3.61it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 507/605 [03:07<00:27,  3.62it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 508/605 [03:08<00:26,  3.62it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 509/605 [03:08<00:26,  3.62it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 510/605 [03:08<00:26,  3.63it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 511/605 [03:09<00:25,  3.63it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 512/605 [03:09<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 513/605 [03:09<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 514/605 [03:09<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 515/605 [03:10<00:24,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 516/605 [03:10<00:24,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 517/605 [03:10<00:24,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 518/605 [03:10<00:23,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 519/605 [03:11<00:23,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 520/605 [03:11<00:23,  3.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 521/605 [03:11<00:23,  3.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 522/605 [03:12<00:22,  3.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 523/605 [03:12<00:22,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 524/605 [03:12<00:22,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 525/605 [03:12<00:21,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 526/605 [03:13<00:21,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 527/605 [03:13<00:21,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 528/605 [03:13<00:21,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 529/605 [03:13<00:20,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 530/605 [03:14<00:20,  3.62it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 531/605 [03:14<00:20,  3.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 532/605 [03:14<00:20,  3.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 533/605 [03:15<00:19,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 534/605 [03:15<00:19,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 535/605 [03:15<00:19,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 536/605 [03:15<00:18,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 537/605 [03:16<00:18,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 538/605 [03:16<00:18,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 539/605 [03:16<00:18,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 540/605 [03:17<00:17,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 541/605 [03:17<00:17,  3.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 542/605 [03:17<00:17,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 543/605 [03:17<00:17,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 544/605 [03:18<00:16,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 545/605 [03:18<00:16,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 546/605 [03:18<00:16,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 547/605 [03:18<00:15,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 548/605 [03:19<00:15,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 549/605 [03:19<00:15,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 550/605 [03:19<00:15,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 551/605 [03:20<00:14,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 552/605 [03:20<00:14,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 553/605 [03:20<00:14,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 554/605 [03:20<00:14,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 555/605 [03:21<00:13,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 556/605 [03:21<00:13,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 557/605 [03:21<00:13,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 558/605 [03:21<00:12,  3.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 559/605 [03:22<00:12,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 560/605 [03:22<00:12,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 561/605 [03:22<00:12,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 562/605 [03:23<00:11,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 563/605 [03:23<00:11,  3.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 564/605 [03:23<00:11,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 565/605 [03:23<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 566/605 [03:24<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 567/605 [03:24<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 568/605 [03:24<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 569/605 [03:24<00:09,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 570/605 [03:25<00:09,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 571/605 [03:25<00:09,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 572/605 [03:25<00:09,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 573/605 [03:26<00:08,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 574/605 [03:26<00:08,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 575/605 [03:26<00:08,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 576/605 [03:26<00:07,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 577/605 [03:27<00:07,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 578/605 [03:27<00:07,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 579/605 [03:27<00:07,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 580/605 [03:28<00:06,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 581/605 [03:28<00:06,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 582/605 [03:28<00:06,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 583/605 [03:28<00:06,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 584/605 [03:29<00:05,  3.61it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 585/605 [03:29<00:05,  3.62it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 586/605 [03:29<00:05,  3.62it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 587/605 [03:29<00:04,  3.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 588/605 [03:30<00:04,  3.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 589/605 [03:30<00:04,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 590/605 [03:30<00:04,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 591/605 [03:31<00:03,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 592/605 [03:31<00:03,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 593/605 [03:31<00:03,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 594/605 [03:31<00:03,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 595/605 [03:32<00:02,  3.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 596/605 [03:32<00:02,  3.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 597/605 [03:32<00:02,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 598/605 [03:32<00:01,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 599/605 [03:33<00:01,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 600/605 [03:33<00:01,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 601/605 [03:33<00:01,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 602/605 [03:34<00:00,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 603/605 [03:34<00:00,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 604/605 [03:34<00:00,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [03:34<00:00,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 02:52:02,004 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:52:02,004 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 02:52:02,004 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.384, 'eval_samples_per_second': 370.737, 'eval_steps_per_second': 46.355, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6797520661157023e-05, 'epoch': 4.13}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.10it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.58it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.78it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.06it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.63it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.35it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.17it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.92it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.76it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.76it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.77it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.75it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.78it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.78it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.76it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.80it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.74it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.65it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.69it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.65it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.67it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.70it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.72it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.75it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.76it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.72it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.67it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.63it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.61it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.68it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.71it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.69it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.73it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.74it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.73it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.72it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.66it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.66it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.69it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.69it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.71it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.73it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.69it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.74it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.68it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.64it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.67it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.71it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.69it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.72it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.73it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.68it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.71it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.68it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.68it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.71it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.68it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.69it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.72it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.68it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.70it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.73it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.68it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.71it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.73it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.65it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.68it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.65it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.67it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.71it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.69it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.70it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.69it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.65it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.69it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.71it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.68it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.66it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.66it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.69it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.71it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.66it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.70it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.67it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.66it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.66it/s][A                                                 
                                                 [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [03:44<00:00,  3.69it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:52:11,343 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605
[INFO|configuration_utils.py:351] 2023-08-28 02:52:11,364 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:52:14,351 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:52:14,368 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:52:14,379 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 02:52:14,630 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 02:52:14,631 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121 (score: 1.0038264989852905).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [03:49<00:00,  3.69it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 605/605 [03:49<00:00,  2.64it/s]
[INFO|trainer.py:1894] 2023-08-28 02:52:16,456 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 02:52:16,469 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:52:18,763 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:52:18,784 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:52:18,793 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:52:18,961 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:18,961 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:18,962 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:18,962 >>   train_runtime            = 0:03:49.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:18,962 >>   train_samples            =       7740
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:18,962 >>   train_samples_per_second =    168.756
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:18,962 >>   train_steps_per_second   =      2.638
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3161, 'eval_samples_per_second': 373.439, 'eval_steps_per_second': 46.693, 'epoch': 5.0}
{'train_runtime': 229.3251, 'train_samples_per_second': 168.756, 'train_steps_per_second': 2.638, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 02:52:18 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 02:52:18,995 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:52:18,996 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 02:52:18,996 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 58.12it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 51.05it/s]  4%|â–         | 18/435 [00:00<00:08, 49.11it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.37it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.92it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.63it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.43it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.25it/s] 11%|â–ˆ         | 48/435 [00:00<00:08, 47.01it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 47.01it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.97it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.93it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.98it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 47.00it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 47.00it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 47.03it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.99it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.93it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.91it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.89it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.86it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.87it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.93it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.95it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.99it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.95it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.91it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.89it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.87it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.84it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.96it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.92it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.90it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.90it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.88it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.88it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.92it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.89it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.84it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.86it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.86it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.86it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.84it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.82it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 46.85it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.88it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.85it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.87it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.87it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.85it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.86it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.85it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.84it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.89it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.87it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.85it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.89it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.89it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.85it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.87it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.86it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.85it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.89it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.87it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.66it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.77it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.79it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.78it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.81it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.82it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.79it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.83it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.83it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.83it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.86it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.84it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.85it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.85it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.84it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:08<00:00, 46.82it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.97it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:52:28,283 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:28,283 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:28,283 >>   eval_loss               =     1.0038
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:28,283 >>   eval_runtime            = 0:00:09.28
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:28,283 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:28,284 >>   eval_samples_per_second =    374.583
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:28,284 >>   eval_steps_per_second   =     46.836
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:52:28,284 >>   perplexity              =     2.7287
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:33,556 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:33,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:33,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:33,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:33,561 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:52:34,216 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:52:34,217 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:52:34,807 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:52:35,845 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:52:35,846 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:38,833 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:38,838 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:38,838 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:38,838 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:52:38,838 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:52:39,472 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:52:39,473 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:52:40,075 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:52:40,220 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:52:40,220 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-242
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-605
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-363
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-484
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/checkpoint-121
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.24it/s]Extractor Predicting: 3it [00:02,  1.28it/s]Extractor Predicting: 4it [00:03,  1.29it/s]Extractor Predicting: 5it [00:03,  1.28it/s]Extractor Predicting: 6it [00:04,  1.27it/s]Extractor Predicting: 7it [00:05,  1.28it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:07,  1.29it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:08,  1.31it/s]Extractor Predicting: 12it [00:09,  1.29it/s]Extractor Predicting: 13it [00:10,  1.28it/s]Extractor Predicting: 14it [00:10,  1.27it/s]Extractor Predicting: 15it [00:11,  1.32it/s]Extractor Predicting: 16it [00:12,  1.30it/s]Extractor Predicting: 17it [00:13,  1.32it/s]Extractor Predicting: 18it [00:13,  1.36it/s]Extractor Predicting: 19it [00:14,  1.33it/s]Extractor Predicting: 20it [00:15,  1.33it/s]Extractor Predicting: 21it [00:16,  1.33it/s]Extractor Predicting: 22it [00:16,  1.29it/s]Extractor Predicting: 23it [00:17,  1.30it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.28it/s]Extractor Predicting: 26it [00:20,  1.26it/s]Extractor Predicting: 27it [00:20,  1.27it/s]Extractor Predicting: 28it [00:21,  1.31it/s]Extractor Predicting: 29it [00:22,  1.29it/s]Extractor Predicting: 30it [00:23,  1.27it/s]Extractor Predicting: 31it [00:23,  1.27it/s]Extractor Predicting: 32it [00:24,  1.28it/s]Extractor Predicting: 33it [00:25,  1.28it/s]Extractor Predicting: 34it [00:26,  1.31it/s]Extractor Predicting: 35it [00:26,  1.32it/s]Extractor Predicting: 36it [00:27,  1.33it/s]Extractor Predicting: 37it [00:28,  1.33it/s]Extractor Predicting: 38it [00:29,  1.34it/s]Extractor Predicting: 39it [00:29,  1.35it/s]Extractor Predicting: 40it [00:30,  1.34it/s]Extractor Predicting: 41it [00:31,  1.34it/s]Extractor Predicting: 42it [00:32,  1.34it/s]Extractor Predicting: 43it [00:32,  1.36it/s]Extractor Predicting: 44it [00:33,  1.38it/s]Extractor Predicting: 45it [00:34,  1.35it/s]Extractor Predicting: 46it [00:35,  1.34it/s]Extractor Predicting: 47it [00:35,  1.33it/s]Extractor Predicting: 48it [00:36,  1.34it/s]Extractor Predicting: 49it [00:37,  1.34it/s]Extractor Predicting: 50it [00:38,  1.35it/s]Extractor Predicting: 51it [00:38,  1.36it/s]Extractor Predicting: 52it [00:39,  1.36it/s]Extractor Predicting: 53it [00:40,  1.36it/s]Extractor Predicting: 54it [00:41,  1.35it/s]Extractor Predicting: 55it [00:41,  1.31it/s]Extractor Predicting: 56it [00:42,  1.34it/s]Extractor Predicting: 57it [00:43,  1.32it/s]Extractor Predicting: 58it [00:44,  1.32it/s]Extractor Predicting: 59it [00:44,  1.35it/s]Extractor Predicting: 60it [00:45,  1.38it/s]Extractor Predicting: 61it [00:46,  1.40it/s]Extractor Predicting: 62it [00:46,  1.37it/s]Extractor Predicting: 63it [00:47,  1.34it/s]Extractor Predicting: 64it [00:48,  1.36it/s]Extractor Predicting: 65it [00:49,  1.37it/s]Extractor Predicting: 66it [00:49,  1.36it/s]Extractor Predicting: 67it [00:50,  1.36it/s]Extractor Predicting: 68it [00:51,  1.36it/s]Extractor Predicting: 69it [00:52,  1.41it/s]Extractor Predicting: 70it [00:52,  1.40it/s]Extractor Predicting: 71it [00:53,  1.41it/s]Extractor Predicting: 72it [00:54,  1.38it/s]Extractor Predicting: 73it [00:54,  1.38it/s]Extractor Predicting: 74it [00:55,  1.38it/s]Extractor Predicting: 75it [00:56,  1.35it/s]Extractor Predicting: 76it [00:57,  1.33it/s]Extractor Predicting: 77it [00:57,  1.36it/s]Extractor Predicting: 78it [00:58,  1.37it/s]Extractor Predicting: 79it [00:59,  1.39it/s]Extractor Predicting: 80it [01:00,  1.38it/s]Extractor Predicting: 81it [01:00,  1.37it/s]Extractor Predicting: 82it [01:01,  1.37it/s]Extractor Predicting: 83it [01:02,  1.33it/s]Extractor Predicting: 84it [01:03,  1.35it/s]Extractor Predicting: 85it [01:03,  1.37it/s]Extractor Predicting: 86it [01:04,  1.38it/s]Extractor Predicting: 87it [01:05,  1.41it/s]Extractor Predicting: 88it [01:05,  1.40it/s]Extractor Predicting: 89it [01:06,  1.40it/s]Extractor Predicting: 90it [01:07,  1.41it/s]Extractor Predicting: 91it [01:07,  1.44it/s]Extractor Predicting: 92it [01:08,  1.47it/s]Extractor Predicting: 93it [01:09,  1.45it/s]Extractor Predicting: 94it [01:10,  1.43it/s]Extractor Predicting: 95it [01:10,  1.44it/s]Extractor Predicting: 96it [01:11,  1.43it/s]Extractor Predicting: 97it [01:12,  1.43it/s]Extractor Predicting: 98it [01:12,  1.42it/s]Extractor Predicting: 99it [01:13,  1.40it/s]Extractor Predicting: 100it [01:14,  1.36it/s]Extractor Predicting: 101it [01:15,  1.36it/s]Extractor Predicting: 102it [01:15,  1.34it/s]Extractor Predicting: 103it [01:16,  1.38it/s]Extractor Predicting: 104it [01:17,  1.39it/s]Extractor Predicting: 105it [01:17,  1.40it/s]Extractor Predicting: 106it [01:18,  1.40it/s]Extractor Predicting: 107it [01:19,  1.41it/s]Extractor Predicting: 108it [01:20,  1.41it/s]Extractor Predicting: 109it [01:20,  1.42it/s]Extractor Predicting: 110it [01:21,  1.42it/s]Extractor Predicting: 111it [01:22,  1.42it/s]Extractor Predicting: 112it [01:22,  1.44it/s]Extractor Predicting: 113it [01:23,  1.48it/s]Extractor Predicting: 114it [01:24,  1.48it/s]Extractor Predicting: 115it [01:24,  1.49it/s]Extractor Predicting: 116it [01:25,  1.45it/s]Extractor Predicting: 117it [01:26,  1.40it/s]Extractor Predicting: 118it [01:26,  1.41it/s]Extractor Predicting: 119it [01:27,  1.38it/s]Extractor Predicting: 120it [01:28,  1.39it/s]Extractor Predicting: 121it [01:29,  1.37it/s]Extractor Predicting: 122it [01:29,  1.35it/s]Extractor Predicting: 123it [01:30,  1.37it/s]Extractor Predicting: 124it [01:31,  1.38it/s]Extractor Predicting: 125it [01:32,  1.39it/s]Extractor Predicting: 126it [01:32,  1.35it/s]Extractor Predicting: 127it [01:33,  1.39it/s]Extractor Predicting: 128it [01:34,  1.39it/s]Extractor Predicting: 129it [01:35,  1.37it/s]Extractor Predicting: 130it [01:35,  1.35it/s]Extractor Predicting: 131it [01:36,  1.38it/s]Extractor Predicting: 132it [01:37,  1.36it/s]Extractor Predicting: 133it [01:38,  1.34it/s]Extractor Predicting: 134it [01:38,  1.33it/s]Extractor Predicting: 135it [01:39,  1.35it/s]Extractor Predicting: 136it [01:40,  1.33it/s]Extractor Predicting: 137it [01:41,  1.34it/s]Extractor Predicting: 138it [01:41,  1.32it/s]Extractor Predicting: 139it [01:42,  1.33it/s]Extractor Predicting: 140it [01:43,  1.31it/s]Extractor Predicting: 141it [01:44,  1.34it/s]Extractor Predicting: 142it [01:44,  1.32it/s]Extractor Predicting: 143it [01:45,  1.34it/s]Extractor Predicting: 144it [01:45,  1.67it/s]Extractor Predicting: 144it [01:45,  1.36it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:33,415 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:33,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:33,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:33,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:33,420 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:54:33,859 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:54:33,860 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:54:34,254 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:54:35,283 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:54:35,283 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:38,190 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:38,194 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:38,194 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:38,194 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:54:38,194 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:54:38,818 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:54:38,819 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:54:39,467 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:54:39,631 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:54:39,631 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:02,  1.36it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.41it/s]Extractor Predicting: 7it [00:05,  1.44it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.45it/s]Extractor Predicting: 10it [00:07,  1.46it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:09,  1.38it/s]Extractor Predicting: 15it [00:10,  1.41it/s]Extractor Predicting: 16it [00:11,  1.42it/s]Extractor Predicting: 17it [00:12,  1.40it/s]Extractor Predicting: 18it [00:12,  1.43it/s]Extractor Predicting: 19it [00:13,  1.46it/s]Extractor Predicting: 20it [00:14,  1.43it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.46it/s]Extractor Predicting: 23it [00:16,  1.46it/s]Extractor Predicting: 24it [00:16,  1.44it/s]Extractor Predicting: 25it [00:17,  1.43it/s]Extractor Predicting: 26it [00:18,  1.44it/s]Extractor Predicting: 27it [00:19,  1.42it/s]Extractor Predicting: 28it [00:19,  1.43it/s]Extractor Predicting: 29it [00:20,  1.43it/s]Extractor Predicting: 30it [00:21,  1.46it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:22,  1.46it/s]Extractor Predicting: 33it [00:23,  1.45it/s]Extractor Predicting: 34it [00:23,  1.42it/s]Extractor Predicting: 35it [00:24,  1.42it/s]Extractor Predicting: 36it [00:25,  1.43it/s]Extractor Predicting: 37it [00:25,  1.45it/s]Extractor Predicting: 38it [00:26,  1.47it/s]Extractor Predicting: 39it [00:27,  1.45it/s]Extractor Predicting: 40it [00:28,  1.43it/s]Extractor Predicting: 41it [00:28,  1.43it/s]Extractor Predicting: 42it [00:29,  1.47it/s]Extractor Predicting: 43it [00:30,  1.42it/s]Extractor Predicting: 44it [00:30,  1.41it/s]Extractor Predicting: 45it [00:31,  1.41it/s]Extractor Predicting: 46it [00:32,  1.38it/s]Extractor Predicting: 47it [00:32,  1.42it/s]Extractor Predicting: 48it [00:33,  1.40it/s]Extractor Predicting: 49it [00:34,  1.43it/s]Extractor Predicting: 50it [00:35,  1.41it/s]Extractor Predicting: 51it [00:35,  1.40it/s]Extractor Predicting: 52it [00:36,  1.39it/s]Extractor Predicting: 53it [00:37,  1.40it/s]Extractor Predicting: 54it [00:37,  1.39it/s]Extractor Predicting: 55it [00:38,  1.42it/s]Extractor Predicting: 56it [00:39,  1.41it/s]Extractor Predicting: 57it [00:40,  1.39it/s]Extractor Predicting: 58it [00:40,  1.39it/s]Extractor Predicting: 59it [00:41,  1.38it/s]Extractor Predicting: 60it [00:42,  1.37it/s]Extractor Predicting: 61it [00:43,  1.36it/s]Extractor Predicting: 62it [00:43,  1.40it/s]Extractor Predicting: 63it [00:44,  1.40it/s]Extractor Predicting: 64it [00:45,  1.43it/s]Extractor Predicting: 65it [00:45,  1.40it/s]Extractor Predicting: 66it [00:46,  1.37it/s]Extractor Predicting: 67it [00:47,  1.37it/s]Extractor Predicting: 68it [00:48,  1.37it/s]Extractor Predicting: 69it [00:48,  1.35it/s]Extractor Predicting: 70it [00:49,  1.36it/s]Extractor Predicting: 71it [00:50,  1.36it/s]Extractor Predicting: 72it [00:51,  1.34it/s]Extractor Predicting: 73it [00:51,  1.37it/s]Extractor Predicting: 74it [00:52,  1.37it/s]Extractor Predicting: 75it [00:53,  1.39it/s]Extractor Predicting: 76it [00:53,  1.37it/s]Extractor Predicting: 77it [00:54,  1.40it/s]Extractor Predicting: 78it [00:55,  1.39it/s]Extractor Predicting: 79it [00:56,  1.43it/s]Extractor Predicting: 80it [00:56,  1.45it/s]Extractor Predicting: 81it [00:57,  1.45it/s]Extractor Predicting: 82it [00:58,  1.44it/s]Extractor Predicting: 83it [00:58,  1.41it/s]Extractor Predicting: 84it [00:59,  1.40it/s]Extractor Predicting: 85it [01:00,  1.37it/s]Extractor Predicting: 86it [01:01,  1.33it/s]Extractor Predicting: 87it [01:01,  1.35it/s]Extractor Predicting: 88it [01:02,  1.35it/s]Extractor Predicting: 89it [01:03,  1.33it/s]Extractor Predicting: 90it [01:04,  1.32it/s]Extractor Predicting: 91it [01:04,  1.31it/s]Extractor Predicting: 92it [01:05,  1.34it/s]Extractor Predicting: 93it [01:06,  1.35it/s]Extractor Predicting: 94it [01:07,  1.36it/s]Extractor Predicting: 95it [01:07,  1.37it/s]Extractor Predicting: 96it [01:08,  1.33it/s]Extractor Predicting: 97it [01:09,  1.32it/s]Extractor Predicting: 98it [01:10,  1.32it/s]Extractor Predicting: 99it [01:10,  1.34it/s]Extractor Predicting: 100it [01:11,  1.35it/s]Extractor Predicting: 101it [01:12,  1.38it/s]Extractor Predicting: 102it [01:13,  1.35it/s]Extractor Predicting: 103it [01:13,  1.33it/s]Extractor Predicting: 104it [01:14,  1.27it/s]Extractor Predicting: 105it [01:15,  1.28it/s]Extractor Predicting: 106it [01:16,  1.30it/s]Extractor Predicting: 107it [01:16,  1.34it/s]Extractor Predicting: 108it [01:17,  1.33it/s]Extractor Predicting: 109it [01:18,  1.33it/s]Extractor Predicting: 110it [01:19,  1.33it/s]Extractor Predicting: 111it [01:19,  1.35it/s]Extractor Predicting: 112it [01:20,  1.34it/s]Extractor Predicting: 113it [01:21,  1.35it/s]Extractor Predicting: 114it [01:22,  1.36it/s]Extractor Predicting: 115it [01:22,  1.31it/s]Extractor Predicting: 116it [01:23,  1.34it/s]Extractor Predicting: 117it [01:24,  1.34it/s]Extractor Predicting: 118it [01:25,  1.33it/s]Extractor Predicting: 119it [01:25,  1.33it/s]Extractor Predicting: 120it [01:26,  1.36it/s]Extractor Predicting: 121it [01:27,  1.38it/s]Extractor Predicting: 122it [01:27,  1.39it/s]Extractor Predicting: 123it [01:28,  1.38it/s]Extractor Predicting: 124it [01:29,  1.39it/s]Extractor Predicting: 125it [01:30,  1.41it/s]Extractor Predicting: 126it [01:30,  1.41it/s]Extractor Predicting: 127it [01:31,  1.39it/s]Extractor Predicting: 128it [01:32,  1.43it/s]Extractor Predicting: 129it [01:32,  1.43it/s]Extractor Predicting: 130it [01:33,  1.40it/s]Extractor Predicting: 131it [01:34,  1.44it/s]Extractor Predicting: 132it [01:34,  1.44it/s]Extractor Predicting: 133it [01:35,  1.46it/s]Extractor Predicting: 134it [01:36,  1.41it/s]Extractor Predicting: 135it [01:37,  1.44it/s]Extractor Predicting: 136it [01:37,  1.45it/s]Extractor Predicting: 137it [01:38,  1.46it/s]Extractor Predicting: 138it [01:39,  1.45it/s]Extractor Predicting: 139it [01:39,  1.44it/s]Extractor Predicting: 140it [01:40,  1.44it/s]Extractor Predicting: 141it [01:41,  1.41it/s]Extractor Predicting: 142it [01:41,  1.41it/s]Extractor Predicting: 143it [01:42,  1.42it/s]Extractor Predicting: 144it [01:43,  1.41it/s]Extractor Predicting: 145it [01:44,  1.45it/s]Extractor Predicting: 146it [01:44,  1.47it/s]Extractor Predicting: 147it [01:45,  1.48it/s]Extractor Predicting: 148it [01:45,  1.50it/s]Extractor Predicting: 149it [01:46,  1.49it/s]Extractor Predicting: 150it [01:47,  1.51it/s]Extractor Predicting: 151it [01:47,  1.51it/s]Extractor Predicting: 152it [01:48,  1.53it/s]Extractor Predicting: 153it [01:49,  1.50it/s]Extractor Predicting: 154it [01:49,  1.50it/s]Extractor Predicting: 155it [01:50,  1.54it/s]Extractor Predicting: 156it [01:51,  1.53it/s]Extractor Predicting: 157it [01:51,  1.61it/s]Extractor Predicting: 158it [01:52,  1.62it/s]Extractor Predicting: 159it [01:53,  1.57it/s]Extractor Predicting: 160it [01:53,  1.52it/s]Extractor Predicting: 161it [01:54,  1.51it/s]Extractor Predicting: 162it [01:55,  1.52it/s]Extractor Predicting: 163it [01:55,  1.54it/s]Extractor Predicting: 164it [01:56,  1.54it/s]Extractor Predicting: 165it [01:57,  1.54it/s]Extractor Predicting: 166it [01:57,  1.51it/s]Extractor Predicting: 167it [01:58,  1.54it/s]Extractor Predicting: 168it [01:59,  1.52it/s]Extractor Predicting: 169it [01:59,  1.58it/s]Extractor Predicting: 170it [02:00,  1.56it/s]Extractor Predicting: 171it [02:00,  1.56it/s]Extractor Predicting: 172it [02:01,  1.49it/s]Extractor Predicting: 173it [02:02,  1.46it/s]Extractor Predicting: 174it [02:03,  1.43it/s]Extractor Predicting: 175it [02:03,  1.38it/s]Extractor Predicting: 176it [02:04,  1.38it/s]Extractor Predicting: 177it [02:05,  1.39it/s]Extractor Predicting: 178it [02:06,  1.37it/s]Extractor Predicting: 179it [02:06,  1.37it/s]Extractor Predicting: 180it [02:07,  1.37it/s]Extractor Predicting: 181it [02:08,  1.38it/s]Extractor Predicting: 182it [02:08,  1.37it/s]Extractor Predicting: 183it [02:09,  1.37it/s]Extractor Predicting: 184it [02:10,  1.35it/s]Extractor Predicting: 185it [02:11,  1.33it/s]Extractor Predicting: 186it [02:11,  1.34it/s]Extractor Predicting: 187it [02:12,  1.33it/s]Extractor Predicting: 188it [02:13,  1.34it/s]Extractor Predicting: 189it [02:14,  1.34it/s]Extractor Predicting: 190it [02:14,  1.36it/s]Extractor Predicting: 191it [02:15,  1.33it/s]Extractor Predicting: 192it [02:16,  1.31it/s]Extractor Predicting: 193it [02:17,  1.32it/s]Extractor Predicting: 194it [02:18,  1.32it/s]Extractor Predicting: 195it [02:18,  1.33it/s]Extractor Predicting: 196it [02:19,  1.34it/s]Extractor Predicting: 197it [02:20,  1.33it/s]Extractor Predicting: 198it [02:20,  1.36it/s]Extractor Predicting: 199it [02:21,  1.37it/s]Extractor Predicting: 200it [02:22,  1.35it/s]Extractor Predicting: 201it [02:23,  1.32it/s]Extractor Predicting: 202it [02:23,  1.39it/s]Extractor Predicting: 203it [02:24,  1.38it/s]Extractor Predicting: 204it [02:25,  1.40it/s]Extractor Predicting: 205it [02:26,  1.38it/s]Extractor Predicting: 206it [02:26,  1.26it/s]Extractor Predicting: 207it [02:27,  1.27it/s]Extractor Predicting: 208it [02:28,  1.30it/s]Extractor Predicting: 209it [02:29,  1.26it/s]Extractor Predicting: 210it [02:30,  1.29it/s]Extractor Predicting: 211it [02:30,  1.29it/s]Extractor Predicting: 212it [02:31,  1.30it/s]Extractor Predicting: 213it [02:32,  1.31it/s]Extractor Predicting: 214it [02:33,  1.29it/s]Extractor Predicting: 215it [02:33,  1.33it/s]Extractor Predicting: 216it [02:34,  1.31it/s]Extractor Predicting: 217it [02:35,  1.32it/s]Extractor Predicting: 218it [02:36,  1.33it/s]Extractor Predicting: 219it [02:36,  1.32it/s]Extractor Predicting: 220it [02:37,  1.33it/s]Extractor Predicting: 221it [02:38,  1.30it/s]Extractor Predicting: 222it [02:39,  1.30it/s]Extractor Predicting: 223it [02:39,  1.33it/s]Extractor Predicting: 224it [02:40,  1.33it/s]Extractor Predicting: 225it [02:41,  1.34it/s]Extractor Predicting: 226it [02:42,  1.33it/s]Extractor Predicting: 227it [02:42,  1.36it/s]Extractor Predicting: 228it [02:43,  1.35it/s]Extractor Predicting: 229it [02:44,  1.35it/s]Extractor Predicting: 230it [02:45,  1.40it/s]Extractor Predicting: 231it [02:45,  1.41it/s]Extractor Predicting: 232it [02:46,  1.43it/s]Extractor Predicting: 233it [02:47,  1.40it/s]Extractor Predicting: 234it [02:47,  1.41it/s]Extractor Predicting: 235it [02:48,  1.40it/s]Extractor Predicting: 236it [02:49,  1.38it/s]Extractor Predicting: 237it [02:50,  1.36it/s]Extractor Predicting: 238it [02:50,  1.37it/s]Extractor Predicting: 239it [02:51,  1.36it/s]Extractor Predicting: 240it [02:52,  1.36it/s]Extractor Predicting: 241it [02:53,  1.31it/s]Extractor Predicting: 242it [02:53,  1.34it/s]Extractor Predicting: 243it [02:54,  1.33it/s]Extractor Predicting: 244it [02:55,  1.35it/s]Extractor Predicting: 245it [02:55,  1.37it/s]Extractor Predicting: 246it [02:56,  1.36it/s]Extractor Predicting: 247it [02:57,  1.37it/s]Extractor Predicting: 248it [02:58,  1.37it/s]Extractor Predicting: 249it [02:58,  1.35it/s]Extractor Predicting: 250it [02:59,  1.36it/s]Extractor Predicting: 251it [03:00,  1.37it/s]Extractor Predicting: 252it [03:01,  1.37it/s]Extractor Predicting: 253it [03:01,  1.35it/s]Extractor Predicting: 254it [03:02,  1.37it/s]Extractor Predicting: 255it [03:03,  1.32it/s]Extractor Predicting: 256it [03:04,  1.29it/s]Extractor Predicting: 257it [03:04,  1.30it/s]Extractor Predicting: 258it [03:05,  1.30it/s]Extractor Predicting: 259it [03:06,  1.32it/s]Extractor Predicting: 260it [03:07,  1.30it/s]Extractor Predicting: 261it [03:07,  1.34it/s]Extractor Predicting: 262it [03:08,  1.32it/s]Extractor Predicting: 263it [03:09,  1.31it/s]Extractor Predicting: 264it [03:10,  1.31it/s]Extractor Predicting: 265it [03:11,  1.26it/s]Extractor Predicting: 266it [03:11,  1.27it/s]Extractor Predicting: 267it [03:12,  1.28it/s]Extractor Predicting: 268it [03:13,  1.25it/s]Extractor Predicting: 269it [03:14,  1.29it/s]Extractor Predicting: 270it [03:15,  1.31it/s]Extractor Predicting: 271it [03:15,  1.28it/s]Extractor Predicting: 272it [03:16,  1.30it/s]Extractor Predicting: 273it [03:17,  1.30it/s]Extractor Predicting: 274it [03:18,  1.33it/s]Extractor Predicting: 275it [03:18,  1.34it/s]Extractor Predicting: 276it [03:19,  1.38it/s]Extractor Predicting: 277it [03:20,  1.38it/s]Extractor Predicting: 278it [03:20,  1.37it/s]Extractor Predicting: 279it [03:21,  1.37it/s]Extractor Predicting: 280it [03:22,  1.32it/s]Extractor Predicting: 281it [03:23,  1.32it/s]Extractor Predicting: 282it [03:23,  1.41it/s]Extractor Predicting: 282it [03:23,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:11,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:11,191 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:11,191 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:11,191 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:11,191 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:58:11,817 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:58:11,818 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:58:12,401 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:58:13,457 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:58:13,457 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:16,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:16,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:16,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:16,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:58:16,330 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:58:16,975 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:58:16,976 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:58:17,540 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:58:17,702 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:58:17,702 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.21it/s]Extractor Predicting: 2it [00:01,  1.22it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.28it/s]Extractor Predicting: 5it [00:03,  1.34it/s]Extractor Predicting: 5it [00:03,  1.30it/s]
[INFO|configuration_utils.py:515] 2023-08-28 02:58:21,967 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:58:21,968 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:58:21,973 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:58:21,974 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 02:58:21,977 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:58:24,917 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 02:58:24,923 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 02:58:24,941 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:58:24,941 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:58:24,949 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:58:24,952 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:58:24,952 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:58:24,952 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:58:24,952 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:58:24,953 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:58:24,953 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 02:58:25,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:26,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:26,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:27,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:28,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:29,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:30,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:31,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:32,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:33,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:34,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:35,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:36,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:37,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:38,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:39,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:40,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:41,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:42,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:43,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:44,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:45,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:46,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:47,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:47,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:48,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:49,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:51,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:51,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:52,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:53,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:29<06:50, 29.36s/it][WARNING|generation_utils.py:914] 2023-08-28 02:58:54,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:55,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:56,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:57,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:58,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:58:59,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:00,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:01,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:02,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:03,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:04,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:05,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:06,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:07,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:08,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:09,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:10,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:11,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:12,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:13,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:13,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:14,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:15,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:16,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:17,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:53<05:41, 26.30s/it][WARNING|generation_utils.py:914] 2023-08-28 02:59:18,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:19,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:20,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:21,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:22,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:23,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:24,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:25,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:26,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:27,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:28,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:29,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:30,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:31,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:32,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:33,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:34,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:35,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:36,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:37,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:38,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:39,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:40,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:41,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:42,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:43,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:18<05:09, 25.82s/it][WARNING|generation_utils.py:914] 2023-08-28 02:59:43,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:44,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:45,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:46,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:47,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:48,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:49,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:50,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:51,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:52,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:53,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:54,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:55,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:55,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:56,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:57,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:58,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:59:59,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:00,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:01,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:02,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:03,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:05,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:06,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:41<04:32, 24.76s/it][WARNING|generation_utils.py:914] 2023-08-28 03:00:07,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:08,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:09,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:10,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:11,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:12,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:13,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:14,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:15,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:16,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:17,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:18,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:19,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:20,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:21,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:22,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:23,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:24,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:25,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:26,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:27,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:28,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:28,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [02:04<04:00, 24.06s/it][WARNING|generation_utils.py:914] 2023-08-28 03:00:29,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:31,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:32,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:33,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:34,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:35,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:36,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:37,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:38,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:39,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:40,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:41,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:42,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:43,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:44,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:45,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:46,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:47,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:48,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:49,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:50,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:51,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:52,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:28<03:34, 23.81s/it][WARNING|generation_utils.py:914] 2023-08-28 03:00:53,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:54,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:55,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:56,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:56,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:57,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:00:58,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:00,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:00,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:01,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:02,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:03,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:04,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:05,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:06,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:07,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:07,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:08,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:09,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:11,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:12,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:13,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:13,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:49<03:04, 23.12s/it][WARNING|generation_utils.py:914] 2023-08-28 03:01:14,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:15,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:17,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:17,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:19,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:19,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:20,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:22,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:23,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:24,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:25,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:26,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:27,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:28,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:29,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:30,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:31,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:32,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:33,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:34,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:35,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:36,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:37,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [03:13<02:42, 23.22s/it][WARNING|generation_utils.py:914] 2023-08-28 03:01:38,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:39,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:40,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:41,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:42,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:43,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:44,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:45,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:46,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:47,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:48,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:49,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:50,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:51,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:52,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:53,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:54,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:55,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:56,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:57,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:58,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:01:59,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:00,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:36<02:19, 23.19s/it][WARNING|generation_utils.py:914] 2023-08-28 03:02:01,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:02,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:03,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:04,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:05,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:06,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:06,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:07,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:08,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:09,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:10,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:11,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:12,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:13,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:14,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:15,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:16,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:17,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:18,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:19,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:20,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:21,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [03:57<01:52, 22.55s/it][WARNING|generation_utils.py:914] 2023-08-28 03:02:22,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:23,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:24,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:25,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:27,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:27,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:28,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:29,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:30,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:31,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:32,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:33,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:34,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:35,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:36,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:37,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:38,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:39,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:40,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:41,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:42,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:43,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:44,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:45,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [04:21<01:31, 22.86s/it][WARNING|generation_utils.py:914] 2023-08-28 03:02:46,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:47,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:48,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:48,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:49,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:51,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:51,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:52,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:53,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:55,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:56,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:57,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:57,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:58,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:02:59,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:00,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:01,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:02,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:03,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:04,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:05,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:05,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:06,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:42<01:07, 22.43s/it][WARNING|generation_utils.py:914] 2023-08-28 03:03:07,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:08,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:09,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:10,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:11,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:12,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:13,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:13,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:14,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:15,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:16,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:17,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:18,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:19,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:20,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:21,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:22,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:23,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:24,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:25,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:26,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:27,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:28,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:29,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:29,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [05:05<00:45, 22.63s/it][WARNING|generation_utils.py:914] 2023-08-28 03:03:30,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:31,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:32,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:33,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:34,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:35,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:36,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:38,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:39,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:40,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:41,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:42,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:43,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:44,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:44,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:45,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:46,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:47,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:48,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:49,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:50,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:51,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:52,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:53,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [05:29<00:22, 22.99s/it][WARNING|generation_utils.py:914] 2023-08-28 03:03:54,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:55,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:56,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:57,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:58,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:03:59,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:00,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:01,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:02,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:03,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:04,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:05,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:05,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:06,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:08,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:09,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:10,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:11,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:12,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:13,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:14,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:15,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:16,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 03:04:17,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:53<00:00, 23.26s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:53<00:00, 23.55s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:25,070 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:25,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:25,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:25,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:25,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:04:25,691 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:04:25,692 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:04:26,260 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:04:27,324 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:04:27,324 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:30,426 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:30,431 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:30,431 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:30,431 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:04:30,431 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:04:31,094 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:04:31,095 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:04:31,670 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:04:31,852 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:04:31,852 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n', 'Relation : country . Context : In 1994 , , he was named as the winner of the Eurovision Song Contest 2000 . Head Entity : Eurovision Song Contest 2000 , Tail Entity : France .\n']
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 137, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 174, 'raw': 288}
{'target': 600, 'success': 190, 'raw': 320}
{'target': 600, 'success': 211, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 252, 'raw': 416}
{'target': 600, 'success': 274, 'raw': 448}
{'target': 600, 'success': 294, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 347, 'raw': 576}
{'target': 600, 'success': 361, 'raw': 608}
{'target': 600, 'success': 388, 'raw': 640}
{'target': 600, 'success': 408, 'raw': 672}
{'target': 600, 'success': 428, 'raw': 704}
{'target': 600, 'success': 443, 'raw': 736}
{'target': 600, 'success': 462, 'raw': 768}
{'target': 600, 'success': 483, 'raw': 800}
{'target': 600, 'success': 501, 'raw': 832}
{'target': 600, 'success': 519, 'raw': 864}
{'target': 600, 'success': 541, 'raw': 896}
{'target': 600, 'success': 562, 'raw': 928}
{'target': 600, 'success': 585, 'raw': 960}
{'target': 600, 'success': 608, 'raw': 992}
{'prompt': 'Relation : country .', 'success_rate': 0.6129032258064516, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 485, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year , the band formed New River Band with two of their members at the end of 2010 , Mikey McLeod ( the lyricist ) and Tim McCarroll ( bass ) . Head Entity : Mikey McNamara , Tail Entity : New River Band .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : member of .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : subsidiary . Context : The CIBN ( CBNI ) , also called CBE ( CBW , CBQ , CBQR , CJAX , CJD , CJEC , CJF , CJFY ) , is a United States National Research Council scientific satellite constellation . Head Entity : CBI , Tail Entity : United States National Research Council .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', "('A', 'subsidiary', '', 'On August 2017 , the company began shipping a new product for children in Europe : A .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Battle of Brackewald', 'field of work', '', 'In 1849 he became a volunteer for the British Army at the Battle of Brackewald in Normandy .')", 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : Later in the year ( 1141â€“1231 ) he met Ferdinand I of Spain and the Duke of Prussia , whom he bore in the name of Christophe , together with Robert I of Belgium . Head Entity : Christophe , Tail Entity : John I of Belgium .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.859375, 'errors': {''}}
["Relation : occupation . Context : On 31 March 2014 , the Romanian Army , under a new President of Romania , Luiz InÃ¡cio Lutcic , announced the departure of Luiz 's second - generation Air Force commander , former Admiral of Romania , Sigmund Kaveliu . Head Entity : GeniÃ§iu Lutcic , Tail Entity : Romanian Army .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 14467
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14567, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.15it/s]Extractor Estimating: 2it [00:01,  1.14it/s]Extractor Estimating: 3it [00:02,  1.15it/s]Extractor Estimating: 4it [00:03,  1.16it/s]Extractor Estimating: 5it [00:04,  1.23it/s]Extractor Estimating: 6it [00:04,  1.25it/s]Extractor Estimating: 7it [00:05,  1.27it/s]Extractor Estimating: 8it [00:06,  1.26it/s]Extractor Estimating: 9it [00:07,  1.27it/s]Extractor Estimating: 10it [00:08,  1.27it/s]Extractor Estimating: 11it [00:08,  1.26it/s]Extractor Estimating: 12it [00:09,  1.23it/s]Extractor Estimating: 13it [00:10,  1.21it/s]Extractor Estimating: 14it [00:11,  1.22it/s]Extractor Estimating: 15it [00:12,  1.22it/s]Extractor Estimating: 16it [00:13,  1.20it/s]Extractor Estimating: 17it [00:13,  1.21it/s]Extractor Estimating: 18it [00:14,  1.17it/s]Extractor Estimating: 19it [00:15,  1.21it/s]Extractor Estimating: 20it [00:16,  1.22it/s]Extractor Estimating: 21it [00:17,  1.23it/s]Extractor Estimating: 22it [00:18,  1.18it/s]Extractor Estimating: 23it [00:18,  1.21it/s]Extractor Estimating: 24it [00:19,  1.21it/s]Extractor Estimating: 25it [00:20,  1.23it/s]Extractor Estimating: 26it [00:21,  1.24it/s]Extractor Estimating: 27it [00:22,  1.14it/s]Extractor Estimating: 28it [00:23,  1.18it/s]Extractor Estimating: 29it [00:23,  1.19it/s]Extractor Estimating: 30it [00:24,  1.17it/s]Extractor Estimating: 31it [00:25,  1.23it/s]Extractor Estimating: 32it [00:26,  1.26it/s]Extractor Estimating: 33it [00:27,  1.22it/s]Extractor Estimating: 34it [00:27,  1.23it/s]Extractor Estimating: 35it [00:28,  1.23it/s]Extractor Estimating: 36it [00:29,  1.23it/s]Extractor Estimating: 37it [00:30,  1.21it/s]Extractor Estimating: 38it [00:31,  1.22it/s]Extractor Estimating: 39it [00:32,  1.22it/s]Extractor Estimating: 40it [00:32,  1.26it/s]Extractor Estimating: 41it [00:33,  1.23it/s]Extractor Estimating: 42it [00:34,  1.19it/s]Extractor Estimating: 43it [00:35,  1.19it/s]Extractor Estimating: 44it [00:36,  1.17it/s]Extractor Estimating: 45it [00:37,  1.20it/s]Extractor Estimating: 46it [00:38,  1.17it/s]Extractor Estimating: 47it [00:38,  1.16it/s]Extractor Estimating: 48it [00:39,  1.21it/s]Extractor Estimating: 49it [00:40,  1.15it/s]Extractor Estimating: 50it [00:41,  1.18it/s]Extractor Estimating: 51it [00:42,  1.18it/s]Extractor Estimating: 52it [00:43,  1.18it/s]Extractor Estimating: 53it [00:43,  1.22it/s]Extractor Estimating: 54it [00:44,  1.21it/s]Extractor Estimating: 55it [00:45,  1.23it/s]Extractor Estimating: 56it [00:46,  1.23it/s]Extractor Estimating: 57it [00:47,  1.22it/s]Extractor Estimating: 58it [00:47,  1.23it/s]Extractor Estimating: 59it [00:48,  1.24it/s]Extractor Estimating: 60it [00:49,  1.22it/s]Extractor Estimating: 61it [00:50,  1.21it/s]Extractor Estimating: 62it [00:51,  1.18it/s]Extractor Estimating: 63it [00:52,  1.22it/s]Extractor Estimating: 64it [00:52,  1.21it/s]Extractor Estimating: 65it [00:53,  1.22it/s]Extractor Estimating: 66it [00:54,  1.24it/s]Extractor Estimating: 67it [00:55,  1.24it/s]Extractor Estimating: 68it [00:56,  1.22it/s]Extractor Estimating: 69it [00:56,  1.20it/s]Extractor Estimating: 70it [00:57,  1.20it/s]Extractor Estimating: 71it [00:58,  1.22it/s]Extractor Estimating: 72it [00:59,  1.22it/s]Extractor Estimating: 73it [01:00,  1.20it/s]Extractor Estimating: 74it [01:01,  1.23it/s]Extractor Estimating: 75it [01:01,  1.22it/s]Extractor Estimating: 76it [01:02,  1.25it/s]Extractor Estimating: 77it [01:03,  1.24it/s]Extractor Estimating: 78it [01:04,  1.23it/s]Extractor Estimating: 79it [01:05,  1.23it/s]Extractor Estimating: 80it [01:05,  1.26it/s]Extractor Estimating: 81it [01:06,  1.25it/s]Extractor Estimating: 82it [01:07,  1.23it/s]Extractor Estimating: 83it [01:08,  1.25it/s]Extractor Estimating: 84it [01:09,  1.26it/s]Extractor Estimating: 85it [01:09,  1.25it/s]Extractor Estimating: 86it [01:10,  1.24it/s]Extractor Estimating: 87it [01:11,  1.22it/s]Extractor Estimating: 88it [01:12,  1.23it/s]Extractor Estimating: 89it [01:13,  1.24it/s]Extractor Estimating: 90it [01:13,  1.22it/s]Extractor Estimating: 91it [01:14,  1.24it/s]Extractor Estimating: 92it [01:15,  1.24it/s]Extractor Estimating: 93it [01:16,  1.27it/s]Extractor Estimating: 94it [01:17,  1.24it/s]Extractor Estimating: 95it [01:17,  1.25it/s]Extractor Estimating: 96it [01:18,  1.26it/s]Extractor Estimating: 97it [01:19,  1.24it/s]Extractor Estimating: 98it [01:20,  1.23it/s]Extractor Estimating: 99it [01:21,  1.25it/s]Extractor Estimating: 100it [01:21,  1.24it/s]Extractor Estimating: 101it [01:22,  1.18it/s]Extractor Estimating: 102it [01:23,  1.24it/s]Extractor Estimating: 103it [01:24,  1.28it/s]Extractor Estimating: 104it [01:25,  1.28it/s]Extractor Estimating: 105it [01:26,  1.23it/s]Extractor Estimating: 106it [01:26,  1.23it/s]Extractor Estimating: 107it [01:27,  1.21it/s]Extractor Estimating: 108it [01:28,  1.23it/s]Extractor Estimating: 109it [01:29,  1.24it/s]Extractor Estimating: 110it [01:30,  1.20it/s]Extractor Estimating: 111it [01:30,  1.24it/s]Extractor Estimating: 112it [01:31,  1.19it/s]Extractor Estimating: 113it [01:32,  1.17it/s]Extractor Estimating: 114it [01:33,  1.11it/s]Extractor Estimating: 115it [01:34,  1.16it/s]Extractor Estimating: 116it [01:35,  1.21it/s]Extractor Estimating: 117it [01:36,  1.21it/s]Extractor Estimating: 118it [01:36,  1.24it/s]Extractor Estimating: 119it [01:37,  1.23it/s]Extractor Estimating: 120it [01:38,  1.24it/s]Extractor Estimating: 121it [01:39,  1.28it/s]Extractor Estimating: 122it [01:40,  1.22it/s]Extractor Estimating: 123it [01:40,  1.27it/s]Extractor Estimating: 124it [01:41,  1.25it/s]Extractor Estimating: 125it [01:42,  1.23it/s]Extractor Estimating: 126it [01:43,  1.28it/s]Extractor Estimating: 127it [01:43,  1.26it/s]Extractor Estimating: 128it [01:44,  1.28it/s]Extractor Estimating: 129it [01:45,  1.28it/s]Extractor Estimating: 130it [01:46,  1.34it/s]Extractor Estimating: 131it [01:46,  1.32it/s]Extractor Estimating: 132it [01:47,  1.32it/s]Extractor Estimating: 133it [01:48,  1.32it/s]Extractor Estimating: 134it [01:49,  1.29it/s]Extractor Estimating: 135it [01:50,  1.32it/s]Extractor Estimating: 136it [01:50,  1.32it/s]Extractor Estimating: 137it [01:51,  1.24it/s]Extractor Estimating: 138it [01:52,  1.25it/s]Extractor Estimating: 139it [01:53,  1.29it/s]Extractor Estimating: 140it [01:53,  1.30it/s]Extractor Estimating: 141it [01:54,  1.29it/s]Extractor Estimating: 142it [01:55,  1.29it/s]Extractor Estimating: 143it [01:56,  1.31it/s]Extractor Estimating: 144it [01:57,  1.30it/s]Extractor Estimating: 145it [01:57,  1.27it/s]Extractor Estimating: 146it [01:58,  1.31it/s]Extractor Estimating: 147it [01:59,  1.29it/s]Extractor Estimating: 148it [02:00,  1.26it/s]Extractor Estimating: 149it [02:01,  1.22it/s]Extractor Estimating: 150it [02:01,  1.22it/s]Extractor Estimating: 151it [02:02,  1.27it/s]Extractor Estimating: 152it [02:03,  1.26it/s]Extractor Estimating: 153it [02:04,  1.27it/s]Extractor Estimating: 154it [02:05,  1.26it/s]Extractor Estimating: 155it [02:05,  1.26it/s]Extractor Estimating: 156it [02:06,  1.26it/s]Extractor Estimating: 157it [02:07,  1.22it/s]Extractor Estimating: 158it [02:08,  1.23it/s]Extractor Estimating: 159it [02:09,  1.22it/s]Extractor Estimating: 160it [02:09,  1.23it/s]Extractor Estimating: 161it [02:10,  1.24it/s]Extractor Estimating: 162it [02:11,  1.25it/s]Extractor Estimating: 163it [02:12,  1.25it/s]Extractor Estimating: 164it [02:13,  1.27it/s]Extractor Estimating: 165it [02:13,  1.30it/s]Extractor Estimating: 166it [02:14,  1.28it/s]Extractor Estimating: 167it [02:15,  1.25it/s]Extractor Estimating: 168it [02:16,  1.25it/s]Extractor Estimating: 169it [02:16,  1.27it/s]Extractor Estimating: 170it [02:17,  1.32it/s]Extractor Estimating: 171it [02:18,  1.31it/s]Extractor Estimating: 172it [02:19,  1.24it/s]Extractor Estimating: 173it [02:20,  1.22it/s]Extractor Estimating: 174it [02:21,  1.21it/s]Extractor Estimating: 175it [02:21,  1.23it/s]Extractor Estimating: 176it [02:22,  1.20it/s]Extractor Estimating: 177it [02:23,  1.22it/s]Extractor Estimating: 178it [02:24,  1.24it/s]Extractor Estimating: 179it [02:25,  1.25it/s]Extractor Estimating: 180it [02:25,  1.24it/s]Extractor Estimating: 181it [02:26,  1.28it/s]Extractor Estimating: 182it [02:27,  1.26it/s]Extractor Estimating: 183it [02:28,  1.25it/s]Extractor Estimating: 184it [02:29,  1.25it/s]Extractor Estimating: 185it [02:29,  1.26it/s]Extractor Estimating: 186it [02:30,  1.26it/s]Extractor Estimating: 187it [02:31,  1.24it/s]Extractor Estimating: 188it [02:32,  1.17it/s]Extractor Estimating: 189it [02:33,  1.09it/s]Extractor Estimating: 190it [02:34,  1.15it/s]Extractor Estimating: 191it [02:34,  1.21it/s]Extractor Estimating: 192it [02:35,  1.19it/s]Extractor Estimating: 193it [02:36,  1.21it/s]Extractor Estimating: 194it [02:37,  1.20it/s]Extractor Estimating: 195it [02:38,  1.23it/s]Extractor Estimating: 196it [02:39,  1.26it/s]Extractor Estimating: 197it [02:39,  1.22it/s]Extractor Estimating: 198it [02:40,  1.23it/s]Extractor Estimating: 199it [02:41,  1.27it/s]Extractor Estimating: 200it [02:42,  1.24it/s]Extractor Estimating: 201it [02:43,  1.21it/s]Extractor Estimating: 202it [02:43,  1.22it/s]Extractor Estimating: 203it [02:44,  1.21it/s]Extractor Estimating: 204it [02:45,  1.22it/s]Extractor Estimating: 205it [02:46,  1.24it/s]Extractor Estimating: 206it [02:47,  1.22it/s]Extractor Estimating: 207it [02:47,  1.26it/s]Extractor Estimating: 208it [02:48,  1.19it/s]Extractor Estimating: 209it [02:49,  1.17it/s]Extractor Estimating: 210it [02:50,  1.20it/s]Extractor Estimating: 211it [02:51,  1.19it/s]Extractor Estimating: 212it [02:52,  1.18it/s]Extractor Estimating: 213it [02:53,  1.15it/s]Extractor Estimating: 214it [02:54,  1.17it/s]Extractor Estimating: 215it [02:54,  1.19it/s]Extractor Estimating: 216it [02:55,  1.15it/s]Extractor Estimating: 217it [02:56,  1.15it/s]Extractor Estimating: 218it [02:57,  1.18it/s]Extractor Estimating: 219it [02:58,  1.16it/s]Extractor Estimating: 220it [02:59,  1.18it/s]Extractor Estimating: 221it [03:00,  1.14it/s]Extractor Estimating: 222it [03:00,  1.14it/s]Extractor Estimating: 223it [03:01,  1.15it/s]Extractor Estimating: 224it [03:02,  1.16it/s]Extractor Estimating: 225it [03:03,  1.18it/s]Extractor Estimating: 226it [03:04,  1.16it/s]Extractor Estimating: 227it [03:05,  1.22it/s]Extractor Estimating: 228it [03:05,  1.26it/s]Extractor Estimating: 229it [03:06,  1.28it/s]Extractor Estimating: 230it [03:07,  1.28it/s]Extractor Estimating: 231it [03:08,  1.31it/s]Extractor Estimating: 232it [03:08,  1.35it/s]Extractor Estimating: 233it [03:09,  1.38it/s]Extractor Estimating: 234it [03:10,  1.36it/s]Extractor Estimating: 235it [03:11,  1.34it/s]Extractor Estimating: 236it [03:11,  1.33it/s]Extractor Estimating: 237it [03:12,  1.32it/s]Extractor Estimating: 238it [03:13,  1.31it/s]Extractor Estimating: 239it [03:14,  1.30it/s]Extractor Estimating: 240it [03:14,  1.31it/s]Extractor Estimating: 241it [03:15,  1.34it/s]Extractor Estimating: 242it [03:16,  1.33it/s]Extractor Estimating: 243it [03:17,  1.29it/s]Extractor Estimating: 244it [03:17,  1.28it/s]Extractor Estimating: 245it [03:18,  1.30it/s]Extractor Estimating: 246it [03:19,  1.32it/s]Extractor Estimating: 247it [03:20,  1.31it/s]Extractor Estimating: 248it [03:21,  1.28it/s]Extractor Estimating: 249it [03:21,  1.28it/s]Extractor Estimating: 250it [03:22,  1.30it/s]Extractor Estimating: 251it [03:23,  1.22it/s]Extractor Estimating: 252it [03:24,  1.26it/s]Extractor Estimating: 253it [03:24,  1.26it/s]Extractor Estimating: 254it [03:25,  1.20it/s]Extractor Estimating: 255it [03:26,  1.24it/s]Extractor Estimating: 256it [03:27,  1.32it/s]Extractor Estimating: 257it [03:28,  1.30it/s]Extractor Estimating: 258it [03:29,  1.21it/s]Extractor Estimating: 259it [03:29,  1.25it/s]Extractor Estimating: 260it [03:30,  1.27it/s]Extractor Estimating: 261it [03:31,  1.23it/s]Extractor Estimating: 262it [03:32,  1.14it/s]Extractor Estimating: 263it [03:33,  1.17it/s]Extractor Estimating: 264it [03:34,  1.20it/s]Extractor Estimating: 265it [03:34,  1.17it/s]Extractor Estimating: 266it [03:35,  1.20it/s]Extractor Estimating: 267it [03:36,  1.18it/s]Extractor Estimating: 268it [03:37,  1.24it/s]Extractor Estimating: 269it [03:38,  1.25it/s]Extractor Estimating: 270it [03:38,  1.22it/s]Extractor Estimating: 271it [03:39,  1.21it/s]Extractor Estimating: 272it [03:40,  1.20it/s]Extractor Estimating: 273it [03:41,  1.22it/s]Extractor Estimating: 274it [03:42,  1.20it/s]Extractor Estimating: 275it [03:43,  1.23it/s]Extractor Estimating: 276it [03:43,  1.28it/s]Extractor Estimating: 277it [03:44,  1.27it/s]Extractor Estimating: 278it [03:45,  1.32it/s]Extractor Estimating: 279it [03:46,  1.31it/s]Extractor Estimating: 280it [03:46,  1.30it/s]Extractor Estimating: 281it [03:47,  1.32it/s]Extractor Estimating: 282it [03:48,  1.35it/s]Extractor Estimating: 283it [03:49,  1.33it/s]Extractor Estimating: 284it [03:49,  1.30it/s]Extractor Estimating: 285it [03:50,  1.32it/s]Extractor Estimating: 286it [03:51,  1.32it/s]Extractor Estimating: 287it [03:52,  1.32it/s]Extractor Estimating: 288it [03:52,  1.28it/s]Extractor Estimating: 289it [03:53,  1.30it/s]Extractor Estimating: 290it [03:54,  1.30it/s]Extractor Estimating: 291it [03:55,  1.34it/s]Extractor Estimating: 292it [03:55,  1.30it/s]Extractor Estimating: 293it [03:56,  1.32it/s]Extractor Estimating: 294it [03:57,  1.31it/s]Extractor Estimating: 295it [03:58,  1.30it/s]Extractor Estimating: 296it [03:58,  1.33it/s]Extractor Estimating: 297it [03:59,  1.35it/s]Extractor Estimating: 298it [04:00,  1.36it/s]Extractor Estimating: 299it [04:01,  1.39it/s]Extractor Estimating: 300it [04:01,  1.40it/s]Extractor Estimating: 301it [04:02,  1.32it/s]Extractor Estimating: 302it [04:03,  1.26it/s]Extractor Estimating: 303it [04:04,  1.26it/s]Extractor Estimating: 304it [04:05,  1.32it/s]Extractor Estimating: 305it [04:05,  1.31it/s]Extractor Estimating: 306it [04:06,  1.34it/s]Extractor Estimating: 307it [04:07,  1.34it/s]Extractor Estimating: 308it [04:07,  1.40it/s]Extractor Estimating: 309it [04:08,  1.39it/s]Extractor Estimating: 310it [04:09,  1.34it/s]Extractor Estimating: 311it [04:10,  1.27it/s]Extractor Estimating: 312it [04:10,  1.32it/s]Extractor Estimating: 313it [04:11,  1.30it/s]Extractor Estimating: 314it [04:12,  1.28it/s]Extractor Estimating: 315it [04:13,  1.16it/s]Extractor Estimating: 316it [04:14,  1.21it/s]Extractor Estimating: 317it [04:15,  1.24it/s]Extractor Estimating: 318it [04:15,  1.23it/s]Extractor Estimating: 319it [04:16,  1.27it/s]Extractor Estimating: 320it [04:17,  1.22it/s]Extractor Estimating: 321it [04:18,  1.27it/s]Extractor Estimating: 322it [04:19,  1.26it/s]Extractor Estimating: 323it [04:19,  1.28it/s]Extractor Estimating: 324it [04:20,  1.29it/s]Extractor Estimating: 325it [04:21,  1.30it/s]Extractor Estimating: 326it [04:22,  1.18it/s]Extractor Estimating: 327it [04:23,  1.21it/s]Extractor Estimating: 328it [04:24,  1.19it/s]Extractor Estimating: 329it [04:24,  1.23it/s]Extractor Estimating: 330it [04:25,  1.23it/s]Extractor Estimating: 331it [04:26,  1.24it/s]Extractor Estimating: 332it [04:27,  1.24it/s]Extractor Estimating: 333it [04:28,  1.23it/s]Extractor Estimating: 334it [04:28,  1.25it/s]Extractor Estimating: 335it [04:29,  1.23it/s]Extractor Estimating: 336it [04:30,  1.24it/s]Extractor Estimating: 337it [04:31,  1.26it/s]Extractor Estimating: 338it [04:32,  1.24it/s]Extractor Estimating: 339it [04:32,  1.25it/s]Extractor Estimating: 340it [04:33,  1.27it/s]Extractor Estimating: 341it [04:34,  1.21it/s]Extractor Estimating: 342it [04:35,  1.23it/s]Extractor Estimating: 343it [04:36,  1.22it/s]Extractor Estimating: 344it [04:36,  1.22it/s]Extractor Estimating: 345it [04:37,  1.24it/s]Extractor Estimating: 346it [04:38,  1.19it/s]Extractor Estimating: 347it [04:39,  1.13it/s]Extractor Estimating: 348it [04:40,  1.12it/s]Extractor Estimating: 349it [04:41,  1.06it/s]Extractor Estimating: 350it [04:42,  1.11it/s]Extractor Estimating: 351it [04:43,  1.15it/s]Extractor Estimating: 352it [04:43,  1.20it/s]Extractor Estimating: 353it [04:44,  1.23it/s]Extractor Estimating: 354it [04:45,  1.28it/s]Extractor Estimating: 355it [04:46,  1.31it/s]Extractor Estimating: 356it [04:46,  1.26it/s]Extractor Estimating: 357it [04:47,  1.27it/s]Extractor Estimating: 358it [04:48,  1.27it/s]Extractor Estimating: 359it [04:49,  1.33it/s]Extractor Estimating: 360it [04:49,  1.34it/s]Extractor Estimating: 361it [04:50,  1.32it/s]Extractor Estimating: 362it [04:51,  1.29it/s]Extractor Estimating: 363it [04:52,  1.33it/s]Extractor Estimating: 364it [04:53,  1.29it/s]Extractor Estimating: 365it [04:53,  1.23it/s]Extractor Estimating: 366it [04:54,  1.24it/s]Extractor Estimating: 367it [04:55,  1.26it/s]Extractor Estimating: 368it [04:56,  1.24it/s]Extractor Estimating: 369it [04:57,  1.25it/s]Extractor Estimating: 370it [04:57,  1.27it/s]Extractor Estimating: 371it [04:58,  1.25it/s]Extractor Estimating: 372it [04:59,  1.29it/s]Extractor Estimating: 373it [05:00,  1.26it/s]Extractor Estimating: 374it [05:01,  1.24it/s]Extractor Estimating: 375it [05:01,  1.51it/s]Extractor Estimating: 375it [05:01,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:46,265 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:46,269 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:46,270 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:46,270 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:46,270 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:09:46,906 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:09:46,907 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:09:47,480 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:09:48,530 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:09:48,530 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:51,470 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:51,475 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:51,475 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:51,475 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:09:51,475 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:09:52,117 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:09:52,118 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:09:52,694 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:09:52,862 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:09:52,862 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:11:34,640 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:11:34,660 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7877 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 26311
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26411, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=26411, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.371, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.396, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.380, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 71, avg_time 1.370, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 171, avg_time 1.383, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 271, avg_time 2.773, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 42, avg_time 1.393, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 142, avg_time 1.385, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 242, avg_time 1.373, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.373, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 113, avg_time 2.779, loss:nan
g_step 1200, step 213, avg_time 1.389, loss:nan
g_step 1300, step 313, avg_time 1.373, loss:nan
g_step 1400, step 84, avg_time 1.373, loss:nan
g_step 1500, step 184, avg_time 1.390, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 284, avg_time 2.756, loss:nan
g_step 1700, step 55, avg_time 1.380, loss:nan
g_step 1800, step 155, avg_time 1.396, loss:nan
g_step 1900, step 255, avg_time 1.376, loss:nan
g_step 2000, step 26, avg_time 1.352, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 126, avg_time 2.771, loss:nan
g_step 2200, step 226, avg_time 1.379, loss:nan
g_step 2300, step 326, avg_time 1.397, loss:nan
g_step 2400, step 97, avg_time 1.365, loss:nan
g_step 2500, step 197, avg_time 1.379, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 297, avg_time 2.781, loss:nan
g_step 2700, step 68, avg_time 1.381, loss:nan
g_step 2800, step 168, avg_time 1.389, loss:nan
g_step 2900, step 268, avg_time 1.371, loss:nan
g_step 3000, step 39, avg_time 1.395, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 139, avg_time 2.763, loss:nan
g_step 3200, step 239, avg_time 1.393, loss:nan
g_step 3300, step 10, avg_time 1.380, loss:nan
g_step 3400, step 110, avg_time 1.393, loss:nan
g_step 3500, step 210, avg_time 1.363, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 310, avg_time 2.767, loss:nan
g_step 3700, step 81, avg_time 1.361, loss:nan
g_step 3800, step 181, avg_time 1.387, loss:nan
g_step 3900, step 281, avg_time 1.396, loss:nan
g_step 4000, step 52, avg_time 1.369, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 152, avg_time 2.786, loss:nan
g_step 4200, step 252, avg_time 1.396, loss:nan
g_step 4300, step 23, avg_time 1.363, loss:nan
g_step 4400, step 123, avg_time 1.387, loss:nan
g_step 4500, step 223, avg_time 1.392, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 323, avg_time 2.774, loss:nan
g_step 4700, step 94, avg_time 1.379, loss:nan
g_step 4800, step 194, avg_time 1.374, loss:nan
g_step 4900, step 294, avg_time 1.398, loss:nan
g_step 5000, step 65, avg_time 1.378, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 165, avg_time 2.782, loss:nan
g_step 5200, step 265, avg_time 1.392, loss:nan
g_step 5300, step 36, avg_time 1.358, loss:nan
g_step 5400, step 136, avg_time 1.401, loss:nan
g_step 5500, step 236, avg_time 1.374, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 7, avg_time 2.775, loss:nan
g_step 5700, step 107, avg_time 1.376, loss:nan
g_step 5800, step 207, avg_time 1.397, loss:nan
g_step 5900, step 307, avg_time 1.380, loss:nan
g_step 6000, step 78, avg_time 1.380, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 178, avg_time 2.782, loss:nan
g_step 6200, step 278, avg_time 1.375, loss:nan
g_step 6300, step 49, avg_time 1.364, loss:nan
g_step 6400, step 149, avg_time 1.392, loss:nan
g_step 6500, step 249, avg_time 1.393, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:11:34 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:11:34 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-11-34_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:11:35 - WARNING - datasets.builder -   Using custom data configuration default-f83d70ea5503dc44
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f83d70ea5503dc44/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:11:35,910 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:11:35,911 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:11:35,911 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:11:35,912 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:11:35,920 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:11:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:11:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:11:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:11:35,926 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:11:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:11:35,926 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:11:36,048 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:11:39,133 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:11:39,134 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f83d70ea5503dc44/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  3.20ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.99ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.28ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  4.43ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.50ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.56ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.61ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.75ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.48ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  4.13ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.40ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.52ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.72ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.15ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  7.11ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:00,  7.69ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  8.01ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  8.15ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  8.27ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:00<00:00,  8.46ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00,  8.53ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.78ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  7.20ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  7.95ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  8.32ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  9.40ba/s]
[INFO|trainer.py:414] 2023-08-28 06:11:43,452 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:11:43,463 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:11:43,463 >>   Num examples = 7900
[INFO|trainer.py:1149] 2023-08-28 06:11:43,464 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:11:43,464 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:11:43,464 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:11:43,464 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:11:43,464 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<02:55,  3.49it/s]  0%|          | 2/615 [00:00<02:51,  3.58it/s]  0%|          | 3/615 [00:00<02:49,  3.61it/s]  1%|          | 4/615 [00:01<02:48,  3.62it/s]  1%|          | 5/615 [00:01<02:48,  3.63it/s]  1%|          | 6/615 [00:01<02:47,  3.63it/s]  1%|          | 7/615 [00:01<02:47,  3.64it/s]  1%|â–         | 8/615 [00:02<02:47,  3.62it/s]  1%|â–         | 9/615 [00:02<02:46,  3.63it/s]  2%|â–         | 10/615 [00:02<02:46,  3.64it/s]  2%|â–         | 11/615 [00:03<02:45,  3.64it/s]  2%|â–         | 12/615 [00:03<02:45,  3.65it/s]  2%|â–         | 13/615 [00:03<02:44,  3.65it/s]  2%|â–         | 14/615 [00:03<02:44,  3.65it/s]  2%|â–         | 15/615 [00:04<02:44,  3.65it/s]  3%|â–Ž         | 16/615 [00:04<02:43,  3.65it/s]  3%|â–Ž         | 17/615 [00:04<02:43,  3.65it/s]  3%|â–Ž         | 18/615 [00:04<02:43,  3.65it/s]  3%|â–Ž         | 19/615 [00:05<02:43,  3.64it/s]  3%|â–Ž         | 20/615 [00:05<02:43,  3.65it/s]  3%|â–Ž         | 21/615 [00:05<02:42,  3.65it/s]  4%|â–Ž         | 22/615 [00:06<02:42,  3.65it/s]  4%|â–Ž         | 23/615 [00:06<02:42,  3.65it/s]  4%|â–         | 24/615 [00:06<02:41,  3.65it/s]  4%|â–         | 25/615 [00:06<02:41,  3.65it/s]  4%|â–         | 26/615 [00:07<02:41,  3.65it/s]  4%|â–         | 27/615 [00:07<02:41,  3.65it/s]  5%|â–         | 28/615 [00:07<02:41,  3.64it/s]  5%|â–         | 29/615 [00:07<02:40,  3.64it/s]  5%|â–         | 30/615 [00:08<02:40,  3.64it/s]  5%|â–Œ         | 31/615 [00:08<02:40,  3.64it/s]  5%|â–Œ         | 32/615 [00:08<02:39,  3.65it/s]  5%|â–Œ         | 33/615 [00:09<02:39,  3.65it/s]  6%|â–Œ         | 34/615 [00:09<02:39,  3.65it/s]  6%|â–Œ         | 35/615 [00:09<02:38,  3.65it/s]  6%|â–Œ         | 36/615 [00:09<02:38,  3.65it/s]  6%|â–Œ         | 37/615 [00:10<02:38,  3.65it/s]  6%|â–Œ         | 38/615 [00:10<02:38,  3.65it/s]  6%|â–‹         | 39/615 [00:10<02:37,  3.65it/s]  7%|â–‹         | 40/615 [00:10<02:37,  3.65it/s]  7%|â–‹         | 41/615 [00:11<02:37,  3.63it/s]  7%|â–‹         | 42/615 [00:11<02:37,  3.64it/s]  7%|â–‹         | 43/615 [00:11<02:37,  3.64it/s]  7%|â–‹         | 44/615 [00:12<02:36,  3.64it/s]  7%|â–‹         | 45/615 [00:12<02:36,  3.64it/s]  7%|â–‹         | 46/615 [00:12<02:36,  3.64it/s]  8%|â–Š         | 47/615 [00:12<02:35,  3.64it/s]  8%|â–Š         | 48/615 [00:13<02:35,  3.65it/s]  8%|â–Š         | 49/615 [00:13<02:35,  3.65it/s]  8%|â–Š         | 50/615 [00:13<02:34,  3.65it/s]  8%|â–Š         | 51/615 [00:14<02:34,  3.65it/s]  8%|â–Š         | 52/615 [00:14<02:34,  3.64it/s]  9%|â–Š         | 53/615 [00:14<02:34,  3.64it/s]  9%|â–‰         | 54/615 [00:14<02:34,  3.64it/s]  9%|â–‰         | 55/615 [00:15<02:33,  3.64it/s]  9%|â–‰         | 56/615 [00:15<02:33,  3.65it/s]  9%|â–‰         | 57/615 [00:15<02:33,  3.65it/s]  9%|â–‰         | 58/615 [00:15<02:32,  3.64it/s] 10%|â–‰         | 59/615 [00:16<02:32,  3.65it/s] 10%|â–‰         | 60/615 [00:16<02:32,  3.65it/s] 10%|â–‰         | 61/615 [00:16<02:31,  3.65it/s] 10%|â–ˆ         | 62/615 [00:17<02:31,  3.65it/s] 10%|â–ˆ         | 63/615 [00:17<02:31,  3.64it/s] 10%|â–ˆ         | 64/615 [00:17<02:31,  3.64it/s] 11%|â–ˆ         | 65/615 [00:17<02:30,  3.64it/s] 11%|â–ˆ         | 66/615 [00:18<02:30,  3.64it/s] 11%|â–ˆ         | 67/615 [00:18<02:30,  3.65it/s] 11%|â–ˆ         | 68/615 [00:18<02:30,  3.65it/s] 11%|â–ˆ         | 69/615 [00:18<02:29,  3.65it/s] 11%|â–ˆâ–        | 70/615 [00:19<02:29,  3.65it/s] 12%|â–ˆâ–        | 71/615 [00:19<02:29,  3.65it/s] 12%|â–ˆâ–        | 72/615 [00:19<02:28,  3.65it/s] 12%|â–ˆâ–        | 73/615 [00:20<02:28,  3.65it/s] 12%|â–ˆâ–        | 74/615 [00:20<02:28,  3.64it/s] 12%|â–ˆâ–        | 75/615 [00:20<02:28,  3.64it/s] 12%|â–ˆâ–        | 76/615 [00:20<02:28,  3.64it/s] 13%|â–ˆâ–Ž        | 77/615 [00:21<02:27,  3.64it/s] 13%|â–ˆâ–Ž        | 78/615 [00:21<02:27,  3.64it/s] 13%|â–ˆâ–Ž        | 79/615 [00:21<02:27,  3.64it/s] 13%|â–ˆâ–Ž        | 80/615 [00:21<02:26,  3.64it/s] 13%|â–ˆâ–Ž        | 81/615 [00:22<02:26,  3.64it/s] 13%|â–ˆâ–Ž        | 82/615 [00:22<02:26,  3.65it/s] 13%|â–ˆâ–Ž        | 83/615 [00:22<02:25,  3.65it/s] 14%|â–ˆâ–Ž        | 84/615 [00:23<02:25,  3.65it/s] 14%|â–ˆâ–        | 85/615 [00:23<02:25,  3.65it/s] 14%|â–ˆâ–        | 86/615 [00:23<02:25,  3.65it/s] 14%|â–ˆâ–        | 87/615 [00:23<02:24,  3.65it/s] 14%|â–ˆâ–        | 88/615 [00:24<02:24,  3.65it/s] 14%|â–ˆâ–        | 89/615 [00:24<02:24,  3.65it/s] 15%|â–ˆâ–        | 90/615 [00:24<02:24,  3.65it/s] 15%|â–ˆâ–        | 91/615 [00:24<02:24,  3.64it/s] 15%|â–ˆâ–        | 92/615 [00:25<02:23,  3.64it/s] 15%|â–ˆâ–Œ        | 93/615 [00:25<02:23,  3.64it/s] 15%|â–ˆâ–Œ        | 94/615 [00:25<02:23,  3.64it/s] 15%|â–ˆâ–Œ        | 95/615 [00:26<02:22,  3.64it/s] 16%|â–ˆâ–Œ        | 96/615 [00:26<02:22,  3.65it/s] 16%|â–ˆâ–Œ        | 97/615 [00:26<02:22,  3.65it/s] 16%|â–ˆâ–Œ        | 98/615 [00:26<02:21,  3.64it/s] 16%|â–ˆâ–Œ        | 99/615 [00:27<02:21,  3.65it/s] 16%|â–ˆâ–‹        | 100/615 [00:27<02:21,  3.65it/s] 16%|â–ˆâ–‹        | 101/615 [00:27<02:21,  3.64it/s] 17%|â–ˆâ–‹        | 102/615 [00:27<02:21,  3.63it/s] 17%|â–ˆâ–‹        | 103/615 [00:28<02:20,  3.64it/s] 17%|â–ˆâ–‹        | 104/615 [00:28<02:20,  3.64it/s] 17%|â–ˆâ–‹        | 105/615 [00:28<02:20,  3.64it/s] 17%|â–ˆâ–‹        | 106/615 [00:29<02:19,  3.64it/s] 17%|â–ˆâ–‹        | 107/615 [00:29<02:19,  3.64it/s] 18%|â–ˆâ–Š        | 108/615 [00:29<02:19,  3.64it/s] 18%|â–ˆâ–Š        | 109/615 [00:29<02:18,  3.64it/s] 18%|â–ˆâ–Š        | 110/615 [00:30<02:18,  3.64it/s] 18%|â–ˆâ–Š        | 111/615 [00:30<02:18,  3.65it/s] 18%|â–ˆâ–Š        | 112/615 [00:30<02:17,  3.65it/s] 18%|â–ˆâ–Š        | 113/615 [00:31<02:18,  3.61it/s] 19%|â–ˆâ–Š        | 114/615 [00:31<02:18,  3.62it/s] 19%|â–ˆâ–Š        | 115/615 [00:31<02:17,  3.63it/s] 19%|â–ˆâ–‰        | 116/615 [00:31<02:17,  3.63it/s] 19%|â–ˆâ–‰        | 117/615 [00:32<02:16,  3.64it/s] 19%|â–ˆâ–‰        | 118/615 [00:32<02:16,  3.64it/s] 19%|â–ˆâ–‰        | 119/615 [00:32<02:16,  3.64it/s] 20%|â–ˆâ–‰        | 120/615 [00:32<02:15,  3.64it/s] 20%|â–ˆâ–‰        | 121/615 [00:33<02:15,  3.64it/s] 20%|â–ˆâ–‰        | 122/615 [00:33<02:15,  3.64it/s] 20%|â–ˆâ–ˆ        | 123/615 [00:33<02:15,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 06:12:17,358 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:12:17,359 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 06:12:17,359 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.50it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.79it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.92it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.20it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.77it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.46it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.26it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 47.00it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.92it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.93it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.91it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.93it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.93it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.91it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.89it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.90it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.81it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.79it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.82it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.83it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.83it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.85it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.82it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.84it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.86it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.85it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.82it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.84it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.83it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.81it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.83it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.83it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.84it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.88it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.80it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.79it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.85it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.78it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.81it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.86it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.80it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.85it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.87it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.77it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.81it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.84it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.78it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.83it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 46.80it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.80it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.82it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.83it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.78it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.81it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.82it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.72it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.70it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.69it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.73it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.78it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.79it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.77it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.81it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.80it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.80it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.78it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.70it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.70it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.68it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.71it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.75it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.81it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.80it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.81it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.83it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.77it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.77it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.72it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.72it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.74it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.72it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.70it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.78it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.74it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.71it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.71it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.71it/s][A 20%|â–ˆâ–ˆ        | 123/615 [00:43<02:15,  3.64it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:12:26,676 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-28 06:12:26,694 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:12:28,760 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:12:28,774 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:12:28,784 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 124/615 [00:45<31:12,  3.81s/it] 20%|â–ˆâ–ˆ        | 125/615 [00:46<22:28,  2.75s/it] 20%|â–ˆâ–ˆ        | 126/615 [00:46<16:23,  2.01s/it] 21%|â–ˆâ–ˆ        | 127/615 [00:46<12:07,  1.49s/it] 21%|â–ˆâ–ˆ        | 128/615 [00:46<09:08,  1.13s/it] 21%|â–ˆâ–ˆ        | 129/615 [00:47<07:02,  1.15it/s] 21%|â–ˆâ–ˆ        | 130/615 [00:47<05:35,  1.45it/s] 21%|â–ˆâ–ˆâ–       | 131/615 [00:47<04:34,  1.77it/s] 21%|â–ˆâ–ˆâ–       | 132/615 [00:48<03:51,  2.09it/s] 22%|â–ˆâ–ˆâ–       | 133/615 [00:48<03:21,  2.40it/s] 22%|â–ˆâ–ˆâ–       | 134/615 [00:48<03:00,  2.67it/s] 22%|â–ˆâ–ˆâ–       | 135/615 [00:48<02:45,  2.91it/s] 22%|â–ˆâ–ˆâ–       | 136/615 [00:49<02:34,  3.10it/s] 22%|â–ˆâ–ˆâ–       | 137/615 [00:49<02:27,  3.24it/s] 22%|â–ˆâ–ˆâ–       | 138/615 [00:49<02:22,  3.35it/s] 23%|â–ˆâ–ˆâ–Ž       | 139/615 [00:49<02:18,  3.43it/s] 23%|â–ˆâ–ˆâ–Ž       | 140/615 [00:50<02:16,  3.49it/s] 23%|â–ˆâ–ˆâ–Ž       | 141/615 [00:50<02:14,  3.54it/s] 23%|â–ˆâ–ˆâ–Ž       | 142/615 [00:50<02:12,  3.57it/s] 23%|â–ˆâ–ˆâ–Ž       | 143/615 [00:51<02:11,  3.59it/s] 23%|â–ˆâ–ˆâ–Ž       | 144/615 [00:51<02:13,  3.53it/s] 24%|â–ˆâ–ˆâ–Ž       | 145/615 [00:51<02:12,  3.54it/s] 24%|â–ˆâ–ˆâ–Ž       | 146/615 [00:51<02:11,  3.57it/s] 24%|â–ˆâ–ˆâ–       | 147/615 [00:52<02:10,  3.60it/s] 24%|â–ˆâ–ˆâ–       | 148/615 [00:52<02:10,  3.57it/s] 24%|â–ˆâ–ˆâ–       | 149/615 [00:52<02:09,  3.60it/s] 24%|â–ˆâ–ˆâ–       | 150/615 [00:53<02:08,  3.61it/s] 25%|â–ˆâ–ˆâ–       | 151/615 [00:53<02:08,  3.62it/s] 25%|â–ˆâ–ˆâ–       | 152/615 [00:53<02:07,  3.63it/s] 25%|â–ˆâ–ˆâ–       | 153/615 [00:53<02:07,  3.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 154/615 [00:54<02:06,  3.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 155/615 [00:54<02:06,  3.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 156/615 [00:54<02:05,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 157/615 [00:54<02:05,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 158/615 [00:55<02:05,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 159/615 [00:55<02:05,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 160/615 [00:55<02:05,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 161/615 [00:56<02:04,  3.64it/s] 26%|â–ˆâ–ˆâ–‹       | 162/615 [00:56<02:04,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 163/615 [00:56<02:04,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 164/615 [00:56<02:03,  3.65it/s] 27%|â–ˆâ–ˆâ–‹       | 165/615 [00:57<02:03,  3.65it/s] 27%|â–ˆâ–ˆâ–‹       | 166/615 [00:57<02:03,  3.64it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|â–ˆâ–ˆâ–‹       | 167/615 [00:57<02:02,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 168/615 [00:57<02:02,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 169/615 [00:58<02:02,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 170/615 [00:58<02:02,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 171/615 [00:58<02:02,  3.62it/s] 28%|â–ˆâ–ˆâ–Š       | 172/615 [00:59<02:02,  3.63it/s] 28%|â–ˆâ–ˆâ–Š       | 173/615 [00:59<02:01,  3.63it/s] 28%|â–ˆâ–ˆâ–Š       | 174/615 [00:59<02:01,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 175/615 [00:59<02:00,  3.64it/s] 29%|â–ˆâ–ˆâ–Š       | 176/615 [01:00<02:00,  3.64it/s] 29%|â–ˆâ–ˆâ–‰       | 177/615 [01:00<02:00,  3.64it/s] 29%|â–ˆâ–ˆâ–‰       | 178/615 [01:00<01:59,  3.64it/s] 29%|â–ˆâ–ˆâ–‰       | 179/615 [01:00<01:59,  3.65it/s] 29%|â–ˆâ–ˆâ–‰       | 180/615 [01:01<01:59,  3.65it/s] 29%|â–ˆâ–ˆâ–‰       | 181/615 [01:01<01:59,  3.65it/s] 30%|â–ˆâ–ˆâ–‰       | 182/615 [01:01<01:59,  3.63it/s] 30%|â–ˆâ–ˆâ–‰       | 183/615 [01:02<01:58,  3.64it/s] 30%|â–ˆâ–ˆâ–‰       | 184/615 [01:02<01:58,  3.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 185/615 [01:02<01:58,  3.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 186/615 [01:02<01:57,  3.65it/s] 30%|â–ˆâ–ˆâ–ˆ       | 187/615 [01:03<01:57,  3.65it/s] 31%|â–ˆâ–ˆâ–ˆ       | 188/615 [01:03<01:57,  3.65it/s] 31%|â–ˆâ–ˆâ–ˆ       | 189/615 [01:03<01:56,  3.65it/s] 31%|â–ˆâ–ˆâ–ˆ       | 190/615 [01:03<01:56,  3.65it/s] 31%|â–ˆâ–ˆâ–ˆ       | 191/615 [01:04<01:56,  3.65it/s] 31%|â–ˆâ–ˆâ–ˆ       | 192/615 [01:04<01:56,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 193/615 [01:04<01:56,  3.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 194/615 [01:05<01:55,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 195/615 [01:05<01:55,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 196/615 [01:05<01:55,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 197/615 [01:05<01:54,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 198/615 [01:06<01:54,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 199/615 [01:06<01:54,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 200/615 [01:06<01:53,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 201/615 [01:07<01:53,  3.65it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 202/615 [01:07<01:53,  3.65it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 203/615 [01:07<01:53,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 204/615 [01:07<01:53,  3.63it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 205/615 [01:08<01:52,  3.63it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 206/615 [01:08<01:52,  3.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 207/615 [01:08<01:52,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 208/615 [01:08<01:51,  3.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 209/615 [01:09<01:51,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 210/615 [01:09<01:51,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 211/615 [01:09<01:51,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 212/615 [01:10<01:50,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 213/615 [01:10<01:50,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 214/615 [01:10<01:50,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 215/615 [01:10<01:50,  3.62it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 216/615 [01:11<01:50,  3.62it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 217/615 [01:11<01:49,  3.63it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 218/615 [01:11<01:49,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 219/615 [01:11<01:48,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 220/615 [01:12<01:48,  3.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 221/615 [01:12<01:48,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 222/615 [01:12<01:48,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 223/615 [01:13<01:47,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 224/615 [01:13<01:47,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 225/615 [01:13<01:47,  3.63it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 226/615 [01:13<01:47,  3.63it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 227/615 [01:14<01:46,  3.63it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 228/615 [01:14<01:46,  3.63it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 229/615 [01:14<01:46,  3.63it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 230/615 [01:15<01:46,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 231/615 [01:15<01:45,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 232/615 [01:15<01:45,  3.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 233/615 [01:15<01:45,  3.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 234/615 [01:16<01:44,  3.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 235/615 [01:16<01:44,  3.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 236/615 [01:16<01:44,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 237/615 [01:16<01:45,  3.60it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 238/615 [01:17<01:44,  3.61it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 239/615 [01:17<01:43,  3.62it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 240/615 [01:17<01:43,  3.62it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 241/615 [01:18<01:43,  3.62it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 242/615 [01:18<01:42,  3.62it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 243/615 [01:18<01:42,  3.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 244/615 [01:18<01:42,  3.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 245/615 [01:19<01:41,  3.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 246/615 [01:19<01:41,  3.63it/s][INFO|trainer.py:2140] 2023-08-28 06:13:03,003 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:13:03,003 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 06:13:03,003 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.2952, 'eval_samples_per_second': 374.28, 'eval_steps_per_second': 46.799, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.60it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.76it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.87it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.17it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.70it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.37it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.16it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.87it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.75it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.75it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.77it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.81it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.79it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.78it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.81it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.82it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.71it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.71it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.72it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.66it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.70it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.72it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.73it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.77it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.77it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.74it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.76it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.76it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.68it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.71it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.71it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.68it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.71it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.69it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.69it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.73it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.68it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.72it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.73it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.70it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.73it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.75it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.70it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.73it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.74it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.67it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.70it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.70it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.71it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.74it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.70it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.71it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.73it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.68it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.71it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.74it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.66it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.70it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.74it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.68it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.72it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.73it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.70it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.73it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.71it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.69it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.67it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.68it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.71it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.73it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.71it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.73it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.75it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.70it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.69it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.65it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.67it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.70it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.71it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.73it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.67it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.66it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.69it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.72it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.63it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.68it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.68it/s][A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 246/615 [01:28<01:41,  3.63it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:13:12,397 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 06:13:12,429 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:13:15,056 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:13:15,077 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:13:15,090 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 247/615 [01:32<24:40,  4.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 248/615 [01:32<17:43,  2.90s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 249/615 [01:32<12:52,  2.11s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 250/615 [01:33<09:29,  1.56s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 251/615 [01:33<07:07,  1.17s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 252/615 [01:33<05:28,  1.11it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 253/615 [01:33<04:18,  1.40it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 254/615 [01:34<03:30,  1.72it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 255/615 [01:34<02:56,  2.04it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/615 [01:34<02:32,  2.35it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/615 [01:34<02:16,  2.63it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/615 [01:35<02:04,  2.87it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/615 [01:35<01:56,  3.06it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/615 [01:35<01:50,  3.22it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/615 [01:36<01:46,  3.34it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 262/615 [01:36<01:43,  3.42it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 263/615 [01:36<01:40,  3.49it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 264/615 [01:36<01:39,  3.53it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 265/615 [01:37<01:38,  3.56it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 266/615 [01:37<01:37,  3.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 267/615 [01:37<01:36,  3.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 268/615 [01:37<01:36,  3.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 269/615 [01:38<01:35,  3.62it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 270/615 [01:38<01:35,  3.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 271/615 [01:38<01:34,  3.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 272/615 [01:39<01:34,  3.64it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 273/615 [01:39<01:34,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 274/615 [01:39<01:33,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 275/615 [01:39<01:33,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 276/615 [01:40<01:33,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 277/615 [01:40<01:32,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 278/615 [01:40<01:32,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 279/615 [01:40<01:32,  3.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 280/615 [01:41<01:32,  3.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 281/615 [01:41<01:31,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 282/615 [01:41<01:31,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 283/615 [01:42<01:31,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 284/615 [01:42<01:30,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 285/615 [01:42<01:30,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 286/615 [01:42<01:30,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 287/615 [01:43<01:29,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 288/615 [01:43<01:29,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 289/615 [01:43<01:29,  3.65it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 290/615 [01:43<01:29,  3.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 291/615 [01:44<01:29,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 292/615 [01:44<01:28,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 293/615 [01:44<01:28,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 294/615 [01:45<01:28,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 295/615 [01:45<01:27,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 296/615 [01:45<01:27,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 297/615 [01:45<01:27,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 298/615 [01:46<01:26,  3.65it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 299/615 [01:46<01:26,  3.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 300/615 [01:46<01:26,  3.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 301/615 [01:47<01:27,  3.59it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 302/615 [01:47<01:26,  3.60it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 303/615 [01:47<01:26,  3.61it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 304/615 [01:47<01:25,  3.62it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 305/615 [01:48<01:25,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 306/615 [01:48<01:25,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 307/615 [01:48<01:24,  3.64it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 308/615 [01:48<01:24,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 309/615 [01:49<01:24,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 310/615 [01:49<01:23,  3.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 311/615 [01:49<01:23,  3.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 312/615 [01:50<01:23,  3.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 313/615 [01:50<01:23,  3.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 314/615 [01:50<01:22,  3.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 315/615 [01:50<01:22,  3.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/615 [01:51<01:22,  3.64it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/615 [01:51<01:23,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/615 [01:51<01:22,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/615 [01:51<01:22,  3.60it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/615 [01:52<01:21,  3.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/615 [01:52<01:21,  3.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 322/615 [01:52<01:20,  3.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 323/615 [01:53<01:20,  3.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 324/615 [01:53<01:20,  3.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 325/615 [01:53<01:19,  3.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 326/615 [01:53<01:19,  3.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 327/615 [01:54<01:19,  3.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 328/615 [01:54<01:18,  3.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 329/615 [01:54<01:18,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 330/615 [01:55<01:18,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 331/615 [01:55<01:18,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 332/615 [01:55<01:17,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 333/615 [01:55<01:17,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 334/615 [01:56<01:17,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 335/615 [01:56<01:16,  3.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 336/615 [01:56<01:16,  3.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 337/615 [01:56<01:16,  3.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 338/615 [01:57<01:16,  3.61it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 339/615 [01:57<01:16,  3.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 340/615 [01:57<01:15,  3.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 341/615 [01:58<01:15,  3.62it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 342/615 [01:58<01:15,  3.62it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 343/615 [01:58<01:14,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 344/615 [01:58<01:14,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 345/615 [01:59<01:14,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 346/615 [01:59<01:14,  3.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 347/615 [01:59<01:13,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 348/615 [01:59<01:13,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 349/615 [02:00<01:13,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 350/615 [02:00<01:12,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 351/615 [02:00<01:12,  3.63it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 352/615 [02:01<01:12,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 353/615 [02:01<01:12,  3.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 354/615 [02:01<01:11,  3.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 355/615 [02:01<01:11,  3.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 356/615 [02:02<01:11,  3.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 357/615 [02:02<01:10,  3.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 358/615 [02:02<01:10,  3.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 359/615 [02:02<01:10,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 360/615 [02:03<01:10,  3.61it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 361/615 [02:03<01:10,  3.62it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 362/615 [02:03<01:09,  3.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 363/615 [02:04<01:09,  3.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 364/615 [02:04<01:09,  3.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 365/615 [02:04<01:08,  3.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 366/615 [02:04<01:08,  3.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 367/615 [02:05<01:08,  3.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 368/615 [02:05<01:07,  3.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 369/615 [02:05<01:07,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 06:13:49,328 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:13:49,329 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 06:13:49,329 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3117, 'eval_samples_per_second': 373.618, 'eval_steps_per_second': 46.716, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.42it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.66it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.82it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.11it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.65it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.39it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.15it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.87it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.79it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.78it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.78it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.84it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.82it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.80it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.80it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.80it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.67it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.64it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.61it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.64it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.68it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.71it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.74it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.75it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.76it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.72it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.74it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.65it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.65it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.68it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.42it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.73it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.74it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.73it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.75it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.76it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.70it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.72it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.73it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.69it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.71it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.72it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.73it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.75it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.71it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.72it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.69it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.65it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.69it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.72it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.68it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.70it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.71it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.70it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.73it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.72it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.65it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.69it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.69it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.70it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.72it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.68it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.71it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.72it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.66it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.70it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.72it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.66it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.70it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.72it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.67it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.71it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.71it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.68it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.71it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.70it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.72it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.74it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.68it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.68it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.73it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.68it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.71it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.71it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.68it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.73it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.73it/s][A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 369/615 [02:15<01:07,  3.64it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:13:58,655 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-28 06:13:58,672 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:14:00,718 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:14:00,731 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:14:00,739 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 370/615 [02:17<15:34,  3.81s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 371/615 [02:18<11:11,  2.75s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 372/615 [02:18<08:07,  2.01s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373/615 [02:18<06:00,  1.49s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 374/615 [02:18<04:31,  1.12s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 375/615 [02:19<03:28,  1.15it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 376/615 [02:19<02:45,  1.45it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/615 [02:19<02:14,  1.77it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/615 [02:20<01:53,  2.09it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/615 [02:20<01:38,  2.40it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/615 [02:20<01:27,  2.67it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 381/615 [02:20<01:20,  2.90it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 382/615 [02:21<01:15,  3.09it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 383/615 [02:21<01:11,  3.24it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/615 [02:21<01:09,  3.35it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 385/615 [02:21<01:07,  3.43it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 386/615 [02:22<01:05,  3.49it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 387/615 [02:22<01:04,  3.54it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 388/615 [02:22<01:03,  3.57it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 389/615 [02:23<01:02,  3.59it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 390/615 [02:23<01:02,  3.61it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 391/615 [02:23<01:01,  3.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 392/615 [02:23<01:01,  3.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 393/615 [02:24<01:01,  3.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 394/615 [02:24<01:00,  3.64it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 395/615 [02:24<01:00,  3.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 396/615 [02:24<01:00,  3.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 397/615 [02:25<01:00,  3.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 398/615 [02:25<00:59,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 399/615 [02:25<00:59,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 400/615 [02:26<00:59,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 401/615 [02:26<00:58,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 402/615 [02:26<00:58,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 403/615 [02:26<00:58,  3.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 404/615 [02:27<00:58,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 405/615 [02:27<00:57,  3.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 406/615 [02:27<00:57,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 407/615 [02:27<00:57,  3.62it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 408/615 [02:28<00:57,  3.63it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 409/615 [02:28<00:56,  3.63it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 410/615 [02:28<00:56,  3.63it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 411/615 [02:29<00:56,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 412/615 [02:29<00:55,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 413/615 [02:29<00:55,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 414/615 [02:29<00:55,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 415/615 [02:30<00:55,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 416/615 [02:30<00:54,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 417/615 [02:30<00:54,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 418/615 [02:31<01:05,  3.03it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 419/615 [02:31<01:01,  3.19it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 420/615 [02:31<00:58,  3.31it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 421/615 [02:32<00:57,  3.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 422/615 [02:32<00:55,  3.47it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 423/615 [02:32<00:54,  3.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 424/615 [02:32<00:53,  3.55it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 425/615 [02:33<00:53,  3.56it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 426/615 [02:33<00:52,  3.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 427/615 [02:33<00:52,  3.60it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 428/615 [02:33<00:51,  3.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 429/615 [02:34<00:51,  3.61it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 430/615 [02:34<00:51,  3.62it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 431/615 [02:34<00:50,  3.62it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 432/615 [02:35<00:50,  3.63it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 433/615 [02:35<00:50,  3.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 434/615 [02:35<00:49,  3.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 435/615 [02:35<00:49,  3.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 436/615 [02:36<00:49,  3.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 437/615 [02:36<00:48,  3.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 438/615 [02:36<00:48,  3.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 439/615 [02:36<00:48,  3.64it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/615 [02:37<00:48,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 441/615 [02:37<00:47,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/615 [02:37<00:47,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 443/615 [02:38<00:47,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/615 [02:38<00:47,  3.63it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 445/615 [02:38<00:46,  3.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 446/615 [02:38<00:46,  3.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 447/615 [02:39<00:46,  3.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 448/615 [02:39<00:45,  3.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 449/615 [02:39<00:45,  3.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 450/615 [02:39<00:45,  3.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 451/615 [02:40<00:45,  3.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 452/615 [02:40<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 453/615 [02:40<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 454/615 [02:41<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 455/615 [02:41<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 456/615 [02:41<00:43,  3.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 457/615 [02:41<00:43,  3.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 458/615 [02:42<00:43,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 459/615 [02:42<00:42,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 460/615 [02:42<00:42,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 461/615 [02:43<00:42,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 462/615 [02:43<00:42,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 463/615 [02:43<00:41,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 464/615 [02:43<00:41,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 465/615 [02:44<00:41,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 466/615 [02:44<00:41,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 467/615 [02:44<00:40,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 468/615 [02:44<00:40,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 469/615 [02:45<00:40,  3.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 470/615 [02:45<00:39,  3.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 471/615 [02:45<00:39,  3.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 472/615 [02:46<00:39,  3.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 473/615 [02:46<00:39,  3.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 474/615 [02:46<00:38,  3.62it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 475/615 [02:46<00:38,  3.63it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 476/615 [02:47<00:38,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 477/615 [02:47<00:37,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 478/615 [02:47<00:37,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 479/615 [02:47<00:37,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 480/615 [02:48<00:37,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 481/615 [02:48<00:36,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 482/615 [02:48<00:36,  3.63it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 483/615 [02:49<00:36,  3.64it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 484/615 [02:49<00:36,  3.61it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 485/615 [02:49<00:35,  3.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 486/615 [02:49<00:35,  3.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 487/615 [02:50<00:35,  3.61it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 488/615 [02:50<00:35,  3.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 489/615 [02:50<00:34,  3.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 490/615 [02:51<00:34,  3.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 491/615 [02:51<00:34,  3.63it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 492/615 [02:51<00:34,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 06:14:35,193 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:14:35,193 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 06:14:35,193 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3132, 'eval_samples_per_second': 373.554, 'eval_steps_per_second': 46.708, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.39it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.64it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.77it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.09it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.64it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.37it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.13it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.87it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.81it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.80it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.77it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.77it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.76it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.75it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.78it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.77it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.66it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.65it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.62it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.64it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.68it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.71it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.72it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.74it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.74it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.70it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.72it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.66it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.65it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.68it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.67it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.70it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.72it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.70it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.73it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.75it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.69it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.71it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.74it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.67it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.70it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.70it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.69it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.70it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.68it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.70it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.73it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.67it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.70it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.74it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.69it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.70it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.74it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.66it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.70it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.57it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.61it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.66it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.64it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.67it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.70it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 45.58it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 45.93it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.16it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.33it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.47it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.55it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.62it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.64it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.62it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.60it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.60it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.65it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.69it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.69it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.71it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.73it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.69it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.67it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.63it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.65it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.69it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.70it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.70it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.72it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.68it/s][A                                                 
                                                 [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 492/615 [03:01<00:34,  3.52it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.68it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:14:44,541 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 06:14:44,558 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:14:46,792 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:14:46,806 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:14:46,817 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 493/615 [03:03<07:53,  3.88s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 494/615 [03:04<05:38,  2.80s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 495/615 [03:04<04:05,  2.04s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 496/615 [03:04<02:59,  1.51s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 497/615 [03:04<02:14,  1.14s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 498/615 [03:05<01:43,  1.14it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 499/615 [03:05<01:21,  1.43it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 500/615 [03:05<01:05,  1.75it/s]                                                  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 500/615 [03:05<01:05,  1.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 501/615 [03:06<00:55,  2.07it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 502/615 [03:06<00:47,  2.38it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/615 [03:06<00:42,  2.65it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 504/615 [03:06<00:38,  2.89it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/615 [03:07<00:35,  3.08it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 506/615 [03:07<00:33,  3.23it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 507/615 [03:07<00:32,  3.35it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 508/615 [03:07<00:31,  3.43it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 509/615 [03:08<00:30,  3.49it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 510/615 [03:08<00:29,  3.54it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 511/615 [03:08<00:29,  3.57it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 512/615 [03:09<00:28,  3.59it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 513/615 [03:09<00:28,  3.60it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 514/615 [03:09<00:27,  3.61it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 515/615 [03:09<00:27,  3.62it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 516/615 [03:10<00:27,  3.63it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 517/615 [03:10<00:26,  3.63it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 518/615 [03:10<00:26,  3.64it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 519/615 [03:11<00:26,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 520/615 [03:11<00:26,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 521/615 [03:11<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 522/615 [03:11<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 523/615 [03:12<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 524/615 [03:12<00:24,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 525/615 [03:12<00:24,  3.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 526/615 [03:12<00:24,  3.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 527/615 [03:13<00:24,  3.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 528/615 [03:13<00:23,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 529/615 [03:13<00:23,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 530/615 [03:14<00:23,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 531/615 [03:14<00:23,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 532/615 [03:14<00:22,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 533/615 [03:14<00:22,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 534/615 [03:15<00:22,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 535/615 [03:15<00:21,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 536/615 [03:15<00:21,  3.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 537/615 [03:15<00:21,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 538/615 [03:16<00:21,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 539/615 [03:16<00:20,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 540/615 [03:16<00:20,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 541/615 [03:17<00:20,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 542/615 [03:17<00:20,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 543/615 [03:17<00:19,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 544/615 [03:17<00:19,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 545/615 [03:18<00:19,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 546/615 [03:18<00:18,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 547/615 [03:18<00:18,  3.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 548/615 [03:18<00:18,  3.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 549/615 [03:19<00:18,  3.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 550/615 [03:19<00:17,  3.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 551/615 [03:19<00:17,  3.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 552/615 [03:20<00:17,  3.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 553/615 [03:20<00:17,  3.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 554/615 [03:20<00:16,  3.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 555/615 [03:20<00:16,  3.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 556/615 [03:21<00:16,  3.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 557/615 [03:21<00:15,  3.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 558/615 [03:21<00:15,  3.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 559/615 [03:22<00:15,  3.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 560/615 [03:22<00:15,  3.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 561/615 [03:22<00:14,  3.62it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 562/615 [03:22<00:14,  3.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/615 [03:23<00:14,  3.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 564/615 [03:23<00:14,  3.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 565/615 [03:23<00:13,  3.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 566/615 [03:23<00:13,  3.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 567/615 [03:24<00:13,  3.63it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 568/615 [03:24<00:12,  3.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 569/615 [03:24<00:12,  3.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 570/615 [03:25<00:12,  3.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 571/615 [03:25<00:12,  3.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 572/615 [03:25<00:11,  3.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 573/615 [03:25<00:11,  3.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 574/615 [03:26<00:11,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 575/615 [03:26<00:11,  3.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 576/615 [03:26<00:10,  3.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 577/615 [03:26<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 578/615 [03:27<00:10,  3.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 579/615 [03:27<00:09,  3.62it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 580/615 [03:27<00:09,  3.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 581/615 [03:28<00:09,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 582/615 [03:28<00:09,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 583/615 [03:28<00:08,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 584/615 [03:28<00:08,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 585/615 [03:29<00:08,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 586/615 [03:29<00:08,  3.60it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 587/615 [03:29<00:07,  3.61it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 588/615 [03:30<00:07,  3.62it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 589/615 [03:30<00:07,  3.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 590/615 [03:30<00:06,  3.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 591/615 [03:30<00:06,  3.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 592/615 [03:31<00:06,  3.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 593/615 [03:31<00:06,  3.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 594/615 [03:31<00:05,  3.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 595/615 [03:31<00:05,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 596/615 [03:32<00:05,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 597/615 [03:32<00:04,  3.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 598/615 [03:32<00:04,  3.63it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 599/615 [03:33<00:04,  3.63it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 600/615 [03:33<00:04,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 601/615 [03:33<00:03,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 602/615 [03:33<00:03,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 603/615 [03:34<00:03,  3.62it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 604/615 [03:34<00:03,  3.63it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 605/615 [03:34<00:02,  3.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 606/615 [03:34<00:02,  3.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 607/615 [03:35<00:02,  3.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 608/615 [03:35<00:01,  3.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 609/615 [03:35<00:01,  3.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 610/615 [03:36<00:01,  3.63it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 611/615 [03:36<00:01,  3.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 612/615 [03:36<00:00,  3.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 613/615 [03:36<00:00,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 614/615 [03:37<00:00,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [03:37<00:00,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 06:15:20,909 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:15:20,909 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 06:15:20,909 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3241, 'eval_samples_per_second': 373.121, 'eval_steps_per_second': 46.653, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.13it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.55it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.75it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.02it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.61it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.34it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.11it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.88it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.75it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.71it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.73it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.75it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.75it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.75it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.77it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.74it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.69it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.65it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.64it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.61it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.65it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.70it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.73it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.72it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.73it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.68it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.66it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.63it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.60it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.64it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.57it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.69it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.70it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.73it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.70it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.71it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.67it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.63it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.66it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.70it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.70it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.71it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.73it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.69it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.71it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.64it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.65it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.69it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.62it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.66it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.70it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.68it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.71it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.74it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.62it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.50it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.55it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.58it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.66it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.66it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.69it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.71it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.68it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.63it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.59it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.65it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.66it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.70it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.68it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.69it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.69it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.71it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.71it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.63it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.63it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.62it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.66it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.69it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.72it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.71it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.73it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.68it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.65it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.62it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.58it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.65it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.65it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [03:46<00:00,  3.64it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:15:30,250 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-28 06:15:30,268 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:15:33,033 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:15:33,045 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:15:33,053 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:15:33,344 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:15:33,344 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123 (score: 1.0038264989852905).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [03:51<00:00,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [03:51<00:00,  2.65it/s]
[INFO|trainer.py:1894] 2023-08-28 06:15:35,140 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 06:15:35,158 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:15:37,461 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:15:37,477 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:15:37,487 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:15:37,658 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:37,658 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:37,658 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:37,658 >>   train_runtime            = 0:03:51.64
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:37,658 >>   train_samples            =       7900
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:37,658 >>   train_samples_per_second =    170.519
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:37,659 >>   train_steps_per_second   =      2.655
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3205, 'eval_samples_per_second': 373.263, 'eval_steps_per_second': 46.671, 'epoch': 5.0}
{'train_runtime': 231.6458, 'train_samples_per_second': 170.519, 'train_steps_per_second': 2.655, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:15:37 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:15:37,698 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:15:37,698 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 06:15:37,698 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 57.93it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 51.00it/s]  4%|â–         | 18/435 [00:00<00:08, 49.12it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.34it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.91it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.63it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.43it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.26it/s] 11%|â–ˆ         | 48/435 [00:00<00:08, 47.00it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.87it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.91it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.95it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.96it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.99it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 47.01it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.99it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.96it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.84it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.73it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.82it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.89it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.88it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.74it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.83it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.84it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.88it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.86it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.79it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.78it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.80it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.84it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.83it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.84it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.90it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.93it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.85it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.87it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.85it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.85it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.85it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.84it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.82it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.89it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.86it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.83it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.85it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.84it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 46.82it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.83it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.84it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.86it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.84it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.84it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.83it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.83it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.83it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.83it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.83it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.85it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.84it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.84it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.83it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.83it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.82it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.83it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.83it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.81it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.85it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.84it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.56it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.70it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.73it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.73it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.78it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.79it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.80it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.83it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.83it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.81it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.82it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.83it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.81it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.93it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:15:46,993 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:46,993 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:46,993 >>   eval_loss               =     1.0038
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:46,993 >>   eval_runtime            = 0:00:09.29
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:46,993 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:46,993 >>   eval_samples_per_second =     374.27
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:46,994 >>   eval_steps_per_second   =     46.797
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:15:46,994 >>   perplexity              =     2.7287
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:51,687 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:51,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:51,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:51,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:51,693 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:15:52,415 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:15:52,416 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:15:52,999 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:15:54,158 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:15:54,158 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:57,078 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:57,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:57,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:57,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:15:57,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:15:57,732 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:15:57,733 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:15:58,309 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:15:58,469 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:15:58,469 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-615
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-123
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/checkpoint-369
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.23it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.27it/s]Extractor Predicting: 5it [00:03,  1.27it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.27it/s]Extractor Predicting: 8it [00:06,  1.28it/s]Extractor Predicting: 9it [00:07,  1.28it/s]Extractor Predicting: 10it [00:07,  1.32it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:10,  1.27it/s]Extractor Predicting: 14it [00:10,  1.26it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:13,  1.31it/s]Extractor Predicting: 18it [00:13,  1.34it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.32it/s]Extractor Predicting: 21it [00:16,  1.32it/s]Extractor Predicting: 22it [00:16,  1.34it/s]Extractor Predicting: 23it [00:17,  1.33it/s]Extractor Predicting: 24it [00:18,  1.27it/s]Extractor Predicting: 25it [00:19,  1.26it/s]Extractor Predicting: 26it [00:20,  1.25it/s]Extractor Predicting: 27it [00:21,  1.26it/s]Extractor Predicting: 28it [00:21,  1.29it/s]Extractor Predicting: 29it [00:22,  1.28it/s]Extractor Predicting: 30it [00:23,  1.26it/s]Extractor Predicting: 31it [00:24,  1.27it/s]Extractor Predicting: 32it [00:24,  1.27it/s]Extractor Predicting: 33it [00:25,  1.28it/s]Extractor Predicting: 34it [00:26,  1.31it/s]Extractor Predicting: 35it [00:27,  1.32it/s]Extractor Predicting: 36it [00:27,  1.32it/s]Extractor Predicting: 37it [00:28,  1.33it/s]Extractor Predicting: 38it [00:29,  1.34it/s]Extractor Predicting: 39it [00:30,  1.35it/s]Extractor Predicting: 40it [00:30,  1.33it/s]Extractor Predicting: 41it [00:31,  1.34it/s]Extractor Predicting: 42it [00:32,  1.33it/s]Extractor Predicting: 43it [00:33,  1.36it/s]Extractor Predicting: 44it [00:33,  1.29it/s]Extractor Predicting: 45it [00:34,  1.29it/s]Extractor Predicting: 46it [00:35,  1.29it/s]Extractor Predicting: 47it [00:36,  1.29it/s]Extractor Predicting: 48it [00:37,  1.31it/s]Extractor Predicting: 49it [00:37,  1.31it/s]Extractor Predicting: 50it [00:38,  1.32it/s]Extractor Predicting: 51it [00:39,  1.34it/s]Extractor Predicting: 52it [00:39,  1.34it/s]Extractor Predicting: 53it [00:40,  1.34it/s]Extractor Predicting: 54it [00:41,  1.33it/s]Extractor Predicting: 55it [00:42,  1.30it/s]Extractor Predicting: 56it [00:43,  1.33it/s]Extractor Predicting: 57it [00:43,  1.31it/s]Extractor Predicting: 58it [00:44,  1.31it/s]Extractor Predicting: 59it [00:45,  1.34it/s]Extractor Predicting: 60it [00:45,  1.38it/s]Extractor Predicting: 61it [00:46,  1.39it/s]Extractor Predicting: 62it [00:47,  1.37it/s]Extractor Predicting: 63it [00:48,  1.34it/s]Extractor Predicting: 64it [00:48,  1.36it/s]Extractor Predicting: 65it [00:49,  1.36it/s]Extractor Predicting: 66it [00:50,  1.36it/s]Extractor Predicting: 67it [00:51,  1.37it/s]Extractor Predicting: 68it [00:51,  1.36it/s]Extractor Predicting: 69it [00:52,  1.41it/s]Extractor Predicting: 70it [00:53,  1.40it/s]Extractor Predicting: 71it [00:53,  1.42it/s]Extractor Predicting: 72it [00:54,  1.38it/s]Extractor Predicting: 73it [00:55,  1.39it/s]Extractor Predicting: 74it [00:56,  1.37it/s]Extractor Predicting: 75it [00:56,  1.35it/s]Extractor Predicting: 76it [00:57,  1.33it/s]Extractor Predicting: 77it [00:58,  1.36it/s]Extractor Predicting: 78it [00:59,  1.38it/s]Extractor Predicting: 79it [00:59,  1.40it/s]Extractor Predicting: 80it [01:00,  1.38it/s]Extractor Predicting: 81it [01:01,  1.37it/s]Extractor Predicting: 82it [01:01,  1.37it/s]Extractor Predicting: 83it [01:02,  1.38it/s]Extractor Predicting: 84it [01:03,  1.38it/s]Extractor Predicting: 85it [01:04,  1.38it/s]Extractor Predicting: 86it [01:04,  1.39it/s]Extractor Predicting: 87it [01:05,  1.42it/s]Extractor Predicting: 88it [01:06,  1.40it/s]Extractor Predicting: 89it [01:06,  1.41it/s]Extractor Predicting: 90it [01:07,  1.41it/s]Extractor Predicting: 91it [01:08,  1.43it/s]Extractor Predicting: 92it [01:08,  1.47it/s]Extractor Predicting: 93it [01:09,  1.44it/s]Extractor Predicting: 94it [01:10,  1.43it/s]Extractor Predicting: 95it [01:11,  1.43it/s]Extractor Predicting: 96it [01:11,  1.43it/s]Extractor Predicting: 97it [01:12,  1.42it/s]Extractor Predicting: 98it [01:13,  1.42it/s]Extractor Predicting: 99it [01:13,  1.40it/s]Extractor Predicting: 100it [01:14,  1.36it/s]Extractor Predicting: 101it [01:15,  1.36it/s]Extractor Predicting: 102it [01:16,  1.44it/s]Extractor Predicting: 103it [01:16,  1.44it/s]Extractor Predicting: 104it [01:17,  1.43it/s]Extractor Predicting: 105it [01:18,  1.43it/s]Extractor Predicting: 106it [01:18,  1.43it/s]Extractor Predicting: 107it [01:19,  1.42it/s]Extractor Predicting: 108it [01:20,  1.42it/s]Extractor Predicting: 109it [01:21,  1.42it/s]Extractor Predicting: 110it [01:21,  1.32it/s]Extractor Predicting: 111it [01:22,  1.35it/s]Extractor Predicting: 112it [01:23,  1.34it/s]Extractor Predicting: 113it [01:24,  1.40it/s]Extractor Predicting: 114it [01:24,  1.42it/s]Extractor Predicting: 115it [01:25,  1.45it/s]Extractor Predicting: 116it [01:26,  1.42it/s]Extractor Predicting: 117it [01:26,  1.38it/s]Extractor Predicting: 118it [01:27,  1.39it/s]Extractor Predicting: 119it [01:28,  1.36it/s]Extractor Predicting: 120it [01:29,  1.38it/s]Extractor Predicting: 121it [01:29,  1.37it/s]Extractor Predicting: 122it [01:30,  1.34it/s]Extractor Predicting: 123it [01:31,  1.36it/s]Extractor Predicting: 124it [01:32,  1.37it/s]Extractor Predicting: 125it [01:32,  1.39it/s]Extractor Predicting: 126it [01:33,  1.36it/s]Extractor Predicting: 127it [01:34,  1.38it/s]Extractor Predicting: 128it [01:34,  1.39it/s]Extractor Predicting: 129it [01:35,  1.37it/s]Extractor Predicting: 130it [01:36,  1.35it/s]Extractor Predicting: 131it [01:37,  1.38it/s]Extractor Predicting: 132it [01:37,  1.36it/s]Extractor Predicting: 133it [01:38,  1.34it/s]Extractor Predicting: 134it [01:39,  1.33it/s]Extractor Predicting: 135it [01:40,  1.35it/s]Extractor Predicting: 136it [01:40,  1.33it/s]Extractor Predicting: 137it [01:41,  1.34it/s]Extractor Predicting: 138it [01:42,  1.32it/s]Extractor Predicting: 139it [01:43,  1.32it/s]Extractor Predicting: 140it [01:43,  1.31it/s]Extractor Predicting: 141it [01:44,  1.34it/s]Extractor Predicting: 142it [01:45,  1.32it/s]Extractor Predicting: 143it [01:46,  1.33it/s]Extractor Predicting: 144it [01:46,  1.66it/s]Extractor Predicting: 144it [01:46,  1.35it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:52,219 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:52,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:52,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:52,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:52,225 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:17:52,540 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:17:52,541 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:17:52,806 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:17:53,856 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:17:53,856 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:55,619 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:55,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:55,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:55,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:17:55,624 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:17:55,954 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:17:55,955 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:17:56,213 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:17:56,377 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:17:56,377 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.27it/s]Extractor Predicting: 3it [00:02,  1.32it/s]Extractor Predicting: 4it [00:03,  1.28it/s]Extractor Predicting: 5it [00:03,  1.34it/s]Extractor Predicting: 6it [00:04,  1.37it/s]Extractor Predicting: 7it [00:05,  1.41it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.42it/s]Extractor Predicting: 10it [00:07,  1.43it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:10,  1.42it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.42it/s]Extractor Predicting: 17it [00:12,  1.41it/s]Extractor Predicting: 18it [00:12,  1.43it/s]Extractor Predicting: 19it [00:13,  1.46it/s]Extractor Predicting: 20it [00:14,  1.42it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.45it/s]Extractor Predicting: 23it [00:16,  1.44it/s]Extractor Predicting: 24it [00:17,  1.43it/s]Extractor Predicting: 25it [00:17,  1.42it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:19,  1.41it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:20,  1.43it/s]Extractor Predicting: 30it [00:21,  1.45it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:22,  1.46it/s]Extractor Predicting: 33it [00:23,  1.43it/s]Extractor Predicting: 34it [00:24,  1.41it/s]Extractor Predicting: 35it [00:24,  1.42it/s]Extractor Predicting: 36it [00:25,  1.43it/s]Extractor Predicting: 37it [00:26,  1.45it/s]Extractor Predicting: 38it [00:26,  1.46it/s]Extractor Predicting: 39it [00:27,  1.45it/s]Extractor Predicting: 40it [00:28,  1.42it/s]Extractor Predicting: 41it [00:28,  1.43it/s]Extractor Predicting: 42it [00:29,  1.47it/s]Extractor Predicting: 43it [00:30,  1.42it/s]Extractor Predicting: 44it [00:31,  1.41it/s]Extractor Predicting: 45it [00:31,  1.41it/s]Extractor Predicting: 46it [00:32,  1.39it/s]Extractor Predicting: 47it [00:33,  1.42it/s]Extractor Predicting: 48it [00:33,  1.40it/s]Extractor Predicting: 49it [00:34,  1.43it/s]Extractor Predicting: 50it [00:35,  1.42it/s]Extractor Predicting: 51it [00:35,  1.41it/s]Extractor Predicting: 52it [00:36,  1.39it/s]Extractor Predicting: 53it [00:37,  1.40it/s]Extractor Predicting: 54it [00:38,  1.39it/s]Extractor Predicting: 55it [00:38,  1.43it/s]Extractor Predicting: 56it [00:39,  1.41it/s]Extractor Predicting: 57it [00:40,  1.39it/s]Extractor Predicting: 58it [00:41,  1.38it/s]Extractor Predicting: 59it [00:41,  1.39it/s]Extractor Predicting: 60it [00:42,  1.37it/s]Extractor Predicting: 61it [00:43,  1.36it/s]Extractor Predicting: 62it [00:43,  1.40it/s]Extractor Predicting: 63it [00:44,  1.40it/s]Extractor Predicting: 64it [00:45,  1.42it/s]Extractor Predicting: 65it [00:46,  1.40it/s]Extractor Predicting: 66it [00:46,  1.37it/s]Extractor Predicting: 67it [00:47,  1.37it/s]Extractor Predicting: 68it [00:48,  1.37it/s]Extractor Predicting: 69it [00:49,  1.35it/s]Extractor Predicting: 70it [00:49,  1.36it/s]Extractor Predicting: 71it [00:50,  1.36it/s]Extractor Predicting: 72it [00:51,  1.35it/s]Extractor Predicting: 73it [00:51,  1.37it/s]Extractor Predicting: 74it [00:52,  1.37it/s]Extractor Predicting: 75it [00:53,  1.38it/s]Extractor Predicting: 76it [00:54,  1.36it/s]Extractor Predicting: 77it [00:54,  1.40it/s]Extractor Predicting: 78it [00:55,  1.40it/s]Extractor Predicting: 79it [00:56,  1.43it/s]Extractor Predicting: 80it [00:56,  1.46it/s]Extractor Predicting: 81it [00:57,  1.45it/s]Extractor Predicting: 82it [00:58,  1.43it/s]Extractor Predicting: 83it [00:59,  1.41it/s]Extractor Predicting: 84it [00:59,  1.40it/s]Extractor Predicting: 85it [01:00,  1.36it/s]Extractor Predicting: 86it [01:01,  1.33it/s]Extractor Predicting: 87it [01:02,  1.35it/s]Extractor Predicting: 88it [01:02,  1.35it/s]Extractor Predicting: 89it [01:03,  1.32it/s]Extractor Predicting: 90it [01:04,  1.33it/s]Extractor Predicting: 91it [01:05,  1.31it/s]Extractor Predicting: 92it [01:05,  1.34it/s]Extractor Predicting: 93it [01:06,  1.35it/s]Extractor Predicting: 94it [01:07,  1.36it/s]Extractor Predicting: 95it [01:07,  1.37it/s]Extractor Predicting: 96it [01:08,  1.33it/s]Extractor Predicting: 97it [01:09,  1.32it/s]Extractor Predicting: 98it [01:10,  1.31it/s]Extractor Predicting: 99it [01:11,  1.34it/s]Extractor Predicting: 100it [01:11,  1.34it/s]Extractor Predicting: 101it [01:12,  1.38it/s]Extractor Predicting: 102it [01:13,  1.35it/s]Extractor Predicting: 103it [01:13,  1.33it/s]Extractor Predicting: 104it [01:14,  1.37it/s]Extractor Predicting: 105it [01:15,  1.25it/s]Extractor Predicting: 106it [01:16,  1.27it/s]Extractor Predicting: 107it [01:17,  1.32it/s]Extractor Predicting: 108it [01:17,  1.31it/s]Extractor Predicting: 109it [01:18,  1.32it/s]Extractor Predicting: 110it [01:19,  1.31it/s]Extractor Predicting: 111it [01:20,  1.34it/s]Extractor Predicting: 112it [01:20,  1.33it/s]Extractor Predicting: 113it [01:21,  1.34it/s]Extractor Predicting: 114it [01:22,  1.35it/s]Extractor Predicting: 115it [01:23,  1.31it/s]Extractor Predicting: 116it [01:23,  1.34it/s]Extractor Predicting: 117it [01:24,  1.33it/s]Extractor Predicting: 118it [01:25,  1.33it/s]Extractor Predicting: 119it [01:26,  1.33it/s]Extractor Predicting: 120it [01:26,  1.36it/s]Extractor Predicting: 121it [01:27,  1.37it/s]Extractor Predicting: 122it [01:28,  1.38it/s]Extractor Predicting: 123it [01:28,  1.37it/s]Extractor Predicting: 124it [01:29,  1.38it/s]Extractor Predicting: 125it [01:30,  1.40it/s]Extractor Predicting: 126it [01:31,  1.40it/s]Extractor Predicting: 127it [01:31,  1.39it/s]Extractor Predicting: 128it [01:32,  1.42it/s]Extractor Predicting: 129it [01:33,  1.42it/s]Extractor Predicting: 130it [01:33,  1.39it/s]Extractor Predicting: 131it [01:34,  1.43it/s]Extractor Predicting: 132it [01:35,  1.44it/s]Extractor Predicting: 133it [01:35,  1.46it/s]Extractor Predicting: 134it [01:36,  1.41it/s]Extractor Predicting: 135it [01:37,  1.44it/s]Extractor Predicting: 136it [01:38,  1.44it/s]Extractor Predicting: 137it [01:38,  1.46it/s]Extractor Predicting: 138it [01:39,  1.44it/s]Extractor Predicting: 139it [01:40,  1.43it/s]Extractor Predicting: 140it [01:40,  1.44it/s]Extractor Predicting: 141it [01:41,  1.41it/s]Extractor Predicting: 142it [01:42,  1.40it/s]Extractor Predicting: 143it [01:42,  1.42it/s]Extractor Predicting: 144it [01:43,  1.40it/s]Extractor Predicting: 145it [01:44,  1.45it/s]Extractor Predicting: 146it [01:45,  1.47it/s]Extractor Predicting: 147it [01:45,  1.48it/s]Extractor Predicting: 148it [01:46,  1.50it/s]Extractor Predicting: 149it [01:47,  1.49it/s]Extractor Predicting: 150it [01:47,  1.50it/s]Extractor Predicting: 151it [01:48,  1.51it/s]Extractor Predicting: 152it [01:48,  1.53it/s]Extractor Predicting: 153it [01:49,  1.50it/s]Extractor Predicting: 154it [01:50,  1.50it/s]Extractor Predicting: 155it [01:50,  1.54it/s]Extractor Predicting: 156it [01:51,  1.53it/s]Extractor Predicting: 157it [01:52,  1.60it/s]Extractor Predicting: 158it [01:52,  1.62it/s]Extractor Predicting: 159it [01:53,  1.57it/s]Extractor Predicting: 160it [01:54,  1.52it/s]Extractor Predicting: 161it [01:54,  1.51it/s]Extractor Predicting: 162it [01:55,  1.52it/s]Extractor Predicting: 163it [01:56,  1.55it/s]Extractor Predicting: 164it [01:56,  1.55it/s]Extractor Predicting: 165it [01:57,  1.55it/s]Extractor Predicting: 166it [01:58,  1.52it/s]Extractor Predicting: 167it [01:58,  1.54it/s]Extractor Predicting: 168it [01:59,  1.52it/s]Extractor Predicting: 169it [01:59,  1.58it/s]Extractor Predicting: 170it [02:00,  1.56it/s]Extractor Predicting: 171it [02:01,  1.56it/s]Extractor Predicting: 172it [02:01,  1.49it/s]Extractor Predicting: 173it [02:02,  1.47it/s]Extractor Predicting: 174it [02:03,  1.43it/s]Extractor Predicting: 175it [02:04,  1.38it/s]Extractor Predicting: 176it [02:04,  1.39it/s]Extractor Predicting: 177it [02:05,  1.39it/s]Extractor Predicting: 178it [02:06,  1.37it/s]Extractor Predicting: 179it [02:07,  1.37it/s]Extractor Predicting: 180it [02:07,  1.38it/s]Extractor Predicting: 181it [02:08,  1.38it/s]Extractor Predicting: 182it [02:09,  1.37it/s]Extractor Predicting: 183it [02:10,  1.37it/s]Extractor Predicting: 184it [02:10,  1.35it/s]Extractor Predicting: 185it [02:11,  1.34it/s]Extractor Predicting: 186it [02:12,  1.33it/s]Extractor Predicting: 187it [02:13,  1.34it/s]Extractor Predicting: 188it [02:13,  1.34it/s]Extractor Predicting: 189it [02:14,  1.34it/s]Extractor Predicting: 190it [02:15,  1.36it/s]Extractor Predicting: 191it [02:16,  1.33it/s]Extractor Predicting: 192it [02:16,  1.31it/s]Extractor Predicting: 193it [02:17,  1.32it/s]Extractor Predicting: 194it [02:18,  1.32it/s]Extractor Predicting: 195it [02:19,  1.33it/s]Extractor Predicting: 196it [02:19,  1.34it/s]Extractor Predicting: 197it [02:20,  1.33it/s]Extractor Predicting: 198it [02:21,  1.36it/s]Extractor Predicting: 199it [02:21,  1.37it/s]Extractor Predicting: 200it [02:22,  1.35it/s]Extractor Predicting: 201it [02:23,  1.32it/s]Extractor Predicting: 202it [02:24,  1.40it/s]Extractor Predicting: 203it [02:24,  1.38it/s]Extractor Predicting: 204it [02:25,  1.40it/s]Extractor Predicting: 205it [02:26,  1.38it/s]Extractor Predicting: 206it [02:27,  1.37it/s]Extractor Predicting: 207it [02:27,  1.36it/s]Extractor Predicting: 208it [02:28,  1.24it/s]Extractor Predicting: 209it [02:29,  1.23it/s]Extractor Predicting: 210it [02:30,  1.26it/s]Extractor Predicting: 211it [02:31,  1.27it/s]Extractor Predicting: 212it [02:31,  1.28it/s]Extractor Predicting: 213it [02:32,  1.29it/s]Extractor Predicting: 214it [02:33,  1.28it/s]Extractor Predicting: 215it [02:34,  1.32it/s]Extractor Predicting: 216it [02:34,  1.29it/s]Extractor Predicting: 217it [02:35,  1.31it/s]Extractor Predicting: 218it [02:36,  1.29it/s]Extractor Predicting: 219it [02:37,  1.28it/s]Extractor Predicting: 220it [02:38,  1.30it/s]Extractor Predicting: 221it [02:38,  1.28it/s]Extractor Predicting: 222it [02:39,  1.28it/s]Extractor Predicting: 223it [02:40,  1.31it/s]Extractor Predicting: 224it [02:41,  1.32it/s]Extractor Predicting: 225it [02:41,  1.33it/s]Extractor Predicting: 226it [02:42,  1.31it/s]Extractor Predicting: 227it [02:43,  1.36it/s]Extractor Predicting: 228it [02:44,  1.35it/s]Extractor Predicting: 229it [02:44,  1.35it/s]Extractor Predicting: 230it [02:45,  1.40it/s]Extractor Predicting: 231it [02:46,  1.40it/s]Extractor Predicting: 232it [02:46,  1.42it/s]Extractor Predicting: 233it [02:47,  1.40it/s]Extractor Predicting: 234it [02:48,  1.40it/s]Extractor Predicting: 235it [02:49,  1.39it/s]Extractor Predicting: 236it [02:49,  1.37it/s]Extractor Predicting: 237it [02:50,  1.35it/s]Extractor Predicting: 238it [02:51,  1.37it/s]Extractor Predicting: 239it [02:52,  1.35it/s]Extractor Predicting: 240it [02:52,  1.36it/s]Extractor Predicting: 241it [02:53,  1.31it/s]Extractor Predicting: 242it [02:54,  1.34it/s]Extractor Predicting: 243it [02:55,  1.33it/s]Extractor Predicting: 244it [02:55,  1.35it/s]Extractor Predicting: 245it [02:56,  1.37it/s]Extractor Predicting: 246it [02:57,  1.37it/s]Extractor Predicting: 247it [02:57,  1.37it/s]Extractor Predicting: 248it [02:58,  1.37it/s]Extractor Predicting: 249it [02:59,  1.35it/s]Extractor Predicting: 250it [03:00,  1.37it/s]Extractor Predicting: 251it [03:00,  1.38it/s]Extractor Predicting: 252it [03:01,  1.37it/s]Extractor Predicting: 253it [03:02,  1.35it/s]Extractor Predicting: 254it [03:03,  1.37it/s]Extractor Predicting: 255it [03:03,  1.32it/s]Extractor Predicting: 256it [03:04,  1.29it/s]Extractor Predicting: 257it [03:05,  1.30it/s]Extractor Predicting: 258it [03:06,  1.30it/s]Extractor Predicting: 259it [03:06,  1.32it/s]Extractor Predicting: 260it [03:07,  1.30it/s]Extractor Predicting: 261it [03:08,  1.34it/s]Extractor Predicting: 262it [03:09,  1.32it/s]Extractor Predicting: 263it [03:10,  1.30it/s]Extractor Predicting: 264it [03:10,  1.31it/s]Extractor Predicting: 265it [03:11,  1.26it/s]Extractor Predicting: 266it [03:12,  1.27it/s]Extractor Predicting: 267it [03:13,  1.28it/s]Extractor Predicting: 268it [03:13,  1.26it/s]Extractor Predicting: 269it [03:14,  1.29it/s]Extractor Predicting: 270it [03:15,  1.31it/s]Extractor Predicting: 271it [03:16,  1.28it/s]Extractor Predicting: 272it [03:17,  1.30it/s]Extractor Predicting: 273it [03:17,  1.30it/s]Extractor Predicting: 274it [03:18,  1.33it/s]Extractor Predicting: 275it [03:19,  1.34it/s]Extractor Predicting: 276it [03:19,  1.38it/s]Extractor Predicting: 277it [03:20,  1.38it/s]Extractor Predicting: 278it [03:21,  1.37it/s]Extractor Predicting: 279it [03:22,  1.37it/s]Extractor Predicting: 280it [03:22,  1.32it/s]Extractor Predicting: 281it [03:23,  1.32it/s]Extractor Predicting: 282it [03:24,  1.42it/s]Extractor Predicting: 282it [03:24,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:28,338 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:28,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:28,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:28,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:28,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:21:28,964 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:21:28,965 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:21:29,542 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:21:30,599 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:21:30,599 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:33,427 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:33,430 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:33,431 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:33,431 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:21:33,431 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:21:34,076 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:21:34,077 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:21:34,657 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:21:34,811 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:21:34,811 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.20it/s]Extractor Predicting: 2it [00:01,  1.14it/s]Extractor Predicting: 3it [00:02,  1.20it/s]Extractor Predicting: 4it [00:03,  1.24it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.26it/s]
[INFO|configuration_utils.py:515] 2023-08-28 06:21:39,223 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:21:39,224 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:21:39,231 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:21:39,232 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 06:21:39,234 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:21:42,321 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 06:21:42,323 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 06:21:42,333 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:21:42,334 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:21:42,338 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:21:42,341 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:21:42,341 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:21:42,341 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:21:42,341 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:21:42,341 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:21:42,341 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 06:21:42,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:43,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:44,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:45,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:46,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:47,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:48,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:49,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:50,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:51,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:52,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:53,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:54,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:54,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:55,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:56,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:57,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:58,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:21:59,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:00,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:01,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:02,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:03,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:04,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:05,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:06,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:07,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:08,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:09,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:10,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:11,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:29<06:52, 29.47s/it][WARNING|generation_utils.py:914] 2023-08-28 06:22:12,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:12,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:13,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:15,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:15,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:16,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:17,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:19,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:20,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:21,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:22,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:23,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:24,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:25,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:25,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:26,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:27,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:28,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:29,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:30,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:31,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:32,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:33,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:34,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:35,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:53<05:43, 26.45s/it][WARNING|generation_utils.py:914] 2023-08-28 06:22:36,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:37,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:38,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:38,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:39,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:40,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:42,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:43,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:44,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:45,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:46,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:47,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:48,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:49,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:50,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:51,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:51,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:53,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:54,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:55,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:56,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:57,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:58,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:59,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:22:59,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:00,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:19<05:11, 25.98s/it][WARNING|generation_utils.py:914] 2023-08-28 06:23:01,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:02,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:03,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:04,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:05,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:06,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:07,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:08,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:09,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:10,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:11,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:11,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:13,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:13,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:14,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:15,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:16,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:17,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:18,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:19,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:20,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:21,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:23,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:24,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:42<04:34, 24.93s/it][WARNING|generation_utils.py:914] 2023-08-28 06:23:25,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:26,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:27,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:28,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:29,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:30,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:31,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:32,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:33,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:34,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:35,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:36,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:37,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:38,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:39,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:40,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:41,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:42,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:43,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:44,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:45,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:46,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:47,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [02:05<04:02, 24.22s/it][WARNING|generation_utils.py:914] 2023-08-28 06:23:48,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:49,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:50,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:51,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:52,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:53,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:54,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:55,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:56,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:58,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:23:58,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:00,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:00,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:01,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:02,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:03,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:04,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:05,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:06,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:07,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:08,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:09,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:10,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:28<03:35, 23.95s/it][WARNING|generation_utils.py:914] 2023-08-28 06:24:11,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:12,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:13,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:14,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:15,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:16,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:17,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:18,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:19,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:20,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:21,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:21,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:22,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:23,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:24,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:25,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:26,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:27,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:28,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:29,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:30,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:31,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:32,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:50<03:06, 23.27s/it][WARNING|generation_utils.py:914] 2023-08-28 06:24:33,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:34,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:35,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:36,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:37,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:38,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:39,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:40,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:41,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:42,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:43,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:44,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:45,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:47,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:48,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:49,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:50,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:50,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:52,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:53,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:54,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:55,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:56,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [03:14<02:43, 23.37s/it][WARNING|generation_utils.py:914] 2023-08-28 06:24:56,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:57,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:58,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:24:59,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:00,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:01,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:02,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:03,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:05,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:05,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:07,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:07,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:08,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:09,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:11,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:12,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:13,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:13,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:15,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:16,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:17,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:18,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:19,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:37<02:19, 23.29s/it][WARNING|generation_utils.py:914] 2023-08-28 06:25:20,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:21,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:22,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:23,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:23,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:24,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:25,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:26,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:27,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:28,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:29,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:30,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:31,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:32,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:32,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:34,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:35,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:36,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:37,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:37,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:39,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:40,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [03:58<01:53, 22.62s/it][WARNING|generation_utils.py:914] 2023-08-28 06:25:41,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:42,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:43,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:44,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:45,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:46,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:47,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:48,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:49,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:50,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:51,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:52,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:53,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:54,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:55,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:56,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:57,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:58,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:25:59,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:00,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:01,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:02,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:02,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:04,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [04:22<01:31, 22.95s/it][WARNING|generation_utils.py:914] 2023-08-28 06:26:04,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:05,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:06,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:07,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:08,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:09,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:10,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:11,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:12,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:14,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:14,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:15,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:16,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:17,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:18,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:19,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:20,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:21,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:21,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:22,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:23,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:24,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:25,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:43<01:07, 22.53s/it][WARNING|generation_utils.py:914] 2023-08-28 06:26:26,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:27,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:28,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:29,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:29,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:30,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:31,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:32,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:33,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:34,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:35,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:36,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:37,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:38,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:39,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:40,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:41,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:42,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:43,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:44,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:45,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:46,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:47,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:47,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:48,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [05:07<00:45, 22.73s/it][WARNING|generation_utils.py:914] 2023-08-28 06:26:49,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:50,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:51,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:52,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:53,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:54,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:55,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:57,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:58,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:26:59,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:00,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:01,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:02,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:03,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:03,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:04,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:05,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:06,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:07,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:08,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:09,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:10,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:11,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:12,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [05:31<00:23, 23.10s/it][WARNING|generation_utils.py:914] 2023-08-28 06:27:13,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:14,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:15,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:16,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:17,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:18,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:19,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:20,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:21,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:22,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:23,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:24,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:25,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:25,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:27,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:28,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:29,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:30,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:31,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:32,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:33,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:34,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:35,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:27:36,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:55<00:00, 23.42s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:55<00:00, 23.68s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:44,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:44,519 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:44,519 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:44,519 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:44,519 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:27:45,134 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:27:45,135 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:27:45,734 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:27:46,791 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:27:46,791 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:49,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:49,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:49,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:49,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:27:49,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:27:50,488 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:27:50,489 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:27:51,074 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:27:51,237 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:27:51,237 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n', 'Relation : country . Context : In 1994 , , he was named as the winner of the Eurovision Song Contest 2000 . Head Entity : Eurovision Song Contest 2000 , Tail Entity : France .\n']
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 137, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 174, 'raw': 288}
{'target': 600, 'success': 190, 'raw': 320}
{'target': 600, 'success': 211, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 252, 'raw': 416}
{'target': 600, 'success': 274, 'raw': 448}
{'target': 600, 'success': 294, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 347, 'raw': 576}
{'target': 600, 'success': 361, 'raw': 608}
{'target': 600, 'success': 388, 'raw': 640}
{'target': 600, 'success': 408, 'raw': 672}
{'target': 600, 'success': 428, 'raw': 704}
{'target': 600, 'success': 443, 'raw': 736}
{'target': 600, 'success': 462, 'raw': 768}
{'target': 600, 'success': 483, 'raw': 800}
{'target': 600, 'success': 501, 'raw': 832}
{'target': 600, 'success': 519, 'raw': 864}
{'target': 600, 'success': 541, 'raw': 896}
{'target': 600, 'success': 562, 'raw': 928}
{'target': 600, 'success': 585, 'raw': 960}
{'target': 600, 'success': 608, 'raw': 992}
{'prompt': 'Relation : country .', 'success_rate': 0.6129032258064516, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 485, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year , the band formed New River Band with two of their members at the end of 2010 , Mikey McLeod ( the lyricist ) and Tim McCarroll ( bass ) . Head Entity : Mikey McNamara , Tail Entity : New River Band .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : member of .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : subsidiary . Context : The CIBN ( CBNI ) , also called CBE ( CBW , CBQ , CBQR , CJAX , CJD , CJEC , CJF , CJFY ) , is a United States National Research Council scientific satellite constellation . Head Entity : CBI , Tail Entity : United States National Research Council .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', "('A', 'subsidiary', '', 'On August 2017 , the company began shipping a new product for children in Europe : A .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Battle of Brackewald', 'field of work', '', 'In 1849 he became a volunteer for the British Army at the Battle of Brackewald in Normandy .')", 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : Later in the year ( 1141â€“1231 ) he met Ferdinand I of Spain and the Duke of Prussia , whom he bore in the name of Christophe , together with Robert I of Belgium . Head Entity : Christophe , Tail Entity : John I of Belgium .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.859375, 'errors': {''}}
["Relation : occupation . Context : On 31 March 2014 , the Romanian Army , under a new President of Romania , Luiz InÃ¡cio Lutcic , announced the departure of Luiz 's second - generation Air Force commander , former Admiral of Romania , Sigmund Kaveliu . Head Entity : GeniÃ§iu Lutcic , Tail Entity : Romanian Army .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 14467
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14567, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.14it/s]Extractor Estimating: 2it [00:01,  1.13it/s]Extractor Estimating: 3it [00:02,  1.14it/s]Extractor Estimating: 4it [00:03,  1.15it/s]Extractor Estimating: 5it [00:04,  1.22it/s]Extractor Estimating: 6it [00:04,  1.24it/s]Extractor Estimating: 7it [00:05,  1.27it/s]Extractor Estimating: 8it [00:06,  1.25it/s]Extractor Estimating: 9it [00:07,  1.27it/s]Extractor Estimating: 10it [00:08,  1.27it/s]Extractor Estimating: 11it [00:08,  1.25it/s]Extractor Estimating: 12it [00:09,  1.22it/s]Extractor Estimating: 13it [00:10,  1.20it/s]Extractor Estimating: 14it [00:11,  1.22it/s]Extractor Estimating: 15it [00:12,  1.21it/s]Extractor Estimating: 16it [00:13,  1.19it/s]Extractor Estimating: 17it [00:13,  1.21it/s]Extractor Estimating: 18it [00:14,  1.17it/s]Extractor Estimating: 19it [00:15,  1.21it/s]Extractor Estimating: 20it [00:16,  1.22it/s]Extractor Estimating: 21it [00:17,  1.22it/s]Extractor Estimating: 22it [00:18,  1.18it/s]Extractor Estimating: 23it [00:18,  1.20it/s]Extractor Estimating: 24it [00:19,  1.21it/s]Extractor Estimating: 25it [00:20,  1.23it/s]Extractor Estimating: 26it [00:21,  1.24it/s]Extractor Estimating: 27it [00:22,  1.15it/s]Extractor Estimating: 28it [00:23,  1.18it/s]Extractor Estimating: 29it [00:24,  1.14it/s]Extractor Estimating: 30it [00:25,  1.14it/s]Extractor Estimating: 31it [00:25,  1.20it/s]Extractor Estimating: 32it [00:26,  1.24it/s]Extractor Estimating: 33it [00:27,  1.20it/s]Extractor Estimating: 34it [00:28,  1.22it/s]Extractor Estimating: 35it [00:29,  1.21it/s]Extractor Estimating: 36it [00:29,  1.21it/s]Extractor Estimating: 37it [00:30,  1.20it/s]Extractor Estimating: 38it [00:31,  1.22it/s]Extractor Estimating: 39it [00:32,  1.21it/s]Extractor Estimating: 40it [00:33,  1.26it/s]Extractor Estimating: 41it [00:33,  1.23it/s]Extractor Estimating: 42it [00:34,  1.19it/s]Extractor Estimating: 43it [00:35,  1.19it/s]Extractor Estimating: 44it [00:36,  1.17it/s]Extractor Estimating: 45it [00:37,  1.20it/s]Extractor Estimating: 46it [00:38,  1.22it/s]Extractor Estimating: 47it [00:39,  1.19it/s]Extractor Estimating: 48it [00:39,  1.23it/s]Extractor Estimating: 49it [00:40,  1.16it/s]Extractor Estimating: 50it [00:41,  1.18it/s]Extractor Estimating: 51it [00:42,  1.19it/s]Extractor Estimating: 52it [00:43,  1.18it/s]Extractor Estimating: 53it [00:43,  1.22it/s]Extractor Estimating: 54it [00:44,  1.21it/s]Extractor Estimating: 55it [00:45,  1.23it/s]Extractor Estimating: 56it [00:46,  1.23it/s]Extractor Estimating: 57it [00:47,  1.22it/s]Extractor Estimating: 58it [00:48,  1.23it/s]Extractor Estimating: 59it [00:48,  1.24it/s]Extractor Estimating: 60it [00:49,  1.21it/s]Extractor Estimating: 61it [00:50,  1.20it/s]Extractor Estimating: 62it [00:51,  1.18it/s]Extractor Estimating: 63it [00:52,  1.21it/s]Extractor Estimating: 64it [00:53,  1.20it/s]Extractor Estimating: 65it [00:53,  1.22it/s]Extractor Estimating: 66it [00:54,  1.24it/s]Extractor Estimating: 67it [00:55,  1.24it/s]Extractor Estimating: 68it [00:56,  1.22it/s]Extractor Estimating: 69it [00:57,  1.20it/s]Extractor Estimating: 70it [00:57,  1.21it/s]Extractor Estimating: 71it [00:58,  1.22it/s]Extractor Estimating: 72it [00:59,  1.22it/s]Extractor Estimating: 73it [01:00,  1.20it/s]Extractor Estimating: 74it [01:01,  1.22it/s]Extractor Estimating: 75it [01:02,  1.22it/s]Extractor Estimating: 76it [01:02,  1.24it/s]Extractor Estimating: 77it [01:03,  1.24it/s]Extractor Estimating: 78it [01:04,  1.23it/s]Extractor Estimating: 79it [01:05,  1.22it/s]Extractor Estimating: 80it [01:06,  1.25it/s]Extractor Estimating: 81it [01:06,  1.25it/s]Extractor Estimating: 82it [01:07,  1.23it/s]Extractor Estimating: 83it [01:08,  1.25it/s]Extractor Estimating: 84it [01:09,  1.26it/s]Extractor Estimating: 85it [01:10,  1.24it/s]Extractor Estimating: 86it [01:10,  1.24it/s]Extractor Estimating: 87it [01:11,  1.22it/s]Extractor Estimating: 88it [01:12,  1.23it/s]Extractor Estimating: 89it [01:13,  1.23it/s]Extractor Estimating: 90it [01:14,  1.22it/s]Extractor Estimating: 91it [01:14,  1.24it/s]Extractor Estimating: 92it [01:15,  1.24it/s]Extractor Estimating: 93it [01:16,  1.27it/s]Extractor Estimating: 94it [01:17,  1.23it/s]Extractor Estimating: 95it [01:18,  1.25it/s]Extractor Estimating: 96it [01:18,  1.26it/s]Extractor Estimating: 97it [01:19,  1.24it/s]Extractor Estimating: 98it [01:20,  1.24it/s]Extractor Estimating: 99it [01:21,  1.26it/s]Extractor Estimating: 100it [01:22,  1.24it/s]Extractor Estimating: 101it [01:23,  1.19it/s]Extractor Estimating: 102it [01:23,  1.25it/s]Extractor Estimating: 103it [01:24,  1.28it/s]Extractor Estimating: 104it [01:25,  1.28it/s]Extractor Estimating: 105it [01:26,  1.23it/s]Extractor Estimating: 106it [01:27,  1.23it/s]Extractor Estimating: 107it [01:27,  1.21it/s]Extractor Estimating: 108it [01:28,  1.23it/s]Extractor Estimating: 109it [01:29,  1.24it/s]Extractor Estimating: 110it [01:30,  1.20it/s]Extractor Estimating: 111it [01:31,  1.24it/s]Extractor Estimating: 112it [01:32,  1.11it/s]Extractor Estimating: 113it [01:33,  1.12it/s]Extractor Estimating: 114it [01:33,  1.14it/s]Extractor Estimating: 115it [01:34,  1.17it/s]Extractor Estimating: 116it [01:35,  1.22it/s]Extractor Estimating: 117it [01:36,  1.21it/s]Extractor Estimating: 118it [01:37,  1.24it/s]Extractor Estimating: 119it [01:37,  1.22it/s]Extractor Estimating: 120it [01:38,  1.23it/s]Extractor Estimating: 121it [01:39,  1.27it/s]Extractor Estimating: 122it [01:40,  1.21it/s]Extractor Estimating: 123it [01:41,  1.26it/s]Extractor Estimating: 124it [01:41,  1.24it/s]Extractor Estimating: 125it [01:42,  1.21it/s]Extractor Estimating: 126it [01:43,  1.27it/s]Extractor Estimating: 127it [01:44,  1.24it/s]Extractor Estimating: 128it [01:45,  1.27it/s]Extractor Estimating: 129it [01:45,  1.27it/s]Extractor Estimating: 130it [01:46,  1.33it/s]Extractor Estimating: 131it [01:47,  1.31it/s]Extractor Estimating: 132it [01:48,  1.31it/s]Extractor Estimating: 133it [01:48,  1.31it/s]Extractor Estimating: 134it [01:49,  1.28it/s]Extractor Estimating: 135it [01:50,  1.31it/s]Extractor Estimating: 136it [01:51,  1.31it/s]Extractor Estimating: 137it [01:52,  1.23it/s]Extractor Estimating: 138it [01:52,  1.24it/s]Extractor Estimating: 139it [01:53,  1.29it/s]Extractor Estimating: 140it [01:54,  1.30it/s]Extractor Estimating: 141it [01:55,  1.29it/s]Extractor Estimating: 142it [01:55,  1.29it/s]Extractor Estimating: 143it [01:56,  1.31it/s]Extractor Estimating: 144it [01:57,  1.30it/s]Extractor Estimating: 145it [01:58,  1.27it/s]Extractor Estimating: 146it [01:58,  1.31it/s]Extractor Estimating: 147it [01:59,  1.29it/s]Extractor Estimating: 148it [02:00,  1.26it/s]Extractor Estimating: 149it [02:01,  1.22it/s]Extractor Estimating: 150it [02:02,  1.22it/s]Extractor Estimating: 151it [02:03,  1.27it/s]Extractor Estimating: 152it [02:03,  1.26it/s]Extractor Estimating: 153it [02:04,  1.27it/s]Extractor Estimating: 154it [02:05,  1.26it/s]Extractor Estimating: 155it [02:06,  1.26it/s]Extractor Estimating: 156it [02:06,  1.26it/s]Extractor Estimating: 157it [02:07,  1.22it/s]Extractor Estimating: 158it [02:08,  1.24it/s]Extractor Estimating: 159it [02:09,  1.23it/s]Extractor Estimating: 160it [02:10,  1.24it/s]Extractor Estimating: 161it [02:11,  1.25it/s]Extractor Estimating: 162it [02:11,  1.25it/s]Extractor Estimating: 163it [02:12,  1.25it/s]Extractor Estimating: 164it [02:13,  1.28it/s]Extractor Estimating: 165it [02:14,  1.30it/s]Extractor Estimating: 166it [02:14,  1.28it/s]Extractor Estimating: 167it [02:15,  1.25it/s]Extractor Estimating: 168it [02:16,  1.26it/s]Extractor Estimating: 169it [02:17,  1.27it/s]Extractor Estimating: 170it [02:18,  1.32it/s]Extractor Estimating: 171it [02:18,  1.31it/s]Extractor Estimating: 172it [02:19,  1.25it/s]Extractor Estimating: 173it [02:20,  1.22it/s]Extractor Estimating: 174it [02:21,  1.21it/s]Extractor Estimating: 175it [02:22,  1.23it/s]Extractor Estimating: 176it [02:23,  1.20it/s]Extractor Estimating: 177it [02:23,  1.21it/s]Extractor Estimating: 178it [02:24,  1.23it/s]Extractor Estimating: 179it [02:25,  1.24it/s]Extractor Estimating: 180it [02:26,  1.16it/s]Extractor Estimating: 181it [02:27,  1.21it/s]Extractor Estimating: 182it [02:27,  1.21it/s]Extractor Estimating: 183it [02:28,  1.22it/s]Extractor Estimating: 184it [02:29,  1.23it/s]Extractor Estimating: 185it [02:30,  1.24it/s]Extractor Estimating: 186it [02:31,  1.25it/s]Extractor Estimating: 187it [02:32,  1.22it/s]Extractor Estimating: 188it [02:32,  1.23it/s]Extractor Estimating: 189it [02:33,  1.12it/s]Extractor Estimating: 190it [02:34,  1.17it/s]Extractor Estimating: 191it [02:35,  1.22it/s]Extractor Estimating: 192it [02:36,  1.20it/s]Extractor Estimating: 193it [02:37,  1.21it/s]Extractor Estimating: 194it [02:37,  1.20it/s]Extractor Estimating: 195it [02:38,  1.22it/s]Extractor Estimating: 196it [02:39,  1.26it/s]Extractor Estimating: 197it [02:40,  1.22it/s]Extractor Estimating: 198it [02:41,  1.23it/s]Extractor Estimating: 199it [02:41,  1.27it/s]Extractor Estimating: 200it [02:42,  1.24it/s]Extractor Estimating: 201it [02:43,  1.21it/s]Extractor Estimating: 202it [02:44,  1.22it/s]Extractor Estimating: 203it [02:45,  1.22it/s]Extractor Estimating: 204it [02:46,  1.22it/s]Extractor Estimating: 205it [02:46,  1.24it/s]Extractor Estimating: 206it [02:47,  1.22it/s]Extractor Estimating: 207it [02:48,  1.26it/s]Extractor Estimating: 208it [02:49,  1.19it/s]Extractor Estimating: 209it [02:50,  1.18it/s]Extractor Estimating: 210it [02:51,  1.20it/s]Extractor Estimating: 211it [02:51,  1.19it/s]Extractor Estimating: 212it [02:52,  1.18it/s]Extractor Estimating: 213it [02:53,  1.16it/s]Extractor Estimating: 214it [02:54,  1.17it/s]Extractor Estimating: 215it [02:55,  1.19it/s]Extractor Estimating: 216it [02:56,  1.15it/s]Extractor Estimating: 217it [02:57,  1.15it/s]Extractor Estimating: 218it [02:57,  1.18it/s]Extractor Estimating: 219it [02:58,  1.17it/s]Extractor Estimating: 220it [02:59,  1.19it/s]Extractor Estimating: 221it [03:00,  1.15it/s]Extractor Estimating: 222it [03:01,  1.15it/s]Extractor Estimating: 223it [03:02,  1.15it/s]Extractor Estimating: 224it [03:03,  1.17it/s]Extractor Estimating: 225it [03:03,  1.18it/s]Extractor Estimating: 226it [03:04,  1.16it/s]Extractor Estimating: 227it [03:05,  1.22it/s]Extractor Estimating: 228it [03:06,  1.26it/s]Extractor Estimating: 229it [03:06,  1.29it/s]Extractor Estimating: 230it [03:07,  1.28it/s]Extractor Estimating: 231it [03:08,  1.31it/s]Extractor Estimating: 232it [03:09,  1.35it/s]Extractor Estimating: 233it [03:09,  1.38it/s]Extractor Estimating: 234it [03:10,  1.37it/s]Extractor Estimating: 235it [03:11,  1.35it/s]Extractor Estimating: 236it [03:12,  1.33it/s]Extractor Estimating: 237it [03:12,  1.32it/s]Extractor Estimating: 238it [03:13,  1.31it/s]Extractor Estimating: 239it [03:14,  1.30it/s]Extractor Estimating: 240it [03:15,  1.30it/s]Extractor Estimating: 241it [03:15,  1.33it/s]Extractor Estimating: 242it [03:16,  1.32it/s]Extractor Estimating: 243it [03:17,  1.28it/s]Extractor Estimating: 244it [03:18,  1.27it/s]Extractor Estimating: 245it [03:19,  1.29it/s]Extractor Estimating: 246it [03:19,  1.32it/s]Extractor Estimating: 247it [03:20,  1.31it/s]Extractor Estimating: 248it [03:21,  1.27it/s]Extractor Estimating: 249it [03:22,  1.20it/s]Extractor Estimating: 250it [03:23,  1.24it/s]Extractor Estimating: 251it [03:24,  1.17it/s]Extractor Estimating: 252it [03:24,  1.22it/s]Extractor Estimating: 253it [03:25,  1.23it/s]Extractor Estimating: 254it [03:26,  1.18it/s]Extractor Estimating: 255it [03:27,  1.23it/s]Extractor Estimating: 256it [03:27,  1.30it/s]Extractor Estimating: 257it [03:28,  1.28it/s]Extractor Estimating: 258it [03:29,  1.20it/s]Extractor Estimating: 259it [03:30,  1.24it/s]Extractor Estimating: 260it [03:31,  1.26it/s]Extractor Estimating: 261it [03:32,  1.23it/s]Extractor Estimating: 262it [03:32,  1.20it/s]Extractor Estimating: 263it [03:33,  1.22it/s]Extractor Estimating: 264it [03:34,  1.24it/s]Extractor Estimating: 265it [03:35,  1.20it/s]Extractor Estimating: 266it [03:36,  1.21it/s]Extractor Estimating: 267it [03:37,  1.19it/s]Extractor Estimating: 268it [03:37,  1.24it/s]Extractor Estimating: 269it [03:38,  1.25it/s]Extractor Estimating: 270it [03:39,  1.22it/s]Extractor Estimating: 271it [03:40,  1.21it/s]Extractor Estimating: 272it [03:41,  1.20it/s]Extractor Estimating: 273it [03:41,  1.22it/s]Extractor Estimating: 274it [03:42,  1.20it/s]Extractor Estimating: 275it [03:43,  1.23it/s]Extractor Estimating: 276it [03:44,  1.28it/s]Extractor Estimating: 277it [03:45,  1.27it/s]Extractor Estimating: 278it [03:45,  1.32it/s]Extractor Estimating: 279it [03:46,  1.30it/s]Extractor Estimating: 280it [03:47,  1.31it/s]Extractor Estimating: 281it [03:48,  1.32it/s]Extractor Estimating: 282it [03:48,  1.35it/s]Extractor Estimating: 283it [03:49,  1.33it/s]Extractor Estimating: 284it [03:50,  1.30it/s]Extractor Estimating: 285it [03:51,  1.33it/s]Extractor Estimating: 286it [03:51,  1.32it/s]Extractor Estimating: 287it [03:52,  1.32it/s]Extractor Estimating: 288it [03:53,  1.28it/s]Extractor Estimating: 289it [03:54,  1.31it/s]Extractor Estimating: 290it [03:54,  1.30it/s]Extractor Estimating: 291it [03:55,  1.34it/s]Extractor Estimating: 292it [03:56,  1.30it/s]Extractor Estimating: 293it [03:57,  1.32it/s]Extractor Estimating: 294it [03:57,  1.30it/s]Extractor Estimating: 295it [03:58,  1.29it/s]Extractor Estimating: 296it [03:59,  1.33it/s]Extractor Estimating: 297it [04:00,  1.34it/s]Extractor Estimating: 298it [04:00,  1.35it/s]Extractor Estimating: 299it [04:01,  1.38it/s]Extractor Estimating: 300it [04:02,  1.39it/s]Extractor Estimating: 301it [04:03,  1.32it/s]Extractor Estimating: 302it [04:04,  1.25it/s]Extractor Estimating: 303it [04:04,  1.25it/s]Extractor Estimating: 304it [04:05,  1.31it/s]Extractor Estimating: 305it [04:06,  1.30it/s]Extractor Estimating: 306it [04:07,  1.33it/s]Extractor Estimating: 307it [04:07,  1.34it/s]Extractor Estimating: 308it [04:08,  1.40it/s]Extractor Estimating: 309it [04:09,  1.39it/s]Extractor Estimating: 310it [04:09,  1.34it/s]Extractor Estimating: 311it [04:10,  1.26it/s]Extractor Estimating: 312it [04:11,  1.32it/s]Extractor Estimating: 313it [04:12,  1.29it/s]Extractor Estimating: 314it [04:13,  1.27it/s]Extractor Estimating: 315it [04:14,  1.15it/s]Extractor Estimating: 316it [04:14,  1.20it/s]Extractor Estimating: 317it [04:15,  1.24it/s]Extractor Estimating: 318it [04:16,  1.22it/s]Extractor Estimating: 319it [04:17,  1.26it/s]Extractor Estimating: 320it [04:18,  1.21it/s]Extractor Estimating: 321it [04:18,  1.27it/s]Extractor Estimating: 322it [04:19,  1.26it/s]Extractor Estimating: 323it [04:20,  1.27it/s]Extractor Estimating: 324it [04:21,  1.28it/s]Extractor Estimating: 325it [04:22,  1.29it/s]Extractor Estimating: 326it [04:23,  1.18it/s]Extractor Estimating: 327it [04:23,  1.21it/s]Extractor Estimating: 328it [04:24,  1.20it/s]Extractor Estimating: 329it [04:25,  1.23it/s]Extractor Estimating: 330it [04:26,  1.23it/s]Extractor Estimating: 331it [04:27,  1.25it/s]Extractor Estimating: 332it [04:27,  1.24it/s]Extractor Estimating: 333it [04:28,  1.23it/s]Extractor Estimating: 334it [04:29,  1.25it/s]Extractor Estimating: 335it [04:30,  1.23it/s]Extractor Estimating: 336it [04:31,  1.24it/s]Extractor Estimating: 337it [04:31,  1.27it/s]Extractor Estimating: 338it [04:32,  1.16it/s]Extractor Estimating: 339it [04:33,  1.19it/s]Extractor Estimating: 340it [04:34,  1.23it/s]Extractor Estimating: 341it [04:35,  1.18it/s]Extractor Estimating: 342it [04:36,  1.20it/s]Extractor Estimating: 343it [04:36,  1.20it/s]Extractor Estimating: 344it [04:37,  1.20it/s]Extractor Estimating: 345it [04:38,  1.22it/s]Extractor Estimating: 346it [04:39,  1.18it/s]Extractor Estimating: 347it [04:40,  1.13it/s]Extractor Estimating: 348it [04:41,  1.12it/s]Extractor Estimating: 349it [04:42,  1.12it/s]Extractor Estimating: 350it [04:43,  1.16it/s]Extractor Estimating: 351it [04:43,  1.19it/s]Extractor Estimating: 352it [04:44,  1.22it/s]Extractor Estimating: 353it [04:45,  1.25it/s]Extractor Estimating: 354it [04:46,  1.29it/s]Extractor Estimating: 355it [04:46,  1.31it/s]Extractor Estimating: 356it [04:47,  1.26it/s]Extractor Estimating: 357it [04:48,  1.27it/s]Extractor Estimating: 358it [04:49,  1.26it/s]Extractor Estimating: 359it [04:49,  1.33it/s]Extractor Estimating: 360it [04:50,  1.34it/s]Extractor Estimating: 361it [04:51,  1.31it/s]Extractor Estimating: 362it [04:52,  1.28it/s]Extractor Estimating: 363it [04:52,  1.32it/s]Extractor Estimating: 364it [04:53,  1.28it/s]Extractor Estimating: 365it [04:54,  1.22it/s]Extractor Estimating: 366it [04:55,  1.23it/s]Extractor Estimating: 367it [04:56,  1.25it/s]Extractor Estimating: 368it [04:57,  1.23it/s]Extractor Estimating: 369it [04:57,  1.24it/s]Extractor Estimating: 370it [04:58,  1.26it/s]Extractor Estimating: 371it [04:59,  1.25it/s]Extractor Estimating: 372it [05:00,  1.28it/s]Extractor Estimating: 373it [05:01,  1.26it/s]Extractor Estimating: 374it [05:01,  1.23it/s]Extractor Estimating: 375it [05:02,  1.50it/s]Extractor Estimating: 375it [05:02,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:05,090 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:05,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:05,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:05,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:05,092 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:33:05,446 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:33:05,447 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:33:06,139 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:33:07,188 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:33:07,188 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:09,740 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:09,744 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:09,744 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:09,744 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:33:09,744 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:33:10,410 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:33:10,411 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:33:10,978 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:33:11,150 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:33:11,150 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 09:38:04,579 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 09:38:04,597 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7899 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 24761
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24861, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24861, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.393, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.395, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.428, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 70, avg_time 1.405, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 170, avg_time 1.421, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 270, avg_time 2.814, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 40, avg_time 1.379, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 140, avg_time 1.403, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 240, avg_time 1.401, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 10, avg_time 1.390, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 110, avg_time 2.830, loss:nan
g_step 1200, step 210, avg_time 1.410, loss:nan
g_step 1300, step 310, avg_time 1.385, loss:nan
g_step 1400, step 80, avg_time 1.402, loss:nan
g_step 1500, step 180, avg_time 1.389, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 280, avg_time 2.828, loss:nan
g_step 1700, step 50, avg_time 1.397, loss:nan
g_step 1800, step 150, avg_time 1.422, loss:nan
g_step 1900, step 250, avg_time 1.398, loss:nan
g_step 2000, step 20, avg_time 1.403, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 120, avg_time 2.828, loss:nan
g_step 2200, step 220, avg_time 1.394, loss:nan
g_step 2300, step 320, avg_time 1.419, loss:nan
g_step 2400, step 90, avg_time 1.399, loss:nan
g_step 2500, step 190, avg_time 1.392, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 290, avg_time 2.818, loss:nan
g_step 2700, step 60, avg_time 1.395, loss:nan
g_step 2800, step 160, avg_time 1.389, loss:nan
g_step 2900, step 260, avg_time 1.425, loss:nan
g_step 3000, step 30, avg_time 1.411, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 130, avg_time 2.792, loss:nan
g_step 3200, step 230, avg_time 1.397, loss:nan
g_step 3300, step 330, avg_time 1.415, loss:nan
g_step 3400, step 100, avg_time 1.391, loss:nan
g_step 3500, step 200, avg_time 1.414, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 300, avg_time 2.813, loss:nan
g_step 3700, step 70, avg_time 1.395, loss:nan
g_step 3800, step 170, avg_time 1.396, loss:nan
g_step 3900, step 270, avg_time 1.402, loss:nan
g_step 4000, step 40, avg_time 1.400, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 140, avg_time 2.811, loss:nan
g_step 4200, step 240, avg_time 1.415, loss:nan
g_step 4300, step 10, avg_time 1.390, loss:nan
g_step 4400, step 110, avg_time 1.397, loss:nan
g_step 4500, step 210, avg_time 1.413, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 310, avg_time 2.801, loss:nan
g_step 4700, step 80, avg_time 1.403, loss:nan
g_step 4800, step 180, avg_time 1.418, loss:nan
g_step 4900, step 280, avg_time 1.391, loss:nan
g_step 5000, step 50, avg_time 1.420, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 150, avg_time 2.804, loss:nan
g_step 5200, step 250, avg_time 1.417, loss:nan
g_step 5300, step 20, avg_time 1.394, loss:nan
g_step 5400, step 120, avg_time 1.399, loss:nan
g_step 5500, step 220, avg_time 1.397, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 320, avg_time 2.813, loss:nan
g_step 5700, step 90, avg_time 1.400, loss:nan
g_step 5800, step 190, avg_time 1.402, loss:nan
g_step 5900, step 290, avg_time 1.404, loss:nan
g_step 6000, step 60, avg_time 1.403, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 160, avg_time 2.819, loss:nan
g_step 6200, step 260, avg_time 1.391, loss:nan
g_step 6300, step 30, avg_time 1.401, loss:nan
g_step 6400, step 130, avg_time 1.413, loss:nan
g_step 6500, step 230, avg_time 1.399, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 330, avg_time 2.810, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 09:38:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 09:38:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_09-38-04_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 09:38:05 - WARNING - datasets.builder -   Using custom data configuration default-0a50436f0a3e1a03
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-0a50436f0a3e1a03/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 09:38:05,906 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:38:05,907 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:38:05,908 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:38:05,909 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:38:05,921 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:38:05,930 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:38:05,930 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:38:05,930 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:38:05,930 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:38:05,930 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:38:05,930 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 09:38:06,063 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:38:09,148 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 09:38:09,151 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-0a50436f0a3e1a03/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  3.14ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.97ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  3.48ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  3.87ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.13ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.30ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.42ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.61ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.20ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  4.12ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.39ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.49ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.68ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.12ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  7.02ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:00,  7.94ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  8.11ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  8.32ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  8.42ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:00<00:00,  8.64ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00,  8.62ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.84ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.49ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.93ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  7.75ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  8.14ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  9.16ba/s]
[INFO|trainer.py:414] 2023-08-28 09:38:13,621 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 09:38:13,635 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 09:38:13,635 >>   Num examples = 7920
[INFO|trainer.py:1149] 2023-08-28 09:38:13,635 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 09:38:13,635 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 09:38:13,635 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 09:38:13,635 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 09:38:13,635 >>   Total optimization steps = 620
  0%|          | 0/620 [00:00<?, ?it/s]  0%|          | 1/620 [00:00<03:01,  3.42it/s]  0%|          | 2/620 [00:00<02:54,  3.54it/s]  0%|          | 3/620 [00:00<02:52,  3.58it/s]  1%|          | 4/620 [00:01<02:50,  3.60it/s]  1%|          | 5/620 [00:01<02:50,  3.62it/s]  1%|          | 6/620 [00:01<02:49,  3.62it/s]  1%|          | 7/620 [00:01<02:49,  3.62it/s]  1%|â–         | 8/620 [00:02<02:48,  3.63it/s]  1%|â–         | 9/620 [00:02<02:48,  3.63it/s]  2%|â–         | 10/620 [00:02<02:47,  3.63it/s]  2%|â–         | 11/620 [00:03<02:47,  3.64it/s]  2%|â–         | 12/620 [00:03<02:47,  3.64it/s]  2%|â–         | 13/620 [00:03<02:46,  3.64it/s]  2%|â–         | 14/620 [00:03<02:46,  3.64it/s]  2%|â–         | 15/620 [00:04<02:45,  3.65it/s]  3%|â–Ž         | 16/620 [00:04<02:45,  3.65it/s]  3%|â–Ž         | 17/620 [00:04<02:45,  3.65it/s]  3%|â–Ž         | 18/620 [00:04<02:45,  3.64it/s]  3%|â–Ž         | 19/620 [00:05<02:45,  3.64it/s]  3%|â–Ž         | 20/620 [00:05<02:44,  3.64it/s]  3%|â–Ž         | 21/620 [00:05<02:44,  3.64it/s]  4%|â–Ž         | 22/620 [00:06<02:43,  3.65it/s]  4%|â–Ž         | 23/620 [00:06<02:43,  3.65it/s]  4%|â–         | 24/620 [00:06<02:43,  3.65it/s]  4%|â–         | 25/620 [00:06<02:43,  3.65it/s]  4%|â–         | 26/620 [00:07<02:42,  3.65it/s]  4%|â–         | 27/620 [00:07<02:42,  3.65it/s]  5%|â–         | 28/620 [00:07<02:42,  3.65it/s]  5%|â–         | 29/620 [00:07<02:43,  3.61it/s]  5%|â–         | 30/620 [00:08<02:43,  3.62it/s]  5%|â–Œ         | 31/620 [00:08<02:42,  3.63it/s]  5%|â–Œ         | 32/620 [00:08<02:41,  3.63it/s]  5%|â–Œ         | 33/620 [00:09<02:41,  3.64it/s]  5%|â–Œ         | 34/620 [00:09<02:41,  3.64it/s]  6%|â–Œ         | 35/620 [00:09<02:40,  3.64it/s]  6%|â–Œ         | 36/620 [00:09<02:40,  3.64it/s]  6%|â–Œ         | 37/620 [00:10<02:39,  3.64it/s]  6%|â–Œ         | 38/620 [00:10<02:39,  3.65it/s]  6%|â–‹         | 39/620 [00:10<02:39,  3.65it/s]  6%|â–‹         | 40/620 [00:11<02:39,  3.64it/s]  7%|â–‹         | 41/620 [00:11<02:39,  3.64it/s]  7%|â–‹         | 42/620 [00:11<02:38,  3.64it/s]  7%|â–‹         | 43/620 [00:11<02:38,  3.64it/s]  7%|â–‹         | 44/620 [00:12<02:38,  3.64it/s]  7%|â–‹         | 45/620 [00:12<02:37,  3.64it/s]  7%|â–‹         | 46/620 [00:12<02:37,  3.64it/s]  8%|â–Š         | 47/620 [00:12<02:37,  3.64it/s]  8%|â–Š         | 48/620 [00:13<02:37,  3.64it/s]  8%|â–Š         | 49/620 [00:13<02:36,  3.64it/s]  8%|â–Š         | 50/620 [00:13<02:36,  3.65it/s]  8%|â–Š         | 51/620 [00:14<02:36,  3.63it/s]  8%|â–Š         | 52/620 [00:14<02:36,  3.64it/s]  9%|â–Š         | 53/620 [00:14<02:35,  3.64it/s]  9%|â–Š         | 54/620 [00:14<02:35,  3.64it/s]  9%|â–‰         | 55/620 [00:15<02:35,  3.64it/s]  9%|â–‰         | 56/620 [00:15<02:34,  3.64it/s]  9%|â–‰         | 57/620 [00:15<02:34,  3.64it/s]  9%|â–‰         | 58/620 [00:15<02:34,  3.64it/s] 10%|â–‰         | 59/620 [00:16<02:33,  3.65it/s] 10%|â–‰         | 60/620 [00:16<02:33,  3.65it/s] 10%|â–‰         | 61/620 [00:16<02:33,  3.65it/s] 10%|â–ˆ         | 62/620 [00:17<02:33,  3.64it/s] 10%|â–ˆ         | 63/620 [00:17<02:33,  3.64it/s] 10%|â–ˆ         | 64/620 [00:17<02:32,  3.64it/s] 10%|â–ˆ         | 65/620 [00:17<02:32,  3.64it/s] 11%|â–ˆ         | 66/620 [00:18<02:32,  3.64it/s] 11%|â–ˆ         | 67/620 [00:18<02:31,  3.64it/s] 11%|â–ˆ         | 68/620 [00:18<02:31,  3.64it/s] 11%|â–ˆ         | 69/620 [00:18<02:31,  3.64it/s] 11%|â–ˆâ–        | 70/620 [00:19<02:30,  3.64it/s] 11%|â–ˆâ–        | 71/620 [00:19<02:30,  3.64it/s] 12%|â–ˆâ–        | 72/620 [00:19<02:30,  3.64it/s] 12%|â–ˆâ–        | 73/620 [00:20<02:30,  3.64it/s] 12%|â–ˆâ–        | 74/620 [00:20<02:30,  3.64it/s] 12%|â–ˆâ–        | 75/620 [00:20<02:29,  3.64it/s] 12%|â–ˆâ–        | 76/620 [00:20<02:29,  3.64it/s] 12%|â–ˆâ–        | 77/620 [00:21<02:29,  3.64it/s] 13%|â–ˆâ–Ž        | 78/620 [00:21<02:28,  3.64it/s] 13%|â–ˆâ–Ž        | 79/620 [00:21<02:28,  3.64it/s] 13%|â–ˆâ–Ž        | 80/620 [00:21<02:28,  3.64it/s] 13%|â–ˆâ–Ž        | 81/620 [00:22<02:27,  3.64it/s] 13%|â–ˆâ–Ž        | 82/620 [00:22<02:27,  3.64it/s] 13%|â–ˆâ–Ž        | 83/620 [00:22<02:27,  3.64it/s] 14%|â–ˆâ–Ž        | 84/620 [00:23<02:27,  3.64it/s] 14%|â–ˆâ–Ž        | 85/620 [00:23<02:26,  3.64it/s] 14%|â–ˆâ–        | 86/620 [00:23<02:26,  3.64it/s] 14%|â–ˆâ–        | 87/620 [00:23<02:26,  3.64it/s] 14%|â–ˆâ–        | 88/620 [00:24<02:26,  3.64it/s] 14%|â–ˆâ–        | 89/620 [00:24<02:25,  3.64it/s] 15%|â–ˆâ–        | 90/620 [00:24<02:26,  3.63it/s] 15%|â–ˆâ–        | 91/620 [00:25<02:25,  3.63it/s] 15%|â–ˆâ–        | 92/620 [00:25<02:25,  3.63it/s] 15%|â–ˆâ–Œ        | 93/620 [00:25<02:24,  3.64it/s] 15%|â–ˆâ–Œ        | 94/620 [00:25<02:24,  3.64it/s] 15%|â–ˆâ–Œ        | 95/620 [00:26<02:24,  3.64it/s] 15%|â–ˆâ–Œ        | 96/620 [00:26<02:23,  3.64it/s] 16%|â–ˆâ–Œ        | 97/620 [00:26<02:23,  3.64it/s] 16%|â–ˆâ–Œ        | 98/620 [00:26<02:23,  3.64it/s] 16%|â–ˆâ–Œ        | 99/620 [00:27<02:23,  3.64it/s] 16%|â–ˆâ–Œ        | 100/620 [00:27<02:22,  3.64it/s] 16%|â–ˆâ–‹        | 101/620 [00:27<02:23,  3.62it/s] 16%|â–ˆâ–‹        | 102/620 [00:28<02:22,  3.63it/s] 17%|â–ˆâ–‹        | 103/620 [00:28<02:22,  3.63it/s] 17%|â–ˆâ–‹        | 104/620 [00:28<02:21,  3.64it/s] 17%|â–ˆâ–‹        | 105/620 [00:28<02:21,  3.64it/s] 17%|â–ˆâ–‹        | 106/620 [00:29<02:21,  3.64it/s] 17%|â–ˆâ–‹        | 107/620 [00:29<02:20,  3.64it/s] 17%|â–ˆâ–‹        | 108/620 [00:29<02:20,  3.64it/s] 18%|â–ˆâ–Š        | 109/620 [00:29<02:20,  3.64it/s] 18%|â–ˆâ–Š        | 110/620 [00:30<02:20,  3.64it/s] 18%|â–ˆâ–Š        | 111/620 [00:30<02:19,  3.64it/s] 18%|â–ˆâ–Š        | 112/620 [00:30<02:19,  3.63it/s] 18%|â–ˆâ–Š        | 113/620 [00:31<02:19,  3.63it/s] 18%|â–ˆâ–Š        | 114/620 [00:31<02:19,  3.64it/s] 19%|â–ˆâ–Š        | 115/620 [00:31<02:18,  3.64it/s] 19%|â–ˆâ–Š        | 116/620 [00:31<02:18,  3.64it/s] 19%|â–ˆâ–‰        | 117/620 [00:32<02:18,  3.64it/s] 19%|â–ˆâ–‰        | 118/620 [00:32<02:17,  3.64it/s] 19%|â–ˆâ–‰        | 119/620 [00:32<02:17,  3.64it/s] 19%|â–ˆâ–‰        | 120/620 [00:32<02:17,  3.64it/s] 20%|â–ˆâ–‰        | 121/620 [00:33<02:17,  3.64it/s] 20%|â–ˆâ–‰        | 122/620 [00:33<02:16,  3.64it/s] 20%|â–ˆâ–‰        | 123/620 [00:33<02:16,  3.63it/s] 20%|â–ˆâ–ˆ        | 124/620 [00:34<02:07,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 09:38:47,664 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:38:47,664 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 09:38:47,664 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.12it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.66it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.89it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.15it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.76it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.49it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.29it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 47.06it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.92it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.88it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.90it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.92it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.90it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.91it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.92it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.90it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.88it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.83it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.78it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.83it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.86it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.84it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.87it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.89it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.86it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.88it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.82it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.76it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.81it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.83it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.83it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.51it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.80it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.79it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.82it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.76it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.76it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.81it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.77it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.80it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.85it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.80it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.82it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.85it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.75it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.81it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.84it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.79it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 46.83it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.86it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.79it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.84it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.85it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.77it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.82it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.84it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.80it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.79it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.76it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.77it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.81it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.83it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.76it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.75it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.73it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.80it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.83it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.80it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.83it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.85it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.80it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.79it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.77it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.72it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.73it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.73it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.75it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.80it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.78it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.78it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.53it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.65it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.69it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.71it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.69it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.75it/s][A                                                 
                                                 [A 20%|â–ˆâ–ˆ        | 124/620 [00:43<02:07,  3.90it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.75it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:38:56,968 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-28 09:38:56,991 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:39:00,149 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:39:00,164 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:39:00,177 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 125/620 [00:47<33:50,  4.10s/it] 20%|â–ˆâ–ˆ        | 126/620 [00:47<24:19,  2.95s/it] 20%|â–ˆâ–ˆ        | 127/620 [00:47<17:39,  2.15s/it] 21%|â–ˆâ–ˆ        | 128/620 [00:47<13:00,  1.59s/it] 21%|â–ˆâ–ˆ        | 129/620 [00:48<09:45,  1.19s/it] 21%|â–ˆâ–ˆ        | 130/620 [00:48<07:29,  1.09it/s] 21%|â–ˆâ–ˆ        | 131/620 [00:48<05:54,  1.38it/s] 21%|â–ˆâ–ˆâ–       | 132/620 [00:49<04:47,  1.70it/s] 21%|â–ˆâ–ˆâ–       | 133/620 [00:49<04:01,  2.02it/s] 22%|â–ˆâ–ˆâ–       | 134/620 [00:49<03:28,  2.33it/s] 22%|â–ˆâ–ˆâ–       | 135/620 [00:49<03:06,  2.61it/s] 22%|â–ˆâ–ˆâ–       | 136/620 [00:50<02:49,  2.85it/s] 22%|â–ˆâ–ˆâ–       | 137/620 [00:50<02:38,  3.05it/s] 22%|â–ˆâ–ˆâ–       | 138/620 [00:50<02:30,  3.21it/s] 22%|â–ˆâ–ˆâ–       | 139/620 [00:50<02:25,  3.30it/s] 23%|â–ˆâ–ˆâ–Ž       | 140/620 [00:51<02:24,  3.32it/s] 23%|â–ˆâ–ˆâ–Ž       | 141/620 [00:51<02:20,  3.40it/s] 23%|â–ˆâ–ˆâ–Ž       | 142/620 [00:51<02:17,  3.47it/s] 23%|â–ˆâ–ˆâ–Ž       | 143/620 [00:52<02:15,  3.52it/s] 23%|â–ˆâ–ˆâ–Ž       | 144/620 [00:52<02:13,  3.56it/s] 23%|â–ˆâ–ˆâ–Ž       | 145/620 [00:52<02:12,  3.58it/s] 24%|â–ˆâ–ˆâ–Ž       | 146/620 [00:52<02:11,  3.60it/s] 24%|â–ˆâ–ˆâ–Ž       | 147/620 [00:53<02:10,  3.61it/s] 24%|â–ˆâ–ˆâ–       | 148/620 [00:53<02:10,  3.62it/s] 24%|â–ˆâ–ˆâ–       | 149/620 [00:53<02:09,  3.63it/s] 24%|â–ˆâ–ˆâ–       | 150/620 [00:53<02:09,  3.63it/s] 24%|â–ˆâ–ˆâ–       | 151/620 [00:54<02:08,  3.64it/s] 25%|â–ˆâ–ˆâ–       | 152/620 [00:54<02:08,  3.64it/s] 25%|â–ˆâ–ˆâ–       | 153/620 [00:54<02:08,  3.64it/s] 25%|â–ˆâ–ˆâ–       | 154/620 [00:55<02:07,  3.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 155/620 [00:55<02:07,  3.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 156/620 [00:55<02:07,  3.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 157/620 [00:55<02:07,  3.64it/s] 25%|â–ˆâ–ˆâ–Œ       | 158/620 [00:56<02:06,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 159/620 [00:56<02:06,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 160/620 [00:56<02:06,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 161/620 [00:57<02:06,  3.64it/s] 26%|â–ˆâ–ˆâ–Œ       | 162/620 [00:57<02:05,  3.64it/s] 26%|â–ˆâ–ˆâ–‹       | 163/620 [00:57<02:05,  3.64it/s] 26%|â–ˆâ–ˆâ–‹       | 164/620 [00:57<02:05,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 165/620 [00:58<02:04,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 166/620 [00:58<02:04,  3.64it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|â–ˆâ–ˆâ–‹       | 167/620 [00:58<02:04,  3.63it/s] 27%|â–ˆâ–ˆâ–‹       | 168/620 [00:58<02:04,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 169/620 [00:59<02:03,  3.64it/s] 27%|â–ˆâ–ˆâ–‹       | 170/620 [00:59<02:03,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 171/620 [00:59<02:03,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 172/620 [01:00<02:02,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 173/620 [01:00<02:02,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 174/620 [01:00<02:02,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 175/620 [01:00<02:02,  3.64it/s] 28%|â–ˆâ–ˆâ–Š       | 176/620 [01:01<02:01,  3.64it/s] 29%|â–ˆâ–ˆâ–Š       | 177/620 [01:01<02:01,  3.64it/s] 29%|â–ˆâ–ˆâ–Š       | 178/620 [01:01<02:01,  3.63it/s] 29%|â–ˆâ–ˆâ–‰       | 179/620 [01:01<02:01,  3.63it/s] 29%|â–ˆâ–ˆâ–‰       | 180/620 [01:02<02:00,  3.64it/s] 29%|â–ˆâ–ˆâ–‰       | 181/620 [01:02<02:00,  3.64it/s] 29%|â–ˆâ–ˆâ–‰       | 182/620 [01:02<02:00,  3.63it/s] 30%|â–ˆâ–ˆâ–‰       | 183/620 [01:03<02:00,  3.63it/s] 30%|â–ˆâ–ˆâ–‰       | 184/620 [01:03<01:59,  3.64it/s] 30%|â–ˆâ–ˆâ–‰       | 185/620 [01:03<01:59,  3.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 186/620 [01:03<01:59,  3.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 187/620 [01:04<01:58,  3.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 188/620 [01:04<01:58,  3.64it/s] 30%|â–ˆâ–ˆâ–ˆ       | 189/620 [01:04<01:58,  3.63it/s] 31%|â–ˆâ–ˆâ–ˆ       | 190/620 [01:04<01:58,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 191/620 [01:05<01:57,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 192/620 [01:05<01:57,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 193/620 [01:05<01:57,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 194/620 [01:06<01:57,  3.64it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 195/620 [01:06<01:56,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 196/620 [01:06<01:56,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 197/620 [01:06<01:56,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 198/620 [01:07<01:55,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 199/620 [01:07<01:55,  3.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 200/620 [01:07<01:55,  3.63it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 201/620 [01:08<01:55,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 202/620 [01:08<01:54,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 203/620 [01:08<01:54,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 204/620 [01:08<01:54,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 205/620 [01:09<01:53,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 206/620 [01:09<01:53,  3.64it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 207/620 [01:09<01:53,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 208/620 [01:09<01:53,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 209/620 [01:10<01:52,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 210/620 [01:10<01:52,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 211/620 [01:10<01:52,  3.63it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 212/620 [01:11<01:52,  3.64it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 213/620 [01:11<01:51,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 214/620 [01:11<01:51,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 215/620 [01:11<01:51,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 216/620 [01:12<01:50,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 217/620 [01:12<01:50,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 218/620 [01:12<01:50,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 219/620 [01:12<01:50,  3.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 220/620 [01:13<01:49,  3.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 221/620 [01:13<01:49,  3.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 222/620 [01:13<01:49,  3.63it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 223/620 [01:14<01:49,  3.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 224/620 [01:14<01:48,  3.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 225/620 [01:14<01:48,  3.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 226/620 [01:14<01:48,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 227/620 [01:15<01:47,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 228/620 [01:15<01:47,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 229/620 [01:15<01:47,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 230/620 [01:15<01:47,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 231/620 [01:16<01:46,  3.64it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 232/620 [01:16<01:46,  3.64it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 233/620 [01:16<01:46,  3.62it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 234/620 [01:17<01:46,  3.62it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 235/620 [01:17<01:46,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 236/620 [01:17<01:45,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 237/620 [01:17<01:45,  3.63it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 238/620 [01:18<01:45,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 239/620 [01:18<01:44,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 240/620 [01:18<01:44,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 241/620 [01:19<01:44,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 242/620 [01:19<01:43,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 243/620 [01:19<01:43,  3.64it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 244/620 [01:19<01:43,  3.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 245/620 [01:20<01:43,  3.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 246/620 [01:20<01:42,  3.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 247/620 [01:20<01:42,  3.64it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 248/620 [01:20<01:35,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 09:39:34,507 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:39:34,507 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 09:39:34,507 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.2949, 'eval_samples_per_second': 374.29, 'eval_steps_per_second': 46.8, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.41it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.72it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.86it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.10it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.72it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.47it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.23it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.94it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.82it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.82it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.84it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.84it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.83it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.85it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.85it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.84it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.79it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.73it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.71it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.74it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.77it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.76it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.78it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.80it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.82it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.81it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.72it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.72it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.75it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.72it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.77it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.79it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.78it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.80it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.81it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.73it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.73it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.69it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.70it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.74it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.76it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.77it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.79it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.79it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.77it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.78it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.69it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.71it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.74it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.71it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.72it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.75it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.72it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.75it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.77it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.53it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.62it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.68it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.67it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.72it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.73it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.72it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.75it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.72it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.71it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.74it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.71it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.75it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.76it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.71it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.74it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.77it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.71it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.75it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.76it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.74it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.75it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.72it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.72it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.74it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.74it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.73it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.76it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.74it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.75it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.76it/s][A                                                 
                                                 [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 248/620 [01:30<01:35,  3.90it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.76it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:39:43,832 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248
[INFO|configuration_utils.py:351] 2023-08-28 09:39:43,857 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:39:46,157 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:39:46,175 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:39:46,182 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 249/620 [01:33<23:45,  3.84s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 250/620 [01:33<17:05,  2.77s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 251/620 [01:33<12:26,  2.02s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 252/620 [01:33<09:11,  1.50s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 253/620 [01:34<06:55,  1.13s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 254/620 [01:34<05:19,  1.14it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 255/620 [01:34<04:13,  1.44it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/620 [01:34<03:26,  1.76it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/620 [01:35<02:54,  2.08it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/620 [01:35<02:31,  2.39it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/620 [01:35<02:15,  2.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/620 [01:36<02:04,  2.90it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/620 [01:36<01:56,  3.09it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 262/620 [01:36<01:50,  3.23it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/620 [01:36<01:46,  3.35it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 264/620 [01:37<01:43,  3.43it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 265/620 [01:37<01:41,  3.49it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 266/620 [01:37<01:40,  3.54it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 267/620 [01:38<01:38,  3.57it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 268/620 [01:38<01:38,  3.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 269/620 [01:38<01:37,  3.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 270/620 [01:38<01:36,  3.61it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 271/620 [01:39<01:36,  3.62it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 272/620 [01:39<01:35,  3.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 273/620 [01:39<01:35,  3.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 274/620 [01:39<01:35,  3.63it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 275/620 [01:40<01:34,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 276/620 [01:40<01:34,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 277/620 [01:40<01:34,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 278/620 [01:41<01:33,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 279/620 [01:41<01:33,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 280/620 [01:41<01:33,  3.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 281/620 [01:41<01:33,  3.63it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 282/620 [01:42<01:33,  3.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 283/620 [01:42<01:32,  3.63it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 284/620 [01:42<01:32,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 285/620 [01:42<01:32,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 286/620 [01:43<01:31,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 287/620 [01:43<01:31,  3.64it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 288/620 [01:43<01:31,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 289/620 [01:44<01:30,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 290/620 [01:44<01:30,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 291/620 [01:44<01:30,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 292/620 [01:44<01:30,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 293/620 [01:45<01:29,  3.64it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 294/620 [01:45<01:29,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 295/620 [01:45<01:29,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 296/620 [01:45<01:28,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 297/620 [01:46<01:28,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 298/620 [01:46<01:28,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 299/620 [01:46<01:28,  3.64it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 300/620 [01:47<01:27,  3.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 301/620 [01:47<01:27,  3.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 302/620 [01:47<01:27,  3.64it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 303/620 [01:47<01:27,  3.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 304/620 [01:48<01:27,  3.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 305/620 [01:48<01:26,  3.63it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 306/620 [01:48<01:26,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 307/620 [01:49<01:26,  3.64it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 308/620 [01:49<01:25,  3.64it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 309/620 [01:49<01:25,  3.64it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 310/620 [01:49<01:25,  3.64it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 311/620 [01:50<01:25,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 312/620 [01:50<01:24,  3.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 313/620 [01:50<01:24,  3.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 314/620 [01:50<01:24,  3.63it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 315/620 [01:51<01:24,  3.61it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 316/620 [01:51<01:25,  3.57it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 317/620 [01:51<01:24,  3.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/620 [01:52<01:23,  3.60it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/620 [01:52<01:23,  3.61it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/620 [01:52<01:22,  3.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/620 [01:52<01:22,  3.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 322/620 [01:53<01:22,  3.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 323/620 [01:53<01:22,  3.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 324/620 [01:53<01:21,  3.62it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 325/620 [01:53<01:21,  3.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 326/620 [01:54<01:20,  3.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 327/620 [01:54<01:20,  3.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 328/620 [01:54<01:20,  3.63it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 329/620 [01:55<01:20,  3.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 330/620 [01:55<01:19,  3.64it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 331/620 [01:55<01:19,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 332/620 [01:55<01:19,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 333/620 [01:56<01:18,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 334/620 [01:56<01:18,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 335/620 [01:56<01:18,  3.64it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 336/620 [01:57<01:18,  3.63it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 337/620 [01:57<01:18,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 338/620 [01:57<01:17,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 339/620 [01:57<01:17,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 340/620 [01:58<01:17,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 341/620 [01:58<01:16,  3.63it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 342/620 [01:58<01:16,  3.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 343/620 [01:58<01:16,  3.64it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 344/620 [01:59<01:15,  3.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 345/620 [01:59<01:15,  3.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 346/620 [01:59<01:15,  3.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 347/620 [02:00<01:15,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 348/620 [02:00<01:14,  3.63it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 349/620 [02:00<01:14,  3.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 350/620 [02:00<01:14,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 351/620 [02:01<01:13,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 352/620 [02:01<01:13,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 353/620 [02:01<01:13,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 354/620 [02:01<01:13,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 355/620 [02:02<01:12,  3.64it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 356/620 [02:02<01:12,  3.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 357/620 [02:02<01:12,  3.64it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 358/620 [02:03<01:12,  3.62it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 359/620 [02:03<01:11,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 360/620 [02:03<01:11,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 361/620 [02:03<01:11,  3.63it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 362/620 [02:04<01:10,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 363/620 [02:04<01:10,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 364/620 [02:04<01:10,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 365/620 [02:04<01:10,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 366/620 [02:05<01:09,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 367/620 [02:05<01:09,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 368/620 [02:05<01:09,  3.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 369/620 [02:06<01:09,  3.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 370/620 [02:06<01:08,  3.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 371/620 [02:06<01:08,  3.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 372/620 [02:06<01:03,  3.90it/s][INFO|trainer.py:2140] 2023-08-28 09:40:20,492 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:40:20,492 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 09:40:20,492 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3052, 'eval_samples_per_second': 373.875, 'eval_steps_per_second': 46.748, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.39it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.75it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.91it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.15it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.77it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.45it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.20it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.86it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.75it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.74it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.80it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.80it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.82it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.84it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.83it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.81it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.77it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.71it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.69it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.72it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.75it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.75it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.78it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.78it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.83it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.80it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.72it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.72it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.69it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.44it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.59it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.66it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.71it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.77it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.78it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.77it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.78it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.72it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.68it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.72it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.72it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.75it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.77it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.75it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.77it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.77it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.67it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.69it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.72it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.70it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.72it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.74it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.72it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.73it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.75it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.69it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.72it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.71it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.70it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.72it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.68it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.69it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.72it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.69it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.72it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.74it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.70it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.73it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.75it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.70it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.73it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.73it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.67it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.70it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.69it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.67it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.70it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.67it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.66it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.70it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.65it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.69it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.72it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.66it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.69it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.72it/s][A                                                 
                                                 [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 372/620 [02:16<01:03,  3.90it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.72it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:40:29,821 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372
[INFO|configuration_utils.py:351] 2023-08-28 09:40:29,843 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:40:32,275 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:40:32,292 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:40:32,301 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373/620 [02:19<16:00,  3.89s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 374/620 [02:19<11:29,  2.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 375/620 [02:19<08:20,  2.04s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 376/620 [02:20<06:09,  1.51s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 377/620 [02:20<04:37,  1.14s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 378/620 [02:20<03:33,  1.13it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 379/620 [02:20<02:48,  1.43it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/620 [02:21<02:17,  1.75it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 381/620 [02:21<01:55,  2.07it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 382/620 [02:21<01:40,  2.38it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 383/620 [02:21<01:29,  2.65it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/620 [02:22<01:21,  2.89it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 385/620 [02:22<01:16,  3.08it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 386/620 [02:22<01:12,  3.23it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 387/620 [02:23<01:09,  3.34it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 388/620 [02:23<01:07,  3.43it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 389/620 [02:23<01:06,  3.49it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 390/620 [02:23<01:05,  3.53it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 391/620 [02:24<01:04,  3.57it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 392/620 [02:24<01:03,  3.59it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 393/620 [02:24<01:02,  3.61it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 394/620 [02:24<01:02,  3.62it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 395/620 [02:25<01:02,  3.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 396/620 [02:25<01:01,  3.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 397/620 [02:25<01:01,  3.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 398/620 [02:26<01:01,  3.64it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 399/620 [02:26<01:00,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 400/620 [02:26<01:00,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 401/620 [02:26<01:00,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 402/620 [02:27<00:59,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 403/620 [02:27<00:59,  3.64it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 404/620 [02:27<00:59,  3.62it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 405/620 [02:28<00:59,  3.63it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 406/620 [02:28<00:58,  3.63it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 407/620 [02:28<00:58,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 408/620 [02:28<00:58,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 409/620 [02:29<00:57,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 410/620 [02:29<00:57,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 411/620 [02:29<00:57,  3.64it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 412/620 [02:29<00:57,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 413/620 [02:30<00:56,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 414/620 [02:30<00:56,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 415/620 [02:30<00:56,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 416/620 [02:31<00:56,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 417/620 [02:31<00:55,  3.64it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 418/620 [02:31<00:55,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 419/620 [02:31<00:55,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 420/620 [02:32<00:54,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 421/620 [02:32<00:54,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 422/620 [02:32<00:54,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 423/620 [02:32<00:54,  3.64it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 424/620 [02:33<00:53,  3.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 425/620 [02:33<00:53,  3.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 426/620 [02:33<00:53,  3.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 427/620 [02:34<00:53,  3.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 428/620 [02:34<00:52,  3.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 429/620 [02:34<00:52,  3.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 430/620 [02:34<00:52,  3.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 431/620 [02:35<00:51,  3.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 432/620 [02:35<00:51,  3.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 433/620 [02:35<00:51,  3.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 434/620 [02:35<00:51,  3.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 435/620 [02:36<00:50,  3.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 436/620 [02:36<00:50,  3.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 437/620 [02:36<00:50,  3.62it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 438/620 [02:37<00:50,  3.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 439/620 [02:37<00:49,  3.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 440/620 [02:37<00:49,  3.63it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 441/620 [02:37<00:49,  3.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/620 [02:38<00:48,  3.64it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 443/620 [02:38<00:48,  3.64it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/620 [02:38<00:48,  3.64it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 445/620 [02:38<00:48,  3.64it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 446/620 [02:39<00:47,  3.64it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 447/620 [02:39<00:47,  3.64it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 448/620 [02:39<00:47,  3.61it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 449/620 [02:40<00:47,  3.62it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 450/620 [02:40<00:46,  3.62it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 451/620 [02:40<00:46,  3.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 452/620 [02:40<00:46,  3.63it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 453/620 [02:41<00:45,  3.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 454/620 [02:41<00:45,  3.64it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 455/620 [02:41<00:45,  3.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 456/620 [02:42<00:45,  3.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 457/620 [02:42<00:44,  3.64it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 458/620 [02:42<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 459/620 [02:42<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 460/620 [02:43<00:44,  3.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 461/620 [02:43<00:43,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 462/620 [02:43<00:43,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 463/620 [02:43<00:43,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 464/620 [02:44<00:42,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 465/620 [02:44<00:42,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 466/620 [02:44<00:42,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 467/620 [02:45<00:42,  3.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 468/620 [02:45<00:41,  3.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 469/620 [02:45<00:41,  3.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 470/620 [02:45<00:41,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 471/620 [02:46<00:40,  3.63it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 472/620 [02:46<00:40,  3.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 473/620 [02:46<00:40,  3.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 474/620 [02:46<00:40,  3.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 475/620 [02:47<00:39,  3.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 476/620 [02:47<00:39,  3.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 477/620 [02:47<00:39,  3.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 478/620 [02:48<00:39,  3.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 479/620 [02:48<00:38,  3.64it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 480/620 [02:48<00:38,  3.64it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 481/620 [02:48<00:38,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 482/620 [02:49<00:37,  3.63it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 483/620 [02:49<00:37,  3.64it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 484/620 [02:49<00:37,  3.64it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 485/620 [02:50<00:37,  3.64it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 486/620 [02:50<00:37,  3.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 487/620 [02:50<00:36,  3.62it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 488/620 [02:50<00:36,  3.63it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 489/620 [02:51<00:36,  3.63it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 490/620 [02:51<00:36,  3.61it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 491/620 [02:51<00:36,  3.53it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 492/620 [02:51<00:36,  3.53it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 493/620 [02:52<00:35,  3.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 494/620 [02:52<00:35,  3.58it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 495/620 [02:52<00:34,  3.60it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 496/620 [02:53<00:32,  3.87it/s][INFO|trainer.py:2140] 2023-08-28 09:41:06,644 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:41:06,644 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 09:41:06,644 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3104, 'eval_samples_per_second': 373.667, 'eval_steps_per_second': 46.722, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.13it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.60it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.79it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.98it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.66it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.36it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.22it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.89it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.76it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.79it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.78it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.78it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.81it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.80it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.83it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.84it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.76it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.69it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.68it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.65it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.72it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.75it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.74it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.79it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.79it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.74it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.75it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.71it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.48it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.58it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.65it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.66it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.70it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.73it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.75it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.76it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.70it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.70it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.73it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.67it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.69it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.72it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.71it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.75it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.76it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.67it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.70it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.72it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.65it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.63it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.63it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.67it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.36it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.48it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.57it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.63it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.66it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.69it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.64it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.64it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.68it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.66it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.68it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.71it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.71it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.73it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.75it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.66it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.64it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.63it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.61it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.66it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.69it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.70it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.72it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.72it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.73it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.69it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.62it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.66it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.65it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.64it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.69it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.72it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.72it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.73it/s][A                                                 
                                                 [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 496/620 [03:02<00:32,  3.87it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:41:15,975 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496
[INFO|configuration_utils.py:351] 2023-08-28 09:41:15,995 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:41:18,288 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:41:18,311 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:41:18,329 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 497/620 [03:05<07:54,  3.86s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 498/620 [03:05<05:39,  2.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 499/620 [03:05<04:05,  2.03s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 500/620 [03:06<03:00,  1.50s/it]                                                  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 500/620 [03:06<03:00,  1.50s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 501/620 [03:06<02:15,  1.14s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 502/620 [03:06<01:43,  1.14it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 503/620 [03:06<01:21,  1.44it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 504/620 [03:07<01:06,  1.75it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/620 [03:07<00:55,  2.08it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 506/620 [03:07<00:47,  2.39it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 507/620 [03:08<00:42,  2.65it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 508/620 [03:08<00:38,  2.89it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 509/620 [03:08<00:36,  3.08it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 510/620 [03:08<00:34,  3.23it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 511/620 [03:09<00:32,  3.34it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 512/620 [03:09<00:31,  3.43it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 513/620 [03:09<00:30,  3.48it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 514/620 [03:09<00:30,  3.53it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 515/620 [03:10<00:29,  3.56it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 516/620 [03:10<00:28,  3.59it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 517/620 [03:10<00:28,  3.60it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 518/620 [03:11<00:28,  3.58it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 519/620 [03:11<00:28,  3.60it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 520/620 [03:11<00:27,  3.61it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 521/620 [03:11<00:27,  3.62it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 522/620 [03:12<00:27,  3.63it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 523/620 [03:12<00:26,  3.63it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 524/620 [03:12<00:26,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 525/620 [03:12<00:26,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 526/620 [03:13<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 527/620 [03:13<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 528/620 [03:13<00:25,  3.64it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 529/620 [03:14<00:25,  3.63it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 530/620 [03:14<00:24,  3.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 531/620 [03:14<00:24,  3.63it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 532/620 [03:14<00:24,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 533/620 [03:15<00:23,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 534/620 [03:15<00:23,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 535/620 [03:15<00:23,  3.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 536/620 [03:15<00:23,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 537/620 [03:16<00:22,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 538/620 [03:16<00:22,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 539/620 [03:16<00:22,  3.64it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 540/620 [03:17<00:22,  3.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 541/620 [03:17<00:21,  3.63it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 542/620 [03:17<00:21,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 543/620 [03:17<00:21,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 544/620 [03:18<00:20,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 545/620 [03:18<00:20,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 546/620 [03:18<00:20,  3.64it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 547/620 [03:19<00:20,  3.63it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 548/620 [03:19<00:19,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 549/620 [03:19<00:19,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 550/620 [03:19<00:19,  3.64it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 551/620 [03:20<00:19,  3.62it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 552/620 [03:20<00:18,  3.63it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 553/620 [03:20<00:18,  3.63it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 554/620 [03:20<00:18,  3.63it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 555/620 [03:21<00:17,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 556/620 [03:21<00:17,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 557/620 [03:21<00:17,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 558/620 [03:22<00:17,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 559/620 [03:22<00:16,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 560/620 [03:22<00:16,  3.64it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 561/620 [03:22<00:16,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 562/620 [03:23<00:15,  3.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 563/620 [03:23<00:15,  3.63it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 564/620 [03:23<00:15,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 565/620 [03:23<00:15,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 566/620 [03:24<00:14,  3.64it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 567/620 [03:24<00:14,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 568/620 [03:24<00:14,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 569/620 [03:25<00:14,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 570/620 [03:25<00:13,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 571/620 [03:25<00:13,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 572/620 [03:25<00:13,  3.64it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 573/620 [03:26<00:12,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 574/620 [03:26<00:12,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 575/620 [03:26<00:12,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 576/620 [03:26<00:12,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 577/620 [03:27<00:11,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 578/620 [03:27<00:11,  3.64it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 579/620 [03:27<00:11,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 580/620 [03:28<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 581/620 [03:28<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 582/620 [03:28<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 583/620 [03:28<00:10,  3.64it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 584/620 [03:29<00:09,  3.63it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 585/620 [03:29<00:09,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 586/620 [03:29<00:09,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 587/620 [03:30<00:09,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 588/620 [03:30<00:08,  3.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 589/620 [03:30<00:08,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 590/620 [03:30<00:08,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 591/620 [03:31<00:07,  3.64it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 592/620 [03:31<00:07,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 593/620 [03:31<00:07,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 594/620 [03:31<00:07,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 595/620 [03:32<00:06,  3.63it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 596/620 [03:32<00:06,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 597/620 [03:32<00:06,  3.64it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 598/620 [03:33<00:06,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 599/620 [03:33<00:05,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 600/620 [03:33<00:05,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 601/620 [03:33<00:05,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 602/620 [03:34<00:04,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 603/620 [03:34<00:04,  3.64it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 604/620 [03:34<00:04,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 605/620 [03:34<00:04,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 606/620 [03:35<00:03,  3.63it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 607/620 [03:35<00:03,  3.63it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 608/620 [03:35<00:03,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 609/620 [03:36<00:03,  3.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 610/620 [03:36<00:02,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 611/620 [03:36<00:02,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 612/620 [03:36<00:02,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 613/620 [03:37<00:01,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 614/620 [03:37<00:01,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 615/620 [03:37<00:01,  3.64it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 616/620 [03:37<00:01,  3.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 617/620 [03:38<00:00,  3.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 618/620 [03:38<00:00,  3.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 619/620 [03:38<00:00,  3.63it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 620/620 [03:39<00:00,  3.89it/s][INFO|trainer.py:2140] 2023-08-28 09:41:52,671 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:41:52,671 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 09:41:52,671 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3163, 'eval_samples_per_second': 373.43, 'eval_steps_per_second': 46.692, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7298387096774197e-05, 'epoch': 4.03}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.54it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.71it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.82it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.12it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.66it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.38it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.22it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.86it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.77it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.78it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.78it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.82it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.81it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.80it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.80it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.80it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.70it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.65it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.61it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.63it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.68it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.71it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.71it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.75it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.76it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.74it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.49it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.77it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.70it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.72it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.71it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.69it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.71it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.70it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.72it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.73it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.69it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.72it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.74it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.70it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.72it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.66it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.66it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.70it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.71it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.69it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.71it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.71it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.72it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.73it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.65it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.67it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.66it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.64it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.68it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.71it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.69it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.72it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.72it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.70it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.71it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.65it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.67it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.63it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.62it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.67it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.69it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.68it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.71it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.73it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.69it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.68it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.62it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.63it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.67it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.65it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.70it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.73it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.70it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.72it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.73it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.63it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.62it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.62it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.65it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.69it/s][A                                                 
                                                 [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 620/620 [03:48<00:00,  3.89it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.69it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 09:42:02,001 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620
[INFO|configuration_utils.py:351] 2023-08-28 09:42:02,025 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:42:04,301 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:42:04,316 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:42:04,327 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 09:42:04,600 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 09:42:04,601 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124 (score: 1.0038264989852905).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 620/620 [03:52<00:00,  3.89it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 620/620 [03:52<00:00,  2.66it/s]
[INFO|trainer.py:1894] 2023-08-28 09:42:06,365 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 09:42:06,381 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 09:42:08,606 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 09:42:08,623 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 09:42:08,634 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:42:08,813 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:08,814 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:08,814 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:08,814 >>   train_runtime            = 0:03:52.69
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:08,814 >>   train_samples            =       7920
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:08,814 >>   train_samples_per_second =    170.183
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:08,814 >>   train_steps_per_second   =      2.664
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3152, 'eval_samples_per_second': 373.475, 'eval_steps_per_second': 46.698, 'epoch': 5.0}
{'train_runtime': 232.6912, 'train_samples_per_second': 170.183, 'train_steps_per_second': 2.664, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 09:42:08 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 09:42:08,850 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 09:42:08,850 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 09:42:08,850 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 57.73it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 50.86it/s]  4%|â–         | 18/435 [00:00<00:08, 49.05it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.32it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.86it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.60it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.42it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.26it/s] 11%|â–ˆ         | 48/435 [00:00<00:08, 47.05it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.97it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.96it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.99it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.96it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.92it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.96it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.99it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.96it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.62it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.67it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.72it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.81it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.82it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.83it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.88it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.88it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.89it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.85it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.80it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.82it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.82it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.80it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.85it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.90it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.88it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.87it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.86it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.83it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.85it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.84it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.80it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.87it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.84it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.86it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.85it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.82it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.84it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.83it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 46.81it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.83it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.83it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.81it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.84it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.83it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.81it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.82it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.82it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.83it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.82it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.83it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.82it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.82it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.82it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.83it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.83it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.40it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.62it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.69it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.73it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.73it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.77it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.78it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.79it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.81it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.81it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.79it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.81it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.81it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.83it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.82it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.82it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.82it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.82it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.81it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.91it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 09:42:18,148 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:18,148 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:18,148 >>   eval_loss               =     1.0038
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:18,148 >>   eval_runtime            = 0:00:09.29
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:18,148 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:18,148 >>   eval_samples_per_second =     374.17
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:18,148 >>   eval_steps_per_second   =     46.785
[INFO|trainer_pt_utils.py:913] 2023-08-28 09:42:18,148 >>   perplexity              =     2.7287
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:22,903 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:22,909 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:22,909 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:22,909 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:22,909 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:42:23,239 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:42:23,240 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:42:23,509 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:42:24,536 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:42:24,537 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:27,378 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:27,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:27,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:27,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:42:27,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:42:28,035 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:42:28,037 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:42:28,612 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:42:28,777 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:42:28,777 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-124
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-620
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-248
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-372
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/checkpoint-496
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.16it/s]Extractor Predicting: 2it [00:01,  1.18it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.26it/s]Extractor Predicting: 5it [00:04,  1.26it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.27it/s]Extractor Predicting: 8it [00:06,  1.28it/s]Extractor Predicting: 9it [00:07,  1.28it/s]Extractor Predicting: 10it [00:07,  1.32it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:10,  1.27it/s]Extractor Predicting: 14it [00:11,  1.26it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:13,  1.31it/s]Extractor Predicting: 18it [00:14,  1.35it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.32it/s]Extractor Predicting: 21it [00:16,  1.32it/s]Extractor Predicting: 22it [00:17,  1.34it/s]Extractor Predicting: 23it [00:17,  1.33it/s]Extractor Predicting: 24it [00:18,  1.31it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.27it/s]Extractor Predicting: 27it [00:20,  1.27it/s]Extractor Predicting: 28it [00:21,  1.31it/s]Extractor Predicting: 29it [00:22,  1.29it/s]Extractor Predicting: 30it [00:23,  1.27it/s]Extractor Predicting: 31it [00:24,  1.27it/s]Extractor Predicting: 32it [00:24,  1.28it/s]Extractor Predicting: 33it [00:25,  1.28it/s]Extractor Predicting: 34it [00:26,  1.31it/s]Extractor Predicting: 35it [00:27,  1.32it/s]Extractor Predicting: 36it [00:27,  1.33it/s]Extractor Predicting: 37it [00:28,  1.33it/s]Extractor Predicting: 38it [00:29,  1.33it/s]Extractor Predicting: 39it [00:30,  1.35it/s]Extractor Predicting: 40it [00:30,  1.33it/s]Extractor Predicting: 41it [00:31,  1.33it/s]Extractor Predicting: 42it [00:32,  1.32it/s]Extractor Predicting: 43it [00:33,  1.35it/s]Extractor Predicting: 44it [00:33,  1.36it/s]Extractor Predicting: 45it [00:34,  1.34it/s]Extractor Predicting: 46it [00:35,  1.33it/s]Extractor Predicting: 47it [00:36,  1.32it/s]Extractor Predicting: 48it [00:36,  1.33it/s]Extractor Predicting: 49it [00:37,  1.33it/s]Extractor Predicting: 50it [00:38,  1.34it/s]Extractor Predicting: 51it [00:39,  1.35it/s]Extractor Predicting: 52it [00:39,  1.35it/s]Extractor Predicting: 53it [00:40,  1.36it/s]Extractor Predicting: 54it [00:41,  1.34it/s]Extractor Predicting: 55it [00:42,  1.31it/s]Extractor Predicting: 56it [00:42,  1.34it/s]Extractor Predicting: 57it [00:43,  1.32it/s]Extractor Predicting: 58it [00:44,  1.32it/s]Extractor Predicting: 59it [00:45,  1.35it/s]Extractor Predicting: 60it [00:45,  1.38it/s]Extractor Predicting: 61it [00:46,  1.40it/s]Extractor Predicting: 62it [00:47,  1.37it/s]Extractor Predicting: 63it [00:47,  1.35it/s]Extractor Predicting: 64it [00:48,  1.36it/s]Extractor Predicting: 65it [00:49,  1.37it/s]Extractor Predicting: 66it [00:50,  1.36it/s]Extractor Predicting: 67it [00:50,  1.37it/s]Extractor Predicting: 68it [00:51,  1.37it/s]Extractor Predicting: 69it [00:52,  1.41it/s]Extractor Predicting: 70it [00:52,  1.40it/s]Extractor Predicting: 71it [00:53,  1.42it/s]Extractor Predicting: 72it [00:54,  1.38it/s]Extractor Predicting: 73it [00:55,  1.38it/s]Extractor Predicting: 74it [00:55,  1.37it/s]Extractor Predicting: 75it [00:56,  1.34it/s]Extractor Predicting: 76it [00:57,  1.33it/s]Extractor Predicting: 77it [00:58,  1.36it/s]Extractor Predicting: 78it [00:59,  1.28it/s]Extractor Predicting: 79it [00:59,  1.33it/s]Extractor Predicting: 80it [01:00,  1.33it/s]Extractor Predicting: 81it [01:01,  1.33it/s]Extractor Predicting: 82it [01:01,  1.34it/s]Extractor Predicting: 83it [01:02,  1.36it/s]Extractor Predicting: 84it [01:03,  1.36it/s]Extractor Predicting: 85it [01:04,  1.38it/s]Extractor Predicting: 86it [01:04,  1.39it/s]Extractor Predicting: 87it [01:05,  1.42it/s]Extractor Predicting: 88it [01:06,  1.40it/s]Extractor Predicting: 89it [01:06,  1.40it/s]Extractor Predicting: 90it [01:07,  1.41it/s]Extractor Predicting: 91it [01:08,  1.43it/s]Extractor Predicting: 92it [01:08,  1.46it/s]Extractor Predicting: 93it [01:09,  1.43it/s]Extractor Predicting: 94it [01:10,  1.42it/s]Extractor Predicting: 95it [01:11,  1.42it/s]Extractor Predicting: 96it [01:11,  1.42it/s]Extractor Predicting: 97it [01:12,  1.41it/s]Extractor Predicting: 98it [01:13,  1.41it/s]Extractor Predicting: 99it [01:14,  1.38it/s]Extractor Predicting: 100it [01:14,  1.35it/s]Extractor Predicting: 101it [01:15,  1.35it/s]Extractor Predicting: 102it [01:16,  1.42it/s]Extractor Predicting: 103it [01:16,  1.43it/s]Extractor Predicting: 104it [01:17,  1.42it/s]Extractor Predicting: 105it [01:18,  1.42it/s]Extractor Predicting: 106it [01:18,  1.41it/s]Extractor Predicting: 107it [01:19,  1.40it/s]Extractor Predicting: 108it [01:20,  1.40it/s]Extractor Predicting: 109it [01:21,  1.41it/s]Extractor Predicting: 110it [01:21,  1.41it/s]Extractor Predicting: 111it [01:22,  1.41it/s]Extractor Predicting: 112it [01:23,  1.43it/s]Extractor Predicting: 113it [01:23,  1.46it/s]Extractor Predicting: 114it [01:24,  1.46it/s]Extractor Predicting: 115it [01:25,  1.47it/s]Extractor Predicting: 116it [01:25,  1.43it/s]Extractor Predicting: 117it [01:26,  1.39it/s]Extractor Predicting: 118it [01:27,  1.40it/s]Extractor Predicting: 119it [01:28,  1.37it/s]Extractor Predicting: 120it [01:28,  1.39it/s]Extractor Predicting: 121it [01:29,  1.37it/s]Extractor Predicting: 122it [01:30,  1.35it/s]Extractor Predicting: 123it [01:31,  1.36it/s]Extractor Predicting: 124it [01:31,  1.37it/s]Extractor Predicting: 125it [01:32,  1.39it/s]Extractor Predicting: 126it [01:33,  1.36it/s]Extractor Predicting: 127it [01:33,  1.39it/s]Extractor Predicting: 128it [01:34,  1.39it/s]Extractor Predicting: 129it [01:35,  1.37it/s]Extractor Predicting: 130it [01:36,  1.35it/s]Extractor Predicting: 131it [01:36,  1.38it/s]Extractor Predicting: 132it [01:37,  1.36it/s]Extractor Predicting: 133it [01:38,  1.34it/s]Extractor Predicting: 134it [01:39,  1.33it/s]Extractor Predicting: 135it [01:39,  1.35it/s]Extractor Predicting: 136it [01:40,  1.33it/s]Extractor Predicting: 137it [01:41,  1.34it/s]Extractor Predicting: 138it [01:42,  1.32it/s]Extractor Predicting: 139it [01:42,  1.33it/s]Extractor Predicting: 140it [01:43,  1.31it/s]Extractor Predicting: 141it [01:44,  1.34it/s]Extractor Predicting: 142it [01:45,  1.32it/s]Extractor Predicting: 143it [01:45,  1.34it/s]Extractor Predicting: 144it [01:46,  1.67it/s]Extractor Predicting: 144it [01:46,  1.36it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:21,663 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:21,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:21,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:21,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:21,668 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:44:21,983 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:44:21,985 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:44:22,244 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:44:23,253 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:44:23,253 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:24,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:24,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:24,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:24,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:44:24,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:44:25,010 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:44:25,014 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:44:25,281 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:44:25,439 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:44:25,439 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.28it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:02,  1.36it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:05,  1.44it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.43it/s]Extractor Predicting: 10it [00:07,  1.45it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.43it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:09,  1.42it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:11,  1.42it/s]Extractor Predicting: 17it [00:12,  1.41it/s]Extractor Predicting: 18it [00:12,  1.43it/s]Extractor Predicting: 19it [00:13,  1.46it/s]Extractor Predicting: 20it [00:14,  1.42it/s]Extractor Predicting: 21it [00:14,  1.44it/s]Extractor Predicting: 22it [00:15,  1.45it/s]Extractor Predicting: 23it [00:16,  1.44it/s]Extractor Predicting: 24it [00:16,  1.43it/s]Extractor Predicting: 25it [00:17,  1.42it/s]Extractor Predicting: 26it [00:18,  1.43it/s]Extractor Predicting: 27it [00:19,  1.40it/s]Extractor Predicting: 28it [00:19,  1.41it/s]Extractor Predicting: 29it [00:20,  1.42it/s]Extractor Predicting: 30it [00:21,  1.44it/s]Extractor Predicting: 31it [00:21,  1.41it/s]Extractor Predicting: 32it [00:22,  1.45it/s]Extractor Predicting: 33it [00:23,  1.43it/s]Extractor Predicting: 34it [00:23,  1.41it/s]Extractor Predicting: 35it [00:24,  1.41it/s]Extractor Predicting: 36it [00:25,  1.43it/s]Extractor Predicting: 37it [00:26,  1.45it/s]Extractor Predicting: 38it [00:26,  1.46it/s]Extractor Predicting: 39it [00:27,  1.45it/s]Extractor Predicting: 40it [00:28,  1.42it/s]Extractor Predicting: 41it [00:28,  1.42it/s]Extractor Predicting: 42it [00:29,  1.46it/s]Extractor Predicting: 43it [00:30,  1.42it/s]Extractor Predicting: 44it [00:30,  1.41it/s]Extractor Predicting: 45it [00:31,  1.40it/s]Extractor Predicting: 46it [00:32,  1.38it/s]Extractor Predicting: 47it [00:33,  1.42it/s]Extractor Predicting: 48it [00:33,  1.40it/s]Extractor Predicting: 49it [00:34,  1.43it/s]Extractor Predicting: 50it [00:35,  1.41it/s]Extractor Predicting: 51it [00:35,  1.40it/s]Extractor Predicting: 52it [00:36,  1.39it/s]Extractor Predicting: 53it [00:37,  1.40it/s]Extractor Predicting: 54it [00:38,  1.39it/s]Extractor Predicting: 55it [00:38,  1.42it/s]Extractor Predicting: 56it [00:39,  1.41it/s]Extractor Predicting: 57it [00:40,  1.39it/s]Extractor Predicting: 58it [00:40,  1.38it/s]Extractor Predicting: 59it [00:41,  1.38it/s]Extractor Predicting: 60it [00:42,  1.37it/s]Extractor Predicting: 61it [00:43,  1.36it/s]Extractor Predicting: 62it [00:43,  1.39it/s]Extractor Predicting: 63it [00:44,  1.40it/s]Extractor Predicting: 64it [00:45,  1.43it/s]Extractor Predicting: 65it [00:45,  1.40it/s]Extractor Predicting: 66it [00:46,  1.37it/s]Extractor Predicting: 67it [00:47,  1.37it/s]Extractor Predicting: 68it [00:48,  1.37it/s]Extractor Predicting: 69it [00:48,  1.35it/s]Extractor Predicting: 70it [00:49,  1.36it/s]Extractor Predicting: 71it [00:50,  1.36it/s]Extractor Predicting: 72it [00:51,  1.35it/s]Extractor Predicting: 73it [00:51,  1.37it/s]Extractor Predicting: 74it [00:52,  1.37it/s]Extractor Predicting: 75it [00:53,  1.39it/s]Extractor Predicting: 76it [00:54,  1.36it/s]Extractor Predicting: 77it [00:54,  1.41it/s]Extractor Predicting: 78it [00:55,  1.40it/s]Extractor Predicting: 79it [00:56,  1.43it/s]Extractor Predicting: 80it [00:56,  1.45it/s]Extractor Predicting: 81it [00:57,  1.45it/s]Extractor Predicting: 82it [00:58,  1.44it/s]Extractor Predicting: 83it [00:58,  1.41it/s]Extractor Predicting: 84it [00:59,  1.39it/s]Extractor Predicting: 85it [01:00,  1.36it/s]Extractor Predicting: 86it [01:01,  1.32it/s]Extractor Predicting: 87it [01:01,  1.34it/s]Extractor Predicting: 88it [01:02,  1.34it/s]Extractor Predicting: 89it [01:03,  1.32it/s]Extractor Predicting: 90it [01:04,  1.32it/s]Extractor Predicting: 91it [01:05,  1.31it/s]Extractor Predicting: 92it [01:05,  1.34it/s]Extractor Predicting: 93it [01:06,  1.35it/s]Extractor Predicting: 94it [01:07,  1.25it/s]Extractor Predicting: 95it [01:08,  1.28it/s]Extractor Predicting: 96it [01:08,  1.27it/s]Extractor Predicting: 97it [01:09,  1.28it/s]Extractor Predicting: 98it [01:10,  1.28it/s]Extractor Predicting: 99it [01:11,  1.31it/s]Extractor Predicting: 100it [01:11,  1.33it/s]Extractor Predicting: 101it [01:12,  1.37it/s]Extractor Predicting: 102it [01:13,  1.33it/s]Extractor Predicting: 103it [01:14,  1.32it/s]Extractor Predicting: 104it [01:14,  1.35it/s]Extractor Predicting: 105it [01:15,  1.34it/s]Extractor Predicting: 106it [01:16,  1.34it/s]Extractor Predicting: 107it [01:17,  1.36it/s]Extractor Predicting: 108it [01:17,  1.34it/s]Extractor Predicting: 109it [01:18,  1.33it/s]Extractor Predicting: 110it [01:19,  1.33it/s]Extractor Predicting: 111it [01:20,  1.35it/s]Extractor Predicting: 112it [01:20,  1.33it/s]Extractor Predicting: 113it [01:21,  1.35it/s]Extractor Predicting: 114it [01:22,  1.35it/s]Extractor Predicting: 115it [01:23,  1.31it/s]Extractor Predicting: 116it [01:23,  1.34it/s]Extractor Predicting: 117it [01:24,  1.33it/s]Extractor Predicting: 118it [01:25,  1.33it/s]Extractor Predicting: 119it [01:26,  1.33it/s]Extractor Predicting: 120it [01:26,  1.36it/s]Extractor Predicting: 121it [01:27,  1.37it/s]Extractor Predicting: 122it [01:28,  1.38it/s]Extractor Predicting: 123it [01:28,  1.37it/s]Extractor Predicting: 124it [01:29,  1.38it/s]Extractor Predicting: 125it [01:30,  1.41it/s]Extractor Predicting: 126it [01:31,  1.40it/s]Extractor Predicting: 127it [01:31,  1.39it/s]Extractor Predicting: 128it [01:32,  1.42it/s]Extractor Predicting: 129it [01:33,  1.43it/s]Extractor Predicting: 130it [01:33,  1.40it/s]Extractor Predicting: 131it [01:34,  1.44it/s]Extractor Predicting: 132it [01:35,  1.44it/s]Extractor Predicting: 133it [01:35,  1.46it/s]Extractor Predicting: 134it [01:36,  1.41it/s]Extractor Predicting: 135it [01:37,  1.44it/s]Extractor Predicting: 136it [01:38,  1.45it/s]Extractor Predicting: 137it [01:38,  1.46it/s]Extractor Predicting: 138it [01:39,  1.44it/s]Extractor Predicting: 139it [01:40,  1.43it/s]Extractor Predicting: 140it [01:40,  1.44it/s]Extractor Predicting: 141it [01:41,  1.41it/s]Extractor Predicting: 142it [01:42,  1.40it/s]Extractor Predicting: 143it [01:42,  1.42it/s]Extractor Predicting: 144it [01:43,  1.40it/s]Extractor Predicting: 145it [01:44,  1.45it/s]Extractor Predicting: 146it [01:45,  1.47it/s]Extractor Predicting: 147it [01:45,  1.47it/s]Extractor Predicting: 148it [01:46,  1.50it/s]Extractor Predicting: 149it [01:47,  1.49it/s]Extractor Predicting: 150it [01:47,  1.49it/s]Extractor Predicting: 151it [01:48,  1.50it/s]Extractor Predicting: 152it [01:48,  1.52it/s]Extractor Predicting: 153it [01:49,  1.50it/s]Extractor Predicting: 154it [01:50,  1.49it/s]Extractor Predicting: 155it [01:50,  1.54it/s]Extractor Predicting: 156it [01:51,  1.52it/s]Extractor Predicting: 157it [01:52,  1.60it/s]Extractor Predicting: 158it [01:52,  1.61it/s]Extractor Predicting: 159it [01:53,  1.56it/s]Extractor Predicting: 160it [01:54,  1.51it/s]Extractor Predicting: 161it [01:54,  1.50it/s]Extractor Predicting: 162it [01:55,  1.52it/s]Extractor Predicting: 163it [01:56,  1.53it/s]Extractor Predicting: 164it [01:56,  1.53it/s]Extractor Predicting: 165it [01:57,  1.54it/s]Extractor Predicting: 166it [01:58,  1.51it/s]Extractor Predicting: 167it [01:58,  1.54it/s]Extractor Predicting: 168it [01:59,  1.51it/s]Extractor Predicting: 169it [02:00,  1.57it/s]Extractor Predicting: 170it [02:00,  1.55it/s]Extractor Predicting: 171it [02:01,  1.55it/s]Extractor Predicting: 172it [02:02,  1.49it/s]Extractor Predicting: 173it [02:02,  1.46it/s]Extractor Predicting: 174it [02:03,  1.43it/s]Extractor Predicting: 175it [02:04,  1.37it/s]Extractor Predicting: 176it [02:05,  1.38it/s]Extractor Predicting: 177it [02:05,  1.38it/s]Extractor Predicting: 178it [02:06,  1.36it/s]Extractor Predicting: 179it [02:07,  1.36it/s]Extractor Predicting: 180it [02:07,  1.37it/s]Extractor Predicting: 181it [02:08,  1.37it/s]Extractor Predicting: 182it [02:09,  1.37it/s]Extractor Predicting: 183it [02:10,  1.37it/s]Extractor Predicting: 184it [02:10,  1.35it/s]Extractor Predicting: 185it [02:11,  1.34it/s]Extractor Predicting: 186it [02:12,  1.34it/s]Extractor Predicting: 187it [02:13,  1.34it/s]Extractor Predicting: 188it [02:13,  1.34it/s]Extractor Predicting: 189it [02:14,  1.34it/s]Extractor Predicting: 190it [02:15,  1.36it/s]Extractor Predicting: 191it [02:16,  1.33it/s]Extractor Predicting: 192it [02:16,  1.31it/s]Extractor Predicting: 193it [02:17,  1.32it/s]Extractor Predicting: 194it [02:18,  1.22it/s]Extractor Predicting: 195it [02:19,  1.25it/s]Extractor Predicting: 196it [02:20,  1.28it/s]Extractor Predicting: 197it [02:20,  1.29it/s]Extractor Predicting: 198it [02:21,  1.32it/s]Extractor Predicting: 199it [02:22,  1.35it/s]Extractor Predicting: 200it [02:23,  1.33it/s]Extractor Predicting: 201it [02:23,  1.31it/s]Extractor Predicting: 202it [02:24,  1.38it/s]Extractor Predicting: 203it [02:25,  1.36it/s]Extractor Predicting: 204it [02:25,  1.38it/s]Extractor Predicting: 205it [02:26,  1.37it/s]Extractor Predicting: 206it [02:27,  1.36it/s]Extractor Predicting: 207it [02:28,  1.34it/s]Extractor Predicting: 208it [02:28,  1.34it/s]Extractor Predicting: 209it [02:29,  1.29it/s]Extractor Predicting: 210it [02:30,  1.30it/s]Extractor Predicting: 211it [02:31,  1.30it/s]Extractor Predicting: 212it [02:32,  1.30it/s]Extractor Predicting: 213it [02:32,  1.30it/s]Extractor Predicting: 214it [02:33,  1.29it/s]Extractor Predicting: 215it [02:34,  1.32it/s]Extractor Predicting: 216it [02:35,  1.30it/s]Extractor Predicting: 217it [02:35,  1.31it/s]Extractor Predicting: 218it [02:36,  1.32it/s]Extractor Predicting: 219it [02:37,  1.31it/s]Extractor Predicting: 220it [02:38,  1.32it/s]Extractor Predicting: 221it [02:39,  1.29it/s]Extractor Predicting: 222it [02:39,  1.28it/s]Extractor Predicting: 223it [02:40,  1.31it/s]Extractor Predicting: 224it [02:41,  1.32it/s]Extractor Predicting: 225it [02:42,  1.33it/s]Extractor Predicting: 226it [02:42,  1.31it/s]Extractor Predicting: 227it [02:43,  1.36it/s]Extractor Predicting: 228it [02:44,  1.35it/s]Extractor Predicting: 229it [02:44,  1.34it/s]Extractor Predicting: 230it [02:45,  1.40it/s]Extractor Predicting: 231it [02:46,  1.40it/s]Extractor Predicting: 232it [02:47,  1.42it/s]Extractor Predicting: 233it [02:47,  1.40it/s]Extractor Predicting: 234it [02:48,  1.41it/s]Extractor Predicting: 235it [02:49,  1.39it/s]Extractor Predicting: 236it [02:49,  1.37it/s]Extractor Predicting: 237it [02:50,  1.35it/s]Extractor Predicting: 238it [02:51,  1.37it/s]Extractor Predicting: 239it [02:52,  1.35it/s]Extractor Predicting: 240it [02:52,  1.36it/s]Extractor Predicting: 241it [02:53,  1.31it/s]Extractor Predicting: 242it [02:54,  1.34it/s]Extractor Predicting: 243it [02:55,  1.33it/s]Extractor Predicting: 244it [02:55,  1.35it/s]Extractor Predicting: 245it [02:56,  1.37it/s]Extractor Predicting: 246it [02:57,  1.36it/s]Extractor Predicting: 247it [02:58,  1.36it/s]Extractor Predicting: 248it [02:58,  1.36it/s]Extractor Predicting: 249it [02:59,  1.35it/s]Extractor Predicting: 250it [03:00,  1.37it/s]Extractor Predicting: 251it [03:01,  1.37it/s]Extractor Predicting: 252it [03:01,  1.37it/s]Extractor Predicting: 253it [03:02,  1.35it/s]Extractor Predicting: 254it [03:03,  1.37it/s]Extractor Predicting: 255it [03:04,  1.33it/s]Extractor Predicting: 256it [03:04,  1.29it/s]Extractor Predicting: 257it [03:05,  1.30it/s]Extractor Predicting: 258it [03:06,  1.30it/s]Extractor Predicting: 259it [03:07,  1.31it/s]Extractor Predicting: 260it [03:07,  1.30it/s]Extractor Predicting: 261it [03:08,  1.33it/s]Extractor Predicting: 262it [03:09,  1.32it/s]Extractor Predicting: 263it [03:10,  1.30it/s]Extractor Predicting: 264it [03:10,  1.30it/s]Extractor Predicting: 265it [03:11,  1.26it/s]Extractor Predicting: 266it [03:12,  1.27it/s]Extractor Predicting: 267it [03:13,  1.28it/s]Extractor Predicting: 268it [03:14,  1.26it/s]Extractor Predicting: 269it [03:14,  1.29it/s]Extractor Predicting: 270it [03:15,  1.31it/s]Extractor Predicting: 271it [03:16,  1.28it/s]Extractor Predicting: 272it [03:17,  1.30it/s]Extractor Predicting: 273it [03:17,  1.30it/s]Extractor Predicting: 274it [03:18,  1.33it/s]Extractor Predicting: 275it [03:19,  1.34it/s]Extractor Predicting: 276it [03:20,  1.38it/s]Extractor Predicting: 277it [03:20,  1.38it/s]Extractor Predicting: 278it [03:21,  1.37it/s]Extractor Predicting: 279it [03:22,  1.36it/s]Extractor Predicting: 280it [03:23,  1.32it/s]Extractor Predicting: 281it [03:24,  1.21it/s]Extractor Predicting: 282it [03:24,  1.32it/s]Extractor Predicting: 282it [03:24,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:57,825 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:57,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:57,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:57,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:47:57,829 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:47:58,455 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:47:58,456 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:47:59,025 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:48:00,038 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:48:00,038 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:48:02,886 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:48:02,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:48:02,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:48:02,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:48:02,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:48:03,555 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:48:03,556 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:48:04,131 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:48:04,293 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:48:04,293 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.20it/s]Extractor Predicting: 2it [00:01,  1.21it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.26it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.28it/s]
[INFO|configuration_utils.py:515] 2023-08-28 09:48:08,614 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:48:08,615 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 09:48:08,623 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:48:08,624 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 09:48:08,626 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 09:48:11,990 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 09:48:11,992 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 09:48:12,003 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 09:48:12,004 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 09:48:12,009 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:48:12,013 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:48:12,013 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:48:12,013 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:48:12,013 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:48:12,013 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 09:48:12,013 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 09:48:12,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:13,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:14,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:15,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:16,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:16,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:18,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:18,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:20,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:20,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:21,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:22,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:23,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:24,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:25,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:26,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:27,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:28,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:29,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:30,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:31,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:32,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:33,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:34,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:35,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:36,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:37,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:38,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:39,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:40,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:41,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:29<06:54, 29.63s/it][WARNING|generation_utils.py:914] 2023-08-28 09:48:41,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:42,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:43,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:44,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:45,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:46,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:47,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:49,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:50,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:51,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:52,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:53,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:54,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:55,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:55,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:56,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:57,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:58,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:48:59,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:00,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:01,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:02,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:03,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:04,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:05,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:54<05:45, 26.59s/it][WARNING|generation_utils.py:914] 2023-08-28 09:49:06,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:07,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:08,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:08,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:09,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:10,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:12,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:13,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:14,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:15,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:16,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:17,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:18,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:19,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:20,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:21,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:21,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:23,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:24,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:25,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:26,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:27,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:28,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:29,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:30,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:30,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:19<05:13, 26.11s/it][WARNING|generation_utils.py:914] 2023-08-28 09:49:31,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:32,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:33,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:34,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:35,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:36,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:37,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:38,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:39,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:40,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:41,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:42,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:43,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:44,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:44,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:46,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:47,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:47,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:48,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:49,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:50,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:51,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:53,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:54,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:43<04:35, 25.05s/it][WARNING|generation_utils.py:914] 2023-08-28 09:49:55,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:56,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:57,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:58,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:49:59,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:00,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:01,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:02,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:03,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:04,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:05,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:06,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:07,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:08,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:09,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:10,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:11,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:12,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:13,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:14,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:15,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:16,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:17,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [02:06<04:03, 24.35s/it][WARNING|generation_utils.py:914] 2023-08-28 09:50:18,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:20,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:20,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:21,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:23,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:24,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:25,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:26,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:27,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:28,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:29,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:30,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:31,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:32,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:33,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:34,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:35,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:35,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:36,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:37,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:38,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:39,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:41,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:29<03:36, 24.09s/it][WARNING|generation_utils.py:914] 2023-08-28 09:50:42,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:42,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:43,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:44,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:45,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:46,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:47,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:48,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:49,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:50,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:51,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:52,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:53,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:54,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:55,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:55,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:56,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:57,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:50:58,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:00,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:01,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:02,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:03,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:51<03:07, 23.40s/it][WARNING|generation_utils.py:914] 2023-08-28 09:51:03,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:04,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:06,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:07,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:08,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:08,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:09,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:11,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:12,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:13,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:14,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:15,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:16,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:17,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:18,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:19,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:20,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:21,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:22,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:23,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:24,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:25,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:26,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [03:15<02:44, 23.50s/it][WARNING|generation_utils.py:914] 2023-08-28 09:51:27,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:28,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:29,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:30,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:31,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:32,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:33,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:34,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:35,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:36,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:37,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:38,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:39,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:40,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:41,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:42,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:43,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:44,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:45,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:47,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:48,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:48,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:49,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:38<02:20, 23.41s/it][WARNING|generation_utils.py:914] 2023-08-28 09:51:50,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:52,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:52,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:53,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:54,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:55,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:56,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:57,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:58,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:51:59,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:00,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:01,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:02,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:02,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:03,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:04,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:05,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:07,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:08,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:08,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:10,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:11,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [03:59<01:53, 22.75s/it][WARNING|generation_utils.py:914] 2023-08-28 09:52:12,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:13,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:14,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:15,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:16,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:17,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:18,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:19,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:20,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:21,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:22,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:23,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:24,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:25,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:26,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:27,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:28,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:29,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:30,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:31,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:32,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:33,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:34,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:35,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [04:23<01:32, 23.09s/it][WARNING|generation_utils.py:914] 2023-08-28 09:52:36,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:37,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:37,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:38,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:39,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:40,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:41,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:42,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:43,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:45,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:46,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:47,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:47,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:48,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:49,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:50,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:51,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:52,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:53,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:54,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:55,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:56,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:56,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:45<01:07, 22.67s/it][WARNING|generation_utils.py:914] 2023-08-28 09:52:57,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:58,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:52:59,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:00,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:01,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:02,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:03,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:04,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:05,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:05,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:06,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:07,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:08,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:09,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:10,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:11,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:12,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:13,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:14,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:15,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:16,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:17,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:18,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:19,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:20,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [05:08<00:45, 22.88s/it][WARNING|generation_utils.py:914] 2023-08-28 09:53:21,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:22,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:23,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:24,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:25,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:26,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:27,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:28,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:29,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:30,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:31,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:32,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:33,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:34,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:35,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:36,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:37,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:38,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:39,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:40,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:41,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:42,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:43,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:44,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [05:32<00:23, 23.25s/it][WARNING|generation_utils.py:914] 2023-08-28 09:53:45,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:46,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:46,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:47,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:48,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:49,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:51,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:51,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:52,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:53,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:54,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:55,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:56,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:57,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:53:58,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:00,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:01,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:02,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:03,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:04,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:05,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:06,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:07,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 09:54:08,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:57<00:00, 23.55s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:57<00:00, 23.81s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:14,708 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:14,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:14,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:14,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:14,712 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:54:15,024 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:54:15,025 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:54:15,285 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:54:16,332 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:54:16,332 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:18,112 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:18,122 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:18,123 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:18,123 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:54:18,123 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:54:18,481 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:54:18,482 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:54:18,741 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:54:18,905 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:54:18,905 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n', 'Relation : country . Context : In 1994 , , he was named as the winner of the Eurovision Song Contest 2000 . Head Entity : Eurovision Song Contest 2000 , Tail Entity : France .\n']
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 137, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 174, 'raw': 288}
{'target': 600, 'success': 190, 'raw': 320}
{'target': 600, 'success': 211, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 252, 'raw': 416}
{'target': 600, 'success': 274, 'raw': 448}
{'target': 600, 'success': 294, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 347, 'raw': 576}
{'target': 600, 'success': 361, 'raw': 608}
{'target': 600, 'success': 388, 'raw': 640}
{'target': 600, 'success': 408, 'raw': 672}
{'target': 600, 'success': 428, 'raw': 704}
{'target': 600, 'success': 443, 'raw': 736}
{'target': 600, 'success': 462, 'raw': 768}
{'target': 600, 'success': 483, 'raw': 800}
{'target': 600, 'success': 501, 'raw': 832}
{'target': 600, 'success': 519, 'raw': 864}
{'target': 600, 'success': 541, 'raw': 896}
{'target': 600, 'success': 562, 'raw': 928}
{'target': 600, 'success': 585, 'raw': 960}
{'target': 600, 'success': 608, 'raw': 992}
{'prompt': 'Relation : country .', 'success_rate': 0.6129032258064516, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 485, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year , the band formed New River Band with two of their members at the end of 2010 , Mikey McLeod ( the lyricist ) and Tim McCarroll ( bass ) . Head Entity : Mikey McNamara , Tail Entity : New River Band .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : member of .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : subsidiary . Context : The CIBN ( CBNI ) , also called CBE ( CBW , CBQ , CBQR , CJAX , CJD , CJEC , CJF , CJFY ) , is a United States National Research Council scientific satellite constellation . Head Entity : CBI , Tail Entity : United States National Research Council .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', "('A', 'subsidiary', '', 'On August 2017 , the company began shipping a new product for children in Europe : A .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Battle of Brackewald', 'field of work', '', 'In 1849 he became a volunteer for the British Army at the Battle of Brackewald in Normandy .')", 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : Later in the year ( 1141â€“1231 ) he met Ferdinand I of Spain and the Duke of Prussia , whom he bore in the name of Christophe , together with Robert I of Belgium . Head Entity : Christophe , Tail Entity : John I of Belgium .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.859375, 'errors': {''}}
["Relation : occupation . Context : On 31 March 2014 , the Romanian Army , under a new President of Romania , Luiz InÃ¡cio Lutcic , announced the departure of Luiz 's second - generation Air Force commander , former Admiral of Romania , Sigmund Kaveliu . Head Entity : GeniÃ§iu Lutcic , Tail Entity : Romanian Army .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 14467
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14567, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.14it/s]Extractor Estimating: 2it [00:01,  1.13it/s]Extractor Estimating: 3it [00:02,  1.14it/s]Extractor Estimating: 4it [00:03,  1.15it/s]Extractor Estimating: 5it [00:04,  1.22it/s]Extractor Estimating: 6it [00:05,  1.24it/s]Extractor Estimating: 7it [00:05,  1.26it/s]Extractor Estimating: 8it [00:06,  1.24it/s]Extractor Estimating: 9it [00:07,  1.26it/s]Extractor Estimating: 10it [00:08,  1.26it/s]Extractor Estimating: 11it [00:08,  1.25it/s]Extractor Estimating: 12it [00:09,  1.21it/s]Extractor Estimating: 13it [00:10,  1.19it/s]Extractor Estimating: 14it [00:11,  1.21it/s]Extractor Estimating: 15it [00:12,  1.21it/s]Extractor Estimating: 16it [00:13,  1.19it/s]Extractor Estimating: 17it [00:14,  1.20it/s]Extractor Estimating: 18it [00:14,  1.16it/s]Extractor Estimating: 19it [00:15,  1.20it/s]Extractor Estimating: 20it [00:16,  1.21it/s]Extractor Estimating: 21it [00:17,  1.22it/s]Extractor Estimating: 22it [00:18,  1.17it/s]Extractor Estimating: 23it [00:19,  1.19it/s]Extractor Estimating: 24it [00:19,  1.20it/s]Extractor Estimating: 25it [00:20,  1.22it/s]Extractor Estimating: 26it [00:21,  1.24it/s]Extractor Estimating: 27it [00:22,  1.14it/s]Extractor Estimating: 28it [00:23,  1.18it/s]Extractor Estimating: 29it [00:24,  1.14it/s]Extractor Estimating: 30it [00:25,  1.13it/s]Extractor Estimating: 31it [00:25,  1.20it/s]Extractor Estimating: 32it [00:26,  1.24it/s]Extractor Estimating: 33it [00:27,  1.20it/s]Extractor Estimating: 34it [00:28,  1.22it/s]Extractor Estimating: 35it [00:29,  1.22it/s]Extractor Estimating: 36it [00:29,  1.22it/s]Extractor Estimating: 37it [00:30,  1.21it/s]Extractor Estimating: 38it [00:31,  1.22it/s]Extractor Estimating: 39it [00:32,  1.21it/s]Extractor Estimating: 40it [00:33,  1.26it/s]Extractor Estimating: 41it [00:34,  1.22it/s]Extractor Estimating: 42it [00:34,  1.19it/s]Extractor Estimating: 43it [00:35,  1.19it/s]Extractor Estimating: 44it [00:36,  1.16it/s]Extractor Estimating: 45it [00:37,  1.20it/s]Extractor Estimating: 46it [00:38,  1.21it/s]Extractor Estimating: 47it [00:39,  1.19it/s]Extractor Estimating: 48it [00:39,  1.23it/s]Extractor Estimating: 49it [00:40,  1.16it/s]Extractor Estimating: 50it [00:41,  1.18it/s]Extractor Estimating: 51it [00:42,  1.19it/s]Extractor Estimating: 52it [00:43,  1.18it/s]Extractor Estimating: 53it [00:44,  1.22it/s]Extractor Estimating: 54it [00:44,  1.21it/s]Extractor Estimating: 55it [00:45,  1.22it/s]Extractor Estimating: 56it [00:46,  1.23it/s]Extractor Estimating: 57it [00:47,  1.22it/s]Extractor Estimating: 58it [00:48,  1.22it/s]Extractor Estimating: 59it [00:48,  1.24it/s]Extractor Estimating: 60it [00:49,  1.21it/s]Extractor Estimating: 61it [00:50,  1.20it/s]Extractor Estimating: 62it [00:51,  1.18it/s]Extractor Estimating: 63it [00:52,  1.21it/s]Extractor Estimating: 64it [00:53,  1.20it/s]Extractor Estimating: 65it [00:53,  1.22it/s]Extractor Estimating: 66it [00:54,  1.23it/s]Extractor Estimating: 67it [00:55,  1.24it/s]Extractor Estimating: 68it [00:56,  1.22it/s]Extractor Estimating: 69it [00:57,  1.19it/s]Extractor Estimating: 70it [00:58,  1.20it/s]Extractor Estimating: 71it [00:58,  1.21it/s]Extractor Estimating: 72it [00:59,  1.21it/s]Extractor Estimating: 73it [01:00,  1.19it/s]Extractor Estimating: 74it [01:01,  1.22it/s]Extractor Estimating: 75it [01:02,  1.21it/s]Extractor Estimating: 76it [01:02,  1.24it/s]Extractor Estimating: 77it [01:03,  1.24it/s]Extractor Estimating: 78it [01:04,  1.23it/s]Extractor Estimating: 79it [01:05,  1.22it/s]Extractor Estimating: 80it [01:06,  1.25it/s]Extractor Estimating: 81it [01:07,  1.25it/s]Extractor Estimating: 82it [01:07,  1.22it/s]Extractor Estimating: 83it [01:08,  1.25it/s]Extractor Estimating: 84it [01:09,  1.26it/s]Extractor Estimating: 85it [01:10,  1.24it/s]Extractor Estimating: 86it [01:11,  1.24it/s]Extractor Estimating: 87it [01:11,  1.22it/s]Extractor Estimating: 88it [01:12,  1.22it/s]Extractor Estimating: 89it [01:13,  1.23it/s]Extractor Estimating: 90it [01:14,  1.22it/s]Extractor Estimating: 91it [01:15,  1.24it/s]Extractor Estimating: 92it [01:15,  1.24it/s]Extractor Estimating: 93it [01:16,  1.27it/s]Extractor Estimating: 94it [01:17,  1.23it/s]Extractor Estimating: 95it [01:18,  1.25it/s]Extractor Estimating: 96it [01:19,  1.25it/s]Extractor Estimating: 97it [01:19,  1.24it/s]Extractor Estimating: 98it [01:20,  1.23it/s]Extractor Estimating: 99it [01:21,  1.26it/s]Extractor Estimating: 100it [01:22,  1.24it/s]Extractor Estimating: 101it [01:23,  1.18it/s]Extractor Estimating: 102it [01:24,  1.24it/s]Extractor Estimating: 103it [01:24,  1.28it/s]Extractor Estimating: 104it [01:25,  1.28it/s]Extractor Estimating: 105it [01:26,  1.23it/s]Extractor Estimating: 106it [01:27,  1.23it/s]Extractor Estimating: 107it [01:28,  1.21it/s]Extractor Estimating: 108it [01:28,  1.23it/s]Extractor Estimating: 109it [01:29,  1.24it/s]Extractor Estimating: 110it [01:30,  1.20it/s]Extractor Estimating: 111it [01:31,  1.24it/s]Extractor Estimating: 112it [01:32,  1.11it/s]Extractor Estimating: 113it [01:33,  1.12it/s]Extractor Estimating: 114it [01:34,  1.14it/s]Extractor Estimating: 115it [01:34,  1.18it/s]Extractor Estimating: 116it [01:35,  1.22it/s]Extractor Estimating: 117it [01:36,  1.22it/s]Extractor Estimating: 118it [01:37,  1.24it/s]Extractor Estimating: 119it [01:38,  1.23it/s]Extractor Estimating: 120it [01:38,  1.24it/s]Extractor Estimating: 121it [01:39,  1.27it/s]Extractor Estimating: 122it [01:40,  1.22it/s]Extractor Estimating: 123it [01:41,  1.26it/s]Extractor Estimating: 124it [01:42,  1.25it/s]Extractor Estimating: 125it [01:42,  1.22it/s]Extractor Estimating: 126it [01:43,  1.27it/s]Extractor Estimating: 127it [01:44,  1.25it/s]Extractor Estimating: 128it [01:45,  1.27it/s]Extractor Estimating: 129it [01:46,  1.27it/s]Extractor Estimating: 130it [01:46,  1.33it/s]Extractor Estimating: 131it [01:47,  1.31it/s]Extractor Estimating: 132it [01:48,  1.31it/s]Extractor Estimating: 133it [01:49,  1.31it/s]Extractor Estimating: 134it [01:49,  1.28it/s]Extractor Estimating: 135it [01:50,  1.31it/s]Extractor Estimating: 136it [01:51,  1.31it/s]Extractor Estimating: 137it [01:52,  1.22it/s]Extractor Estimating: 138it [01:53,  1.24it/s]Extractor Estimating: 139it [01:53,  1.28it/s]Extractor Estimating: 140it [01:54,  1.30it/s]Extractor Estimating: 141it [01:55,  1.28it/s]Extractor Estimating: 142it [01:56,  1.28it/s]Extractor Estimating: 143it [01:56,  1.30it/s]Extractor Estimating: 144it [01:57,  1.30it/s]Extractor Estimating: 145it [01:58,  1.26it/s]Extractor Estimating: 146it [01:59,  1.30it/s]Extractor Estimating: 147it [01:59,  1.28it/s]Extractor Estimating: 148it [02:00,  1.25it/s]Extractor Estimating: 149it [02:01,  1.21it/s]Extractor Estimating: 150it [02:02,  1.21it/s]Extractor Estimating: 151it [02:03,  1.26it/s]Extractor Estimating: 152it [02:04,  1.25it/s]Extractor Estimating: 153it [02:04,  1.26it/s]Extractor Estimating: 154it [02:05,  1.26it/s]Extractor Estimating: 155it [02:06,  1.26it/s]Extractor Estimating: 156it [02:07,  1.26it/s]Extractor Estimating: 157it [02:08,  1.22it/s]Extractor Estimating: 158it [02:08,  1.23it/s]Extractor Estimating: 159it [02:09,  1.22it/s]Extractor Estimating: 160it [02:10,  1.24it/s]Extractor Estimating: 161it [02:11,  1.24it/s]Extractor Estimating: 162it [02:12,  1.25it/s]Extractor Estimating: 163it [02:12,  1.25it/s]Extractor Estimating: 164it [02:13,  1.27it/s]Extractor Estimating: 165it [02:14,  1.30it/s]Extractor Estimating: 166it [02:15,  1.29it/s]Extractor Estimating: 167it [02:16,  1.26it/s]Extractor Estimating: 168it [02:16,  1.25it/s]Extractor Estimating: 169it [02:17,  1.27it/s]Extractor Estimating: 170it [02:18,  1.33it/s]Extractor Estimating: 171it [02:19,  1.31it/s]Extractor Estimating: 172it [02:19,  1.25it/s]Extractor Estimating: 173it [02:20,  1.22it/s]Extractor Estimating: 174it [02:21,  1.21it/s]Extractor Estimating: 175it [02:22,  1.23it/s]Extractor Estimating: 176it [02:23,  1.20it/s]Extractor Estimating: 177it [02:24,  1.22it/s]Extractor Estimating: 178it [02:24,  1.24it/s]Extractor Estimating: 179it [02:25,  1.25it/s]Extractor Estimating: 180it [02:26,  1.16it/s]Extractor Estimating: 181it [02:27,  1.21it/s]Extractor Estimating: 182it [02:28,  1.22it/s]Extractor Estimating: 183it [02:29,  1.22it/s]Extractor Estimating: 184it [02:29,  1.23it/s]Extractor Estimating: 185it [02:30,  1.25it/s]Extractor Estimating: 186it [02:31,  1.25it/s]Extractor Estimating: 187it [02:32,  1.23it/s]Extractor Estimating: 188it [02:33,  1.23it/s]Extractor Estimating: 189it [02:34,  1.12it/s]Extractor Estimating: 190it [02:34,  1.17it/s]Extractor Estimating: 191it [02:35,  1.22it/s]Extractor Estimating: 192it [02:36,  1.20it/s]Extractor Estimating: 193it [02:37,  1.21it/s]Extractor Estimating: 194it [02:38,  1.20it/s]Extractor Estimating: 195it [02:38,  1.22it/s]Extractor Estimating: 196it [02:39,  1.25it/s]Extractor Estimating: 197it [02:40,  1.21it/s]Extractor Estimating: 198it [02:41,  1.22it/s]Extractor Estimating: 199it [02:42,  1.26it/s]Extractor Estimating: 200it [02:42,  1.23it/s]Extractor Estimating: 201it [02:43,  1.21it/s]Extractor Estimating: 202it [02:44,  1.22it/s]Extractor Estimating: 203it [02:45,  1.21it/s]Extractor Estimating: 204it [02:46,  1.22it/s]Extractor Estimating: 205it [02:47,  1.23it/s]Extractor Estimating: 206it [02:47,  1.21it/s]Extractor Estimating: 207it [02:48,  1.25it/s]Extractor Estimating: 208it [02:49,  1.17it/s]Extractor Estimating: 209it [02:50,  1.17it/s]Extractor Estimating: 210it [02:51,  1.19it/s]Extractor Estimating: 211it [02:52,  1.18it/s]Extractor Estimating: 212it [02:53,  1.18it/s]Extractor Estimating: 213it [02:53,  1.15it/s]Extractor Estimating: 214it [02:54,  1.17it/s]Extractor Estimating: 215it [02:55,  1.19it/s]Extractor Estimating: 216it [02:56,  1.15it/s]Extractor Estimating: 217it [02:57,  1.15it/s]Extractor Estimating: 218it [02:58,  1.18it/s]Extractor Estimating: 219it [02:59,  1.16it/s]Extractor Estimating: 220it [02:59,  1.19it/s]Extractor Estimating: 221it [03:00,  1.15it/s]Extractor Estimating: 222it [03:01,  1.14it/s]Extractor Estimating: 223it [03:02,  1.15it/s]Extractor Estimating: 224it [03:03,  1.16it/s]Extractor Estimating: 225it [03:04,  1.18it/s]Extractor Estimating: 226it [03:05,  1.16it/s]Extractor Estimating: 227it [03:05,  1.22it/s]Extractor Estimating: 228it [03:06,  1.26it/s]Extractor Estimating: 229it [03:07,  1.29it/s]Extractor Estimating: 230it [03:08,  1.28it/s]Extractor Estimating: 231it [03:08,  1.31it/s]Extractor Estimating: 232it [03:09,  1.35it/s]Extractor Estimating: 233it [03:10,  1.38it/s]Extractor Estimating: 234it [03:10,  1.37it/s]Extractor Estimating: 235it [03:11,  1.34it/s]Extractor Estimating: 236it [03:12,  1.33it/s]Extractor Estimating: 237it [03:13,  1.32it/s]Extractor Estimating: 238it [03:14,  1.31it/s]Extractor Estimating: 239it [03:14,  1.30it/s]Extractor Estimating: 240it [03:15,  1.30it/s]Extractor Estimating: 241it [03:16,  1.33it/s]Extractor Estimating: 242it [03:17,  1.33it/s]Extractor Estimating: 243it [03:17,  1.28it/s]Extractor Estimating: 244it [03:18,  1.27it/s]Extractor Estimating: 245it [03:19,  1.29it/s]Extractor Estimating: 246it [03:20,  1.32it/s]Extractor Estimating: 247it [03:20,  1.30it/s]Extractor Estimating: 248it [03:21,  1.27it/s]Extractor Estimating: 249it [03:22,  1.19it/s]Extractor Estimating: 250it [03:23,  1.24it/s]Extractor Estimating: 251it [03:24,  1.18it/s]Extractor Estimating: 252it [03:25,  1.22it/s]Extractor Estimating: 253it [03:25,  1.24it/s]Extractor Estimating: 254it [03:26,  1.18it/s]Extractor Estimating: 255it [03:27,  1.23it/s]Extractor Estimating: 256it [03:28,  1.30it/s]Extractor Estimating: 257it [03:29,  1.28it/s]Extractor Estimating: 258it [03:30,  1.20it/s]Extractor Estimating: 259it [03:30,  1.24it/s]Extractor Estimating: 260it [03:31,  1.25it/s]Extractor Estimating: 261it [03:32,  1.22it/s]Extractor Estimating: 262it [03:33,  1.20it/s]Extractor Estimating: 263it [03:34,  1.21it/s]Extractor Estimating: 264it [03:34,  1.22it/s]Extractor Estimating: 265it [03:35,  1.19it/s]Extractor Estimating: 266it [03:36,  1.21it/s]Extractor Estimating: 267it [03:37,  1.18it/s]Extractor Estimating: 268it [03:38,  1.23it/s]Extractor Estimating: 269it [03:39,  1.25it/s]Extractor Estimating: 270it [03:39,  1.22it/s]Extractor Estimating: 271it [03:40,  1.20it/s]Extractor Estimating: 272it [03:41,  1.19it/s]Extractor Estimating: 273it [03:42,  1.21it/s]Extractor Estimating: 274it [03:43,  1.19it/s]Extractor Estimating: 275it [03:44,  1.22it/s]Extractor Estimating: 276it [03:44,  1.27it/s]Extractor Estimating: 277it [03:45,  1.26it/s]Extractor Estimating: 278it [03:46,  1.32it/s]Extractor Estimating: 279it [03:47,  1.30it/s]Extractor Estimating: 280it [03:47,  1.30it/s]Extractor Estimating: 281it [03:48,  1.32it/s]Extractor Estimating: 282it [03:49,  1.35it/s]Extractor Estimating: 283it [03:50,  1.32it/s]Extractor Estimating: 284it [03:50,  1.30it/s]Extractor Estimating: 285it [03:51,  1.32it/s]Extractor Estimating: 286it [03:52,  1.32it/s]Extractor Estimating: 287it [03:53,  1.31it/s]Extractor Estimating: 288it [03:53,  1.28it/s]Extractor Estimating: 289it [03:54,  1.30it/s]Extractor Estimating: 290it [03:55,  1.29it/s]Extractor Estimating: 291it [03:56,  1.34it/s]Extractor Estimating: 292it [03:56,  1.30it/s]Extractor Estimating: 293it [03:57,  1.32it/s]Extractor Estimating: 294it [03:58,  1.30it/s]Extractor Estimating: 295it [03:59,  1.29it/s]Extractor Estimating: 296it [04:00,  1.33it/s]Extractor Estimating: 297it [04:00,  1.34it/s]Extractor Estimating: 298it [04:01,  1.35it/s]Extractor Estimating: 299it [04:02,  1.38it/s]Extractor Estimating: 300it [04:02,  1.39it/s]Extractor Estimating: 301it [04:03,  1.32it/s]Extractor Estimating: 302it [04:04,  1.25it/s]Extractor Estimating: 303it [04:05,  1.26it/s]Extractor Estimating: 304it [04:06,  1.31it/s]Extractor Estimating: 305it [04:06,  1.30it/s]Extractor Estimating: 306it [04:07,  1.33it/s]Extractor Estimating: 307it [04:08,  1.33it/s]Extractor Estimating: 308it [04:08,  1.39it/s]Extractor Estimating: 309it [04:09,  1.38it/s]Extractor Estimating: 310it [04:10,  1.34it/s]Extractor Estimating: 311it [04:11,  1.27it/s]Extractor Estimating: 312it [04:12,  1.32it/s]Extractor Estimating: 313it [04:12,  1.29it/s]Extractor Estimating: 314it [04:13,  1.27it/s]Extractor Estimating: 315it [04:14,  1.15it/s]Extractor Estimating: 316it [04:15,  1.20it/s]Extractor Estimating: 317it [04:16,  1.24it/s]Extractor Estimating: 318it [04:17,  1.22it/s]Extractor Estimating: 319it [04:17,  1.26it/s]Extractor Estimating: 320it [04:18,  1.22it/s]Extractor Estimating: 321it [04:19,  1.27it/s]Extractor Estimating: 322it [04:20,  1.26it/s]Extractor Estimating: 323it [04:21,  1.27it/s]Extractor Estimating: 324it [04:21,  1.28it/s]Extractor Estimating: 325it [04:22,  1.29it/s]Extractor Estimating: 326it [04:23,  1.17it/s]Extractor Estimating: 327it [04:24,  1.20it/s]Extractor Estimating: 328it [04:25,  1.19it/s]Extractor Estimating: 329it [04:25,  1.23it/s]Extractor Estimating: 330it [04:26,  1.22it/s]Extractor Estimating: 331it [04:27,  1.24it/s]Extractor Estimating: 332it [04:28,  1.23it/s]Extractor Estimating: 333it [04:29,  1.23it/s]Extractor Estimating: 334it [04:29,  1.25it/s]Extractor Estimating: 335it [04:30,  1.23it/s]Extractor Estimating: 336it [04:31,  1.24it/s]Extractor Estimating: 337it [04:32,  1.26it/s]Extractor Estimating: 338it [04:33,  1.16it/s]Extractor Estimating: 339it [04:34,  1.19it/s]Extractor Estimating: 340it [04:34,  1.23it/s]Extractor Estimating: 341it [04:35,  1.18it/s]Extractor Estimating: 342it [04:36,  1.20it/s]Extractor Estimating: 343it [04:37,  1.20it/s]Extractor Estimating: 344it [04:38,  1.20it/s]Extractor Estimating: 345it [04:39,  1.22it/s]Extractor Estimating: 346it [04:40,  1.18it/s]Extractor Estimating: 347it [04:41,  1.12it/s]Extractor Estimating: 348it [04:41,  1.12it/s]Extractor Estimating: 349it [04:42,  1.12it/s]Extractor Estimating: 350it [04:43,  1.16it/s]Extractor Estimating: 351it [04:44,  1.18it/s]Extractor Estimating: 352it [04:45,  1.22it/s]Extractor Estimating: 353it [04:45,  1.24it/s]Extractor Estimating: 354it [04:46,  1.28it/s]Extractor Estimating: 355it [04:47,  1.30it/s]Extractor Estimating: 356it [04:48,  1.26it/s]Extractor Estimating: 357it [04:49,  1.27it/s]Extractor Estimating: 358it [04:49,  1.26it/s]Extractor Estimating: 359it [04:50,  1.32it/s]Extractor Estimating: 360it [04:51,  1.33it/s]Extractor Estimating: 361it [04:52,  1.31it/s]Extractor Estimating: 362it [04:52,  1.28it/s]Extractor Estimating: 363it [04:53,  1.32it/s]Extractor Estimating: 364it [04:54,  1.28it/s]Extractor Estimating: 365it [04:55,  1.23it/s]Extractor Estimating: 366it [04:56,  1.24it/s]Extractor Estimating: 367it [04:56,  1.25it/s]Extractor Estimating: 368it [04:57,  1.23it/s]Extractor Estimating: 369it [04:58,  1.24it/s]Extractor Estimating: 370it [04:59,  1.26it/s]Extractor Estimating: 371it [05:00,  1.25it/s]Extractor Estimating: 372it [05:00,  1.28it/s]Extractor Estimating: 373it [05:01,  1.25it/s]Extractor Estimating: 374it [05:02,  1.24it/s]Extractor Estimating: 375it [05:02,  1.50it/s]Extractor Estimating: 375it [05:02,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:34,810 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:34,812 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:34,812 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:34,812 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:34,812 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 09:59:35,441 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 09:59:35,442 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:59:36,007 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 09:59:37,049 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:59:37,049 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:40,095 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:40,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:40,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:40,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 09:59:40,099 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 09:59:40,746 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 09:59:40,747 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 09:59:41,331 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 09:59:41,496 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 09:59:41,496 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:10:12,031 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:10:12,054 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7883 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 22746
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22846, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22846, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.420, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.468, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.433, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 71, avg_time 1.430, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 171, avg_time 1.429, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 271, avg_time 2.831, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 42, avg_time 1.449, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 142, avg_time 1.419, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 242, avg_time 1.451, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.416, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 113, avg_time 2.823, loss:nan
g_step 1200, step 213, avg_time 1.434, loss:nan
g_step 1300, step 313, avg_time 1.441, loss:nan
g_step 1400, step 84, avg_time 1.434, loss:nan
g_step 1500, step 184, avg_time 1.429, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 284, avg_time 2.839, loss:nan
g_step 1700, step 55, avg_time 1.422, loss:nan
g_step 1800, step 155, avg_time 1.444, loss:nan
g_step 1900, step 255, avg_time 1.465, loss:nan
g_step 2000, step 26, avg_time 1.436, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 126, avg_time 2.875, loss:nan
g_step 2200, step 226, avg_time 1.464, loss:nan
g_step 2300, step 326, avg_time 1.477, loss:nan
g_step 2400, step 97, avg_time 1.452, loss:nan
g_step 2500, step 197, avg_time 1.484, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 297, avg_time 2.888, loss:nan
g_step 2700, step 68, avg_time 1.430, loss:nan
g_step 2800, step 168, avg_time 1.463, loss:nan
g_step 2900, step 268, avg_time 1.476, loss:nan
g_step 3000, step 39, avg_time 1.475, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 139, avg_time 2.895, loss:nan
g_step 3200, step 239, avg_time 1.486, loss:nan
g_step 3300, step 10, avg_time 1.452, loss:nan
g_step 3400, step 110, avg_time 1.472, loss:nan
g_step 3500, step 210, avg_time 1.465, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 310, avg_time 2.889, loss:nan
g_step 3700, step 81, avg_time 1.493, loss:nan
g_step 3800, step 181, avg_time 1.469, loss:nan
g_step 3900, step 281, avg_time 1.440, loss:nan
g_step 4000, step 52, avg_time 1.453, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 152, avg_time 2.900, loss:nan
g_step 4200, step 252, avg_time 1.442, loss:nan
g_step 4300, step 23, avg_time 1.470, loss:nan
g_step 4400, step 123, avg_time 1.481, loss:nan
g_step 4500, step 223, avg_time 1.463, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 323, avg_time 2.884, loss:nan
g_step 4700, step 94, avg_time 1.445, loss:nan
g_step 4800, step 194, avg_time 1.486, loss:nan
g_step 4900, step 294, avg_time 1.450, loss:nan
g_step 5000, step 65, avg_time 1.467, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 165, avg_time 2.883, loss:nan
g_step 5200, step 265, avg_time 1.460, loss:nan
g_step 5300, step 36, avg_time 1.474, loss:nan
g_step 5400, step 136, avg_time 1.461, loss:nan
g_step 5500, step 236, avg_time 1.452, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 7, avg_time 2.901, loss:nan
g_step 5700, step 107, avg_time 1.467, loss:nan
g_step 5800, step 207, avg_time 1.461, loss:nan
g_step 5900, step 307, avg_time 1.485, loss:nan
g_step 6000, step 78, avg_time 1.465, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 178, avg_time 2.901, loss:nan
g_step 6200, step 278, avg_time 1.455, loss:nan
g_step 6300, step 49, avg_time 1.459, loss:nan
g_step 6400, step 149, avg_time 1.497, loss:nan
g_step 6500, step 249, avg_time 1.474, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:10:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:10:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-10-12_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:10:13 - WARNING - datasets.builder -   Using custom data configuration default-a4be3bbaa0e8a256
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a4be3bbaa0e8a256/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:10:13,714 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:10:13,715 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:10:13,716 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:10:13,717 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:10:13,726 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:10:13,729 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:10:13,729 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:10:13,729 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:10:13,730 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:10:13,730 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:10:13,730 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:10:13,888 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:10:17,016 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:10:17,020 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a4be3bbaa0e8a256/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  3.12ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.90ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.20ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  4.36ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.44ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.48ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.51ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.68ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.40ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  4.05ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.29ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.39ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.52ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.99ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  7.11ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:00,  7.67ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  7.92ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  8.09ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  8.13ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:00<00:00,  8.19ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00,  8.33ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.59ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.23ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.82ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  7.50ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  7.82ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  8.84ba/s]
[INFO|trainer.py:414] 2023-08-28 13:10:21,457 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:10:21,474 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:10:21,474 >>   Num examples = 7899
[INFO|trainer.py:1149] 2023-08-28 13:10:21,474 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:10:21,474 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:10:21,474 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:10:21,474 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:10:21,474 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<02:58,  3.44it/s]  0%|          | 2/615 [00:00<02:53,  3.53it/s]  0%|          | 3/615 [00:00<02:51,  3.56it/s]  1%|          | 4/615 [00:01<02:50,  3.58it/s]  1%|          | 5/615 [00:01<02:50,  3.58it/s]  1%|          | 6/615 [00:01<02:49,  3.59it/s]  1%|          | 7/615 [00:01<02:51,  3.55it/s]  1%|â–         | 8/615 [00:02<02:50,  3.57it/s]  1%|â–         | 9/615 [00:02<02:49,  3.58it/s]  2%|â–         | 10/615 [00:02<02:48,  3.58it/s]  2%|â–         | 11/615 [00:03<02:48,  3.59it/s]  2%|â–         | 12/615 [00:03<02:47,  3.59it/s]  2%|â–         | 13/615 [00:03<02:47,  3.59it/s]  2%|â–         | 14/615 [00:03<02:47,  3.59it/s]  2%|â–         | 15/615 [00:04<02:46,  3.59it/s]  3%|â–Ž         | 16/615 [00:04<02:46,  3.59it/s]  3%|â–Ž         | 17/615 [00:04<02:46,  3.59it/s]  3%|â–Ž         | 18/615 [00:05<02:46,  3.59it/s]  3%|â–Ž         | 19/615 [00:05<02:45,  3.59it/s]  3%|â–Ž         | 20/615 [00:05<02:45,  3.59it/s]  3%|â–Ž         | 21/615 [00:05<02:45,  3.59it/s]  4%|â–Ž         | 22/615 [00:06<02:45,  3.59it/s]  4%|â–Ž         | 23/615 [00:06<02:44,  3.59it/s]  4%|â–         | 24/615 [00:06<02:44,  3.59it/s]  4%|â–         | 25/615 [00:06<02:44,  3.59it/s]  4%|â–         | 26/615 [00:07<02:44,  3.58it/s]  4%|â–         | 27/615 [00:07<02:43,  3.59it/s]  5%|â–         | 28/615 [00:07<02:43,  3.59it/s]  5%|â–         | 29/615 [00:08<02:43,  3.58it/s]  5%|â–         | 30/615 [00:08<02:43,  3.59it/s]  5%|â–Œ         | 31/615 [00:08<02:42,  3.59it/s]  5%|â–Œ         | 32/615 [00:08<02:42,  3.59it/s]  5%|â–Œ         | 33/615 [00:09<02:42,  3.59it/s]  6%|â–Œ         | 34/615 [00:09<02:41,  3.59it/s]  6%|â–Œ         | 35/615 [00:09<02:41,  3.59it/s]  6%|â–Œ         | 36/615 [00:10<02:41,  3.59it/s]  6%|â–Œ         | 37/615 [00:10<02:41,  3.59it/s]  6%|â–Œ         | 38/615 [00:10<02:40,  3.59it/s]  6%|â–‹         | 39/615 [00:10<02:40,  3.59it/s]  7%|â–‹         | 40/615 [00:11<02:40,  3.59it/s]  7%|â–‹         | 41/615 [00:11<02:40,  3.59it/s]  7%|â–‹         | 42/615 [00:11<02:39,  3.58it/s]  7%|â–‹         | 43/615 [00:11<02:39,  3.58it/s]  7%|â–‹         | 44/615 [00:12<02:39,  3.58it/s]  7%|â–‹         | 45/615 [00:12<02:39,  3.58it/s]  7%|â–‹         | 46/615 [00:12<02:38,  3.58it/s]  8%|â–Š         | 47/615 [00:13<02:38,  3.58it/s]  8%|â–Š         | 48/615 [00:13<02:38,  3.58it/s]  8%|â–Š         | 49/615 [00:13<02:37,  3.58it/s]  8%|â–Š         | 50/615 [00:13<02:37,  3.58it/s]  8%|â–Š         | 51/615 [00:14<02:38,  3.56it/s]  8%|â–Š         | 52/615 [00:14<02:38,  3.56it/s]  9%|â–Š         | 53/615 [00:14<02:37,  3.57it/s]  9%|â–‰         | 54/615 [00:15<02:37,  3.57it/s]  9%|â–‰         | 55/615 [00:15<02:36,  3.57it/s]  9%|â–‰         | 56/615 [00:15<02:36,  3.58it/s]  9%|â–‰         | 57/615 [00:15<02:35,  3.58it/s]  9%|â–‰         | 58/615 [00:16<02:35,  3.58it/s] 10%|â–‰         | 59/615 [00:16<02:35,  3.58it/s] 10%|â–‰         | 60/615 [00:16<02:34,  3.58it/s] 10%|â–‰         | 61/615 [00:17<02:34,  3.58it/s] 10%|â–ˆ         | 62/615 [00:17<02:34,  3.58it/s] 10%|â–ˆ         | 63/615 [00:17<02:33,  3.58it/s] 10%|â–ˆ         | 64/615 [00:17<02:33,  3.59it/s] 11%|â–ˆ         | 65/615 [00:18<02:33,  3.59it/s] 11%|â–ˆ         | 66/615 [00:18<02:33,  3.58it/s] 11%|â–ˆ         | 67/615 [00:18<02:32,  3.58it/s] 11%|â–ˆ         | 68/615 [00:18<02:32,  3.58it/s] 11%|â–ˆ         | 69/615 [00:19<02:32,  3.58it/s] 11%|â–ˆâ–        | 70/615 [00:19<02:32,  3.58it/s] 12%|â–ˆâ–        | 71/615 [00:19<02:31,  3.58it/s] 12%|â–ˆâ–        | 72/615 [00:20<02:31,  3.58it/s] 12%|â–ˆâ–        | 73/615 [00:20<02:31,  3.58it/s] 12%|â–ˆâ–        | 74/615 [00:20<02:31,  3.58it/s] 12%|â–ˆâ–        | 75/615 [00:20<02:30,  3.58it/s] 12%|â–ˆâ–        | 76/615 [00:21<02:30,  3.58it/s] 13%|â–ˆâ–Ž        | 77/615 [00:21<02:30,  3.58it/s] 13%|â–ˆâ–Ž        | 78/615 [00:21<02:30,  3.58it/s] 13%|â–ˆâ–Ž        | 79/615 [00:22<02:29,  3.58it/s] 13%|â–ˆâ–Ž        | 80/615 [00:22<02:29,  3.58it/s] 13%|â–ˆâ–Ž        | 81/615 [00:22<02:29,  3.58it/s] 13%|â–ˆâ–Ž        | 82/615 [00:22<02:28,  3.58it/s] 13%|â–ˆâ–Ž        | 83/615 [00:23<02:28,  3.58it/s] 14%|â–ˆâ–Ž        | 84/615 [00:23<02:28,  3.58it/s] 14%|â–ˆâ–        | 85/615 [00:23<02:28,  3.57it/s] 14%|â–ˆâ–        | 86/615 [00:24<02:28,  3.57it/s] 14%|â–ˆâ–        | 87/615 [00:24<02:27,  3.57it/s] 14%|â–ˆâ–        | 88/615 [00:24<02:27,  3.57it/s] 14%|â–ˆâ–        | 89/615 [00:24<02:27,  3.57it/s] 15%|â–ˆâ–        | 90/615 [00:25<02:26,  3.58it/s] 15%|â–ˆâ–        | 91/615 [00:25<02:26,  3.58it/s] 15%|â–ˆâ–        | 92/615 [00:25<02:26,  3.58it/s] 15%|â–ˆâ–Œ        | 93/615 [00:25<02:25,  3.58it/s] 15%|â–ˆâ–Œ        | 94/615 [00:26<02:25,  3.58it/s] 15%|â–ˆâ–Œ        | 95/615 [00:26<02:25,  3.58it/s] 16%|â–ˆâ–Œ        | 96/615 [00:26<02:25,  3.58it/s] 16%|â–ˆâ–Œ        | 97/615 [00:27<02:24,  3.57it/s] 16%|â–ˆâ–Œ        | 98/615 [00:27<02:24,  3.57it/s] 16%|â–ˆâ–Œ        | 99/615 [00:27<02:24,  3.57it/s] 16%|â–ˆâ–‹        | 100/615 [00:27<02:24,  3.57it/s] 16%|â–ˆâ–‹        | 101/615 [00:28<02:23,  3.58it/s] 17%|â–ˆâ–‹        | 102/615 [00:28<02:23,  3.58it/s] 17%|â–ˆâ–‹        | 103/615 [00:28<02:23,  3.58it/s] 17%|â–ˆâ–‹        | 104/615 [00:29<02:22,  3.58it/s] 17%|â–ˆâ–‹        | 105/615 [00:29<02:22,  3.58it/s] 17%|â–ˆâ–‹        | 106/615 [00:29<02:22,  3.58it/s] 17%|â–ˆâ–‹        | 107/615 [00:29<02:21,  3.58it/s] 18%|â–ˆâ–Š        | 108/615 [00:30<02:21,  3.58it/s] 18%|â–ˆâ–Š        | 109/615 [00:30<02:21,  3.58it/s] 18%|â–ˆâ–Š        | 110/615 [00:30<02:21,  3.58it/s] 18%|â–ˆâ–Š        | 111/615 [00:30<02:20,  3.58it/s] 18%|â–ˆâ–Š        | 112/615 [00:31<02:20,  3.58it/s] 18%|â–ˆâ–Š        | 113/615 [00:31<02:20,  3.58it/s] 19%|â–ˆâ–Š        | 114/615 [00:31<02:19,  3.58it/s] 19%|â–ˆâ–Š        | 115/615 [00:32<02:19,  3.58it/s] 19%|â–ˆâ–‰        | 116/615 [00:32<02:19,  3.58it/s] 19%|â–ˆâ–‰        | 117/615 [00:32<02:19,  3.58it/s] 19%|â–ˆâ–‰        | 118/615 [00:32<02:18,  3.58it/s] 19%|â–ˆâ–‰        | 119/615 [00:33<02:18,  3.58it/s] 20%|â–ˆâ–‰        | 120/615 [00:33<02:18,  3.58it/s] 20%|â–ˆâ–‰        | 121/615 [00:33<02:18,  3.58it/s] 20%|â–ˆâ–‰        | 122/615 [00:34<02:17,  3.58it/s] 20%|â–ˆâ–ˆ        | 123/615 [00:34<02:17,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 13:10:55,949 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:10:55,949 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:10:55,949 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.57it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.44it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.63it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.90it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.38it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.17it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.03it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.80it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.76it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.63it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.65it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.67it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.68it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.66it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.59it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.58it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.58it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.64it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.65it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.54it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.60it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.62it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.61it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.63it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.56it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.63it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.66it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.57it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.64it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.57it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.59it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.62it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.64it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.57it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.57it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.61it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.57it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.64it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.61it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.52it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.26it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.44it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.46it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.54it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.57it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.60it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.57it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.58it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.60it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.55it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.52it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.53it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.56it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.60it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.58it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.64it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.58it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.58it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.62it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.46it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.60it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.61it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.53it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.55it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.61it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.56it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.56it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.60it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.52it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.53it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.52it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.55it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.57it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.57it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.59it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.60it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.61it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.60it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.56it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.57it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.56it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.52it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.56it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.62it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.63it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.56it/s][A                                                 
                                                 [A 20%|â–ˆâ–ˆ        | 123/615 [00:43<02:17,  3.57it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:11:05,312 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-28 13:11:05,334 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:11:08,005 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:11:08,026 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:11:08,038 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 124/615 [00:47<32:58,  4.03s/it] 20%|â–ˆâ–ˆ        | 125/615 [00:47<23:43,  2.90s/it] 20%|â–ˆâ–ˆ        | 126/615 [00:47<17:15,  2.12s/it] 21%|â–ˆâ–ˆ        | 127/615 [00:47<12:43,  1.57s/it] 21%|â–ˆâ–ˆ        | 128/615 [00:48<09:34,  1.18s/it] 21%|â–ˆâ–ˆ        | 129/615 [00:48<07:22,  1.10it/s] 21%|â–ˆâ–ˆ        | 130/615 [00:48<05:49,  1.39it/s] 21%|â–ˆâ–ˆâ–       | 131/615 [00:49<04:44,  1.70it/s] 21%|â–ˆâ–ˆâ–       | 132/615 [00:49<04:00,  2.01it/s] 22%|â–ˆâ–ˆâ–       | 133/615 [00:49<03:28,  2.31it/s] 22%|â–ˆâ–ˆâ–       | 134/615 [00:49<03:06,  2.58it/s] 22%|â–ˆâ–ˆâ–       | 135/615 [00:50<02:50,  2.82it/s] 22%|â–ˆâ–ˆâ–       | 136/615 [00:50<02:38,  3.01it/s] 22%|â–ˆâ–ˆâ–       | 137/615 [00:50<02:35,  3.08it/s] 22%|â–ˆâ–ˆâ–       | 138/615 [00:51<02:29,  3.19it/s] 23%|â–ˆâ–ˆâ–Ž       | 139/615 [00:51<02:24,  3.30it/s] 23%|â–ˆâ–ˆâ–Ž       | 140/615 [00:51<02:20,  3.38it/s] 23%|â–ˆâ–ˆâ–Ž       | 141/615 [00:51<02:17,  3.44it/s] 23%|â–ˆâ–ˆâ–Ž       | 142/615 [00:52<02:16,  3.48it/s] 23%|â–ˆâ–ˆâ–Ž       | 143/615 [00:52<02:14,  3.51it/s] 23%|â–ˆâ–ˆâ–Ž       | 144/615 [00:52<02:13,  3.53it/s] 24%|â–ˆâ–ˆâ–Ž       | 145/615 [00:53<02:12,  3.54it/s] 24%|â–ˆâ–ˆâ–Ž       | 146/615 [00:53<02:12,  3.55it/s] 24%|â–ˆâ–ˆâ–       | 147/615 [00:53<02:11,  3.56it/s] 24%|â–ˆâ–ˆâ–       | 148/615 [00:53<02:11,  3.56it/s] 24%|â–ˆâ–ˆâ–       | 149/615 [00:54<02:10,  3.57it/s] 24%|â–ˆâ–ˆâ–       | 150/615 [00:54<02:10,  3.57it/s] 25%|â–ˆâ–ˆâ–       | 151/615 [00:54<02:09,  3.57it/s] 25%|â–ˆâ–ˆâ–       | 152/615 [00:54<02:09,  3.58it/s] 25%|â–ˆâ–ˆâ–       | 153/615 [00:55<02:09,  3.55it/s] 25%|â–ˆâ–ˆâ–Œ       | 154/615 [00:55<02:09,  3.56it/s] 25%|â–ˆâ–ˆâ–Œ       | 155/615 [00:55<02:08,  3.57it/s] 25%|â–ˆâ–ˆâ–Œ       | 156/615 [00:56<02:08,  3.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 157/615 [00:56<02:08,  3.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 158/615 [00:56<02:07,  3.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 159/615 [00:56<02:07,  3.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 160/615 [00:57<02:07,  3.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 161/615 [00:57<02:06,  3.58it/s] 26%|â–ˆâ–ˆâ–‹       | 162/615 [00:57<02:06,  3.58it/s] 27%|â–ˆâ–ˆâ–‹       | 163/615 [00:58<02:06,  3.58it/s] 27%|â–ˆâ–ˆâ–‹       | 164/615 [00:58<02:06,  3.57it/s] 27%|â–ˆâ–ˆâ–‹       | 165/615 [00:58<02:06,  3.57it/s] 27%|â–ˆâ–ˆâ–‹       | 166/615 [00:58<02:05,  3.57it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|â–ˆâ–ˆâ–‹       | 167/615 [00:59<02:05,  3.57it/s] 27%|â–ˆâ–ˆâ–‹       | 168/615 [00:59<02:04,  3.58it/s] 27%|â–ˆâ–ˆâ–‹       | 169/615 [00:59<02:04,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 170/615 [01:00<02:04,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 171/615 [01:00<02:04,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 172/615 [01:00<02:03,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 173/615 [01:00<02:03,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 174/615 [01:01<02:03,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 175/615 [01:01<02:03,  3.57it/s] 29%|â–ˆâ–ˆâ–Š       | 176/615 [01:01<02:02,  3.57it/s] 29%|â–ˆâ–ˆâ–‰       | 177/615 [01:01<02:02,  3.58it/s] 29%|â–ˆâ–ˆâ–‰       | 178/615 [01:02<02:02,  3.58it/s] 29%|â–ˆâ–ˆâ–‰       | 179/615 [01:02<02:01,  3.58it/s] 29%|â–ˆâ–ˆâ–‰       | 180/615 [01:02<02:01,  3.58it/s] 29%|â–ˆâ–ˆâ–‰       | 181/615 [01:03<02:01,  3.58it/s] 30%|â–ˆâ–ˆâ–‰       | 182/615 [01:03<02:00,  3.58it/s] 30%|â–ˆâ–ˆâ–‰       | 183/615 [01:03<02:00,  3.59it/s] 30%|â–ˆâ–ˆâ–‰       | 184/615 [01:03<02:00,  3.59it/s] 30%|â–ˆâ–ˆâ–ˆ       | 185/615 [01:04<01:59,  3.59it/s] 30%|â–ˆâ–ˆâ–ˆ       | 186/615 [01:04<02:00,  3.57it/s] 30%|â–ˆâ–ˆâ–ˆ       | 187/615 [01:04<01:59,  3.57it/s] 31%|â–ˆâ–ˆâ–ˆ       | 188/615 [01:05<01:59,  3.58it/s] 31%|â–ˆâ–ˆâ–ˆ       | 189/615 [01:05<01:58,  3.58it/s] 31%|â–ˆâ–ˆâ–ˆ       | 190/615 [01:05<01:58,  3.58it/s] 31%|â–ˆâ–ˆâ–ˆ       | 191/615 [01:05<01:58,  3.58it/s] 31%|â–ˆâ–ˆâ–ˆ       | 192/615 [01:06<01:58,  3.58it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 193/615 [01:06<01:57,  3.58it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 194/615 [01:06<01:57,  3.58it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 195/615 [01:07<01:57,  3.58it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 196/615 [01:07<01:57,  3.58it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 197/615 [01:07<01:57,  3.56it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 198/615 [01:07<01:56,  3.57it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 199/615 [01:08<01:56,  3.57it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 200/615 [01:08<01:56,  3.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 201/615 [01:08<01:55,  3.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 202/615 [01:08<01:55,  3.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 203/615 [01:09<01:55,  3.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 204/615 [01:09<01:54,  3.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 205/615 [01:09<01:54,  3.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 206/615 [01:10<01:54,  3.58it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 207/615 [01:10<01:54,  3.58it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 208/615 [01:10<01:54,  3.56it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 209/615 [01:10<01:53,  3.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 210/615 [01:11<01:53,  3.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 211/615 [01:11<01:53,  3.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 212/615 [01:11<01:52,  3.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 213/615 [01:12<01:52,  3.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 214/615 [01:12<01:52,  3.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 215/615 [01:12<01:51,  3.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 216/615 [01:12<01:51,  3.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 217/615 [01:13<01:51,  3.57it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 218/615 [01:13<01:51,  3.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 219/615 [01:13<01:51,  3.56it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 220/615 [01:14<01:50,  3.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 221/615 [01:14<01:50,  3.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 222/615 [01:14<01:49,  3.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 223/615 [01:14<01:49,  3.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 224/615 [01:15<01:49,  3.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 225/615 [01:15<01:49,  3.57it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 226/615 [01:15<01:48,  3.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 227/615 [01:15<01:48,  3.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 228/615 [01:16<01:48,  3.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 229/615 [01:16<01:47,  3.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 230/615 [01:16<01:48,  3.56it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 231/615 [01:17<01:47,  3.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 232/615 [01:17<01:47,  3.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 233/615 [01:17<01:46,  3.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 234/615 [01:17<01:46,  3.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 235/615 [01:18<01:46,  3.58it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 236/615 [01:18<01:46,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 237/615 [01:18<01:45,  3.57it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 238/615 [01:19<01:45,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 239/615 [01:19<01:45,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 240/615 [01:19<01:44,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 241/615 [01:19<01:45,  3.56it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 242/615 [01:20<01:44,  3.56it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 243/615 [01:20<01:44,  3.57it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 244/615 [01:20<01:43,  3.57it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 245/615 [01:21<01:43,  3.57it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 246/615 [01:21<01:43,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 13:11:42,891 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:11:42,892 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:11:42,892 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3412, 'eval_samples_per_second': 372.436, 'eval_steps_per_second': 46.568, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.03it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.44it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.57it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.86it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.49it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.27it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.11it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.77it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.66it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.68it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.62it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:09, 39.42it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:08, 41.36it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:08, 42.80it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:08, 43.96it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 44.77it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 45.26it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:02<00:07, 45.71it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.01it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 45.95it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.12it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.20it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.34it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.47it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.56it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.57it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:03<00:06, 46.61it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.59it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.50it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.43it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.51it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.32it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.46it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.58it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.57it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.64it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.64it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.53it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.54it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.61it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.44it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.49it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.50it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.49it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.63it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.71it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.65it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.55it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.60it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.50it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.48it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.54it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.52it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.53it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:06<00:03, 46.62it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.32it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.44it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.49it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.47it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.48it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.55it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.49it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.52it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.52it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.45it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.54it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.69it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.47it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.57it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.57it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.50it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.57it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.50it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.43it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.47it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.58it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.51it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.52it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.55it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.54it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.55it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.60it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 46.53it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.49it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.53it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.54it/s][A                                                 
                                                 [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 246/615 [01:30<01:43,  3.57it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:11:52,330 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 13:11:52,347 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:11:55,003 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:11:55,015 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:11:55,024 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 247/615 [01:34<24:49,  4.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 248/615 [01:34<17:50,  2.92s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 249/615 [01:34<12:58,  2.13s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 250/615 [01:34<09:33,  1.57s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 251/615 [01:35<07:11,  1.18s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 252/615 [01:35<05:31,  1.10it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 253/615 [01:35<04:21,  1.38it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 254/615 [01:36<03:32,  1.70it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 255/615 [01:36<02:58,  2.01it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/615 [01:36<02:34,  2.32it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/615 [01:36<02:18,  2.59it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/615 [01:37<02:06,  2.83it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/615 [01:37<01:58,  3.01it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/615 [01:37<01:52,  3.16it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/615 [01:38<01:48,  3.28it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 262/615 [01:38<01:45,  3.36it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 263/615 [01:38<01:42,  3.42it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 264/615 [01:38<01:41,  3.47it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 265/615 [01:39<01:40,  3.50it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 266/615 [01:39<01:38,  3.53it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 267/615 [01:39<01:38,  3.54it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 268/615 [01:40<01:37,  3.55it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 269/615 [01:40<01:37,  3.56it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 270/615 [01:40<01:37,  3.56it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 271/615 [01:40<01:36,  3.56it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 272/615 [01:41<01:36,  3.57it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 273/615 [01:41<01:35,  3.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 274/615 [01:41<01:35,  3.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 275/615 [01:41<01:35,  3.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 276/615 [01:42<01:34,  3.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 277/615 [01:42<01:34,  3.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 278/615 [01:42<01:34,  3.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 279/615 [01:43<01:33,  3.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 280/615 [01:43<01:33,  3.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 281/615 [01:43<01:33,  3.56it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 282/615 [01:43<01:33,  3.57it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 283/615 [01:44<01:32,  3.57it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 284/615 [01:44<01:32,  3.57it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 285/615 [01:44<01:32,  3.57it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 286/615 [01:45<01:32,  3.57it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 287/615 [01:45<01:31,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 288/615 [01:45<01:31,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 289/615 [01:45<01:31,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 290/615 [01:46<01:30,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 291/615 [01:46<01:30,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 292/615 [01:46<01:30,  3.57it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 293/615 [01:46<01:30,  3.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 294/615 [01:47<01:29,  3.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 295/615 [01:47<01:29,  3.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 296/615 [01:47<01:29,  3.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 297/615 [01:48<01:28,  3.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 298/615 [01:48<01:28,  3.58it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 299/615 [01:48<01:28,  3.58it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 300/615 [01:48<01:28,  3.58it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 301/615 [01:49<01:27,  3.58it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 302/615 [01:49<01:27,  3.57it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 303/615 [01:49<01:27,  3.55it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 304/615 [01:50<01:27,  3.56it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 305/615 [01:50<01:26,  3.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 306/615 [01:50<01:26,  3.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 307/615 [01:50<01:28,  3.49it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 308/615 [01:51<01:28,  3.49it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 309/615 [01:51<01:27,  3.51it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 310/615 [01:51<01:26,  3.53it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 311/615 [01:52<01:25,  3.55it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 312/615 [01:52<01:25,  3.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 313/615 [01:52<01:24,  3.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 314/615 [01:52<01:24,  3.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 315/615 [01:53<01:24,  3.56it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/615 [01:53<01:23,  3.57it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/615 [01:53<01:23,  3.57it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/615 [01:54<01:23,  3.57it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/615 [01:54<01:22,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/615 [01:54<01:22,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/615 [01:54<01:22,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 322/615 [01:55<01:21,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 323/615 [01:55<01:21,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 324/615 [01:55<01:21,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 325/615 [01:55<01:21,  3.56it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 326/615 [01:56<01:21,  3.57it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 327/615 [01:56<01:20,  3.57it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 328/615 [01:56<01:20,  3.57it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 329/615 [01:57<01:20,  3.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 330/615 [01:57<01:19,  3.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 331/615 [01:57<01:19,  3.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 332/615 [01:57<01:19,  3.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 333/615 [01:58<01:18,  3.57it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 334/615 [01:58<01:18,  3.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 335/615 [01:58<01:18,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 336/615 [01:59<01:18,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 337/615 [01:59<01:17,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 338/615 [01:59<01:17,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 339/615 [01:59<01:17,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 340/615 [02:00<01:16,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 341/615 [02:00<01:16,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 342/615 [02:00<01:16,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 343/615 [02:01<01:16,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 344/615 [02:01<01:15,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 345/615 [02:01<01:15,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 346/615 [02:01<01:15,  3.56it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 347/615 [02:02<01:15,  3.56it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 348/615 [02:02<01:14,  3.56it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 349/615 [02:02<01:14,  3.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 350/615 [02:02<01:14,  3.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 351/615 [02:03<01:13,  3.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 352/615 [02:03<01:13,  3.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 353/615 [02:03<01:13,  3.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 354/615 [02:04<01:12,  3.58it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 355/615 [02:04<01:12,  3.58it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 356/615 [02:04<01:12,  3.58it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 357/615 [02:04<01:12,  3.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 358/615 [02:05<01:11,  3.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 359/615 [02:05<01:11,  3.57it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 360/615 [02:05<01:11,  3.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 361/615 [02:06<01:11,  3.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 362/615 [02:06<01:10,  3.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 363/615 [02:06<01:10,  3.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 364/615 [02:06<01:10,  3.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 365/615 [02:07<01:09,  3.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 366/615 [02:07<01:09,  3.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 367/615 [02:07<01:09,  3.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 368/615 [02:08<01:09,  3.56it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 369/615 [02:08<01:08,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 13:12:29,892 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:12:29,892 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:12:29,892 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.4107, 'eval_samples_per_second': 369.686, 'eval_steps_per_second': 46.224, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.08it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.27it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.60it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.85it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.52it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.28it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.97it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.76it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.73it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.57it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.63it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.69it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.61it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.66it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.71it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.65it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.61it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.60it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.46it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.53it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.56it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.58it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.62it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.56it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.59it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.59it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.56it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.56it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.48it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.54it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.52it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.56it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.60it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.61it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.63it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.63it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.49it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.54it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.46it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.51it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.59it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.63it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.61it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.60it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.51it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.58it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.39it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.37it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.45it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.45it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.53it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.63it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.58it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.61it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.59it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.55it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.55it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.59it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.47it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.60it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.61it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.58it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.58it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.56it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.50it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.57it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.47it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.51it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.48it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.58it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.58it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.54it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.51it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.57it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.60it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.58it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.53it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.47it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.52it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.59it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.60it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.59it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.56it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.52it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.61it/s][A                                                 
                                                 [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 369/615 [02:17<01:08,  3.57it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.61it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:12:39,250 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-28 13:12:39,271 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:12:41,574 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:12:41,589 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:12:41,596 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 370/615 [02:20<15:57,  3.91s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 371/615 [02:20<11:27,  2.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 372/615 [02:21<08:19,  2.06s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 373/615 [02:21<06:08,  1.52s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 374/615 [02:21<04:37,  1.15s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 375/615 [02:22<03:33,  1.13it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 376/615 [02:22<02:48,  1.42it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/615 [02:22<02:17,  1.73it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/615 [02:22<01:55,  2.05it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/615 [02:23<01:40,  2.35it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/615 [02:23<01:29,  2.62it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 381/615 [02:23<01:22,  2.85it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 382/615 [02:24<01:16,  3.03it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 383/615 [02:24<01:12,  3.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/615 [02:24<01:10,  3.29it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 385/615 [02:24<01:08,  3.37it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 386/615 [02:25<01:06,  3.43it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 387/615 [02:25<01:05,  3.47it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 388/615 [02:25<01:04,  3.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 389/615 [02:25<01:04,  3.53it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 390/615 [02:26<01:03,  3.54it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 391/615 [02:26<01:03,  3.55it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 392/615 [02:26<01:02,  3.56it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 393/615 [02:27<01:02,  3.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 394/615 [02:27<01:01,  3.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 395/615 [02:27<01:01,  3.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 396/615 [02:27<01:01,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 397/615 [02:28<01:00,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 398/615 [02:28<01:00,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 399/615 [02:28<01:00,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 400/615 [02:29<01:00,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 401/615 [02:29<00:59,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 402/615 [02:29<00:59,  3.57it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 403/615 [02:29<00:59,  3.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 404/615 [02:30<00:58,  3.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 405/615 [02:30<00:58,  3.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 406/615 [02:30<00:58,  3.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 407/615 [02:31<00:58,  3.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 408/615 [02:31<00:57,  3.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 409/615 [02:31<00:57,  3.59it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 410/615 [02:31<00:57,  3.59it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 411/615 [02:32<00:56,  3.59it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 412/615 [02:32<00:56,  3.59it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 413/615 [02:32<00:56,  3.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 414/615 [02:32<00:56,  3.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 415/615 [02:33<00:55,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 416/615 [02:33<00:55,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 417/615 [02:33<00:55,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 418/615 [02:34<00:55,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 419/615 [02:34<00:54,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 420/615 [02:34<00:54,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 421/615 [02:34<00:54,  3.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 422/615 [02:35<00:53,  3.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 423/615 [02:35<00:53,  3.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 424/615 [02:35<00:53,  3.57it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 425/615 [02:36<00:53,  3.57it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 426/615 [02:36<00:52,  3.57it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 427/615 [02:36<00:52,  3.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 428/615 [02:36<00:52,  3.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 429/615 [02:37<00:52,  3.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 430/615 [02:37<00:51,  3.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 431/615 [02:37<00:51,  3.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 432/615 [02:37<00:51,  3.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 433/615 [02:38<00:50,  3.58it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 434/615 [02:38<00:50,  3.58it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 435/615 [02:38<00:50,  3.57it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 436/615 [02:39<00:50,  3.57it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 437/615 [02:39<00:49,  3.57it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 438/615 [02:39<00:49,  3.58it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 439/615 [02:39<00:49,  3.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/615 [02:40<00:48,  3.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 441/615 [02:40<00:48,  3.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/615 [02:40<00:48,  3.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 443/615 [02:41<00:48,  3.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/615 [02:41<00:47,  3.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 445/615 [02:41<00:47,  3.58it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 446/615 [02:41<00:47,  3.56it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 447/615 [02:42<00:47,  3.57it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 448/615 [02:42<00:46,  3.57it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 449/615 [02:42<00:46,  3.57it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 450/615 [02:43<00:46,  3.57it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 451/615 [02:43<00:45,  3.58it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 452/615 [02:43<00:45,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 453/615 [02:43<00:45,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 454/615 [02:44<00:45,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 455/615 [02:44<00:44,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 456/615 [02:44<00:44,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 457/615 [02:44<00:44,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 458/615 [02:45<00:43,  3.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 459/615 [02:45<00:43,  3.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 460/615 [02:45<00:43,  3.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 461/615 [02:46<00:43,  3.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 462/615 [02:46<00:42,  3.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 463/615 [02:46<00:42,  3.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 464/615 [02:46<00:42,  3.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 465/615 [02:47<00:41,  3.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 466/615 [02:47<00:41,  3.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 467/615 [02:47<00:41,  3.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 468/615 [02:48<00:41,  3.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 469/615 [02:48<00:40,  3.58it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 470/615 [02:48<00:40,  3.58it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 471/615 [02:48<00:40,  3.58it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 472/615 [02:49<00:39,  3.58it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 473/615 [02:49<00:39,  3.58it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 474/615 [02:49<00:39,  3.57it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 475/615 [02:50<00:39,  3.58it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 476/615 [02:50<00:38,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 477/615 [02:50<00:38,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 478/615 [02:50<00:38,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 479/615 [02:51<00:38,  3.51it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 480/615 [02:51<00:38,  3.51it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 481/615 [02:51<00:37,  3.53it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 482/615 [02:51<00:37,  3.55it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 483/615 [02:52<00:37,  3.56it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 484/615 [02:52<00:36,  3.57it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 485/615 [02:52<00:36,  3.57it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 486/615 [02:53<00:36,  3.54it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 487/615 [02:53<00:36,  3.55it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 488/615 [02:53<00:35,  3.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 489/615 [02:53<00:35,  3.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 490/615 [02:54<00:34,  3.57it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 491/615 [02:54<00:34,  3.58it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 492/615 [02:54<00:34,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 13:13:16,388 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:13:16,389 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:13:16,389 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3435, 'eval_samples_per_second': 372.344, 'eval_steps_per_second': 46.556, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.74it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.32it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.51it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.88it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.48it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.27it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.03it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.67it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.63it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.56it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.58it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.66it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.60it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.63it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.67it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.51it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.46it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.43it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.43it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.48it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.56it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.54it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.53it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.58it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.67it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.63it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.59it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.52it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.52it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.57it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.58it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.54it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.59it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.58it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.58it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.63it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.52it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.50it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.57it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:05, 46.23it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.60it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.58it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.63it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.63it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.54it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.55it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.50it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.53it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.55it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.52it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.53it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.54it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.57it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.62it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.64it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.48it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.51it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.52it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.48it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.56it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.58it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.49it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.56it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.64it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.47it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.50it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.45it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.44it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.54it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.53it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.48it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.49it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.54it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.58it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.58it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.59it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.49it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.52it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.56it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.55it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.50it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.48it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.47it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.51it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.52it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.50it/s][A                                                 
                                                 [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 492/615 [03:04<00:34,  3.58it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:13:25,759 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 13:13:25,778 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:13:28,377 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:13:28,391 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:13:28,398 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 493/615 [03:07<08:07,  3.99s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 494/615 [03:07<05:48,  2.88s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 495/615 [03:08<04:11,  2.10s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 496/615 [03:08<03:04,  1.55s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 497/615 [03:08<02:18,  1.17s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 498/615 [03:08<01:45,  1.11it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 499/615 [03:09<01:23,  1.40it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 500/615 [03:09<01:07,  1.71it/s]                                                  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 500/615 [03:09<01:07,  1.71it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 501/615 [03:09<00:56,  2.03it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 502/615 [03:09<00:48,  2.33it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/615 [03:10<00:43,  2.60it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 504/615 [03:10<00:39,  2.83it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/615 [03:10<00:36,  3.02it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 506/615 [03:11<00:34,  3.17it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 507/615 [03:11<00:32,  3.28it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 508/615 [03:11<00:31,  3.37it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 509/615 [03:11<00:30,  3.43it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 510/615 [03:12<00:30,  3.47it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 511/615 [03:12<00:29,  3.50it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 512/615 [03:12<00:29,  3.52it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 513/615 [03:13<00:28,  3.54it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 514/615 [03:13<00:28,  3.54it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 515/615 [03:13<00:28,  3.56it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 516/615 [03:13<00:27,  3.56it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 517/615 [03:14<00:27,  3.57it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 518/615 [03:14<00:27,  3.57it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 519/615 [03:14<00:26,  3.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 520/615 [03:15<00:26,  3.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 521/615 [03:15<00:26,  3.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 522/615 [03:15<00:26,  3.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 523/615 [03:15<00:25,  3.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 524/615 [03:16<00:25,  3.57it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 525/615 [03:16<00:25,  3.56it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 526/615 [03:16<00:24,  3.57it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 527/615 [03:16<00:24,  3.57it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 528/615 [03:17<00:24,  3.57it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 529/615 [03:17<00:24,  3.57it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 530/615 [03:17<00:23,  3.57it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 531/615 [03:18<00:23,  3.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 532/615 [03:18<00:23,  3.58it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 533/615 [03:18<00:22,  3.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 534/615 [03:18<00:22,  3.58it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 535/615 [03:19<00:22,  3.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 536/615 [03:19<00:22,  3.56it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 537/615 [03:19<00:21,  3.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 538/615 [03:20<00:21,  3.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 539/615 [03:20<00:21,  3.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 540/615 [03:20<00:20,  3.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 541/615 [03:20<00:20,  3.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 542/615 [03:21<00:20,  3.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 543/615 [03:21<00:20,  3.58it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 544/615 [03:21<00:19,  3.58it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 545/615 [03:21<00:19,  3.58it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 546/615 [03:22<00:19,  3.58it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 547/615 [03:22<00:19,  3.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 548/615 [03:22<00:18,  3.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 549/615 [03:23<00:18,  3.57it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 550/615 [03:23<00:18,  3.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 551/615 [03:23<00:17,  3.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 552/615 [03:23<00:17,  3.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 553/615 [03:24<00:17,  3.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 554/615 [03:24<00:17,  3.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 555/615 [03:24<00:16,  3.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 556/615 [03:25<00:16,  3.58it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 557/615 [03:25<00:16,  3.58it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 558/615 [03:25<00:15,  3.57it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 559/615 [03:25<00:15,  3.57it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 560/615 [03:26<00:15,  3.57it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 561/615 [03:26<00:15,  3.57it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 562/615 [03:26<00:14,  3.57it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/615 [03:27<00:14,  3.58it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 564/615 [03:27<00:14,  3.58it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 565/615 [03:27<00:13,  3.57it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 566/615 [03:27<00:13,  3.58it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 567/615 [03:28<00:13,  3.57it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 568/615 [03:28<00:13,  3.57it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 569/615 [03:28<00:12,  3.56it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 570/615 [03:28<00:12,  3.57it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 571/615 [03:29<00:12,  3.57it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 572/615 [03:29<00:12,  3.57it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 573/615 [03:29<00:11,  3.57it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 574/615 [03:30<00:11,  3.57it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 575/615 [03:30<00:11,  3.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 576/615 [03:30<00:10,  3.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 577/615 [03:30<00:10,  3.58it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 578/615 [03:31<00:10,  3.58it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 579/615 [03:31<00:10,  3.57it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 580/615 [03:31<00:09,  3.58it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 581/615 [03:32<00:09,  3.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 582/615 [03:32<00:09,  3.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 583/615 [03:32<00:08,  3.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 584/615 [03:32<00:08,  3.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 585/615 [03:33<00:08,  3.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 586/615 [03:33<00:08,  3.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 587/615 [03:33<00:07,  3.58it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 588/615 [03:34<00:07,  3.56it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 589/615 [03:34<00:07,  3.57it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 590/615 [03:34<00:06,  3.57it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 591/615 [03:34<00:06,  3.57it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 592/615 [03:35<00:06,  3.57it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 593/615 [03:35<00:06,  3.57it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 594/615 [03:35<00:05,  3.57it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 595/615 [03:35<00:05,  3.57it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 596/615 [03:36<00:05,  3.57it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 597/615 [03:36<00:05,  3.57it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 598/615 [03:36<00:04,  3.57it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 599/615 [03:37<00:04,  3.54it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 600/615 [03:37<00:04,  3.56it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 601/615 [03:37<00:03,  3.57it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 602/615 [03:37<00:03,  3.57it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 603/615 [03:38<00:03,  3.57it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 604/615 [03:38<00:03,  3.58it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 605/615 [03:38<00:02,  3.58it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 606/615 [03:39<00:02,  3.58it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 607/615 [03:39<00:02,  3.58it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 608/615 [03:39<00:01,  3.58it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 609/615 [03:39<00:01,  3.58it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 610/615 [03:40<00:01,  3.57it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 611/615 [03:40<00:01,  3.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 612/615 [03:40<00:00,  3.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 613/615 [03:41<00:00,  3.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 614/615 [03:41<00:00,  3.57it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [03:41<00:00,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 13:14:03,073 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:14:03,073 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:14:03,073 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3475, 'eval_samples_per_second': 372.185, 'eval_steps_per_second': 46.537, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.53it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.36it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.50it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.92it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.52it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.18it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.06it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.83it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.68it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.58it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.62it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.59it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:08, 45.71it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.04it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.28it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.36it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.16it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.56it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.58it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.49it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.46it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.47it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.50it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.53it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.62it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.65it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.59it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.57it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.62it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.65it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.46it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.45it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.49it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.56it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.62it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.59it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.53it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.57it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.56it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.53it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.56it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.55it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.47it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.54it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.48it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.54it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.57it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.52it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.53it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.48it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.51it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.51it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.58it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.48it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.53it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.57it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.53it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.55it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.53it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.58it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.58it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.58it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.51it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.48it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.55it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.45it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.49it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.59it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.53it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.56it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.53it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.54it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.61it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.57it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.45it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.52it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.55it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.55it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.56it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.49it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.54it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.55it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.60it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.59it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.52it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.49it/s][A                                                 
                                                 [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [03:50<00:00,  3.58it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:14:12,439 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-28 13:14:12,452 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:14:14,742 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:14:14,755 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:14:14,763 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:14:15,050 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:14:15,051 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123 (score: 1.0038264989852905).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [03:55<00:00,  3.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [03:55<00:00,  2.61it/s]
[INFO|trainer.py:1894] 2023-08-28 13:14:16,674 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 13:14:16,692 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:14:19,082 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:14:19,096 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:14:19,106 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:14:19,299 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:19,299 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:19,299 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:19,299 >>   train_runtime            = 0:03:55.19
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:19,299 >>   train_samples            =       7899
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:19,300 >>   train_samples_per_second =    167.927
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:19,300 >>   train_steps_per_second   =      2.615
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3523, 'eval_samples_per_second': 371.994, 'eval_steps_per_second': 46.513, 'epoch': 5.0}
{'train_runtime': 235.1916, 'train_samples_per_second': 167.927, 'train_steps_per_second': 2.615, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 13:14:19 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:14:19,338 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:14:19,338 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 13:14:19,338 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 57.32it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 50.82it/s]  4%|â–         | 18/435 [00:00<00:08, 48.92it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.06it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.67it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.45it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.15it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.08it/s] 11%|â–ˆ         | 48/435 [00:01<00:08, 46.87it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.78it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.74it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.75it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.82it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.71it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.71it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.70it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.76it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.75it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.71it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.70it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.71it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.77it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.80it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.75it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.67it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.64it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.70it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.71it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.69it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.64it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.65it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.69it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 45.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 45.84it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.03it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.39it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.53it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.63it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.58it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.60it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.66it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.62it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.62it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.66it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.63it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.65it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.70it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.70it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.61it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.66it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.69it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.70it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.65it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.70it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.62it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.71it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.63it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.61it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.67it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.65it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.70it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.75it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.63it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.64it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.66it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.62it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.64it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.59it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.55it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.67it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.61it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.58it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.63it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.52it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.51it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.57it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.65it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.60it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.71it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:14:28,677 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:28,678 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:28,678 >>   eval_loss               =     1.0038
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:28,678 >>   eval_runtime            = 0:00:09.33
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:28,678 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:28,678 >>   eval_samples_per_second =    372.517
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:28,678 >>   eval_steps_per_second   =     46.578
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:14:28,678 >>   perplexity              =     2.7287
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:35,241 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:35,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:35,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:35,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:35,246 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:14:35,870 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:14:35,871 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:14:36,441 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:14:37,488 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:14:37,488 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:40,363 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:40,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:40,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:40,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:14:40,372 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:14:41,011 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:14:41,012 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:14:41,586 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:14:41,746 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:14:41,746 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-369
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-123
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-615
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/checkpoint-492
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.22it/s]Extractor Predicting: 3it [00:02,  1.26it/s]Extractor Predicting: 4it [00:03,  1.26it/s]Extractor Predicting: 5it [00:03,  1.26it/s]Extractor Predicting: 6it [00:04,  1.25it/s]Extractor Predicting: 7it [00:05,  1.26it/s]Extractor Predicting: 8it [00:06,  1.27it/s]Extractor Predicting: 9it [00:07,  1.27it/s]Extractor Predicting: 10it [00:07,  1.31it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:09,  1.26it/s]Extractor Predicting: 13it [00:10,  1.26it/s]Extractor Predicting: 14it [00:11,  1.25it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.27it/s]Extractor Predicting: 17it [00:13,  1.30it/s]Extractor Predicting: 18it [00:14,  1.33it/s]Extractor Predicting: 19it [00:14,  1.30it/s]Extractor Predicting: 20it [00:15,  1.31it/s]Extractor Predicting: 21it [00:16,  1.31it/s]Extractor Predicting: 22it [00:17,  1.33it/s]Extractor Predicting: 23it [00:17,  1.32it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.27it/s]Extractor Predicting: 26it [00:20,  1.25it/s]Extractor Predicting: 27it [00:21,  1.25it/s]Extractor Predicting: 28it [00:21,  1.29it/s]Extractor Predicting: 29it [00:22,  1.27it/s]Extractor Predicting: 30it [00:23,  1.24it/s]Extractor Predicting: 31it [00:24,  1.24it/s]Extractor Predicting: 32it [00:25,  1.25it/s]Extractor Predicting: 33it [00:25,  1.25it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:27,  1.29it/s]Extractor Predicting: 36it [00:28,  1.29it/s]Extractor Predicting: 37it [00:28,  1.29it/s]Extractor Predicting: 38it [00:29,  1.30it/s]Extractor Predicting: 39it [00:30,  1.31it/s]Extractor Predicting: 40it [00:31,  1.29it/s]Extractor Predicting: 41it [00:32,  1.30it/s]Extractor Predicting: 42it [00:32,  1.30it/s]Extractor Predicting: 43it [00:33,  1.32it/s]Extractor Predicting: 44it [00:34,  1.34it/s]Extractor Predicting: 45it [00:35,  1.23it/s]Extractor Predicting: 46it [00:35,  1.25it/s]Extractor Predicting: 47it [00:36,  1.26it/s]Extractor Predicting: 48it [00:37,  1.28it/s]Extractor Predicting: 49it [00:38,  1.29it/s]Extractor Predicting: 50it [00:39,  1.30it/s]Extractor Predicting: 51it [00:39,  1.32it/s]Extractor Predicting: 52it [00:40,  1.31it/s]Extractor Predicting: 53it [00:41,  1.32it/s]Extractor Predicting: 54it [00:42,  1.31it/s]Extractor Predicting: 55it [00:42,  1.28it/s]Extractor Predicting: 56it [00:43,  1.31it/s]Extractor Predicting: 57it [00:44,  1.29it/s]Extractor Predicting: 58it [00:45,  1.28it/s]Extractor Predicting: 59it [00:45,  1.32it/s]Extractor Predicting: 60it [00:46,  1.35it/s]Extractor Predicting: 61it [00:47,  1.37it/s]Extractor Predicting: 62it [00:48,  1.34it/s]Extractor Predicting: 63it [00:48,  1.31it/s]Extractor Predicting: 64it [00:49,  1.32it/s]Extractor Predicting: 65it [00:50,  1.33it/s]Extractor Predicting: 66it [00:51,  1.33it/s]Extractor Predicting: 67it [00:51,  1.34it/s]Extractor Predicting: 68it [00:52,  1.34it/s]Extractor Predicting: 69it [00:53,  1.38it/s]Extractor Predicting: 70it [00:54,  1.37it/s]Extractor Predicting: 71it [00:54,  1.38it/s]Extractor Predicting: 72it [00:55,  1.34it/s]Extractor Predicting: 73it [00:56,  1.35it/s]Extractor Predicting: 74it [00:57,  1.34it/s]Extractor Predicting: 75it [00:57,  1.31it/s]Extractor Predicting: 76it [00:58,  1.30it/s]Extractor Predicting: 77it [00:59,  1.32it/s]Extractor Predicting: 78it [01:00,  1.34it/s]Extractor Predicting: 79it [01:00,  1.36it/s]Extractor Predicting: 80it [01:01,  1.34it/s]Extractor Predicting: 81it [01:02,  1.33it/s]Extractor Predicting: 82it [01:03,  1.33it/s]Extractor Predicting: 83it [01:03,  1.34it/s]Extractor Predicting: 84it [01:04,  1.35it/s]Extractor Predicting: 85it [01:05,  1.36it/s]Extractor Predicting: 86it [01:05,  1.37it/s]Extractor Predicting: 87it [01:06,  1.39it/s]Extractor Predicting: 88it [01:07,  1.38it/s]Extractor Predicting: 89it [01:08,  1.38it/s]Extractor Predicting: 90it [01:08,  1.39it/s]Extractor Predicting: 91it [01:09,  1.41it/s]Extractor Predicting: 92it [01:10,  1.44it/s]Extractor Predicting: 93it [01:10,  1.41it/s]Extractor Predicting: 94it [01:11,  1.40it/s]Extractor Predicting: 95it [01:12,  1.41it/s]Extractor Predicting: 96it [01:13,  1.40it/s]Extractor Predicting: 97it [01:13,  1.40it/s]Extractor Predicting: 98it [01:14,  1.39it/s]Extractor Predicting: 99it [01:15,  1.37it/s]Extractor Predicting: 100it [01:16,  1.34it/s]Extractor Predicting: 101it [01:16,  1.34it/s]Extractor Predicting: 102it [01:17,  1.42it/s]Extractor Predicting: 103it [01:18,  1.43it/s]Extractor Predicting: 104it [01:18,  1.42it/s]Extractor Predicting: 105it [01:19,  1.41it/s]Extractor Predicting: 106it [01:20,  1.41it/s]Extractor Predicting: 107it [01:20,  1.40it/s]Extractor Predicting: 108it [01:21,  1.40it/s]Extractor Predicting: 109it [01:22,  1.41it/s]Extractor Predicting: 110it [01:23,  1.41it/s]Extractor Predicting: 111it [01:23,  1.41it/s]Extractor Predicting: 112it [01:24,  1.43it/s]Extractor Predicting: 113it [01:25,  1.46it/s]Extractor Predicting: 114it [01:25,  1.47it/s]Extractor Predicting: 115it [01:26,  1.47it/s]Extractor Predicting: 116it [01:27,  1.43it/s]Extractor Predicting: 117it [01:27,  1.39it/s]Extractor Predicting: 118it [01:28,  1.40it/s]Extractor Predicting: 119it [01:29,  1.36it/s]Extractor Predicting: 120it [01:30,  1.38it/s]Extractor Predicting: 121it [01:31,  1.26it/s]Extractor Predicting: 122it [01:31,  1.26it/s]Extractor Predicting: 123it [01:32,  1.29it/s]Extractor Predicting: 124it [01:33,  1.31it/s]Extractor Predicting: 125it [01:34,  1.34it/s]Extractor Predicting: 126it [01:34,  1.31it/s]Extractor Predicting: 127it [01:35,  1.35it/s]Extractor Predicting: 128it [01:36,  1.35it/s]Extractor Predicting: 129it [01:37,  1.34it/s]Extractor Predicting: 130it [01:37,  1.32it/s]Extractor Predicting: 131it [01:38,  1.35it/s]Extractor Predicting: 132it [01:39,  1.33it/s]Extractor Predicting: 133it [01:40,  1.31it/s]Extractor Predicting: 134it [01:40,  1.30it/s]Extractor Predicting: 135it [01:41,  1.32it/s]Extractor Predicting: 136it [01:42,  1.31it/s]Extractor Predicting: 137it [01:43,  1.31it/s]Extractor Predicting: 138it [01:43,  1.29it/s]Extractor Predicting: 139it [01:44,  1.30it/s]Extractor Predicting: 140it [01:45,  1.28it/s]Extractor Predicting: 141it [01:46,  1.30it/s]Extractor Predicting: 142it [01:47,  1.29it/s]Extractor Predicting: 143it [01:47,  1.30it/s]Extractor Predicting: 144it [01:48,  1.61it/s]Extractor Predicting: 144it [01:48,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:36,637 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:36,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:36,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:36,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:36,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:16:37,316 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:16:37,317 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:16:37,898 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:16:38,935 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:16:38,935 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:41,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:41,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:41,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:41,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:16:41,801 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:16:42,450 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:16:42,451 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:16:43,047 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:16:43,201 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:16:43,201 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.25it/s]Extractor Predicting: 3it [00:02,  1.30it/s]Extractor Predicting: 4it [00:03,  1.33it/s]Extractor Predicting: 5it [00:03,  1.28it/s]Extractor Predicting: 6it [00:04,  1.29it/s]Extractor Predicting: 7it [00:05,  1.34it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.40it/s]Extractor Predicting: 11it [00:08,  1.37it/s]Extractor Predicting: 12it [00:08,  1.37it/s]Extractor Predicting: 13it [00:09,  1.37it/s]Extractor Predicting: 14it [00:10,  1.39it/s]Extractor Predicting: 15it [00:11,  1.40it/s]Extractor Predicting: 16it [00:11,  1.40it/s]Extractor Predicting: 17it [00:12,  1.38it/s]Extractor Predicting: 18it [00:13,  1.41it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:14,  1.40it/s]Extractor Predicting: 21it [00:15,  1.41it/s]Extractor Predicting: 22it [00:15,  1.42it/s]Extractor Predicting: 23it [00:16,  1.42it/s]Extractor Predicting: 24it [00:17,  1.41it/s]Extractor Predicting: 25it [00:18,  1.40it/s]Extractor Predicting: 26it [00:18,  1.41it/s]Extractor Predicting: 27it [00:19,  1.39it/s]Extractor Predicting: 28it [00:20,  1.40it/s]Extractor Predicting: 29it [00:20,  1.40it/s]Extractor Predicting: 30it [00:21,  1.42it/s]Extractor Predicting: 31it [00:22,  1.38it/s]Extractor Predicting: 32it [00:23,  1.42it/s]Extractor Predicting: 33it [00:23,  1.41it/s]Extractor Predicting: 34it [00:24,  1.38it/s]Extractor Predicting: 35it [00:25,  1.38it/s]Extractor Predicting: 36it [00:26,  1.40it/s]Extractor Predicting: 37it [00:26,  1.42it/s]Extractor Predicting: 38it [00:27,  1.43it/s]Extractor Predicting: 39it [00:28,  1.41it/s]Extractor Predicting: 40it [00:28,  1.39it/s]Extractor Predicting: 41it [00:29,  1.39it/s]Extractor Predicting: 42it [00:30,  1.43it/s]Extractor Predicting: 43it [00:30,  1.38it/s]Extractor Predicting: 44it [00:31,  1.38it/s]Extractor Predicting: 45it [00:32,  1.38it/s]Extractor Predicting: 46it [00:33,  1.36it/s]Extractor Predicting: 47it [00:33,  1.39it/s]Extractor Predicting: 48it [00:34,  1.37it/s]Extractor Predicting: 49it [00:35,  1.40it/s]Extractor Predicting: 50it [00:36,  1.39it/s]Extractor Predicting: 51it [00:36,  1.38it/s]Extractor Predicting: 52it [00:37,  1.36it/s]Extractor Predicting: 53it [00:38,  1.36it/s]Extractor Predicting: 54it [00:39,  1.35it/s]Extractor Predicting: 55it [00:39,  1.38it/s]Extractor Predicting: 56it [00:40,  1.37it/s]Extractor Predicting: 57it [00:41,  1.35it/s]Extractor Predicting: 58it [00:41,  1.35it/s]Extractor Predicting: 59it [00:42,  1.34it/s]Extractor Predicting: 60it [00:43,  1.33it/s]Extractor Predicting: 61it [00:44,  1.32it/s]Extractor Predicting: 62it [00:44,  1.35it/s]Extractor Predicting: 63it [00:45,  1.35it/s]Extractor Predicting: 64it [00:46,  1.38it/s]Extractor Predicting: 65it [00:47,  1.36it/s]Extractor Predicting: 66it [00:47,  1.33it/s]Extractor Predicting: 67it [00:48,  1.33it/s]Extractor Predicting: 68it [00:49,  1.33it/s]Extractor Predicting: 69it [00:50,  1.32it/s]Extractor Predicting: 70it [00:50,  1.33it/s]Extractor Predicting: 71it [00:51,  1.33it/s]Extractor Predicting: 72it [00:52,  1.32it/s]Extractor Predicting: 73it [00:53,  1.34it/s]Extractor Predicting: 74it [00:53,  1.34it/s]Extractor Predicting: 75it [00:54,  1.36it/s]Extractor Predicting: 76it [00:55,  1.34it/s]Extractor Predicting: 77it [00:56,  1.38it/s]Extractor Predicting: 78it [00:56,  1.37it/s]Extractor Predicting: 79it [00:57,  1.41it/s]Extractor Predicting: 80it [00:58,  1.43it/s]Extractor Predicting: 81it [00:58,  1.43it/s]Extractor Predicting: 82it [00:59,  1.42it/s]Extractor Predicting: 83it [01:00,  1.37it/s]Extractor Predicting: 84it [01:01,  1.36it/s]Extractor Predicting: 85it [01:01,  1.33it/s]Extractor Predicting: 86it [01:02,  1.31it/s]Extractor Predicting: 87it [01:03,  1.32it/s]Extractor Predicting: 88it [01:04,  1.32it/s]Extractor Predicting: 89it [01:05,  1.30it/s]Extractor Predicting: 90it [01:05,  1.31it/s]Extractor Predicting: 91it [01:06,  1.27it/s]Extractor Predicting: 92it [01:07,  1.30it/s]Extractor Predicting: 93it [01:08,  1.33it/s]Extractor Predicting: 94it [01:08,  1.34it/s]Extractor Predicting: 95it [01:09,  1.35it/s]Extractor Predicting: 96it [01:10,  1.32it/s]Extractor Predicting: 97it [01:11,  1.30it/s]Extractor Predicting: 98it [01:11,  1.29it/s]Extractor Predicting: 99it [01:12,  1.32it/s]Extractor Predicting: 100it [01:13,  1.33it/s]Extractor Predicting: 101it [01:14,  1.37it/s]Extractor Predicting: 102it [01:14,  1.34it/s]Extractor Predicting: 103it [01:15,  1.32it/s]Extractor Predicting: 104it [01:16,  1.24it/s]Extractor Predicting: 105it [01:17,  1.25it/s]Extractor Predicting: 106it [01:18,  1.27it/s]Extractor Predicting: 107it [01:18,  1.31it/s]Extractor Predicting: 108it [01:19,  1.29it/s]Extractor Predicting: 109it [01:20,  1.30it/s]Extractor Predicting: 110it [01:21,  1.30it/s]Extractor Predicting: 111it [01:21,  1.32it/s]Extractor Predicting: 112it [01:22,  1.29it/s]Extractor Predicting: 113it [01:23,  1.31it/s]Extractor Predicting: 114it [01:24,  1.32it/s]Extractor Predicting: 115it [01:24,  1.28it/s]Extractor Predicting: 116it [01:25,  1.31it/s]Extractor Predicting: 117it [01:26,  1.31it/s]Extractor Predicting: 118it [01:27,  1.31it/s]Extractor Predicting: 119it [01:27,  1.30it/s]Extractor Predicting: 120it [01:28,  1.33it/s]Extractor Predicting: 121it [01:29,  1.34it/s]Extractor Predicting: 122it [01:30,  1.35it/s]Extractor Predicting: 123it [01:30,  1.34it/s]Extractor Predicting: 124it [01:31,  1.35it/s]Extractor Predicting: 125it [01:32,  1.38it/s]Extractor Predicting: 126it [01:33,  1.38it/s]Extractor Predicting: 127it [01:33,  1.37it/s]Extractor Predicting: 128it [01:34,  1.40it/s]Extractor Predicting: 129it [01:35,  1.40it/s]Extractor Predicting: 130it [01:35,  1.37it/s]Extractor Predicting: 131it [01:36,  1.41it/s]Extractor Predicting: 132it [01:37,  1.41it/s]Extractor Predicting: 133it [01:38,  1.43it/s]Extractor Predicting: 134it [01:38,  1.38it/s]Extractor Predicting: 135it [01:39,  1.41it/s]Extractor Predicting: 136it [01:40,  1.42it/s]Extractor Predicting: 137it [01:40,  1.43it/s]Extractor Predicting: 138it [01:41,  1.42it/s]Extractor Predicting: 139it [01:42,  1.41it/s]Extractor Predicting: 140it [01:42,  1.41it/s]Extractor Predicting: 141it [01:43,  1.38it/s]Extractor Predicting: 142it [01:44,  1.37it/s]Extractor Predicting: 143it [01:45,  1.39it/s]Extractor Predicting: 144it [01:45,  1.35it/s]Extractor Predicting: 145it [01:46,  1.41it/s]Extractor Predicting: 146it [01:47,  1.43it/s]Extractor Predicting: 147it [01:47,  1.43it/s]Extractor Predicting: 148it [01:48,  1.46it/s]Extractor Predicting: 149it [01:49,  1.43it/s]Extractor Predicting: 150it [01:50,  1.45it/s]Extractor Predicting: 151it [01:50,  1.46it/s]Extractor Predicting: 152it [01:51,  1.48it/s]Extractor Predicting: 153it [01:52,  1.46it/s]Extractor Predicting: 154it [01:52,  1.43it/s]Extractor Predicting: 155it [01:53,  1.48it/s]Extractor Predicting: 156it [01:54,  1.47it/s]Extractor Predicting: 157it [01:54,  1.54it/s]Extractor Predicting: 158it [01:55,  1.56it/s]Extractor Predicting: 159it [01:56,  1.52it/s]Extractor Predicting: 160it [01:56,  1.47it/s]Extractor Predicting: 161it [01:57,  1.46it/s]Extractor Predicting: 162it [01:58,  1.45it/s]Extractor Predicting: 163it [01:58,  1.48it/s]Extractor Predicting: 164it [01:59,  1.48it/s]Extractor Predicting: 165it [02:00,  1.49it/s]Extractor Predicting: 166it [02:00,  1.46it/s]Extractor Predicting: 167it [02:01,  1.49it/s]Extractor Predicting: 168it [02:02,  1.48it/s]Extractor Predicting: 169it [02:02,  1.54it/s]Extractor Predicting: 170it [02:03,  1.52it/s]Extractor Predicting: 171it [02:04,  1.52it/s]Extractor Predicting: 172it [02:04,  1.46it/s]Extractor Predicting: 173it [02:05,  1.44it/s]Extractor Predicting: 174it [02:06,  1.40it/s]Extractor Predicting: 175it [02:07,  1.35it/s]Extractor Predicting: 176it [02:07,  1.35it/s]Extractor Predicting: 177it [02:08,  1.36it/s]Extractor Predicting: 178it [02:09,  1.34it/s]Extractor Predicting: 179it [02:10,  1.35it/s]Extractor Predicting: 180it [02:10,  1.36it/s]Extractor Predicting: 181it [02:11,  1.36it/s]Extractor Predicting: 182it [02:12,  1.35it/s]Extractor Predicting: 183it [02:13,  1.35it/s]Extractor Predicting: 184it [02:13,  1.34it/s]Extractor Predicting: 185it [02:14,  1.32it/s]Extractor Predicting: 186it [02:15,  1.32it/s]Extractor Predicting: 187it [02:16,  1.33it/s]Extractor Predicting: 188it [02:16,  1.33it/s]Extractor Predicting: 189it [02:17,  1.33it/s]Extractor Predicting: 190it [02:18,  1.35it/s]Extractor Predicting: 191it [02:19,  1.31it/s]Extractor Predicting: 192it [02:19,  1.30it/s]Extractor Predicting: 193it [02:20,  1.31it/s]Extractor Predicting: 194it [02:21,  1.31it/s]Extractor Predicting: 195it [02:22,  1.32it/s]Extractor Predicting: 196it [02:22,  1.33it/s]Extractor Predicting: 197it [02:23,  1.32it/s]Extractor Predicting: 198it [02:24,  1.34it/s]Extractor Predicting: 199it [02:25,  1.36it/s]Extractor Predicting: 200it [02:25,  1.33it/s]Extractor Predicting: 201it [02:26,  1.31it/s]Extractor Predicting: 202it [02:27,  1.39it/s]Extractor Predicting: 203it [02:28,  1.37it/s]Extractor Predicting: 204it [02:28,  1.38it/s]Extractor Predicting: 205it [02:29,  1.37it/s]Extractor Predicting: 206it [02:30,  1.24it/s]Extractor Predicting: 207it [02:31,  1.25it/s]Extractor Predicting: 208it [02:32,  1.27it/s]Extractor Predicting: 209it [02:32,  1.24it/s]Extractor Predicting: 210it [02:33,  1.27it/s]Extractor Predicting: 211it [02:34,  1.27it/s]Extractor Predicting: 212it [02:35,  1.27it/s]Extractor Predicting: 213it [02:35,  1.28it/s]Extractor Predicting: 214it [02:36,  1.27it/s]Extractor Predicting: 215it [02:37,  1.30it/s]Extractor Predicting: 216it [02:38,  1.27it/s]Extractor Predicting: 217it [02:39,  1.29it/s]Extractor Predicting: 218it [02:39,  1.30it/s]Extractor Predicting: 219it [02:40,  1.29it/s]Extractor Predicting: 220it [02:41,  1.29it/s]Extractor Predicting: 221it [02:42,  1.26it/s]Extractor Predicting: 222it [02:43,  1.26it/s]Extractor Predicting: 223it [02:43,  1.29it/s]Extractor Predicting: 224it [02:44,  1.29it/s]Extractor Predicting: 225it [02:45,  1.30it/s]Extractor Predicting: 226it [02:46,  1.29it/s]Extractor Predicting: 227it [02:46,  1.33it/s]Extractor Predicting: 228it [02:47,  1.30it/s]Extractor Predicting: 229it [02:48,  1.30it/s]Extractor Predicting: 230it [02:49,  1.36it/s]Extractor Predicting: 231it [02:49,  1.36it/s]Extractor Predicting: 232it [02:50,  1.39it/s]Extractor Predicting: 233it [02:51,  1.37it/s]Extractor Predicting: 234it [02:51,  1.37it/s]Extractor Predicting: 235it [02:52,  1.36it/s]Extractor Predicting: 236it [02:53,  1.34it/s]Extractor Predicting: 237it [02:54,  1.32it/s]Extractor Predicting: 238it [02:54,  1.34it/s]Extractor Predicting: 239it [02:55,  1.32it/s]Extractor Predicting: 240it [02:56,  1.33it/s]Extractor Predicting: 241it [02:57,  1.27it/s]Extractor Predicting: 242it [02:58,  1.31it/s]Extractor Predicting: 243it [02:58,  1.30it/s]Extractor Predicting: 244it [02:59,  1.31it/s]Extractor Predicting: 245it [03:00,  1.34it/s]Extractor Predicting: 246it [03:01,  1.33it/s]Extractor Predicting: 247it [03:01,  1.33it/s]Extractor Predicting: 248it [03:02,  1.33it/s]Extractor Predicting: 249it [03:03,  1.31it/s]Extractor Predicting: 250it [03:04,  1.33it/s]Extractor Predicting: 251it [03:04,  1.34it/s]Extractor Predicting: 252it [03:05,  1.34it/s]Extractor Predicting: 253it [03:06,  1.32it/s]Extractor Predicting: 254it [03:07,  1.34it/s]Extractor Predicting: 255it [03:07,  1.30it/s]Extractor Predicting: 256it [03:08,  1.27it/s]Extractor Predicting: 257it [03:09,  1.27it/s]Extractor Predicting: 258it [03:10,  1.27it/s]Extractor Predicting: 259it [03:11,  1.29it/s]Extractor Predicting: 260it [03:11,  1.28it/s]Extractor Predicting: 261it [03:12,  1.32it/s]Extractor Predicting: 262it [03:13,  1.30it/s]Extractor Predicting: 263it [03:14,  1.29it/s]Extractor Predicting: 264it [03:14,  1.29it/s]Extractor Predicting: 265it [03:15,  1.25it/s]Extractor Predicting: 266it [03:16,  1.26it/s]Extractor Predicting: 267it [03:17,  1.27it/s]Extractor Predicting: 268it [03:18,  1.25it/s]Extractor Predicting: 269it [03:18,  1.28it/s]Extractor Predicting: 270it [03:19,  1.29it/s]Extractor Predicting: 271it [03:20,  1.27it/s]Extractor Predicting: 272it [03:21,  1.29it/s]Extractor Predicting: 273it [03:21,  1.29it/s]Extractor Predicting: 274it [03:22,  1.31it/s]Extractor Predicting: 275it [03:23,  1.33it/s]Extractor Predicting: 276it [03:24,  1.36it/s]Extractor Predicting: 277it [03:24,  1.36it/s]Extractor Predicting: 278it [03:25,  1.35it/s]Extractor Predicting: 279it [03:26,  1.33it/s]Extractor Predicting: 280it [03:27,  1.27it/s]Extractor Predicting: 281it [03:28,  1.28it/s]Extractor Predicting: 282it [03:28,  1.37it/s]Extractor Predicting: 282it [03:28,  1.35it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:21,593 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:21,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:21,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:21,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:21,629 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:20:22,567 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:20:22,569 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:20:23,315 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:20:24,394 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:20:24,394 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:27,390 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:27,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:27,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:27,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:20:27,396 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:20:28,056 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:20:28,057 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:20:28,611 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:20:28,759 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:20:28,759 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.18it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.22it/s]Extractor Predicting: 4it [00:03,  1.24it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.26it/s]
[INFO|configuration_utils.py:515] 2023-08-28 13:20:33,153 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:20:33,155 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:20:33,160 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:20:33,161 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 13:20:33,165 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:20:36,791 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 13:20:36,795 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 13:20:36,807 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:20:36,808 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:20:36,823 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:36,827 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:36,827 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:36,827 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:36,827 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:36,827 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:20:36,827 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 13:20:37,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:38,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:38,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:39,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:40,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:41,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:42,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:43,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:44,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:45,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:46,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:47,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:48,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:49,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:50,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:51,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:52,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:53,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:54,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:55,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:56,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:57,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:58,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:20:59,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:00,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:01,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:02,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:03,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:04,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:05,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:06,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:29<06:59, 29.94s/it][WARNING|generation_utils.py:914] 2023-08-28 13:21:07,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:07,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:08,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:10,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:11,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:11,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:12,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:14,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:15,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:16,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:17,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:18,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:19,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:20,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:21,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:22,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:23,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:24,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:25,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:26,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:27,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:27,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:29,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:29,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:30,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:54<05:50, 26.96s/it][WARNING|generation_utils.py:914] 2023-08-28 13:21:31,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:32,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:33,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:34,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:35,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:36,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:38,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:38,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:39,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:40,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:41,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:43,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:44,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:44,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:45,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:46,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:47,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:48,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:49,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:51,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:52,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:53,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:53,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:54,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:55,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:56,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:20<05:17, 26.43s/it][WARNING|generation_utils.py:914] 2023-08-28 13:21:57,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:58,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:21:59,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:00,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:01,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:02,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:03,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:04,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:05,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:06,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:07,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:08,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:09,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:09,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:10,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:12,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:13,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:13,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:14,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:15,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:16,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:17,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:19,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:20,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:44<04:39, 25.37s/it][WARNING|generation_utils.py:914] 2023-08-28 13:22:21,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:22,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:23,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:24,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:25,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:26,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:27,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:28,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:29,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:31,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:32,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:33,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:34,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:35,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:36,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:36,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:37,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:38,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:40,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:41,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:42,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:42,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:43,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [02:07<04:06, 24.65s/it][WARNING|generation_utils.py:914] 2023-08-28 13:22:44,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:46,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:47,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:48,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:49,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:50,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:51,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:52,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:53,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:55,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:55,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:56,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:57,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:58,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:22:59,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:00,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:01,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:02,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:03,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:04,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:05,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:06,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:07,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:31<03:39, 24.40s/it][WARNING|generation_utils.py:914] 2023-08-28 13:23:08,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:09,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:10,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:11,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:12,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:13,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:14,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:15,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:16,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:17,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:18,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:19,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:20,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:21,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:21,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:22,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:23,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:24,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:25,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:27,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:28,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:29,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:29,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:53<03:09, 23.68s/it][WARNING|generation_utils.py:914] 2023-08-28 13:23:30,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:31,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:33,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:34,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:35,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:35,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:36,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:38,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:39,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:40,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:41,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:42,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:43,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:45,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:45,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:46,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:47,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:48,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:49,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:51,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:52,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:53,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:54,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [03:17<02:46, 23.79s/it][WARNING|generation_utils.py:914] 2023-08-28 13:23:54,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:55,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:56,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:58,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:58,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:23:59,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:00,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:01,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:03,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:04,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:05,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:06,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:07,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:08,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:09,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:10,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:11,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:12,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:13,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:14,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:15,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:16,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:17,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:41<02:22, 23.71s/it][WARNING|generation_utils.py:914] 2023-08-28 13:24:18,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:19,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:20,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:21,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:22,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:23,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:24,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:25,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:25,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:26,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:27,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:28,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:29,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:30,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:31,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:32,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:33,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:34,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:35,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:36,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:37,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:38,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [04:02<01:55, 23.03s/it][WARNING|generation_utils.py:914] 2023-08-28 13:24:40,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:41,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:42,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:43,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:44,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:45,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:46,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:47,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:48,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:49,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:50,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:51,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:52,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:53,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:54,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:55,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:56,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:57,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:58,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:24:59,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:00,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:01,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:02,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:03,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [04:27<01:33, 23.39s/it][WARNING|generation_utils.py:914] 2023-08-28 13:25:04,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:05,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:06,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:06,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:07,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:09,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:10,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:10,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:11,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:13,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:14,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:15,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:16,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:17,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:18,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:18,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:19,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:20,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:21,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:22,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:23,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:24,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:25,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:49<01:08, 22.94s/it][WARNING|generation_utils.py:914] 2023-08-28 13:25:26,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:27,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:28,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:28,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:29,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:30,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:31,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:32,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:33,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:34,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:35,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:36,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:37,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:38,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:39,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:40,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:41,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:42,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:43,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:44,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:45,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:46,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:47,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:48,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:49,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [05:12<00:46, 23.21s/it][WARNING|generation_utils.py:914] 2023-08-28 13:25:49,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:51,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:51,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:53,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:53,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:54,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:55,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:57,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:58,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:25:59,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:00,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:01,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:02,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:03,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:04,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:05,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:06,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:07,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:08,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:09,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:10,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:11,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:12,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:13,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [05:37<00:23, 23.56s/it][WARNING|generation_utils.py:914] 2023-08-28 13:26:14,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:15,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:16,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:17,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:18,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:19,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:20,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:21,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:21,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:22,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:24,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:25,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:26,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:26,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:28,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:29,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:30,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:31,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:32,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:33,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:34,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:35,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:36,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 13:26:37,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:01<00:00, 23.81s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:01<00:00, 24.11s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:45,279 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:45,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:45,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:45,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:45,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:26:45,893 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:26:45,894 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:26:46,463 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:26:47,511 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:26:47,511 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:50,365 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:50,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:50,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:50,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:26:50,370 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:26:51,007 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:26:51,009 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:26:51,563 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:26:51,718 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:26:51,718 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n', 'Relation : country . Context : In 1994 , , he was named as the winner of the Eurovision Song Contest 2000 . Head Entity : Eurovision Song Contest 2000 , Tail Entity : France .\n']
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 137, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 174, 'raw': 288}
{'target': 600, 'success': 190, 'raw': 320}
{'target': 600, 'success': 211, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 252, 'raw': 416}
{'target': 600, 'success': 274, 'raw': 448}
{'target': 600, 'success': 294, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 347, 'raw': 576}
{'target': 600, 'success': 361, 'raw': 608}
{'target': 600, 'success': 388, 'raw': 640}
{'target': 600, 'success': 408, 'raw': 672}
{'target': 600, 'success': 428, 'raw': 704}
{'target': 600, 'success': 443, 'raw': 736}
{'target': 600, 'success': 462, 'raw': 768}
{'target': 600, 'success': 483, 'raw': 800}
{'target': 600, 'success': 501, 'raw': 832}
{'target': 600, 'success': 519, 'raw': 864}
{'target': 600, 'success': 541, 'raw': 896}
{'target': 600, 'success': 562, 'raw': 928}
{'target': 600, 'success': 585, 'raw': 960}
{'target': 600, 'success': 608, 'raw': 992}
{'prompt': 'Relation : country .', 'success_rate': 0.6129032258064516, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 485, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year , the band formed New River Band with two of their members at the end of 2010 , Mikey McLeod ( the lyricist ) and Tim McCarroll ( bass ) . Head Entity : Mikey McNamara , Tail Entity : New River Band .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : member of .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : subsidiary . Context : The CIBN ( CBNI ) , also called CBE ( CBW , CBQ , CBQR , CJAX , CJD , CJEC , CJF , CJFY ) , is a United States National Research Council scientific satellite constellation . Head Entity : CBI , Tail Entity : United States National Research Council .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', "('A', 'subsidiary', '', 'On August 2017 , the company began shipping a new product for children in Europe : A .')", 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Battle of Brackewald', 'field of work', '', 'In 1849 he became a volunteer for the British Army at the Battle of Brackewald in Normandy .')", 'too many values to unpack (expected 2)'}}
['Relation : instrument . Context : Later in the year ( 1141â€“1231 ) he met Ferdinand I of Spain and the Duke of Prussia , whom he bore in the name of Christophe , together with Robert I of Belgium . Head Entity : Christophe , Tail Entity : John I of Belgium .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.859375, 'errors': {''}}
["Relation : occupation . Context : On 31 March 2014 , the Romanian Army , under a new President of Romania , Luiz InÃ¡cio Lutcic , announced the departure of Luiz 's second - generation Air Force commander , former Admiral of Romania , Sigmund Kaveliu . Head Entity : GeniÃ§iu Lutcic , Tail Entity : Romanian Army .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 14467
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14567, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.12it/s]Extractor Estimating: 2it [00:01,  1.10it/s]Extractor Estimating: 3it [00:02,  1.12it/s]Extractor Estimating: 4it [00:03,  1.13it/s]Extractor Estimating: 5it [00:04,  1.20it/s]Extractor Estimating: 6it [00:05,  1.22it/s]Extractor Estimating: 7it [00:05,  1.25it/s]Extractor Estimating: 8it [00:06,  1.23it/s]Extractor Estimating: 9it [00:07,  1.25it/s]Extractor Estimating: 10it [00:08,  1.25it/s]Extractor Estimating: 11it [00:09,  1.24it/s]Extractor Estimating: 12it [00:09,  1.20it/s]Extractor Estimating: 13it [00:10,  1.18it/s]Extractor Estimating: 14it [00:11,  1.19it/s]Extractor Estimating: 15it [00:12,  1.19it/s]Extractor Estimating: 16it [00:13,  1.17it/s]Extractor Estimating: 17it [00:14,  1.18it/s]Extractor Estimating: 18it [00:15,  1.15it/s]Extractor Estimating: 19it [00:15,  1.18it/s]Extractor Estimating: 20it [00:16,  1.19it/s]Extractor Estimating: 21it [00:17,  1.19it/s]Extractor Estimating: 22it [00:18,  1.15it/s]Extractor Estimating: 23it [00:19,  1.18it/s]Extractor Estimating: 24it [00:20,  1.18it/s]Extractor Estimating: 25it [00:20,  1.21it/s]Extractor Estimating: 26it [00:21,  1.22it/s]Extractor Estimating: 27it [00:22,  1.12it/s]Extractor Estimating: 28it [00:23,  1.15it/s]Extractor Estimating: 29it [00:24,  1.16it/s]Extractor Estimating: 30it [00:25,  1.13it/s]Extractor Estimating: 31it [00:26,  1.19it/s]Extractor Estimating: 32it [00:26,  1.23it/s]Extractor Estimating: 33it [00:27,  1.18it/s]Extractor Estimating: 34it [00:28,  1.19it/s]Extractor Estimating: 35it [00:29,  1.19it/s]Extractor Estimating: 36it [00:30,  1.19it/s]Extractor Estimating: 37it [00:31,  1.18it/s]Extractor Estimating: 38it [00:32,  1.19it/s]Extractor Estimating: 39it [00:32,  1.19it/s]Extractor Estimating: 40it [00:33,  1.23it/s]Extractor Estimating: 41it [00:34,  1.20it/s]Extractor Estimating: 42it [00:35,  1.16it/s]Extractor Estimating: 43it [00:36,  1.17it/s]Extractor Estimating: 44it [00:37,  1.14it/s]Extractor Estimating: 45it [00:38,  1.18it/s]Extractor Estimating: 46it [00:38,  1.14it/s]Extractor Estimating: 47it [00:39,  1.14it/s]Extractor Estimating: 48it [00:40,  1.19it/s]Extractor Estimating: 49it [00:41,  1.13it/s]Extractor Estimating: 50it [00:42,  1.15it/s]Extractor Estimating: 51it [00:43,  1.16it/s]Extractor Estimating: 52it [00:44,  1.16it/s]Extractor Estimating: 53it [00:44,  1.19it/s]Extractor Estimating: 54it [00:45,  1.19it/s]Extractor Estimating: 55it [00:46,  1.21it/s]Extractor Estimating: 56it [00:47,  1.21it/s]Extractor Estimating: 57it [00:48,  1.20it/s]Extractor Estimating: 58it [00:49,  1.20it/s]Extractor Estimating: 59it [00:49,  1.22it/s]Extractor Estimating: 60it [00:50,  1.19it/s]Extractor Estimating: 61it [00:51,  1.18it/s]Extractor Estimating: 62it [00:52,  1.16it/s]Extractor Estimating: 63it [00:53,  1.19it/s]Extractor Estimating: 64it [00:54,  1.18it/s]Extractor Estimating: 65it [00:54,  1.20it/s]Extractor Estimating: 66it [00:55,  1.21it/s]Extractor Estimating: 67it [00:56,  1.22it/s]Extractor Estimating: 68it [00:57,  1.20it/s]Extractor Estimating: 69it [00:58,  1.17it/s]Extractor Estimating: 70it [00:59,  1.18it/s]Extractor Estimating: 71it [00:59,  1.19it/s]Extractor Estimating: 72it [01:00,  1.19it/s]Extractor Estimating: 73it [01:01,  1.17it/s]Extractor Estimating: 74it [01:02,  1.19it/s]Extractor Estimating: 75it [01:03,  1.19it/s]Extractor Estimating: 76it [01:04,  1.22it/s]Extractor Estimating: 77it [01:04,  1.22it/s]Extractor Estimating: 78it [01:05,  1.20it/s]Extractor Estimating: 79it [01:06,  1.20it/s]Extractor Estimating: 80it [01:07,  1.22it/s]Extractor Estimating: 81it [01:08,  1.22it/s]Extractor Estimating: 82it [01:09,  1.20it/s]Extractor Estimating: 83it [01:09,  1.22it/s]Extractor Estimating: 84it [01:10,  1.23it/s]Extractor Estimating: 85it [01:11,  1.22it/s]Extractor Estimating: 86it [01:12,  1.22it/s]Extractor Estimating: 87it [01:13,  1.20it/s]Extractor Estimating: 88it [01:14,  1.20it/s]Extractor Estimating: 89it [01:14,  1.21it/s]Extractor Estimating: 90it [01:15,  1.21it/s]Extractor Estimating: 91it [01:16,  1.23it/s]Extractor Estimating: 92it [01:17,  1.23it/s]Extractor Estimating: 93it [01:18,  1.25it/s]Extractor Estimating: 94it [01:18,  1.22it/s]Extractor Estimating: 95it [01:19,  1.24it/s]Extractor Estimating: 96it [01:20,  1.24it/s]Extractor Estimating: 97it [01:21,  1.23it/s]Extractor Estimating: 98it [01:22,  1.22it/s]Extractor Estimating: 99it [01:22,  1.24it/s]Extractor Estimating: 100it [01:23,  1.23it/s]Extractor Estimating: 101it [01:24,  1.17it/s]Extractor Estimating: 102it [01:25,  1.23it/s]Extractor Estimating: 103it [01:26,  1.27it/s]Extractor Estimating: 104it [01:26,  1.27it/s]Extractor Estimating: 105it [01:27,  1.22it/s]Extractor Estimating: 106it [01:28,  1.21it/s]Extractor Estimating: 107it [01:29,  1.19it/s]Extractor Estimating: 108it [01:30,  1.22it/s]Extractor Estimating: 109it [01:31,  1.22it/s]Extractor Estimating: 110it [01:32,  1.18it/s]Extractor Estimating: 111it [01:32,  1.22it/s]Extractor Estimating: 112it [01:33,  1.17it/s]Extractor Estimating: 113it [01:34,  1.16it/s]Extractor Estimating: 114it [01:35,  1.09it/s]Extractor Estimating: 115it [01:36,  1.13it/s]Extractor Estimating: 116it [01:37,  1.19it/s]Extractor Estimating: 117it [01:38,  1.19it/s]Extractor Estimating: 118it [01:38,  1.22it/s]Extractor Estimating: 119it [01:39,  1.20it/s]Extractor Estimating: 120it [01:40,  1.22it/s]Extractor Estimating: 121it [01:41,  1.25it/s]Extractor Estimating: 122it [01:42,  1.20it/s]Extractor Estimating: 123it [01:42,  1.25it/s]Extractor Estimating: 124it [01:43,  1.23it/s]Extractor Estimating: 125it [01:44,  1.20it/s]Extractor Estimating: 126it [01:45,  1.25it/s]Extractor Estimating: 127it [01:46,  1.23it/s]Extractor Estimating: 128it [01:46,  1.26it/s]Extractor Estimating: 129it [01:47,  1.25it/s]Extractor Estimating: 130it [01:48,  1.31it/s]Extractor Estimating: 131it [01:49,  1.29it/s]Extractor Estimating: 132it [01:50,  1.29it/s]Extractor Estimating: 133it [01:50,  1.29it/s]Extractor Estimating: 134it [01:51,  1.26it/s]Extractor Estimating: 135it [01:52,  1.29it/s]Extractor Estimating: 136it [01:53,  1.29it/s]Extractor Estimating: 137it [01:54,  1.21it/s]Extractor Estimating: 138it [01:54,  1.22it/s]Extractor Estimating: 139it [01:55,  1.26it/s]Extractor Estimating: 140it [01:56,  1.27it/s]Extractor Estimating: 141it [01:57,  1.26it/s]Extractor Estimating: 142it [01:57,  1.25it/s]Extractor Estimating: 143it [01:58,  1.28it/s]Extractor Estimating: 144it [01:59,  1.27it/s]Extractor Estimating: 145it [02:00,  1.23it/s]Extractor Estimating: 146it [02:01,  1.27it/s]Extractor Estimating: 147it [02:01,  1.25it/s]Extractor Estimating: 148it [02:02,  1.23it/s]Extractor Estimating: 149it [02:03,  1.19it/s]Extractor Estimating: 150it [02:04,  1.18it/s]Extractor Estimating: 151it [02:05,  1.24it/s]Extractor Estimating: 152it [02:06,  1.22it/s]Extractor Estimating: 153it [02:06,  1.24it/s]Extractor Estimating: 154it [02:07,  1.23it/s]Extractor Estimating: 155it [02:08,  1.23it/s]Extractor Estimating: 156it [02:09,  1.23it/s]Extractor Estimating: 157it [02:10,  1.19it/s]Extractor Estimating: 158it [02:11,  1.21it/s]Extractor Estimating: 159it [02:11,  1.20it/s]Extractor Estimating: 160it [02:12,  1.22it/s]Extractor Estimating: 161it [02:13,  1.22it/s]Extractor Estimating: 162it [02:14,  1.22it/s]Extractor Estimating: 163it [02:15,  1.22it/s]Extractor Estimating: 164it [02:15,  1.25it/s]Extractor Estimating: 165it [02:16,  1.27it/s]Extractor Estimating: 166it [02:17,  1.26it/s]Extractor Estimating: 167it [02:18,  1.24it/s]Extractor Estimating: 168it [02:19,  1.23it/s]Extractor Estimating: 169it [02:19,  1.25it/s]Extractor Estimating: 170it [02:20,  1.31it/s]Extractor Estimating: 171it [02:21,  1.29it/s]Extractor Estimating: 172it [02:22,  1.23it/s]Extractor Estimating: 173it [02:23,  1.20it/s]Extractor Estimating: 174it [02:24,  1.20it/s]Extractor Estimating: 175it [02:24,  1.21it/s]Extractor Estimating: 176it [02:25,  1.19it/s]Extractor Estimating: 177it [02:26,  1.20it/s]Extractor Estimating: 178it [02:27,  1.22it/s]Extractor Estimating: 179it [02:28,  1.24it/s]Extractor Estimating: 180it [02:28,  1.23it/s]Extractor Estimating: 181it [02:29,  1.26it/s]Extractor Estimating: 182it [02:30,  1.25it/s]Extractor Estimating: 183it [02:31,  1.25it/s]Extractor Estimating: 184it [02:32,  1.24it/s]Extractor Estimating: 185it [02:32,  1.25it/s]Extractor Estimating: 186it [02:33,  1.26it/s]Extractor Estimating: 187it [02:34,  1.23it/s]Extractor Estimating: 188it [02:35,  1.14it/s]Extractor Estimating: 189it [02:36,  1.06it/s]Extractor Estimating: 190it [02:37,  1.12it/s]Extractor Estimating: 191it [02:38,  1.18it/s]Extractor Estimating: 192it [02:39,  1.17it/s]Extractor Estimating: 193it [02:39,  1.18it/s]Extractor Estimating: 194it [02:40,  1.18it/s]Extractor Estimating: 195it [02:41,  1.20it/s]Extractor Estimating: 196it [02:42,  1.23it/s]Extractor Estimating: 197it [02:43,  1.20it/s]Extractor Estimating: 198it [02:43,  1.21it/s]Extractor Estimating: 199it [02:44,  1.24it/s]Extractor Estimating: 200it [02:45,  1.22it/s]Extractor Estimating: 201it [02:46,  1.19it/s]Extractor Estimating: 202it [02:47,  1.20it/s]Extractor Estimating: 203it [02:48,  1.19it/s]Extractor Estimating: 204it [02:48,  1.20it/s]Extractor Estimating: 205it [02:49,  1.22it/s]Extractor Estimating: 206it [02:50,  1.19it/s]Extractor Estimating: 207it [02:51,  1.23it/s]Extractor Estimating: 208it [02:52,  1.16it/s]Extractor Estimating: 209it [02:53,  1.15it/s]Extractor Estimating: 210it [02:54,  1.18it/s]Extractor Estimating: 211it [02:54,  1.16it/s]Extractor Estimating: 212it [02:55,  1.16it/s]Extractor Estimating: 213it [02:56,  1.13it/s]Extractor Estimating: 214it [02:57,  1.15it/s]Extractor Estimating: 215it [02:58,  1.16it/s]Extractor Estimating: 216it [02:59,  1.12it/s]Extractor Estimating: 217it [03:00,  1.12it/s]Extractor Estimating: 218it [03:01,  1.15it/s]Extractor Estimating: 219it [03:01,  1.14it/s]Extractor Estimating: 220it [03:02,  1.16it/s]Extractor Estimating: 221it [03:03,  1.11it/s]Extractor Estimating: 222it [03:04,  1.11it/s]Extractor Estimating: 223it [03:05,  1.12it/s]Extractor Estimating: 224it [03:06,  1.12it/s]Extractor Estimating: 225it [03:07,  1.14it/s]Extractor Estimating: 226it [03:08,  1.12it/s]Extractor Estimating: 227it [03:08,  1.18it/s]Extractor Estimating: 228it [03:09,  1.23it/s]Extractor Estimating: 229it [03:10,  1.25it/s]Extractor Estimating: 230it [03:11,  1.25it/s]Extractor Estimating: 231it [03:11,  1.28it/s]Extractor Estimating: 232it [03:12,  1.32it/s]Extractor Estimating: 233it [03:13,  1.35it/s]Extractor Estimating: 234it [03:14,  1.33it/s]Extractor Estimating: 235it [03:14,  1.31it/s]Extractor Estimating: 236it [03:15,  1.30it/s]Extractor Estimating: 237it [03:16,  1.30it/s]Extractor Estimating: 238it [03:17,  1.29it/s]Extractor Estimating: 239it [03:18,  1.28it/s]Extractor Estimating: 240it [03:18,  1.29it/s]Extractor Estimating: 241it [03:19,  1.32it/s]Extractor Estimating: 242it [03:20,  1.31it/s]Extractor Estimating: 243it [03:21,  1.27it/s]Extractor Estimating: 244it [03:21,  1.26it/s]Extractor Estimating: 245it [03:22,  1.28it/s]Extractor Estimating: 246it [03:23,  1.31it/s]Extractor Estimating: 247it [03:24,  1.29it/s]Extractor Estimating: 248it [03:25,  1.26it/s]Extractor Estimating: 249it [03:25,  1.27it/s]Extractor Estimating: 250it [03:26,  1.29it/s]Extractor Estimating: 251it [03:27,  1.21it/s]Extractor Estimating: 252it [03:28,  1.25it/s]Extractor Estimating: 253it [03:29,  1.25it/s]Extractor Estimating: 254it [03:30,  1.19it/s]Extractor Estimating: 255it [03:30,  1.23it/s]Extractor Estimating: 256it [03:31,  1.30it/s]Extractor Estimating: 257it [03:32,  1.29it/s]Extractor Estimating: 258it [03:33,  1.20it/s]Extractor Estimating: 259it [03:33,  1.23it/s]Extractor Estimating: 260it [03:34,  1.26it/s]Extractor Estimating: 261it [03:35,  1.22it/s]Extractor Estimating: 262it [03:36,  1.12it/s]Extractor Estimating: 263it [03:37,  1.15it/s]Extractor Estimating: 264it [03:38,  1.18it/s]Extractor Estimating: 265it [03:39,  1.15it/s]Extractor Estimating: 266it [03:40,  1.18it/s]Extractor Estimating: 267it [03:40,  1.16it/s]Extractor Estimating: 268it [03:41,  1.21it/s]Extractor Estimating: 269it [03:42,  1.23it/s]Extractor Estimating: 270it [03:43,  1.20it/s]Extractor Estimating: 271it [03:44,  1.19it/s]Extractor Estimating: 272it [03:45,  1.17it/s]Extractor Estimating: 273it [03:45,  1.19it/s]Extractor Estimating: 274it [03:46,  1.18it/s]Extractor Estimating: 275it [03:47,  1.20it/s]Extractor Estimating: 276it [03:48,  1.25it/s]Extractor Estimating: 277it [03:49,  1.24it/s]Extractor Estimating: 278it [03:49,  1.30it/s]Extractor Estimating: 279it [03:50,  1.28it/s]Extractor Estimating: 280it [03:51,  1.28it/s]Extractor Estimating: 281it [03:52,  1.29it/s]Extractor Estimating: 282it [03:52,  1.32it/s]Extractor Estimating: 283it [03:53,  1.29it/s]Extractor Estimating: 284it [03:54,  1.27it/s]Extractor Estimating: 285it [03:55,  1.30it/s]Extractor Estimating: 286it [03:55,  1.29it/s]Extractor Estimating: 287it [03:56,  1.28it/s]Extractor Estimating: 288it [03:57,  1.25it/s]Extractor Estimating: 289it [03:58,  1.27it/s]Extractor Estimating: 290it [03:59,  1.26it/s]Extractor Estimating: 291it [03:59,  1.30it/s]Extractor Estimating: 292it [04:00,  1.26it/s]Extractor Estimating: 293it [04:01,  1.28it/s]Extractor Estimating: 294it [04:02,  1.27it/s]Extractor Estimating: 295it [04:03,  1.26it/s]Extractor Estimating: 296it [04:03,  1.29it/s]Extractor Estimating: 297it [04:04,  1.30it/s]Extractor Estimating: 298it [04:05,  1.31it/s]Extractor Estimating: 299it [04:06,  1.34it/s]Extractor Estimating: 300it [04:06,  1.36it/s]Extractor Estimating: 301it [04:07,  1.28it/s]Extractor Estimating: 302it [04:08,  1.22it/s]Extractor Estimating: 303it [04:09,  1.22it/s]Extractor Estimating: 304it [04:10,  1.28it/s]Extractor Estimating: 305it [04:10,  1.26it/s]Extractor Estimating: 306it [04:11,  1.30it/s]Extractor Estimating: 307it [04:12,  1.30it/s]Extractor Estimating: 308it [04:13,  1.36it/s]Extractor Estimating: 309it [04:13,  1.35it/s]Extractor Estimating: 310it [04:14,  1.30it/s]Extractor Estimating: 311it [04:15,  1.23it/s]Extractor Estimating: 312it [04:16,  1.28it/s]Extractor Estimating: 313it [04:17,  1.26it/s]Extractor Estimating: 314it [04:17,  1.24it/s]Extractor Estimating: 315it [04:18,  1.12it/s]Extractor Estimating: 316it [04:19,  1.17it/s]Extractor Estimating: 317it [04:20,  1.21it/s]Extractor Estimating: 318it [04:21,  1.20it/s]Extractor Estimating: 319it [04:22,  1.23it/s]Extractor Estimating: 320it [04:23,  1.19it/s]Extractor Estimating: 321it [04:23,  1.25it/s]Extractor Estimating: 322it [04:24,  1.24it/s]Extractor Estimating: 323it [04:25,  1.25it/s]Extractor Estimating: 324it [04:26,  1.27it/s]Extractor Estimating: 325it [04:26,  1.27it/s]Extractor Estimating: 326it [04:27,  1.17it/s]Extractor Estimating: 327it [04:28,  1.19it/s]Extractor Estimating: 328it [04:29,  1.18it/s]Extractor Estimating: 329it [04:30,  1.21it/s]Extractor Estimating: 330it [04:31,  1.21it/s]Extractor Estimating: 331it [04:31,  1.23it/s]Extractor Estimating: 332it [04:32,  1.22it/s]Extractor Estimating: 333it [04:33,  1.22it/s]Extractor Estimating: 334it [04:34,  1.24it/s]Extractor Estimating: 335it [04:35,  1.21it/s]Extractor Estimating: 336it [04:36,  1.23it/s]Extractor Estimating: 337it [04:36,  1.25it/s]Extractor Estimating: 338it [04:37,  1.23it/s]Extractor Estimating: 339it [04:38,  1.24it/s]Extractor Estimating: 340it [04:39,  1.26it/s]Extractor Estimating: 341it [04:40,  1.20it/s]Extractor Estimating: 342it [04:40,  1.21it/s]Extractor Estimating: 343it [04:41,  1.21it/s]Extractor Estimating: 344it [04:42,  1.21it/s]Extractor Estimating: 345it [04:43,  1.23it/s]Extractor Estimating: 346it [04:44,  1.18it/s]Extractor Estimating: 347it [04:45,  1.13it/s]Extractor Estimating: 348it [04:46,  1.12it/s]Extractor Estimating: 349it [04:47,  1.04it/s]Extractor Estimating: 350it [04:48,  1.10it/s]Extractor Estimating: 351it [04:48,  1.13it/s]Extractor Estimating: 352it [04:49,  1.17it/s]Extractor Estimating: 353it [04:50,  1.21it/s]Extractor Estimating: 354it [04:51,  1.25it/s]Extractor Estimating: 355it [04:51,  1.27it/s]Extractor Estimating: 356it [04:52,  1.23it/s]Extractor Estimating: 357it [04:53,  1.24it/s]Extractor Estimating: 358it [04:54,  1.24it/s]Extractor Estimating: 359it [04:55,  1.30it/s]Extractor Estimating: 360it [04:55,  1.31it/s]Extractor Estimating: 361it [04:56,  1.28it/s]Extractor Estimating: 362it [04:57,  1.26it/s]Extractor Estimating: 363it [04:58,  1.30it/s]Extractor Estimating: 364it [04:59,  1.26it/s]Extractor Estimating: 365it [05:00,  1.21it/s]Extractor Estimating: 366it [05:00,  1.22it/s]Extractor Estimating: 367it [05:01,  1.23it/s]Extractor Estimating: 368it [05:02,  1.21it/s]Extractor Estimating: 369it [05:03,  1.22it/s]Extractor Estimating: 370it [05:04,  1.24it/s]Extractor Estimating: 371it [05:04,  1.22it/s]Extractor Estimating: 372it [05:05,  1.26it/s]Extractor Estimating: 373it [05:06,  1.23it/s]Extractor Estimating: 374it [05:07,  1.21it/s]Extractor Estimating: 375it [05:07,  1.46it/s]Extractor Estimating: 375it [05:07,  1.22it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:12,185 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:12,192 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:12,192 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:12,192 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:12,192 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:32:12,815 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:32:12,816 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:32:13,378 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:32:14,427 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:32:14,427 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:17,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:17,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:17,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:17,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:32:17,240 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:32:17,966 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:32:17,967 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:32:18,538 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:32:18,694 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:32:18,694 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:40:43,056 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:40:43,089 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7686 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 20751
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20851, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20851, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.532, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.481, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.479, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 79, avg_time 1.515, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 179, avg_time 1.483, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 279, avg_time 2.926, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 58, avg_time 1.487, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 158, avg_time 1.483, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 258, avg_time 1.500, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 37, avg_time 1.500, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 137, avg_time 2.925, loss:nan
g_step 1200, step 237, avg_time 1.491, loss:nan
g_step 1300, step 16, avg_time 1.496, loss:nan
g_step 1400, step 116, avg_time 1.493, loss:nan
g_step 1500, step 216, avg_time 1.484, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 316, avg_time 2.920, loss:nan
g_step 1700, step 95, avg_time 1.511, loss:nan
g_step 1800, step 195, avg_time 1.514, loss:nan
g_step 1900, step 295, avg_time 1.484, loss:nan
g_step 2000, step 74, avg_time 1.475, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 174, avg_time 2.926, loss:nan
g_step 2200, step 274, avg_time 1.487, loss:nan
g_step 2300, step 53, avg_time 1.475, loss:nan
g_step 2400, step 153, avg_time 1.469, loss:nan
g_step 2500, step 253, avg_time 1.528, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.901, loss:nan
g_step 2700, step 132, avg_time 1.492, loss:nan
g_step 2800, step 232, avg_time 1.513, loss:nan
g_step 2900, step 11, avg_time 1.480, loss:nan
g_step 3000, step 111, avg_time 1.497, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 211, avg_time 2.920, loss:nan
g_step 3200, step 311, avg_time 1.505, loss:nan
g_step 3300, step 90, avg_time 1.503, loss:nan
g_step 3400, step 190, avg_time 1.472, loss:nan
g_step 3500, step 290, avg_time 1.495, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 69, avg_time 2.943, loss:nan
g_step 3700, step 169, avg_time 1.486, loss:nan
g_step 3800, step 269, avg_time 1.467, loss:nan
g_step 3900, step 48, avg_time 1.501, loss:nan
g_step 4000, step 148, avg_time 1.481, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 248, avg_time 2.935, loss:nan
g_step 4200, step 27, avg_time 1.504, loss:nan
g_step 4300, step 127, avg_time 1.492, loss:nan
g_step 4400, step 227, avg_time 1.499, loss:nan
g_step 4500, step 6, avg_time 1.479, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 106, avg_time 2.946, loss:nan
g_step 4700, step 206, avg_time 1.491, loss:nan
g_step 4800, step 306, avg_time 1.472, loss:nan
g_step 4900, step 85, avg_time 1.479, loss:nan
g_step 5000, step 185, avg_time 1.488, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 285, avg_time 2.917, loss:nan
g_step 5200, step 64, avg_time 1.489, loss:nan
g_step 5300, step 164, avg_time 1.487, loss:nan
g_step 5400, step 264, avg_time 1.501, loss:nan
g_step 5500, step 43, avg_time 1.501, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 143, avg_time 2.933, loss:nan
g_step 5700, step 243, avg_time 1.490, loss:nan
g_step 5800, step 22, avg_time 1.485, loss:nan
g_step 5900, step 122, avg_time 1.479, loss:nan
g_step 6000, step 222, avg_time 1.515, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 1, avg_time 2.917, loss:nan
g_step 6200, step 101, avg_time 1.548, loss:nan
g_step 6300, step 201, avg_time 1.482, loss:nan
g_step 6400, step 301, avg_time 1.475, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:40:43 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:40:43 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-40-43_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:40:44 - WARNING - datasets.builder -   Using custom data configuration default-c544ecae968585e2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c544ecae968585e2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:40:44,859 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:40:44,861 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:40:44,872 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:40:44,873 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:40:44,901 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:44,909 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:44,909 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:44,910 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:44,910 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:44,910 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:40:44,910 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:40:45,145 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:40:48,288 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:40:48,288 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c544ecae968585e2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  3.05ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.86ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.23ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  3.57ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  3.89ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.11ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.26ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.80ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.22ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  3.34ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  3.91ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.17ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.20ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.62ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:01,  5.60ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:00,  7.10ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  7.63ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  7.92ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  8.09ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:00<00:00,  8.23ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00,  8.31ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.25ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  6.30ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  7.37ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  7.81ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  7.79ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  7.60ba/s]
[INFO|trainer.py:414] 2023-08-28 16:40:53,671 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:40:53,703 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:40:53,703 >>   Num examples = 7700
[INFO|trainer.py:1149] 2023-08-28 16:40:53,703 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:40:53,703 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:40:53,703 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:40:53,703 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:40:53,703 >>   Total optimization steps = 600
  0%|          | 0/600 [00:00<?, ?it/s]  0%|          | 1/600 [00:00<02:56,  3.39it/s]  0%|          | 2/600 [00:00<02:50,  3.51it/s]  0%|          | 3/600 [00:00<02:48,  3.55it/s]  1%|          | 4/600 [00:01<02:47,  3.57it/s]  1%|          | 5/600 [00:01<02:46,  3.58it/s]  1%|          | 6/600 [00:01<02:45,  3.58it/s]  1%|          | 7/600 [00:01<02:45,  3.59it/s]  1%|â–         | 8/600 [00:02<02:44,  3.59it/s]  2%|â–         | 9/600 [00:02<02:44,  3.60it/s]  2%|â–         | 10/600 [00:02<02:45,  3.56it/s]  2%|â–         | 11/600 [00:03<02:44,  3.57it/s]  2%|â–         | 12/600 [00:03<02:44,  3.58it/s]  2%|â–         | 13/600 [00:03<02:43,  3.58it/s]  2%|â–         | 14/600 [00:03<02:43,  3.59it/s]  2%|â–Ž         | 15/600 [00:04<02:42,  3.59it/s]  3%|â–Ž         | 16/600 [00:04<02:42,  3.59it/s]  3%|â–Ž         | 17/600 [00:04<02:42,  3.59it/s]  3%|â–Ž         | 18/600 [00:05<02:41,  3.59it/s]  3%|â–Ž         | 19/600 [00:05<02:41,  3.59it/s]  3%|â–Ž         | 20/600 [00:05<02:41,  3.59it/s]  4%|â–Ž         | 21/600 [00:05<02:41,  3.59it/s]  4%|â–Ž         | 22/600 [00:06<02:40,  3.59it/s]  4%|â–         | 23/600 [00:06<02:40,  3.59it/s]  4%|â–         | 24/600 [00:06<02:40,  3.60it/s]  4%|â–         | 25/600 [00:06<02:39,  3.60it/s]  4%|â–         | 26/600 [00:07<02:39,  3.60it/s]  4%|â–         | 27/600 [00:07<02:39,  3.60it/s]  5%|â–         | 28/600 [00:07<02:39,  3.59it/s]  5%|â–         | 29/600 [00:08<02:40,  3.55it/s]  5%|â–Œ         | 30/600 [00:08<02:39,  3.56it/s]  5%|â–Œ         | 31/600 [00:08<02:39,  3.57it/s]  5%|â–Œ         | 32/600 [00:08<02:38,  3.58it/s]  6%|â–Œ         | 33/600 [00:09<02:38,  3.59it/s]  6%|â–Œ         | 34/600 [00:09<02:37,  3.59it/s]  6%|â–Œ         | 35/600 [00:09<02:37,  3.59it/s]  6%|â–Œ         | 36/600 [00:10<02:37,  3.59it/s]  6%|â–Œ         | 37/600 [00:10<02:36,  3.59it/s]  6%|â–‹         | 38/600 [00:10<02:36,  3.59it/s]  6%|â–‹         | 39/600 [00:10<02:36,  3.59it/s]  7%|â–‹         | 40/600 [00:11<02:36,  3.59it/s]  7%|â–‹         | 41/600 [00:11<02:35,  3.59it/s]  7%|â–‹         | 42/600 [00:11<02:35,  3.59it/s]  7%|â–‹         | 43/600 [00:11<02:35,  3.59it/s]  7%|â–‹         | 44/600 [00:12<02:34,  3.59it/s]  8%|â–Š         | 45/600 [00:12<02:34,  3.59it/s]  8%|â–Š         | 46/600 [00:12<02:34,  3.59it/s]  8%|â–Š         | 47/600 [00:13<03:04,  2.99it/s]  8%|â–Š         | 48/600 [00:13<02:55,  3.15it/s]  8%|â–Š         | 49/600 [00:13<02:48,  3.27it/s]  8%|â–Š         | 50/600 [00:14<02:43,  3.36it/s]  8%|â–Š         | 51/600 [00:14<02:40,  3.43it/s]  9%|â–Š         | 52/600 [00:14<02:37,  3.48it/s]  9%|â–‰         | 53/600 [00:14<02:35,  3.51it/s]  9%|â–‰         | 54/600 [00:15<02:34,  3.53it/s]  9%|â–‰         | 55/600 [00:15<02:33,  3.55it/s]  9%|â–‰         | 56/600 [00:15<02:32,  3.56it/s] 10%|â–‰         | 57/600 [00:16<02:32,  3.57it/s] 10%|â–‰         | 58/600 [00:16<02:31,  3.57it/s] 10%|â–‰         | 59/600 [00:16<02:31,  3.58it/s] 10%|â–ˆ         | 60/600 [00:16<02:30,  3.58it/s] 10%|â–ˆ         | 61/600 [00:17<02:30,  3.58it/s] 10%|â–ˆ         | 62/600 [00:17<02:30,  3.58it/s] 10%|â–ˆ         | 63/600 [00:17<02:29,  3.58it/s] 11%|â–ˆ         | 64/600 [00:18<02:29,  3.59it/s] 11%|â–ˆ         | 65/600 [00:18<02:30,  3.55it/s] 11%|â–ˆ         | 66/600 [00:18<02:30,  3.56it/s] 11%|â–ˆ         | 67/600 [00:18<02:29,  3.57it/s] 11%|â–ˆâ–        | 68/600 [00:19<02:28,  3.57it/s] 12%|â–ˆâ–        | 69/600 [00:19<02:28,  3.57it/s] 12%|â–ˆâ–        | 70/600 [00:19<02:28,  3.58it/s] 12%|â–ˆâ–        | 71/600 [00:19<02:27,  3.58it/s] 12%|â–ˆâ–        | 72/600 [00:20<02:27,  3.58it/s] 12%|â–ˆâ–        | 73/600 [00:20<02:27,  3.58it/s] 12%|â–ˆâ–        | 74/600 [00:20<02:26,  3.58it/s] 12%|â–ˆâ–Ž        | 75/600 [00:21<02:26,  3.58it/s] 13%|â–ˆâ–Ž        | 76/600 [00:21<02:26,  3.59it/s] 13%|â–ˆâ–Ž        | 77/600 [00:21<02:25,  3.59it/s] 13%|â–ˆâ–Ž        | 78/600 [00:21<02:25,  3.59it/s] 13%|â–ˆâ–Ž        | 79/600 [00:22<02:25,  3.59it/s] 13%|â–ˆâ–Ž        | 80/600 [00:22<02:24,  3.59it/s] 14%|â–ˆâ–Ž        | 81/600 [00:22<02:24,  3.59it/s] 14%|â–ˆâ–Ž        | 82/600 [00:23<02:24,  3.59it/s] 14%|â–ˆâ–        | 83/600 [00:23<02:25,  3.54it/s] 14%|â–ˆâ–        | 84/600 [00:23<02:24,  3.56it/s] 14%|â–ˆâ–        | 85/600 [00:23<02:24,  3.57it/s] 14%|â–ˆâ–        | 86/600 [00:24<02:23,  3.58it/s] 14%|â–ˆâ–        | 87/600 [00:24<02:23,  3.58it/s] 15%|â–ˆâ–        | 88/600 [00:24<02:22,  3.58it/s] 15%|â–ˆâ–        | 89/600 [00:25<02:22,  3.58it/s] 15%|â–ˆâ–Œ        | 90/600 [00:25<02:22,  3.58it/s] 15%|â–ˆâ–Œ        | 91/600 [00:25<02:22,  3.58it/s] 15%|â–ˆâ–Œ        | 92/600 [00:25<02:21,  3.58it/s] 16%|â–ˆâ–Œ        | 93/600 [00:26<02:21,  3.58it/s] 16%|â–ˆâ–Œ        | 94/600 [00:26<02:21,  3.58it/s] 16%|â–ˆâ–Œ        | 95/600 [00:26<02:20,  3.58it/s] 16%|â–ˆâ–Œ        | 96/600 [00:26<02:20,  3.58it/s] 16%|â–ˆâ–Œ        | 97/600 [00:27<02:20,  3.58it/s] 16%|â–ˆâ–‹        | 98/600 [00:27<02:20,  3.58it/s] 16%|â–ˆâ–‹        | 99/600 [00:27<02:19,  3.58it/s] 17%|â–ˆâ–‹        | 100/600 [00:28<02:19,  3.59it/s] 17%|â–ˆâ–‹        | 101/600 [00:28<02:30,  3.31it/s] 17%|â–ˆâ–‹        | 102/600 [00:28<02:26,  3.39it/s] 17%|â–ˆâ–‹        | 103/600 [00:29<02:24,  3.45it/s] 17%|â–ˆâ–‹        | 104/600 [00:29<02:22,  3.49it/s] 18%|â–ˆâ–Š        | 105/600 [00:29<02:20,  3.52it/s] 18%|â–ˆâ–Š        | 106/600 [00:29<02:19,  3.54it/s] 18%|â–ˆâ–Š        | 107/600 [00:30<02:18,  3.55it/s] 18%|â–ˆâ–Š        | 108/600 [00:30<02:18,  3.56it/s] 18%|â–ˆâ–Š        | 109/600 [00:30<02:17,  3.57it/s] 18%|â–ˆâ–Š        | 110/600 [00:30<02:17,  3.57it/s] 18%|â–ˆâ–Š        | 111/600 [00:31<02:16,  3.58it/s] 19%|â–ˆâ–Š        | 112/600 [00:31<02:16,  3.58it/s] 19%|â–ˆâ–‰        | 113/600 [00:31<02:16,  3.58it/s] 19%|â–ˆâ–‰        | 114/600 [00:32<02:15,  3.58it/s] 19%|â–ˆâ–‰        | 115/600 [00:32<02:15,  3.58it/s] 19%|â–ˆâ–‰        | 116/600 [00:32<02:14,  3.59it/s] 20%|â–ˆâ–‰        | 117/600 [00:32<02:14,  3.59it/s] 20%|â–ˆâ–‰        | 118/600 [00:33<02:14,  3.59it/s] 20%|â–ˆâ–‰        | 119/600 [00:33<02:14,  3.57it/s] 20%|â–ˆâ–ˆ        | 120/600 [00:33<02:14,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 16:41:27,521 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:41:27,521 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:41:27,521 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.11it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.28it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.56it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.83it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.41it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.08it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.92it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.83it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.74it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.69it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.56it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.58it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.57it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.59it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.62it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.59it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.55it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.63it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.65it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.55it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.56it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.62it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.54it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.61it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.59it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.64it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.63it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.56it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.66it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.58it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.56it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.55it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.57it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.55it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.60it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.56it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.57it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.64it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.56it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.64it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.57it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:05, 43.67it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 44.55it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 45.14it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 45.56it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 45.89it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.07it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.21it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.33it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.38it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.52it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.46it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.47it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.50it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.55it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.57it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.60it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.54it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.55it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.57it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.63it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.56it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.56it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.52it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.58it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.59it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.58it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.52it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.47it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.53it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.57it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.55it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.55it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.44it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.55it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.58it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.48it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.59it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.52it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.57it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.53it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.54it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.59it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.55it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.59it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.60it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.60it/s][A 20%|â–ˆâ–ˆ        | 120/600 [00:43<02:14,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:41:37,013 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 16:41:37,065 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:41:39,619 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:41:39,642 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:41:39,657 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 121/600 [00:46<32:10,  4.03s/it] 20%|â–ˆâ–ˆ        | 122/600 [00:46<23:08,  2.91s/it] 20%|â–ˆâ–ˆ        | 123/600 [00:47<16:49,  2.12s/it] 21%|â–ˆâ–ˆ        | 124/600 [00:47<12:25,  1.57s/it] 21%|â–ˆâ–ˆ        | 125/600 [00:47<09:20,  1.18s/it] 21%|â–ˆâ–ˆ        | 126/600 [00:47<07:10,  1.10it/s] 21%|â–ˆâ–ˆ        | 127/600 [00:48<05:41,  1.39it/s] 21%|â–ˆâ–ˆâ–       | 128/600 [00:48<04:37,  1.70it/s] 22%|â–ˆâ–ˆâ–       | 129/600 [00:48<03:53,  2.02it/s] 22%|â–ˆâ–ˆâ–       | 130/600 [00:49<03:22,  2.32it/s] 22%|â–ˆâ–ˆâ–       | 131/600 [00:49<03:01,  2.59it/s] 22%|â–ˆâ–ˆâ–       | 132/600 [00:49<02:48,  2.77it/s] 22%|â–ˆâ–ˆâ–       | 133/600 [00:49<02:37,  2.96it/s] 22%|â–ˆâ–ˆâ–       | 134/600 [00:50<02:29,  3.12it/s] 22%|â–ˆâ–ˆâ–Ž       | 135/600 [00:50<02:23,  3.25it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/600 [00:50<02:18,  3.34it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/600 [00:51<02:15,  3.41it/s] 23%|â–ˆâ–ˆâ–Ž       | 138/600 [00:51<02:13,  3.46it/s] 23%|â–ˆâ–ˆâ–Ž       | 139/600 [00:51<02:11,  3.50it/s] 23%|â–ˆâ–ˆâ–Ž       | 140/600 [00:51<02:10,  3.52it/s] 24%|â–ˆâ–ˆâ–Ž       | 141/600 [00:52<02:09,  3.54it/s] 24%|â–ˆâ–ˆâ–Ž       | 142/600 [00:52<02:13,  3.43it/s] 24%|â–ˆâ–ˆâ–       | 143/600 [00:52<02:11,  3.47it/s] 24%|â–ˆâ–ˆâ–       | 144/600 [00:53<02:09,  3.51it/s] 24%|â–ˆâ–ˆâ–       | 145/600 [00:53<02:08,  3.53it/s] 24%|â–ˆâ–ˆâ–       | 146/600 [00:53<02:07,  3.55it/s] 24%|â–ˆâ–ˆâ–       | 147/600 [00:53<02:07,  3.56it/s] 25%|â–ˆâ–ˆâ–       | 148/600 [00:54<02:06,  3.57it/s] 25%|â–ˆâ–ˆâ–       | 149/600 [00:54<02:06,  3.57it/s] 25%|â–ˆâ–ˆâ–Œ       | 150/600 [00:54<02:05,  3.57it/s] 25%|â–ˆâ–ˆâ–Œ       | 151/600 [00:54<02:05,  3.57it/s] 25%|â–ˆâ–ˆâ–Œ       | 152/600 [00:55<02:05,  3.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/600 [00:55<02:05,  3.55it/s] 26%|â–ˆâ–ˆâ–Œ       | 154/600 [00:55<02:05,  3.56it/s] 26%|â–ˆâ–ˆâ–Œ       | 155/600 [00:56<02:04,  3.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 156/600 [00:56<02:04,  3.58it/s] 26%|â–ˆâ–ˆâ–Œ       | 157/600 [00:56<02:03,  3.58it/s] 26%|â–ˆâ–ˆâ–‹       | 158/600 [00:56<02:03,  3.58it/s] 26%|â–ˆâ–ˆâ–‹       | 159/600 [00:57<02:03,  3.58it/s] 27%|â–ˆâ–ˆâ–‹       | 160/600 [00:57<02:02,  3.58it/s] 27%|â–ˆâ–ˆâ–‹       | 161/600 [00:57<02:02,  3.58it/s] 27%|â–ˆâ–ˆâ–‹       | 162/600 [00:58<02:02,  3.58it/s] 27%|â–ˆâ–ˆâ–‹       | 163/600 [00:58<02:02,  3.58it/s] 27%|â–ˆâ–ˆâ–‹       | 164/600 [00:58<02:02,  3.57it/s] 28%|â–ˆâ–ˆâ–Š       | 165/600 [00:58<02:01,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 166/600 [00:59<02:01,  3.58it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|â–ˆâ–ˆâ–Š       | 167/600 [00:59<02:00,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 168/600 [00:59<02:00,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 169/600 [01:00<02:00,  3.58it/s] 28%|â–ˆâ–ˆâ–Š       | 170/600 [01:00<02:00,  3.57it/s] 28%|â–ˆâ–ˆâ–Š       | 171/600 [01:00<01:59,  3.58it/s] 29%|â–ˆâ–ˆâ–Š       | 172/600 [01:00<01:59,  3.58it/s] 29%|â–ˆâ–ˆâ–‰       | 173/600 [01:01<01:59,  3.58it/s] 29%|â–ˆâ–ˆâ–‰       | 174/600 [01:01<01:59,  3.58it/s] 29%|â–ˆâ–ˆâ–‰       | 175/600 [01:01<02:01,  3.51it/s] 29%|â–ˆâ–ˆâ–‰       | 176/600 [01:01<02:00,  3.53it/s] 30%|â–ˆâ–ˆâ–‰       | 177/600 [01:02<01:59,  3.55it/s] 30%|â–ˆâ–ˆâ–‰       | 178/600 [01:02<01:58,  3.56it/s] 30%|â–ˆâ–ˆâ–‰       | 179/600 [01:02<01:58,  3.57it/s] 30%|â–ˆâ–ˆâ–ˆ       | 180/600 [01:03<01:57,  3.57it/s] 30%|â–ˆâ–ˆâ–ˆ       | 181/600 [01:03<01:57,  3.58it/s] 30%|â–ˆâ–ˆâ–ˆ       | 182/600 [01:03<01:56,  3.58it/s] 30%|â–ˆâ–ˆâ–ˆ       | 183/600 [01:03<01:56,  3.58it/s] 31%|â–ˆâ–ˆâ–ˆ       | 184/600 [01:04<01:56,  3.58it/s] 31%|â–ˆâ–ˆâ–ˆ       | 185/600 [01:04<01:55,  3.58it/s] 31%|â–ˆâ–ˆâ–ˆ       | 186/600 [01:04<02:00,  3.45it/s] 31%|â–ˆâ–ˆâ–ˆ       | 187/600 [01:05<01:58,  3.49it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 188/600 [01:05<01:57,  3.51it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/600 [01:05<01:56,  3.53it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/600 [01:05<01:55,  3.55it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 191/600 [01:06<01:55,  3.55it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 192/600 [01:06<01:54,  3.56it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 193/600 [01:06<01:54,  3.57it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 194/600 [01:07<01:53,  3.57it/s] 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/600 [01:07<01:53,  3.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/600 [01:07<01:52,  3.58it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/600 [01:07<01:53,  3.55it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 198/600 [01:08<01:52,  3.56it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 199/600 [01:08<01:52,  3.57it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 200/600 [01:08<01:51,  3.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 201/600 [01:09<01:51,  3.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 202/600 [01:09<01:51,  3.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 203/600 [01:09<01:51,  3.58it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 204/600 [01:09<01:50,  3.58it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 205/600 [01:10<01:50,  3.58it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 206/600 [01:10<01:50,  3.58it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 207/600 [01:10<01:49,  3.58it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 208/600 [01:10<01:53,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 209/600 [01:11<01:52,  3.48it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/600 [01:11<01:51,  3.51it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/600 [01:11<01:50,  3.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/600 [01:12<01:49,  3.55it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 213/600 [01:12<01:48,  3.56it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 214/600 [01:12<01:48,  3.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 215/600 [01:12<01:47,  3.57it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 216/600 [01:13<01:47,  3.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 217/600 [01:13<01:47,  3.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 218/600 [01:13<01:46,  3.58it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 219/600 [01:14<01:46,  3.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 220/600 [01:14<01:46,  3.58it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 221/600 [01:14<01:48,  3.49it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 222/600 [01:14<01:47,  3.52it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 223/600 [01:15<01:46,  3.54it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 224/600 [01:15<01:45,  3.55it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/600 [01:15<01:45,  3.56it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 226/600 [01:16<01:44,  3.56it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 227/600 [01:16<01:44,  3.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 228/600 [01:16<01:44,  3.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 229/600 [01:16<01:43,  3.57it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 230/600 [01:17<01:43,  3.58it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 231/600 [01:17<01:43,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 232/600 [01:17<01:42,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 233/600 [01:18<01:42,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 234/600 [01:18<01:42,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 235/600 [01:18<01:42,  3.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 236/600 [01:18<01:41,  3.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 237/600 [01:19<01:41,  3.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 238/600 [01:19<01:41,  3.58it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 239/600 [01:19<01:44,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 240/600 [01:19<01:43,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 16:42:13,762 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:42:13,762 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:42:13,762 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3909, 'eval_samples_per_second': 370.465, 'eval_steps_per_second': 46.321, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.05it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.28it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.51it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.80it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.40it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.15it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.91it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.74it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.69it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.58it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.60it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.60it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.54it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.57it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.50it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.55it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.62it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.56it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.56it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.50it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.55it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.51it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.57it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.61it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.53it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.56it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.64it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.56it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.53it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.51it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.54it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.60it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.58it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.50it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.56it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.63it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.62it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.54it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.48it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.54it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:08, 27.74it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:07, 31.58it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:06, 34.95it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:05<00:05, 37.82it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:05<00:05, 40.00it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 41.81it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 43.06it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 44.07it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 44.70it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:04, 45.21it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 45.61it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 45.88it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.15it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:06<00:03, 46.27it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:06<00:03, 46.23it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.37it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.42it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.44it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.53it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.56it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.53it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.52it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:07<00:02, 46.56it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:07<00:02, 46.50it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.48it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.49it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.55it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.58it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.58it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.58it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.55it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:08<00:01, 46.64it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:08<00:01, 46.62it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.43it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.49it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.45it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.51it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.57it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.51it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.57it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.51it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:09<00:00, 46.61it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 46.53it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.21it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.41it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.46it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.46it/s][A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 240/600 [01:29<01:43,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:42:23,772 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 16:42:24,053 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:42:29,278 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:42:29,295 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:42:29,305 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/600 [01:36<30:48,  5.15s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 242/600 [01:36<21:59,  3.69s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 243/600 [01:37<15:51,  2.66s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 244/600 [01:37<11:33,  1.95s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 245/600 [01:37<08:33,  1.45s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 246/600 [01:37<06:28,  1.10s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 247/600 [01:38<05:00,  1.17it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/600 [01:38<03:59,  1.47it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249/600 [01:38<03:16,  1.79it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 250/600 [01:38<02:46,  2.10it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 251/600 [01:39<02:25,  2.40it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 252/600 [01:39<02:10,  2.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 253/600 [01:39<02:00,  2.89it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 254/600 [01:40<01:52,  3.06it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/600 [01:40<01:47,  3.21it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 256/600 [01:40<01:43,  3.31it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 257/600 [01:40<01:41,  3.39it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 258/600 [01:41<01:39,  3.45it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 259/600 [01:41<01:37,  3.49it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 260/600 [01:41<01:36,  3.52it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 261/600 [01:42<01:36,  3.53it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 262/600 [01:42<01:35,  3.55it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/600 [01:42<01:34,  3.56it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 264/600 [01:42<01:34,  3.57it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 265/600 [01:43<01:33,  3.57it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 266/600 [01:43<01:33,  3.58it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 267/600 [01:43<01:33,  3.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 268/600 [01:44<01:32,  3.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 269/600 [01:44<01:32,  3.57it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 270/600 [01:44<01:32,  3.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 271/600 [01:44<01:31,  3.58it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 272/600 [01:45<01:31,  3.57it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 273/600 [01:45<01:31,  3.57it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 274/600 [01:45<01:31,  3.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 275/600 [01:45<01:30,  3.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 276/600 [01:46<01:30,  3.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 277/600 [01:46<01:30,  3.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 278/600 [01:46<01:29,  3.58it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 279/600 [01:47<01:29,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 280/600 [01:47<01:29,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 281/600 [01:47<01:29,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 282/600 [01:47<01:28,  3.58it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 283/600 [01:48<01:35,  3.33it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 284/600 [01:48<01:33,  3.37it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/600 [01:48<01:31,  3.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 286/600 [01:49<01:30,  3.48it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 287/600 [01:49<01:29,  3.51it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 288/600 [01:49<01:30,  3.45it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 289/600 [01:49<01:29,  3.46it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 290/600 [01:50<01:28,  3.50it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 291/600 [01:50<01:27,  3.53it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 292/600 [01:50<01:26,  3.55it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 293/600 [01:51<01:26,  3.56it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 294/600 [01:51<01:26,  3.55it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 295/600 [01:51<01:25,  3.56it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 296/600 [01:51<01:25,  3.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 297/600 [01:52<01:24,  3.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 298/600 [01:52<01:24,  3.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 299/600 [01:52<01:24,  3.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 300/600 [01:53<01:23,  3.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 301/600 [01:53<01:23,  3.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 302/600 [01:53<01:23,  3.58it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 303/600 [01:53<01:22,  3.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 304/600 [01:54<01:22,  3.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 305/600 [01:54<01:22,  3.57it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 306/600 [01:54<01:22,  3.57it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 307/600 [01:55<01:21,  3.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 308/600 [01:55<01:21,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 309/600 [01:55<01:21,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 310/600 [01:55<01:21,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 311/600 [01:56<01:20,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 312/600 [01:56<01:20,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 313/600 [01:56<01:20,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 314/600 [01:56<01:19,  3.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 315/600 [01:57<01:19,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 316/600 [01:57<01:19,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 317/600 [01:57<01:18,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 318/600 [01:58<01:18,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 319/600 [01:58<01:18,  3.58it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 320/600 [01:58<01:18,  3.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 321/600 [01:58<01:17,  3.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 322/600 [01:59<01:17,  3.58it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 323/600 [01:59<01:17,  3.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 324/600 [01:59<01:16,  3.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 325/600 [02:00<01:16,  3.59it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 326/600 [02:00<01:16,  3.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 327/600 [02:00<01:16,  3.57it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 328/600 [02:00<01:16,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 329/600 [02:01<01:15,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 330/600 [02:01<01:15,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 331/600 [02:01<01:15,  3.58it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 332/600 [02:01<01:14,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 333/600 [02:02<01:14,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 334/600 [02:02<01:14,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 335/600 [02:02<01:13,  3.59it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 336/600 [02:03<01:13,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 337/600 [02:03<01:13,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 338/600 [02:03<01:13,  3.58it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 339/600 [02:03<01:12,  3.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 340/600 [02:04<01:12,  3.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 341/600 [02:04<01:12,  3.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 342/600 [02:04<01:12,  3.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 343/600 [02:05<01:11,  3.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 344/600 [02:05<01:11,  3.58it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 345/600 [02:05<01:13,  3.48it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 346/600 [02:05<01:12,  3.51it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 347/600 [02:06<01:11,  3.53it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 348/600 [02:06<01:11,  3.55it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 349/600 [02:06<01:10,  3.56it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 350/600 [02:07<01:10,  3.56it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 351/600 [02:07<01:09,  3.57it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 352/600 [02:07<01:09,  3.57it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 353/600 [02:07<01:09,  3.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 354/600 [02:08<01:08,  3.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 355/600 [02:08<01:08,  3.58it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 356/600 [02:08<01:08,  3.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 357/600 [02:08<01:07,  3.58it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 358/600 [02:09<01:07,  3.59it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 359/600 [02:09<01:07,  3.59it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 360/600 [02:09<01:06,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 16:43:03,599 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:43:03,599 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:43:03,599 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.6184, 'eval_samples_per_second': 361.702, 'eval_steps_per_second': 45.226, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.95it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.33it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.61it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.89it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.44it/s][A
  8%|â–Š         | 33/435 [00:00<00:09, 43.97it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 44.75it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 45.37it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 45.66it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 45.89it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.09it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 46.25it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.39it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.44it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.35it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.45it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.49it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.54it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.49it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.56it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.47it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.53it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.57it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.57it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 45.96it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.12it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.27it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.31it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.36it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.43it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.36it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.45it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.52it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.57it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.56it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.45it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.51it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.58it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.50it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.56it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.22it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.38it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.46it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.45it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.50it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.50it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.48it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.51it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.46it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.48it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.54it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.54it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 45.86it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 45.98it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.17it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.25it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.36it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.45it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.45it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.51it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.54it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.53it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.62it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.49it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.57it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.49it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.53it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.52it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.57it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.53it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.54it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.61it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.62it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.53it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.47it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.45it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.54it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.50it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.45it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.54it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.56it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.61it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.49it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.55it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.59it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.52it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.52it/s][A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 360/600 [02:19<01:06,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:43:13,019 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360
[INFO|configuration_utils.py:351] 2023-08-28 16:43:13,037 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:43:18,025 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:43:18,101 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:43:18,133 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 361/600 [02:25<19:11,  4.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 362/600 [02:25<13:42,  3.45s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 363/600 [02:25<09:52,  2.50s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 364/600 [02:26<07:13,  1.84s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 365/600 [02:26<05:21,  1.37s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 366/600 [02:26<04:03,  1.04s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 367/600 [02:26<03:09,  1.23it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 368/600 [02:27<02:31,  1.53it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 369/600 [02:27<02:04,  1.85it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 370/600 [02:27<01:47,  2.15it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 371/600 [02:28<01:33,  2.44it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 372/600 [02:28<01:24,  2.70it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 373/600 [02:28<01:17,  2.92it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/600 [02:28<01:13,  3.09it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 375/600 [02:29<01:09,  3.23it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 376/600 [02:29<01:07,  3.33it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 377/600 [02:29<01:05,  3.40it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 378/600 [02:29<01:04,  3.46it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 379/600 [02:30<01:03,  3.50it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 380/600 [02:30<01:02,  3.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 381/600 [02:30<01:02,  3.52it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 382/600 [02:31<01:01,  3.54it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 383/600 [02:31<01:01,  3.55it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 384/600 [02:31<01:00,  3.56it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 385/600 [02:31<01:00,  3.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 386/600 [02:32<00:59,  3.57it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 387/600 [02:32<00:59,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 388/600 [02:32<00:59,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 389/600 [02:33<00:58,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 390/600 [02:33<00:58,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 391/600 [02:33<00:58,  3.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 392/600 [02:33<00:58,  3.53it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 393/600 [02:34<00:58,  3.54it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 394/600 [02:34<00:57,  3.56it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 395/600 [02:34<00:57,  3.57it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 396/600 [02:35<00:57,  3.57it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 397/600 [02:35<00:56,  3.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 398/600 [02:35<00:56,  3.58it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 399/600 [02:35<00:56,  3.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 400/600 [02:36<00:55,  3.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 401/600 [02:36<00:55,  3.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 402/600 [02:36<00:55,  3.58it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 403/600 [02:36<00:55,  3.55it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 404/600 [02:37<00:55,  3.56it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 405/600 [02:37<00:54,  3.57it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 406/600 [02:37<00:54,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 407/600 [02:38<00:53,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 408/600 [02:38<00:53,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 409/600 [02:38<00:53,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 410/600 [02:38<00:53,  3.58it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 411/600 [02:39<00:52,  3.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 412/600 [02:39<00:52,  3.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 413/600 [02:39<00:52,  3.58it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 414/600 [02:40<00:54,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 415/600 [02:40<00:53,  3.48it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 416/600 [02:40<00:52,  3.51it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 417/600 [02:40<00:51,  3.53it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 418/600 [02:41<00:51,  3.55it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 419/600 [02:41<00:50,  3.56it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 420/600 [02:41<00:50,  3.57it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 421/600 [02:42<00:50,  3.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 422/600 [02:42<00:49,  3.58it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 423/600 [02:42<00:49,  3.59it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 424/600 [02:42<00:49,  3.58it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 425/600 [02:43<00:52,  3.32it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 426/600 [02:43<00:51,  3.40it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 427/600 [02:43<00:50,  3.45it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 428/600 [02:44<00:49,  3.49it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 429/600 [02:44<00:48,  3.52it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 430/600 [02:44<00:48,  3.54it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 431/600 [02:44<00:47,  3.56it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 432/600 [02:45<00:47,  3.57it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 433/600 [02:45<00:46,  3.57it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 434/600 [02:45<00:46,  3.58it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 435/600 [02:46<00:46,  3.58it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 436/600 [02:46<00:46,  3.52it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 437/600 [02:46<00:46,  3.54it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 438/600 [02:46<00:45,  3.55it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 439/600 [02:47<00:45,  3.56it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 440/600 [02:47<00:44,  3.57it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 441/600 [02:47<00:44,  3.57it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 442/600 [02:47<00:44,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 443/600 [02:48<00:44,  3.57it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 444/600 [02:48<00:43,  3.57it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 445/600 [02:48<00:43,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 446/600 [02:49<00:43,  3.58it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 447/600 [02:49<00:42,  3.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 448/600 [02:49<00:42,  3.58it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 449/600 [02:50<00:45,  3.34it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 450/600 [02:50<00:44,  3.38it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 451/600 [02:50<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 452/600 [02:50<00:42,  3.48it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 453/600 [02:51<00:41,  3.51it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 454/600 [02:51<00:41,  3.53it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 455/600 [02:51<00:40,  3.55it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 456/600 [02:52<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 457/600 [02:52<00:41,  3.48it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 458/600 [02:52<00:40,  3.52it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 459/600 [02:52<00:39,  3.54it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 460/600 [02:53<00:39,  3.55it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 461/600 [02:53<00:39,  3.56it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 462/600 [02:53<00:38,  3.57it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 463/600 [02:53<00:38,  3.58it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 464/600 [02:54<00:37,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 465/600 [02:54<00:37,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 466/600 [02:54<00:37,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 467/600 [02:55<00:37,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 468/600 [02:55<00:36,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 469/600 [02:55<00:36,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 470/600 [02:55<00:36,  3.58it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 471/600 [02:56<00:36,  3.58it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 472/600 [02:56<00:35,  3.58it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 473/600 [02:56<00:35,  3.58it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 474/600 [02:57<00:36,  3.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 475/600 [02:57<00:35,  3.49it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 476/600 [02:57<00:35,  3.52it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 477/600 [02:57<00:34,  3.54it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 478/600 [02:58<00:34,  3.55it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 479/600 [02:58<00:33,  3.56it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 480/600 [02:58<00:33,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 16:43:52,502 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:43:52,502 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:43:52,502 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.4114, 'eval_samples_per_second': 369.657, 'eval_steps_per_second': 46.22, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.95it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.41it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.49it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.86it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.44it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.18it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.03it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.79it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.60it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.53it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.52it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.53it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.52it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.55it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.58it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.59it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.61it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.66it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.61it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.54it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.49it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.49it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.55it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.53it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.58it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.59it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.42it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.44it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.51it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.43it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.49it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.50it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.58it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.61it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.56it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.53it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.29it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.59it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.61it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.49it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.51it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.58it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.56it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.61it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.54it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.59it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.61it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.58it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.64it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.53it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.50it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.56it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.59it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.59it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.54it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.55it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.61it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.61it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.57it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.48it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.49it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.55it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.58it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.59it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.30it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.61it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.51it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.59it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.46it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.50it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.55it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.52it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.57it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.56it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.49it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.61it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.64it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.50it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.44it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.46it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.53it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.53it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.52it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.50it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.47it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.54it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.54it/s][A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 480/600 [03:08<00:33,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:44:01,939 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480
[INFO|configuration_utils.py:351] 2023-08-28 16:44:01,964 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:44:06,234 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:44:06,259 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:44:06,268 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 481/600 [03:13<09:18,  4.69s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 482/600 [03:14<06:37,  3.37s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 483/600 [03:14<04:45,  2.44s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 484/600 [03:14<03:27,  1.79s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 485/600 [03:14<02:33,  1.34s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 486/600 [03:15<01:56,  1.02s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 487/600 [03:15<01:30,  1.25it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 488/600 [03:15<01:11,  1.56it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 489/600 [03:16<01:05,  1.71it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 490/600 [03:16<00:54,  2.02it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 491/600 [03:16<00:46,  2.33it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 492/600 [03:16<00:41,  2.60it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 493/600 [03:17<00:37,  2.83it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 494/600 [03:17<00:35,  3.02it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 495/600 [03:17<00:33,  3.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 496/600 [03:18<00:31,  3.29it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 497/600 [03:18<00:30,  3.37it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 498/600 [03:18<00:29,  3.43it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 499/600 [03:18<00:29,  3.48it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 500/600 [03:19<00:28,  3.49it/s]                                                  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 500/600 [03:19<00:28,  3.49it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 501/600 [03:19<00:28,  3.52it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 502/600 [03:19<00:27,  3.53it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/600 [03:20<00:27,  3.55it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 504/600 [03:20<00:26,  3.56it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 505/600 [03:20<00:26,  3.57it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 506/600 [03:20<00:26,  3.57it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 507/600 [03:21<00:26,  3.58it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 508/600 [03:21<00:25,  3.58it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 509/600 [03:21<00:25,  3.58it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 510/600 [03:21<00:25,  3.59it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 511/600 [03:22<00:26,  3.33it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 512/600 [03:22<00:25,  3.41it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 513/600 [03:22<00:25,  3.46it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 514/600 [03:23<00:24,  3.49it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 515/600 [03:23<00:24,  3.52it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 516/600 [03:23<00:23,  3.54it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 517/600 [03:24<00:23,  3.55it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 518/600 [03:24<00:23,  3.56it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 519/600 [03:24<00:22,  3.57it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 520/600 [03:24<00:22,  3.58it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 521/600 [03:25<00:22,  3.58it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 522/600 [03:25<00:21,  3.55it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 523/600 [03:25<00:21,  3.56it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 524/600 [03:25<00:21,  3.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 525/600 [03:26<00:20,  3.57it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 526/600 [03:26<00:20,  3.58it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 527/600 [03:26<00:20,  3.58it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 528/600 [03:27<00:20,  3.59it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 529/600 [03:27<00:19,  3.59it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 530/600 [03:27<00:19,  3.59it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 531/600 [03:27<00:19,  3.59it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 532/600 [03:28<00:18,  3.59it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 533/600 [03:28<00:18,  3.54it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 534/600 [03:28<00:18,  3.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 535/600 [03:29<00:18,  3.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 536/600 [03:29<00:17,  3.57it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 537/600 [03:29<00:17,  3.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 538/600 [03:29<00:17,  3.58it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 539/600 [03:30<00:17,  3.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 540/600 [03:30<00:16,  3.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 541/600 [03:30<00:16,  3.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 542/600 [03:31<00:16,  3.59it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 543/600 [03:31<00:15,  3.58it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 544/600 [03:31<00:16,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 545/600 [03:31<00:15,  3.48it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 546/600 [03:32<00:15,  3.51it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 547/600 [03:32<00:15,  3.53it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 548/600 [03:32<00:14,  3.54it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/600 [03:33<00:14,  3.55it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 550/600 [03:33<00:14,  3.56it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 551/600 [03:33<00:13,  3.57it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 552/600 [03:33<00:13,  3.57it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 553/600 [03:34<00:13,  3.57it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 554/600 [03:34<00:12,  3.58it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 555/600 [03:34<00:16,  2.69it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 556/600 [03:35<00:15,  2.91it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 557/600 [03:35<00:13,  3.08it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 558/600 [03:35<00:13,  3.22it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 559/600 [03:36<00:12,  3.32it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 560/600 [03:36<00:11,  3.39it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 561/600 [03:36<00:11,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 562/600 [03:36<00:10,  3.48it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/600 [03:37<00:10,  3.51it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 564/600 [03:37<00:10,  3.53it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 565/600 [03:37<00:09,  3.51it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 566/600 [03:38<00:09,  3.53it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 567/600 [03:38<00:09,  3.55it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 568/600 [03:38<00:08,  3.56it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 569/600 [03:38<00:08,  3.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 570/600 [03:39<00:08,  3.57it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 571/600 [03:39<00:08,  3.58it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 572/600 [03:39<00:07,  3.58it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 573/600 [03:40<00:07,  3.58it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 574/600 [03:40<00:07,  3.57it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 575/600 [03:40<00:06,  3.57it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 576/600 [03:40<00:06,  3.58it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 577/600 [03:41<00:06,  3.58it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 578/600 [03:41<00:06,  3.58it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 579/600 [03:41<00:05,  3.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 580/600 [03:41<00:05,  3.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 581/600 [03:42<00:05,  3.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 582/600 [03:42<00:05,  3.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 583/600 [03:42<00:04,  3.58it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 584/600 [03:43<00:04,  3.58it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 585/600 [03:43<00:04,  3.54it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 586/600 [03:43<00:03,  3.55it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 587/600 [03:43<00:03,  3.56it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 588/600 [03:44<00:03,  3.57it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 589/600 [03:44<00:03,  3.57it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 590/600 [03:44<00:02,  3.57it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 591/600 [03:45<00:02,  3.58it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 592/600 [03:45<00:02,  3.55it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 593/600 [03:45<00:01,  3.56it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 594/600 [03:45<00:01,  3.57it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 595/600 [03:46<00:01,  3.57it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 596/600 [03:46<00:01,  3.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 597/600 [03:46<00:00,  3.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 598/600 [03:47<00:00,  3.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 599/600 [03:47<00:00,  3.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [03:47<00:00,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 16:44:41,287 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:44:41,287 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:44:41,287 >>   Batch size = 8
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.3767, 'eval_samples_per_second': 371.024, 'eval_steps_per_second': 46.391, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6625e-05, 'epoch': 4.17}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.70it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.12it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.51it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.72it/s][A
  6%|â–‹         | 28/435 [00:00<00:09, 43.12it/s][A
  8%|â–Š         | 33/435 [00:00<00:09, 43.86it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 44.70it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 45.29it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 45.71it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 45.99it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.23it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 46.21it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.39it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.40it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.36it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.46it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.50it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:02<00:07, 46.51it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.54it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.60it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.58it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:08, 36.33it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:08, 38.83it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:07, 40.81it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:07, 42.41it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 43.56it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:03<00:06, 44.48it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 45.07it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 45.40it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 45.71it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:06, 45.96it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.11it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.29it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.28it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.38it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:04<00:05, 46.43it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.46it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.57it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.54it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.59it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.53it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.54it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.60it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.51it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:05<00:04, 46.47it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.53it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.56it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.58it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.63it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.60it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 44.37it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 45.04it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 45.51it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 45.74it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:06<00:03, 46.03it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.08it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.03it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.23it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.34it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.40it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.42it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.50it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.53it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:07<00:02, 46.46it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.51it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.48it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.46it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.55it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.57it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.58it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.59it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:08<00:01, 46.52it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.49it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.45it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.48it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.52it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.52it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.46it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.50it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.55it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:09<00:00, 46.52it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 46.54it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.55it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.23it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.42it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.42it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [03:57<00:00,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:44:50,897 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600
[INFO|configuration_utils.py:351] 2023-08-28 16:44:50,942 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:44:57,802 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:44:57,884 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:44:57,901 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:44:58,973 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:44:58,973 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120 (score: 1.0038264989852905).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [04:11<00:00,  3.58it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [04:11<00:00,  2.39it/s]
[INFO|trainer.py:1894] 2023-08-28 16:45:04,943 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 16:45:04,968 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:45:11,280 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:45:11,295 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:45:11,310 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:45:11,554 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:11,555 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:11,555 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:11,555 >>   train_runtime            = 0:04:11.22
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:11,555 >>   train_samples            =       7700
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:11,555 >>   train_samples_per_second =    153.248
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:11,555 >>   train_steps_per_second   =      2.388
{'eval_loss': 1.0038264989852905, 'eval_runtime': 9.5024, 'eval_samples_per_second': 366.116, 'eval_steps_per_second': 45.778, 'epoch': 5.0}
{'train_runtime': 251.2266, 'train_samples_per_second': 153.248, 'train_steps_per_second': 2.388, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 16:45:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:45:11,602 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:45:11,603 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 16:45:11,603 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 56.90it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 50.58it/s]  4%|â–         | 18/435 [00:00<00:08, 48.82it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.06it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.59it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.26it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.26it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.15it/s] 11%|â–ˆ         | 48/435 [00:01<00:08, 47.11it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 47.12it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 47.00it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 47.09it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 47.14it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 47.08it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 47.01it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 47.05it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 47.07it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 47.04it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 47.00it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 47.06it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.91it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 47.04it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 47.07it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 47.01it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 47.05it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.46it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.63it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.80it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.94it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.95it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.91it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.96it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.90it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.96it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.90it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.90it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 47.00it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 47.08it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 47.02it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 47.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 47.06it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.94it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.93it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.97it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.82it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.98it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.98it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.98it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 47.08it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 47.04it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.95it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.90it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.94it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.89it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:04, 39.01it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 41.14it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 42.76it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 43.96it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:03, 44.90it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 45.49it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 45.83it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.17it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.35it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.49it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.59it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.75it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.80it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.91it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.93it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 47.00it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.86it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.86it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.83it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.84it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.97it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.89it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.91it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.90it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.99it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.96it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.88it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.87it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.82it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.91it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.94it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.88it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.66it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:45:20,949 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:20,949 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:20,949 >>   eval_loss               =     1.0038
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:20,949 >>   eval_runtime            = 0:00:09.34
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:20,949 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:20,949 >>   eval_samples_per_second =    372.245
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:20,949 >>   eval_steps_per_second   =     46.544
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:45:20,949 >>   perplexity              =     2.7287
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:28,496 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:28,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:28,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:28,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:28,502 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:45:29,198 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:45:29,199 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:45:29,808 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:45:30,878 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:45:30,878 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:33,990 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:34,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:34,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:34,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:45:34,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:45:34,673 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:45:34,674 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:45:35,290 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:45:35,544 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:45:35,544 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-480
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-240
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-600
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-120
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/generator/iter5/model/checkpoint-360
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.22it/s]Extractor Predicting: 3it [00:02,  1.26it/s]Extractor Predicting: 4it [00:03,  1.26it/s]Extractor Predicting: 5it [00:03,  1.26it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.26it/s]Extractor Predicting: 8it [00:06,  1.27it/s]Extractor Predicting: 9it [00:07,  1.27it/s]Extractor Predicting: 10it [00:07,  1.31it/s]Extractor Predicting: 11it [00:08,  1.29it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:10,  1.26it/s]Extractor Predicting: 14it [00:11,  1.20it/s]Extractor Predicting: 15it [00:11,  1.25it/s]Extractor Predicting: 16it [00:12,  1.24it/s]Extractor Predicting: 17it [00:13,  1.28it/s]Extractor Predicting: 18it [00:14,  1.32it/s]Extractor Predicting: 19it [00:14,  1.30it/s]Extractor Predicting: 20it [00:15,  1.30it/s]Extractor Predicting: 21it [00:16,  1.29it/s]Extractor Predicting: 22it [00:17,  1.32it/s]Extractor Predicting: 23it [00:18,  1.31it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.27it/s]Extractor Predicting: 26it [00:20,  1.25it/s]Extractor Predicting: 27it [00:21,  1.24it/s]Extractor Predicting: 28it [00:22,  1.27it/s]Extractor Predicting: 29it [00:22,  1.26it/s]Extractor Predicting: 30it [00:23,  1.23it/s]Extractor Predicting: 31it [00:24,  1.24it/s]Extractor Predicting: 32it [00:25,  1.24it/s]Extractor Predicting: 33it [00:26,  1.24it/s]Extractor Predicting: 34it [00:26,  1.26it/s]Extractor Predicting: 35it [00:27,  1.27it/s]Extractor Predicting: 36it [00:28,  1.28it/s]Extractor Predicting: 37it [00:29,  1.29it/s]Extractor Predicting: 38it [00:29,  1.29it/s]Extractor Predicting: 39it [00:30,  1.30it/s]Extractor Predicting: 40it [00:31,  1.29it/s]Extractor Predicting: 41it [00:32,  1.29it/s]Extractor Predicting: 42it [00:33,  1.29it/s]Extractor Predicting: 43it [00:33,  1.27it/s]Extractor Predicting: 44it [00:34,  1.30it/s]Extractor Predicting: 45it [00:35,  1.28it/s]Extractor Predicting: 46it [00:36,  1.28it/s]Extractor Predicting: 47it [00:36,  1.26it/s]Extractor Predicting: 48it [00:37,  1.28it/s]Extractor Predicting: 49it [00:38,  1.28it/s]Extractor Predicting: 50it [00:39,  1.29it/s]Extractor Predicting: 51it [00:40,  1.31it/s]Extractor Predicting: 52it [00:40,  1.31it/s]Extractor Predicting: 53it [00:41,  1.31it/s]Extractor Predicting: 54it [00:42,  1.29it/s]Extractor Predicting: 55it [00:43,  1.27it/s]Extractor Predicting: 56it [00:43,  1.30it/s]Extractor Predicting: 57it [00:44,  1.28it/s]Extractor Predicting: 58it [00:45,  1.28it/s]Extractor Predicting: 59it [00:46,  1.31it/s]Extractor Predicting: 60it [00:46,  1.35it/s]Extractor Predicting: 61it [00:47,  1.35it/s]Extractor Predicting: 62it [00:48,  1.33it/s]Extractor Predicting: 63it [00:49,  1.31it/s]Extractor Predicting: 64it [00:49,  1.32it/s]Extractor Predicting: 65it [00:50,  1.33it/s]Extractor Predicting: 66it [00:51,  1.33it/s]Extractor Predicting: 67it [00:52,  1.34it/s]Extractor Predicting: 68it [00:52,  1.33it/s]Extractor Predicting: 69it [00:53,  1.37it/s]Extractor Predicting: 70it [00:54,  1.37it/s]Extractor Predicting: 71it [00:55,  1.38it/s]Extractor Predicting: 72it [00:55,  1.35it/s]Extractor Predicting: 73it [00:56,  1.35it/s]Extractor Predicting: 74it [00:57,  1.35it/s]Extractor Predicting: 75it [00:58,  1.31it/s]Extractor Predicting: 76it [00:58,  1.30it/s]Extractor Predicting: 77it [00:59,  1.30it/s]Extractor Predicting: 78it [01:00,  1.33it/s]Extractor Predicting: 79it [01:01,  1.36it/s]Extractor Predicting: 80it [01:01,  1.35it/s]Extractor Predicting: 81it [01:02,  1.34it/s]Extractor Predicting: 82it [01:03,  1.33it/s]Extractor Predicting: 83it [01:04,  1.34it/s]Extractor Predicting: 84it [01:04,  1.35it/s]Extractor Predicting: 85it [01:05,  1.37it/s]Extractor Predicting: 86it [01:06,  1.38it/s]Extractor Predicting: 87it [01:06,  1.40it/s]Extractor Predicting: 88it [01:07,  1.39it/s]Extractor Predicting: 89it [01:08,  1.39it/s]Extractor Predicting: 90it [01:09,  1.40it/s]Extractor Predicting: 91it [01:09,  1.43it/s]Extractor Predicting: 92it [01:10,  1.46it/s]Extractor Predicting: 93it [01:11,  1.43it/s]Extractor Predicting: 94it [01:11,  1.42it/s]Extractor Predicting: 95it [01:12,  1.43it/s]Extractor Predicting: 96it [01:13,  1.41it/s]Extractor Predicting: 97it [01:13,  1.41it/s]Extractor Predicting: 98it [01:14,  1.40it/s]Extractor Predicting: 99it [01:15,  1.39it/s]Extractor Predicting: 100it [01:16,  1.35it/s]Extractor Predicting: 101it [01:16,  1.35it/s]Extractor Predicting: 102it [01:17,  1.43it/s]Extractor Predicting: 103it [01:18,  1.42it/s]Extractor Predicting: 104it [01:18,  1.41it/s]Extractor Predicting: 105it [01:19,  1.41it/s]Extractor Predicting: 106it [01:20,  1.41it/s]Extractor Predicting: 107it [01:21,  1.41it/s]Extractor Predicting: 108it [01:21,  1.41it/s]Extractor Predicting: 109it [01:22,  1.41it/s]Extractor Predicting: 110it [01:23,  1.41it/s]Extractor Predicting: 111it [01:24,  1.30it/s]Extractor Predicting: 112it [01:24,  1.34it/s]Extractor Predicting: 113it [01:25,  1.40it/s]Extractor Predicting: 114it [01:26,  1.41it/s]Extractor Predicting: 115it [01:26,  1.43it/s]Extractor Predicting: 116it [01:27,  1.40it/s]Extractor Predicting: 117it [01:28,  1.32it/s]Extractor Predicting: 118it [01:29,  1.34it/s]Extractor Predicting: 119it [01:29,  1.32it/s]Extractor Predicting: 120it [01:30,  1.34it/s]Extractor Predicting: 121it [01:31,  1.33it/s]Extractor Predicting: 122it [01:32,  1.32it/s]Extractor Predicting: 123it [01:32,  1.34it/s]Extractor Predicting: 124it [01:33,  1.28it/s]Extractor Predicting: 125it [01:34,  1.32it/s]Extractor Predicting: 126it [01:35,  1.31it/s]Extractor Predicting: 127it [01:35,  1.34it/s]Extractor Predicting: 128it [01:36,  1.35it/s]Extractor Predicting: 129it [01:37,  1.33it/s]Extractor Predicting: 130it [01:38,  1.32it/s]Extractor Predicting: 131it [01:39,  1.33it/s]Extractor Predicting: 132it [01:39,  1.32it/s]Extractor Predicting: 133it [01:40,  1.31it/s]Extractor Predicting: 134it [01:41,  1.29it/s]Extractor Predicting: 135it [01:42,  1.31it/s]Extractor Predicting: 136it [01:42,  1.30it/s]Extractor Predicting: 137it [01:43,  1.30it/s]Extractor Predicting: 138it [01:44,  1.27it/s]Extractor Predicting: 139it [01:45,  1.28it/s]Extractor Predicting: 140it [01:46,  1.27it/s]Extractor Predicting: 141it [01:46,  1.30it/s]Extractor Predicting: 142it [01:47,  1.29it/s]Extractor Predicting: 143it [01:48,  1.30it/s]Extractor Predicting: 144it [01:48,  1.61it/s]Extractor Predicting: 144it [01:48,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:33,637 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:33,642 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:33,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:33,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:33,643 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:47:34,111 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:47:34,112 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:47:34,486 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:47:35,672 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:47:35,672 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:37,558 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:37,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:37,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:37,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:47:37,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:47:38,073 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:47:38,074 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:47:38,422 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:47:38,595 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:47:38,595 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.18it/s]Extractor Predicting: 3it [00:02,  1.26it/s]Extractor Predicting: 4it [00:03,  1.31it/s]Extractor Predicting: 5it [00:03,  1.27it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.35it/s]Extractor Predicting: 8it [00:06,  1.41it/s]Extractor Predicting: 9it [00:06,  1.38it/s]Extractor Predicting: 10it [00:07,  1.40it/s]Extractor Predicting: 11it [00:08,  1.41it/s]Extractor Predicting: 12it [00:08,  1.39it/s]Extractor Predicting: 13it [00:09,  1.39it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:11,  1.41it/s]Extractor Predicting: 16it [00:11,  1.41it/s]Extractor Predicting: 17it [00:12,  1.36it/s]Extractor Predicting: 18it [00:13,  1.40it/s]Extractor Predicting: 19it [00:13,  1.43it/s]Extractor Predicting: 20it [00:14,  1.39it/s]Extractor Predicting: 21it [00:15,  1.41it/s]Extractor Predicting: 22it [00:15,  1.42it/s]Extractor Predicting: 23it [00:16,  1.42it/s]Extractor Predicting: 24it [00:17,  1.39it/s]Extractor Predicting: 25it [00:18,  1.38it/s]Extractor Predicting: 26it [00:18,  1.39it/s]Extractor Predicting: 27it [00:19,  1.38it/s]Extractor Predicting: 28it [00:20,  1.38it/s]Extractor Predicting: 29it [00:21,  1.39it/s]Extractor Predicting: 30it [00:21,  1.41it/s]Extractor Predicting: 31it [00:22,  1.38it/s]Extractor Predicting: 32it [00:23,  1.42it/s]Extractor Predicting: 33it [00:23,  1.40it/s]Extractor Predicting: 34it [00:24,  1.37it/s]Extractor Predicting: 35it [00:25,  1.38it/s]Extractor Predicting: 36it [00:26,  1.39it/s]Extractor Predicting: 37it [00:26,  1.41it/s]Extractor Predicting: 38it [00:27,  1.41it/s]Extractor Predicting: 39it [00:28,  1.40it/s]Extractor Predicting: 40it [00:28,  1.38it/s]Extractor Predicting: 41it [00:29,  1.38it/s]Extractor Predicting: 42it [00:30,  1.42it/s]Extractor Predicting: 43it [00:31,  1.38it/s]Extractor Predicting: 44it [00:31,  1.37it/s]Extractor Predicting: 45it [00:32,  1.36it/s]Extractor Predicting: 46it [00:33,  1.34it/s]Extractor Predicting: 47it [00:34,  1.38it/s]Extractor Predicting: 48it [00:34,  1.36it/s]Extractor Predicting: 49it [00:35,  1.39it/s]Extractor Predicting: 50it [00:36,  1.37it/s]Extractor Predicting: 51it [00:36,  1.37it/s]Extractor Predicting: 52it [00:37,  1.35it/s]Extractor Predicting: 53it [00:38,  1.36it/s]Extractor Predicting: 54it [00:39,  1.35it/s]Extractor Predicting: 55it [00:39,  1.39it/s]Extractor Predicting: 56it [00:40,  1.38it/s]Extractor Predicting: 57it [00:41,  1.36it/s]Extractor Predicting: 58it [00:42,  1.36it/s]Extractor Predicting: 59it [00:42,  1.36it/s]Extractor Predicting: 60it [00:43,  1.35it/s]Extractor Predicting: 61it [00:44,  1.34it/s]Extractor Predicting: 62it [00:45,  1.36it/s]Extractor Predicting: 63it [00:45,  1.36it/s]Extractor Predicting: 64it [00:46,  1.39it/s]Extractor Predicting: 65it [00:47,  1.37it/s]Extractor Predicting: 66it [00:48,  1.33it/s]Extractor Predicting: 67it [00:48,  1.33it/s]Extractor Predicting: 68it [00:49,  1.33it/s]Extractor Predicting: 69it [00:50,  1.32it/s]Extractor Predicting: 70it [00:51,  1.33it/s]Extractor Predicting: 71it [00:51,  1.34it/s]Extractor Predicting: 72it [00:52,  1.32it/s]Extractor Predicting: 73it [00:53,  1.34it/s]Extractor Predicting: 74it [00:54,  1.35it/s]Extractor Predicting: 75it [00:54,  1.36it/s]Extractor Predicting: 76it [00:55,  1.34it/s]Extractor Predicting: 77it [00:56,  1.39it/s]Extractor Predicting: 78it [00:56,  1.38it/s]Extractor Predicting: 79it [00:57,  1.41it/s]Extractor Predicting: 80it [00:58,  1.43it/s]Extractor Predicting: 81it [00:58,  1.43it/s]Extractor Predicting: 82it [00:59,  1.42it/s]Extractor Predicting: 83it [01:00,  1.40it/s]Extractor Predicting: 84it [01:01,  1.38it/s]Extractor Predicting: 85it [01:01,  1.35it/s]Extractor Predicting: 86it [01:02,  1.32it/s]Extractor Predicting: 87it [01:03,  1.30it/s]Extractor Predicting: 88it [01:04,  1.31it/s]Extractor Predicting: 89it [01:05,  1.30it/s]Extractor Predicting: 90it [01:05,  1.30it/s]Extractor Predicting: 91it [01:06,  1.29it/s]Extractor Predicting: 92it [01:07,  1.32it/s]Extractor Predicting: 93it [01:08,  1.34it/s]Extractor Predicting: 94it [01:08,  1.34it/s]Extractor Predicting: 95it [01:09,  1.35it/s]Extractor Predicting: 96it [01:10,  1.32it/s]Extractor Predicting: 97it [01:11,  1.31it/s]Extractor Predicting: 98it [01:11,  1.30it/s]Extractor Predicting: 99it [01:12,  1.33it/s]Extractor Predicting: 100it [01:13,  1.34it/s]Extractor Predicting: 101it [01:14,  1.37it/s]Extractor Predicting: 102it [01:14,  1.34it/s]Extractor Predicting: 103it [01:15,  1.28it/s]Extractor Predicting: 104it [01:16,  1.21it/s]Extractor Predicting: 105it [01:17,  1.23it/s]Extractor Predicting: 106it [01:18,  1.26it/s]Extractor Predicting: 107it [01:18,  1.29it/s]Extractor Predicting: 108it [01:19,  1.29it/s]Extractor Predicting: 109it [01:20,  1.29it/s]Extractor Predicting: 110it [01:21,  1.30it/s]Extractor Predicting: 111it [01:21,  1.32it/s]Extractor Predicting: 112it [01:22,  1.30it/s]Extractor Predicting: 113it [01:23,  1.32it/s]Extractor Predicting: 114it [01:24,  1.33it/s]Extractor Predicting: 115it [01:25,  1.29it/s]Extractor Predicting: 116it [01:25,  1.31it/s]Extractor Predicting: 117it [01:26,  1.31it/s]Extractor Predicting: 118it [01:27,  1.31it/s]Extractor Predicting: 119it [01:28,  1.31it/s]Extractor Predicting: 120it [01:28,  1.33it/s]Extractor Predicting: 121it [01:29,  1.32it/s]Extractor Predicting: 122it [01:30,  1.33it/s]Extractor Predicting: 123it [01:31,  1.33it/s]Extractor Predicting: 124it [01:31,  1.35it/s]Extractor Predicting: 125it [01:32,  1.37it/s]Extractor Predicting: 126it [01:33,  1.36it/s]Extractor Predicting: 127it [01:33,  1.35it/s]Extractor Predicting: 128it [01:34,  1.38it/s]Extractor Predicting: 129it [01:35,  1.38it/s]Extractor Predicting: 130it [01:36,  1.35it/s]Extractor Predicting: 131it [01:36,  1.39it/s]Extractor Predicting: 132it [01:37,  1.40it/s]Extractor Predicting: 133it [01:38,  1.42it/s]Extractor Predicting: 134it [01:38,  1.37it/s]Extractor Predicting: 135it [01:39,  1.39it/s]Extractor Predicting: 136it [01:40,  1.40it/s]Extractor Predicting: 137it [01:41,  1.41it/s]Extractor Predicting: 138it [01:41,  1.39it/s]Extractor Predicting: 139it [01:42,  1.39it/s]Extractor Predicting: 140it [01:43,  1.39it/s]Extractor Predicting: 141it [01:44,  1.36it/s]Extractor Predicting: 142it [01:44,  1.35it/s]Extractor Predicting: 143it [01:45,  1.37it/s]Extractor Predicting: 144it [01:46,  1.36it/s]Extractor Predicting: 145it [01:46,  1.40it/s]Extractor Predicting: 146it [01:47,  1.43it/s]Extractor Predicting: 147it [01:48,  1.43it/s]Extractor Predicting: 148it [01:48,  1.46it/s]Extractor Predicting: 149it [01:49,  1.43it/s]Extractor Predicting: 150it [01:50,  1.45it/s]Extractor Predicting: 151it [01:50,  1.46it/s]Extractor Predicting: 152it [01:51,  1.48it/s]Extractor Predicting: 153it [01:52,  1.45it/s]Extractor Predicting: 154it [01:53,  1.46it/s]Extractor Predicting: 155it [01:53,  1.50it/s]Extractor Predicting: 156it [01:54,  1.49it/s]Extractor Predicting: 157it [01:54,  1.55it/s]Extractor Predicting: 158it [01:55,  1.57it/s]Extractor Predicting: 159it [01:56,  1.53it/s]Extractor Predicting: 160it [01:56,  1.48it/s]Extractor Predicting: 161it [01:57,  1.47it/s]Extractor Predicting: 162it [01:58,  1.49it/s]Extractor Predicting: 163it [01:58,  1.51it/s]Extractor Predicting: 164it [01:59,  1.51it/s]Extractor Predicting: 165it [02:00,  1.51it/s]Extractor Predicting: 166it [02:00,  1.48it/s]Extractor Predicting: 167it [02:01,  1.51it/s]Extractor Predicting: 168it [02:02,  1.49it/s]Extractor Predicting: 169it [02:02,  1.55it/s]Extractor Predicting: 170it [02:03,  1.53it/s]Extractor Predicting: 171it [02:04,  1.53it/s]Extractor Predicting: 172it [02:04,  1.46it/s]Extractor Predicting: 173it [02:05,  1.44it/s]Extractor Predicting: 174it [02:06,  1.41it/s]Extractor Predicting: 175it [02:07,  1.36it/s]Extractor Predicting: 176it [02:07,  1.37it/s]Extractor Predicting: 177it [02:08,  1.37it/s]Extractor Predicting: 178it [02:09,  1.35it/s]Extractor Predicting: 179it [02:10,  1.35it/s]Extractor Predicting: 180it [02:10,  1.36it/s]Extractor Predicting: 181it [02:11,  1.36it/s]Extractor Predicting: 182it [02:12,  1.35it/s]Extractor Predicting: 183it [02:13,  1.35it/s]Extractor Predicting: 184it [02:13,  1.34it/s]Extractor Predicting: 185it [02:14,  1.32it/s]Extractor Predicting: 186it [02:15,  1.30it/s]Extractor Predicting: 187it [02:16,  1.31it/s]Extractor Predicting: 188it [02:16,  1.32it/s]Extractor Predicting: 189it [02:17,  1.33it/s]Extractor Predicting: 190it [02:18,  1.35it/s]Extractor Predicting: 191it [02:19,  1.32it/s]Extractor Predicting: 192it [02:20,  1.29it/s]Extractor Predicting: 193it [02:20,  1.29it/s]Extractor Predicting: 194it [02:21,  1.30it/s]Extractor Predicting: 195it [02:22,  1.29it/s]Extractor Predicting: 196it [02:23,  1.31it/s]Extractor Predicting: 197it [02:23,  1.31it/s]Extractor Predicting: 198it [02:24,  1.34it/s]Extractor Predicting: 199it [02:25,  1.33it/s]Extractor Predicting: 200it [02:26,  1.32it/s]Extractor Predicting: 201it [02:26,  1.30it/s]Extractor Predicting: 202it [02:27,  1.37it/s]Extractor Predicting: 203it [02:28,  1.36it/s]Extractor Predicting: 204it [02:28,  1.38it/s]Extractor Predicting: 205it [02:29,  1.36it/s]Extractor Predicting: 206it [02:30,  1.23it/s]Extractor Predicting: 207it [02:31,  1.24it/s]Extractor Predicting: 208it [02:32,  1.27it/s]Extractor Predicting: 209it [02:33,  1.23it/s]Extractor Predicting: 210it [02:33,  1.26it/s]Extractor Predicting: 211it [02:34,  1.27it/s]Extractor Predicting: 212it [02:35,  1.27it/s]Extractor Predicting: 213it [02:36,  1.28it/s]Extractor Predicting: 214it [02:37,  1.27it/s]Extractor Predicting: 215it [02:37,  1.27it/s]Extractor Predicting: 216it [02:38,  1.25it/s]Extractor Predicting: 217it [02:39,  1.27it/s]Extractor Predicting: 218it [02:40,  1.29it/s]Extractor Predicting: 219it [02:40,  1.27it/s]Extractor Predicting: 220it [02:41,  1.29it/s]Extractor Predicting: 221it [02:42,  1.26it/s]Extractor Predicting: 222it [02:43,  1.26it/s]Extractor Predicting: 223it [02:44,  1.29it/s]Extractor Predicting: 224it [02:44,  1.29it/s]Extractor Predicting: 225it [02:45,  1.30it/s]Extractor Predicting: 226it [02:46,  1.28it/s]Extractor Predicting: 227it [02:47,  1.33it/s]Extractor Predicting: 228it [02:47,  1.32it/s]Extractor Predicting: 229it [02:48,  1.31it/s]Extractor Predicting: 230it [02:49,  1.36it/s]Extractor Predicting: 231it [02:50,  1.37it/s]Extractor Predicting: 232it [02:50,  1.39it/s]Extractor Predicting: 233it [02:51,  1.34it/s]Extractor Predicting: 234it [02:52,  1.35it/s]Extractor Predicting: 235it [02:52,  1.35it/s]Extractor Predicting: 236it [02:53,  1.33it/s]Extractor Predicting: 237it [02:54,  1.31it/s]Extractor Predicting: 238it [02:55,  1.33it/s]Extractor Predicting: 239it [02:56,  1.31it/s]Extractor Predicting: 240it [02:56,  1.28it/s]Extractor Predicting: 241it [02:57,  1.25it/s]Extractor Predicting: 242it [02:58,  1.29it/s]Extractor Predicting: 243it [02:59,  1.28it/s]Extractor Predicting: 244it [02:59,  1.31it/s]Extractor Predicting: 245it [03:00,  1.33it/s]Extractor Predicting: 246it [03:01,  1.30it/s]Extractor Predicting: 247it [03:02,  1.32it/s]Extractor Predicting: 248it [03:02,  1.32it/s]Extractor Predicting: 249it [03:03,  1.32it/s]Extractor Predicting: 250it [03:04,  1.34it/s]Extractor Predicting: 251it [03:05,  1.35it/s]Extractor Predicting: 252it [03:05,  1.35it/s]Extractor Predicting: 253it [03:06,  1.33it/s]Extractor Predicting: 254it [03:07,  1.34it/s]Extractor Predicting: 255it [03:08,  1.30it/s]Extractor Predicting: 256it [03:09,  1.27it/s]Extractor Predicting: 257it [03:09,  1.28it/s]Extractor Predicting: 258it [03:10,  1.28it/s]Extractor Predicting: 259it [03:11,  1.30it/s]Extractor Predicting: 260it [03:12,  1.28it/s]Extractor Predicting: 261it [03:12,  1.31it/s]Extractor Predicting: 262it [03:13,  1.30it/s]Extractor Predicting: 263it [03:14,  1.28it/s]Extractor Predicting: 264it [03:15,  1.28it/s]Extractor Predicting: 265it [03:16,  1.24it/s]Extractor Predicting: 266it [03:16,  1.25it/s]Extractor Predicting: 267it [03:17,  1.25it/s]Extractor Predicting: 268it [03:18,  1.24it/s]Extractor Predicting: 269it [03:19,  1.27it/s]Extractor Predicting: 270it [03:20,  1.29it/s]Extractor Predicting: 271it [03:20,  1.27it/s]Extractor Predicting: 272it [03:21,  1.28it/s]Extractor Predicting: 273it [03:22,  1.29it/s]Extractor Predicting: 274it [03:23,  1.31it/s]Extractor Predicting: 275it [03:23,  1.33it/s]Extractor Predicting: 276it [03:24,  1.36it/s]Extractor Predicting: 277it [03:25,  1.36it/s]Extractor Predicting: 278it [03:26,  1.35it/s]Extractor Predicting: 279it [03:26,  1.35it/s]Extractor Predicting: 280it [03:27,  1.28it/s]Extractor Predicting: 281it [03:28,  1.29it/s]Extractor Predicting: 282it [03:29,  1.38it/s]Extractor Predicting: 282it [03:29,  1.35it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:15,239 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:15,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:15,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:15,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:15,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:51:15,886 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:51:15,887 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:51:16,456 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:51:17,506 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:51:17,506 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:20,444 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:20,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:20,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:20,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:51:20,446 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:51:21,096 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:51:21,097 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:51:21,728 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:51:21,943 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:51:21,943 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.23it/s]Extractor Predicting: 2it [00:01,  1.21it/s]Extractor Predicting: 3it [00:02,  1.23it/s]Extractor Predicting: 4it [00:03,  1.25it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 5it [00:03,  1.27it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_1', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_1/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:31<07:19, 31.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:57<06:07, 28.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:23<05:28, 27.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:48<04:49, 26.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [02:12<04:13, 25.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:36<03:44, 24.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:59<03:13, 24.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [03:23<02:49, 24.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:47<02:24, 24.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [04:09<01:57, 23.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [04:33<01:35, 23.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:55<01:10, 23.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [05:20<00:47, 23.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [05:45<00:23, 23.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:09<00:00, 24.27s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [06:09<00:00, 24.66s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n']
['Relation : country . Context : Later in 2008 , the band became a rock band called The Three Kingdoms , which included vocalist Eric Bonta and keyboardist Dave Martin . Head Entity : The Three Kingdoms , Tail Entity : American .\n', 'Relation : country . Context : After the death of his son Richard , his elder sister Ellen entered the church congregation and began publishing his book . Head Entity : Richard , Tail Entity : Wales .\n', 'Relation : country . Context : This film explores the social implications of the events leading up to and following the genocide of Armenians by Soviet forces during World War Two . Head Entity : Holocaust , Tail Entity : Soviet .\n', 'Relation : country . Context : In 1994 , , he was named as the winner of the Eurovision Song Contest 2000 . Head Entity : Eurovision Song Contest 2000 , Tail Entity : France .\n']
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 63, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 101, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 137, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 174, 'raw': 288}
{'target': 600, 'success': 190, 'raw': 320}
{'target': 600, 'success': 211, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 252, 'raw': 416}
{'target': 600, 'success': 274, 'raw': 448}
{'target': 600, 'success': 294, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 329, 'raw': 544}
{'target': 600, 'success': 347, 'raw': 576}
{'target': 600, 'success': 361, 'raw': 608}
{'target': 600, 'success': 388, 'raw': 640}
{'target': 600, 'success': 408, 'raw': 672}
{'target': 600, 'success': 428, 'raw': 704}
{'target': 600, 'success': 443, 'raw': 736}
{'target': 600, 'success': 462, 'raw': 768}
{'target': 600, 'success': 483, 'raw': 800}
{'target': 600, 'success': 501, 'raw': 832}
{'target': 600, 'success': 519, 'raw': 864}
{'target': 600, 'success': 541, 'raw': 896}
{'target': 600, 'success': 562, 'raw': 928}
{'target': 600, 'success': 585, 'raw': 960}
{'target': 600, 'success': 608, 'raw': 992}
{'prompt': 'Relation : country .', 'success_rate': 0.6129032258064516, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 296, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 145, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 233, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 360, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 425, 'raw': 576}
{'target': 600, 'success': 446, 'raw': 608}
{'target': 600, 'success': 464, 'raw': 640}
{'target': 600, 'success': 485, 'raw': 672}
{'target': 600, 'success': 510, 'raw': 704}
{'target': 600, 'success': 533, 'raw': 736}
{'target': 600, 'success': 561, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 609, 'raw': 832}
{'prompt': 'Relation : genre .', 'success_rate': 0.7319711538461539, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year , the band formed New River Band with two of their members at the end of 2010 , Mikey McLeod ( the lyricist ) and Tim McCarroll ( bass ) . Head Entity : Mikey McNamara , Tail Entity : New River Band .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 466, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 515, 'raw': 640}
{'target': 600, 'success': 539, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : member of .', 'success_rate': 0.8111979166666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
['Relation : subsidiary . Context : The CIBN ( CBNI ) , also called CBE ( CBW , CBQ , CBQR , CJAX , CJD , CJEC , CJF , CJFY ) , is a United States National Research Council scientific satellite constellation . Head Entity : CBI , Tail Entity : United States National Research Council .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 579, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8220108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('A', 'subsidiary', '', 'On August 2017 , the company began shipping a new product for children in Europe : A .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', "('Battle of Brackewald', 'field of work', '', 'In 1849 he became a volunteer for the British Army at the Battle of Brackewald in Normandy .')"}}
['Relation : instrument . Context : Later in the year ( 1141â€“1231 ) he met Ferdinand I of Spain and the Duke of Prussia , whom he bore in the name of Christophe , together with Robert I of Belgium . Head Entity : Christophe , Tail Entity : John I of Belgium .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.859375, 'errors': {''}}
["Relation : occupation . Context : On 31 March 2014 , the Romanian Army , under a new President of Romania , Luiz InÃ¡cio Lutcic , announced the departure of Luiz 's second - generation Air Force commander , former Admiral of Romania , Sigmund Kaveliu . Head Entity : GeniÃ§iu Lutcic , Tail Entity : Romanian Army .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8072916666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 600, 'raw': 736}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8152173913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 302, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 348, 'raw': 448}
{'target': 600, 'success': 372, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 427, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 469, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 523, 'raw': 672}
{'target': 600, 'success': 546, 'raw': 704}
{'target': 600, 'success': 572, 'raw': 736}
{'target': 600, 'success': 596, 'raw': 768}
{'target': 600, 'success': 620, 'raw': 800}
{'prompt': 'Relation : platform .', 'success_rate': 0.775, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 364, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 624, 'raw': 768}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 432, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 534, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.7994791666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/0_ext.jsonl'}}
estimate vocab size: 14467
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14567, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_1/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.43s/it]Extractor Estimating: 2it [00:19,  8.28s/it]Extractor Estimating: 3it [00:20,  5.14s/it]Extractor Estimating: 4it [00:21,  3.48s/it]Extractor Estimating: 5it [00:22,  2.49s/it]Extractor Estimating: 6it [00:22,  1.90s/it]Extractor Estimating: 7it [00:23,  1.53s/it]Extractor Estimating: 8it [00:24,  1.30s/it]Extractor Estimating: 9it [00:25,  1.14s/it]Extractor Estimating: 10it [00:25,  1.03s/it]Extractor Estimating: 11it [00:26,  1.04it/s]Extractor Estimating: 12it [00:27,  1.07it/s]Extractor Estimating: 13it [00:28,  1.09it/s]Extractor Estimating: 14it [00:29,  1.13it/s]Extractor Estimating: 15it [00:30,  1.16it/s]Extractor Estimating: 16it [00:31,  1.04s/it]Extractor Estimating: 17it [00:32,  1.03it/s]Extractor Estimating: 18it [00:34,  1.29s/it]Extractor Estimating: 19it [00:35,  1.13s/it]Extractor Estimating: 20it [00:36,  1.04s/it]Extractor Estimating: 21it [00:36,  1.03it/s]Extractor Estimating: 22it [00:37,  1.05it/s]Extractor Estimating: 23it [00:38,  1.10it/s]Extractor Estimating: 24it [00:39,  1.13it/s]Extractor Estimating: 25it [00:40,  1.17it/s]Extractor Estimating: 26it [00:41,  1.12it/s]Extractor Estimating: 27it [00:42,  1.07it/s]Extractor Estimating: 28it [00:43,  1.11it/s]Extractor Estimating: 29it [00:43,  1.14it/s]Extractor Estimating: 30it [00:44,  1.13it/s]Extractor Estimating: 31it [00:45,  1.20it/s]Extractor Estimating: 32it [00:46,  1.23it/s]Extractor Estimating: 33it [00:47,  1.20it/s]Extractor Estimating: 34it [00:47,  1.21it/s]Extractor Estimating: 35it [00:48,  1.21it/s]Extractor Estimating: 36it [00:49,  1.21it/s]Extractor Estimating: 37it [00:50,  1.21it/s]Extractor Estimating: 38it [00:51,  1.18it/s]Extractor Estimating: 39it [00:52,  1.18it/s]Extractor Estimating: 40it [00:52,  1.24it/s]Extractor Estimating: 41it [00:53,  1.21it/s]Extractor Estimating: 42it [00:54,  1.18it/s]Extractor Estimating: 43it [00:55,  1.13it/s]Extractor Estimating: 44it [00:56,  1.13it/s]Extractor Estimating: 45it [00:57,  1.17it/s]Extractor Estimating: 46it [00:58,  1.19it/s]Extractor Estimating: 47it [00:58,  1.18it/s]Extractor Estimating: 48it [00:59,  1.22it/s]Extractor Estimating: 49it [01:01,  1.20s/it]Extractor Estimating: 50it [01:02,  1.09s/it]Extractor Estimating: 51it [01:03,  1.01s/it]Extractor Estimating: 52it [01:04,  1.04it/s]Extractor Estimating: 53it [01:05,  1.10it/s]Extractor Estimating: 54it [01:05,  1.13it/s]Extractor Estimating: 55it [01:06,  1.17it/s]Extractor Estimating: 56it [01:07,  1.18it/s]Extractor Estimating: 57it [01:08,  1.18it/s]Extractor Estimating: 58it [01:09,  1.20it/s]Extractor Estimating: 59it [01:09,  1.22it/s]Extractor Estimating: 60it [01:10,  1.20it/s]Extractor Estimating: 61it [01:11,  1.17it/s]Extractor Estimating: 62it [01:12,  1.16it/s]Extractor Estimating: 63it [01:13,  1.19it/s]Extractor Estimating: 64it [01:14,  1.19it/s]Extractor Estimating: 65it [01:15,  1.21it/s]Extractor Estimating: 66it [01:15,  1.23it/s]Extractor Estimating: 67it [01:16,  1.23it/s]Extractor Estimating: 68it [01:17,  1.21it/s]Extractor Estimating: 69it [01:18,  1.19it/s]Extractor Estimating: 70it [01:19,  1.19it/s]Extractor Estimating: 71it [01:19,  1.21it/s]Extractor Estimating: 72it [01:20,  1.21it/s]Extractor Estimating: 73it [01:21,  1.19it/s]Extractor Estimating: 74it [01:22,  1.21it/s]Extractor Estimating: 75it [01:23,  1.21it/s]Extractor Estimating: 76it [01:24,  1.24it/s]Extractor Estimating: 77it [01:24,  1.23it/s]Extractor Estimating: 78it [01:25,  1.22it/s]Extractor Estimating: 79it [01:26,  1.22it/s]Extractor Estimating: 80it [01:27,  1.20it/s]Extractor Estimating: 81it [01:28,  1.21it/s]Extractor Estimating: 82it [01:29,  1.19it/s]Extractor Estimating: 83it [01:29,  1.23it/s]Extractor Estimating: 84it [01:30,  1.24it/s]Extractor Estimating: 85it [01:31,  1.23it/s]Extractor Estimating: 86it [01:32,  1.22it/s]Extractor Estimating: 87it [01:33,  1.20it/s]Extractor Estimating: 88it [01:33,  1.21it/s]Extractor Estimating: 89it [01:34,  1.22it/s]Extractor Estimating: 90it [01:35,  1.21it/s]Extractor Estimating: 91it [01:36,  1.23it/s]Extractor Estimating: 92it [01:37,  1.23it/s]Extractor Estimating: 93it [01:37,  1.25it/s]Extractor Estimating: 94it [01:38,  1.22it/s]Extractor Estimating: 95it [01:39,  1.24it/s]Extractor Estimating: 96it [01:40,  1.25it/s]Extractor Estimating: 97it [01:41,  1.23it/s]Extractor Estimating: 98it [01:42,  1.23it/s]Extractor Estimating: 99it [01:42,  1.25it/s]Extractor Estimating: 100it [01:43,  1.23it/s]Extractor Estimating: 101it [01:44,  1.18it/s]Extractor Estimating: 102it [01:45,  1.24it/s]Extractor Estimating: 103it [01:46,  1.27it/s]Extractor Estimating: 104it [01:46,  1.27it/s]Extractor Estimating: 105it [01:47,  1.22it/s]Extractor Estimating: 106it [01:48,  1.22it/s]Extractor Estimating: 107it [01:49,  1.20it/s]Extractor Estimating: 108it [01:50,  1.22it/s]Extractor Estimating: 109it [01:51,  1.23it/s]Extractor Estimating: 110it [01:51,  1.19it/s]Extractor Estimating: 111it [01:52,  1.22it/s]Extractor Estimating: 112it [01:53,  1.17it/s]Extractor Estimating: 113it [01:54,  1.16it/s]Extractor Estimating: 114it [01:55,  1.17it/s]Extractor Estimating: 115it [01:56,  1.19it/s]Extractor Estimating: 116it [01:56,  1.23it/s]Extractor Estimating: 117it [01:57,  1.13it/s]Extractor Estimating: 118it [01:58,  1.18it/s]Extractor Estimating: 119it [01:59,  1.18it/s]Extractor Estimating: 120it [02:00,  1.21it/s]Extractor Estimating: 121it [02:01,  1.25it/s]Extractor Estimating: 122it [02:01,  1.20it/s]Extractor Estimating: 123it [02:02,  1.25it/s]Extractor Estimating: 124it [02:03,  1.24it/s]Extractor Estimating: 125it [02:04,  1.21it/s]Extractor Estimating: 126it [02:05,  1.27it/s]Extractor Estimating: 127it [02:05,  1.25it/s]Extractor Estimating: 128it [02:06,  1.27it/s]Extractor Estimating: 129it [02:07,  1.27it/s]Extractor Estimating: 130it [02:08,  1.32it/s]Extractor Estimating: 131it [02:08,  1.30it/s]Extractor Estimating: 132it [02:09,  1.31it/s]Extractor Estimating: 133it [02:10,  1.31it/s]Extractor Estimating: 134it [02:11,  1.28it/s]Extractor Estimating: 135it [02:12,  1.31it/s]Extractor Estimating: 136it [02:12,  1.31it/s]Extractor Estimating: 137it [02:13,  1.21it/s]Extractor Estimating: 138it [02:14,  1.23it/s]Extractor Estimating: 139it [02:15,  1.28it/s]Extractor Estimating: 140it [02:16,  1.29it/s]Extractor Estimating: 141it [02:16,  1.28it/s]Extractor Estimating: 142it [02:17,  1.28it/s]Extractor Estimating: 143it [02:18,  1.29it/s]Extractor Estimating: 144it [02:19,  1.29it/s]Extractor Estimating: 145it [02:19,  1.26it/s]Extractor Estimating: 146it [02:20,  1.30it/s]Extractor Estimating: 147it [02:21,  1.28it/s]Extractor Estimating: 148it [02:22,  1.26it/s]Extractor Estimating: 149it [02:23,  1.22it/s]Extractor Estimating: 150it [02:24,  1.21it/s]Extractor Estimating: 151it [02:24,  1.26it/s]Extractor Estimating: 152it [02:25,  1.25it/s]Extractor Estimating: 153it [02:26,  1.26it/s]Extractor Estimating: 154it [02:27,  1.26it/s]Extractor Estimating: 155it [02:27,  1.26it/s]Extractor Estimating: 156it [02:28,  1.24it/s]Extractor Estimating: 157it [02:29,  1.20it/s]Extractor Estimating: 158it [02:30,  1.22it/s]Extractor Estimating: 159it [02:31,  1.22it/s]Extractor Estimating: 160it [02:32,  1.23it/s]Extractor Estimating: 161it [02:32,  1.24it/s]Extractor Estimating: 162it [02:33,  1.20it/s]Extractor Estimating: 163it [02:34,  1.21it/s]Extractor Estimating: 164it [02:35,  1.25it/s]Extractor Estimating: 165it [02:36,  1.27it/s]Extractor Estimating: 166it [02:36,  1.26it/s]Extractor Estimating: 167it [02:37,  1.24it/s]Extractor Estimating: 168it [02:38,  1.24it/s]Extractor Estimating: 169it [02:39,  1.25it/s]Extractor Estimating: 170it [02:39,  1.30it/s]Extractor Estimating: 171it [02:40,  1.29it/s]Extractor Estimating: 172it [02:41,  1.23it/s]Extractor Estimating: 173it [02:42,  1.20it/s]Extractor Estimating: 174it [02:43,  1.20it/s]Extractor Estimating: 175it [02:44,  1.22it/s]Extractor Estimating: 176it [02:45,  1.19it/s]Extractor Estimating: 177it [02:45,  1.20it/s]Extractor Estimating: 178it [02:46,  1.23it/s]Extractor Estimating: 179it [02:47,  1.24it/s]Extractor Estimating: 180it [02:48,  1.23it/s]Extractor Estimating: 181it [02:49,  1.26it/s]Extractor Estimating: 182it [02:49,  1.25it/s]Extractor Estimating: 183it [02:50,  1.24it/s]Extractor Estimating: 184it [02:51,  1.24it/s]Extractor Estimating: 185it [02:52,  1.26it/s]Extractor Estimating: 186it [02:53,  1.26it/s]Extractor Estimating: 187it [02:53,  1.23it/s]Extractor Estimating: 188it [02:54,  1.23it/s]Extractor Estimating: 189it [02:57,  1.29s/it]Extractor Estimating: 190it [02:57,  1.13s/it]Extractor Estimating: 191it [02:58,  1.02s/it]Extractor Estimating: 192it [02:59,  1.03it/s]Extractor Estimating: 193it [03:00,  1.08it/s]Extractor Estimating: 194it [03:01,  1.11it/s]Extractor Estimating: 195it [03:01,  1.15it/s]Extractor Estimating: 196it [03:02,  1.20it/s]Extractor Estimating: 197it [03:03,  1.18it/s]Extractor Estimating: 198it [03:04,  1.20it/s]Extractor Estimating: 199it [03:05,  1.24it/s]Extractor Estimating: 200it [03:05,  1.22it/s]Extractor Estimating: 201it [03:06,  1.12it/s]Extractor Estimating: 202it [03:07,  1.15it/s]Extractor Estimating: 203it [03:08,  1.16it/s]Extractor Estimating: 204it [03:09,  1.17it/s]Extractor Estimating: 205it [03:10,  1.20it/s]Extractor Estimating: 206it [03:11,  1.19it/s]Extractor Estimating: 207it [03:11,  1.24it/s]Extractor Estimating: 208it [03:12,  1.17it/s]Extractor Estimating: 209it [03:13,  1.16it/s]Extractor Estimating: 210it [03:14,  1.18it/s]Extractor Estimating: 211it [03:15,  1.17it/s]Extractor Estimating: 212it [03:16,  1.17it/s]Extractor Estimating: 213it [03:17,  1.14it/s]Extractor Estimating: 214it [03:17,  1.16it/s]Extractor Estimating: 215it [03:18,  1.18it/s]Extractor Estimating: 216it [03:19,  1.13it/s]Extractor Estimating: 217it [03:20,  1.14it/s]Extractor Estimating: 218it [03:21,  1.17it/s]Extractor Estimating: 219it [03:22,  1.16it/s]Extractor Estimating: 220it [03:23,  1.18it/s]Extractor Estimating: 221it [03:24,  1.14it/s]Extractor Estimating: 222it [03:24,  1.13it/s]Extractor Estimating: 223it [03:25,  1.14it/s]Extractor Estimating: 224it [03:26,  1.16it/s]Extractor Estimating: 225it [03:27,  1.17it/s]Extractor Estimating: 226it [03:28,  1.15it/s]Extractor Estimating: 227it [03:29,  1.21it/s]Extractor Estimating: 228it [03:29,  1.23it/s]Extractor Estimating: 229it [03:30,  1.26it/s]Extractor Estimating: 230it [03:31,  1.26it/s]Extractor Estimating: 231it [03:32,  1.30it/s]Extractor Estimating: 232it [03:32,  1.34it/s]Extractor Estimating: 233it [03:33,  1.37it/s]Extractor Estimating: 234it [03:34,  1.35it/s]Extractor Estimating: 235it [03:35,  1.33it/s]Extractor Estimating: 236it [03:35,  1.32it/s]Extractor Estimating: 237it [03:36,  1.31it/s]Extractor Estimating: 238it [03:37,  1.31it/s]Extractor Estimating: 239it [03:38,  1.29it/s]Extractor Estimating: 240it [03:38,  1.30it/s]Extractor Estimating: 241it [03:39,  1.33it/s]Extractor Estimating: 242it [03:40,  1.32it/s]Extractor Estimating: 243it [03:41,  1.28it/s]Extractor Estimating: 244it [03:42,  1.27it/s]Extractor Estimating: 245it [03:42,  1.29it/s]Extractor Estimating: 246it [03:43,  1.31it/s]Extractor Estimating: 247it [03:44,  1.30it/s]Extractor Estimating: 248it [03:45,  1.27it/s]Extractor Estimating: 249it [03:45,  1.26it/s]Extractor Estimating: 250it [03:46,  1.29it/s]Extractor Estimating: 251it [03:47,  1.21it/s]Extractor Estimating: 252it [03:48,  1.24it/s]Extractor Estimating: 253it [03:49,  1.25it/s]Extractor Estimating: 254it [03:50,  1.19it/s]Extractor Estimating: 255it [03:50,  1.22it/s]Extractor Estimating: 256it [03:51,  1.30it/s]Extractor Estimating: 257it [03:52,  1.28it/s]Extractor Estimating: 258it [03:53,  1.20it/s]Extractor Estimating: 259it [03:54,  1.24it/s]Extractor Estimating: 260it [03:54,  1.26it/s]Extractor Estimating: 261it [03:55,  1.20it/s]Extractor Estimating: 262it [03:56,  1.19it/s]Extractor Estimating: 263it [03:57,  1.20it/s]Extractor Estimating: 264it [03:58,  1.22it/s]Extractor Estimating: 265it [03:59,  1.19it/s]Extractor Estimating: 266it [03:59,  1.21it/s]Extractor Estimating: 267it [04:00,  1.15it/s]Extractor Estimating: 268it [04:01,  1.20it/s]Extractor Estimating: 269it [04:02,  1.15it/s]Extractor Estimating: 270it [04:03,  1.15it/s]Extractor Estimating: 271it [04:04,  1.16it/s]Extractor Estimating: 272it [04:05,  1.16it/s]Extractor Estimating: 273it [04:05,  1.18it/s]Extractor Estimating: 274it [04:06,  1.18it/s]Extractor Estimating: 275it [04:07,  1.21it/s]Extractor Estimating: 276it [04:08,  1.26it/s]Extractor Estimating: 277it [04:09,  1.25it/s]Extractor Estimating: 278it [04:09,  1.30it/s]Extractor Estimating: 279it [04:10,  1.29it/s]Extractor Estimating: 280it [04:11,  1.28it/s]Extractor Estimating: 281it [04:12,  1.30it/s]Extractor Estimating: 282it [04:12,  1.33it/s]Extractor Estimating: 283it [04:13,  1.31it/s]Extractor Estimating: 284it [04:14,  1.29it/s]Extractor Estimating: 285it [04:15,  1.32it/s]Extractor Estimating: 286it [04:15,  1.31it/s]Extractor Estimating: 287it [04:16,  1.30it/s]Extractor Estimating: 288it [04:17,  1.26it/s]Extractor Estimating: 289it [04:18,  1.29it/s]Extractor Estimating: 290it [04:19,  1.28it/s]Extractor Estimating: 291it [04:19,  1.33it/s]Extractor Estimating: 292it [04:20,  1.29it/s]Extractor Estimating: 293it [04:21,  1.30it/s]Extractor Estimating: 294it [04:22,  1.29it/s]Extractor Estimating: 295it [04:22,  1.28it/s]Extractor Estimating: 296it [04:23,  1.32it/s]Extractor Estimating: 297it [04:24,  1.33it/s]Extractor Estimating: 298it [04:25,  1.34it/s]Extractor Estimating: 299it [04:25,  1.37it/s]Extractor Estimating: 300it [04:26,  1.37it/s]Extractor Estimating: 301it [04:27,  1.30it/s]Extractor Estimating: 302it [04:28,  1.24it/s]Extractor Estimating: 303it [04:29,  1.25it/s]Extractor Estimating: 304it [04:29,  1.30it/s]Extractor Estimating: 305it [04:30,  1.29it/s]Extractor Estimating: 306it [04:31,  1.33it/s]Extractor Estimating: 307it [04:32,  1.32it/s]Extractor Estimating: 308it [04:32,  1.38it/s]Extractor Estimating: 309it [04:33,  1.38it/s]Extractor Estimating: 310it [04:34,  1.33it/s]Extractor Estimating: 311it [04:35,  1.26it/s]Extractor Estimating: 312it [04:35,  1.31it/s]Extractor Estimating: 313it [04:36,  1.28it/s]Extractor Estimating: 314it [04:37,  1.26it/s]Extractor Estimating: 315it [04:38,  1.16it/s]Extractor Estimating: 316it [04:39,  1.20it/s]Extractor Estimating: 317it [04:40,  1.23it/s]Extractor Estimating: 318it [04:40,  1.22it/s]Extractor Estimating: 319it [04:41,  1.26it/s]Extractor Estimating: 320it [04:42,  1.20it/s]Extractor Estimating: 321it [04:43,  1.26it/s]Extractor Estimating: 322it [04:44,  1.17it/s]Extractor Estimating: 323it [04:44,  1.21it/s]Extractor Estimating: 324it [04:45,  1.24it/s]Extractor Estimating: 325it [04:46,  1.26it/s]Extractor Estimating: 326it [04:47,  1.12it/s]Extractor Estimating: 327it [04:48,  1.16it/s]Extractor Estimating: 328it [04:49,  1.16it/s]Extractor Estimating: 329it [04:50,  1.20it/s]Extractor Estimating: 330it [04:50,  1.20it/s]Extractor Estimating: 331it [04:51,  1.23it/s]Extractor Estimating: 332it [04:52,  1.22it/s]Extractor Estimating: 333it [04:53,  1.22it/s]Extractor Estimating: 334it [04:54,  1.24it/s]Extractor Estimating: 335it [04:54,  1.22it/s]Extractor Estimating: 336it [04:55,  1.23it/s]Extractor Estimating: 337it [04:56,  1.25it/s]Extractor Estimating: 338it [04:57,  1.23it/s]Extractor Estimating: 339it [04:58,  1.22it/s]Extractor Estimating: 340it [04:58,  1.25it/s]Extractor Estimating: 341it [04:59,  1.19it/s]Extractor Estimating: 342it [05:00,  1.21it/s]Extractor Estimating: 343it [05:01,  1.12it/s]Extractor Estimating: 344it [05:02,  1.15it/s]Extractor Estimating: 345it [05:03,  1.15it/s]Extractor Estimating: 346it [05:04,  1.13it/s]Extractor Estimating: 347it [05:05,  1.10it/s]Extractor Estimating: 348it [05:06,  1.10it/s]Extractor Estimating: 349it [05:07,  1.10it/s]Extractor Estimating: 350it [05:07,  1.14it/s]Extractor Estimating: 351it [05:08,  1.17it/s]Extractor Estimating: 352it [05:09,  1.21it/s]Extractor Estimating: 353it [05:10,  1.24it/s]Extractor Estimating: 354it [05:10,  1.28it/s]Extractor Estimating: 355it [05:11,  1.30it/s]Extractor Estimating: 356it [05:12,  1.26it/s]Extractor Estimating: 357it [05:13,  1.26it/s]Extractor Estimating: 358it [05:14,  1.26it/s]Extractor Estimating: 359it [05:14,  1.32it/s]Extractor Estimating: 360it [05:15,  1.32it/s]Extractor Estimating: 361it [05:16,  1.31it/s]Extractor Estimating: 362it [05:17,  1.28it/s]Extractor Estimating: 363it [05:17,  1.31it/s]Extractor Estimating: 364it [05:18,  1.27it/s]Extractor Estimating: 365it [05:19,  1.22it/s]Extractor Estimating: 366it [05:20,  1.23it/s]Extractor Estimating: 367it [05:21,  1.24it/s]Extractor Estimating: 368it [05:22,  1.22it/s]Extractor Estimating: 369it [05:22,  1.23it/s]Extractor Estimating: 370it [05:23,  1.25it/s]Extractor Estimating: 371it [05:24,  1.23it/s]Extractor Estimating: 372it [05:25,  1.27it/s]Extractor Estimating: 373it [05:26,  1.24it/s]Extractor Estimating: 374it [05:26,  1.22it/s]Extractor Estimating: 375it [05:27,  1.29it/s]Extractor Estimating: 375it [05:27,  1.14it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7476 mean pseudo reward: 0.9455284826721623
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 27599
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27699, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_1/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27699, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.617, loss:1100.5778
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.324, loss:1059.4008
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.301, loss:1061.0387
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.309, loss:1005.0240
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.308, loss:1017.5645
>> valid entity prec:0.4930, rec:0.4634, f1:0.4778
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.985, loss:1029.8860
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.314, loss:945.8715
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.322, loss:998.8945
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.300, loss:994.4796
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.308, loss:929.2448
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.3994, rec:0.4675, f1:0.4308
>> valid relation prec:0.0105, rec:0.0017, f1:0.0030
>> valid relation with NER prec:0.0105, rec:0.0017, f1:0.0030
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.979, loss:950.4438
g_step 1200, step 264, avg_time 1.319, loss:966.4875
g_step 1300, step 52, avg_time 1.305, loss:948.5939
g_step 1400, step 152, avg_time 1.308, loss:863.7254
g_step 1500, step 252, avg_time 1.305, loss:921.2538
>> valid entity prec:0.5021, rec:0.4494, f1:0.4743
>> valid relation prec:0.0151, rec:0.0023, f1:0.0040
>> valid relation with NER prec:0.0151, rec:0.0023, f1:0.0040
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 40, avg_time 2.985, loss:892.1272
g_step 1700, step 140, avg_time 1.314, loss:863.9146
g_step 1800, step 240, avg_time 1.318, loss:858.2639
g_step 1900, step 28, avg_time 1.296, loss:845.3695
g_step 2000, step 128, avg_time 1.301, loss:822.1900
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4769, rec:0.4082, f1:0.4399
>> valid relation prec:0.0186, rec:0.0037, f1:0.0062
>> valid relation with NER prec:0.0186, rec:0.0037, f1:0.0062
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.996, loss:808.8366
g_step 2200, step 16, avg_time 1.308, loss:814.6984
g_step 2300, step 116, avg_time 1.304, loss:768.8756
g_step 2400, step 216, avg_time 1.317, loss:759.5728
g_step 2500, step 4, avg_time 1.304, loss:786.3887
>> valid entity prec:0.4601, rec:0.3770, f1:0.4145
>> valid relation prec:0.0113, rec:0.0023, f1:0.0038
>> valid relation with NER prec:0.0113, rec:0.0023, f1:0.0038
g_step 2600, step 104, avg_time 2.977, loss:731.0515
g_step 2700, step 204, avg_time 1.322, loss:751.5716
g_step 2800, step 304, avg_time 1.310, loss:752.9997
g_step 2900, step 92, avg_time 1.317, loss:710.3169
g_step 3000, step 192, avg_time 1.319, loss:705.6462
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4832, rec:0.3517, f1:0.4071
>> valid relation prec:0.0145, rec:0.0023, f1:0.0040
>> valid relation with NER prec:0.0145, rec:0.0023, f1:0.0040
g_step 3100, step 292, avg_time 2.968, loss:706.7964
g_step 3200, step 80, avg_time 1.307, loss:675.1307
g_step 3300, step 180, avg_time 1.311, loss:666.3816
g_step 3400, step 280, avg_time 1.310, loss:677.6560
g_step 3500, step 68, avg_time 1.310, loss:660.1077
>> valid entity prec:0.4996, rec:0.4844, f1:0.4919
>> valid relation prec:0.0171, rec:0.0057, f1:0.0086
>> valid relation with NER prec:0.0171, rec:0.0057, f1:0.0086
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 168, avg_time 2.985, loss:652.0856
g_step 3700, step 268, avg_time 1.313, loss:648.7845
g_step 3800, step 56, avg_time 1.309, loss:617.8273
g_step 3900, step 156, avg_time 1.302, loss:590.9463
g_step 4000, step 256, avg_time 1.302, loss:630.7057
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4747, rec:0.4672, f1:0.4709
>> valid relation prec:0.0140, rec:0.0046, f1:0.0069
>> valid relation with NER prec:0.0140, rec:0.0046, f1:0.0069
g_step 4100, step 44, avg_time 2.996, loss:592.2370
g_step 4200, step 144, avg_time 1.299, loss:565.9281
g_step 4300, step 244, avg_time 1.309, loss:617.0046
g_step 4400, step 32, avg_time 1.295, loss:589.8957
g_step 4500, step 132, avg_time 1.300, loss:543.7761
>> valid entity prec:0.4948, rec:0.4691, f1:0.4816
>> valid relation prec:0.0215, rec:0.0075, f1:0.0111
>> valid relation with NER prec:0.0215, rec:0.0075, f1:0.0111
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 232, avg_time 2.981, loss:581.6017
g_step 4700, step 20, avg_time 1.310, loss:575.2134
g_step 4800, step 120, avg_time 1.318, loss:515.4982
g_step 4900, step 220, avg_time 1.308, loss:545.7679
g_step 5000, step 8, avg_time 1.308, loss:577.0489
learning rate was adjusted to 0.0008
>> valid entity prec:0.4903, rec:0.4542, f1:0.4716
>> valid relation prec:0.0047, rec:0.0014, f1:0.0022
>> valid relation with NER prec:0.0047, rec:0.0014, f1:0.0022
g_step 5100, step 108, avg_time 2.988, loss:502.5374
g_step 5200, step 208, avg_time 1.315, loss:519.0216
g_step 5300, step 308, avg_time 1.304, loss:543.3266
g_step 5400, step 96, avg_time 1.301, loss:482.7830
g_step 5500, step 196, avg_time 1.310, loss:508.7571
>> valid entity prec:0.5016, rec:0.4260, f1:0.4607
>> valid relation prec:0.0130, rec:0.0043, f1:0.0065
>> valid relation with NER prec:0.0130, rec:0.0043, f1:0.0065
g_step 5600, step 296, avg_time 2.982, loss:523.3517
g_step 5700, step 84, avg_time 1.305, loss:470.3713
g_step 5800, step 184, avg_time 1.314, loss:487.5645
g_step 5900, step 284, avg_time 1.303, loss:484.7534
g_step 6000, step 72, avg_time 1.308, loss:461.6284
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5345, rec:0.3312, f1:0.4090
>> valid relation prec:0.0059, rec:0.0017, f1:0.0027
>> valid relation with NER prec:0.0059, rec:0.0017, f1:0.0027
g_step 6100, step 172, avg_time 2.987, loss:444.7483
g_step 6200, step 272, avg_time 1.306, loss:468.3447
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:55:00 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:55:00 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-55-00_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:55:02 - WARNING - datasets.builder -   Using custom data configuration default-5ea49e42a72248be
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5ea49e42a72248be/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 19:55:04,514 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:55:04,549 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:55:04,549 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:55:04,550 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:55:05,096 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:55:05,623 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:55:05,623 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:55:05,623 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:55:05,624 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:55:05,624 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:55:05,624 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:55:06,227 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:55:09,570 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:55:09,634 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_1/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5ea49e42a72248be/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 19:55:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x15366c8cecb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:01<00:11,  1.58s/ba] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:01<00:04,  1.28ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:02<00:02,  1.89ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:01,  2.43ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:02<00:01,  2.90ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:02<00:00,  3.25ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:02<00:00,  3.56ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  3.81ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.52ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  3.26ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  3.81ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.03ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.12ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.52ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  7.74ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  9.63ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  9.62ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  9.66ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:00<00:00,  9.74ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 11.22ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.32ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  7.57ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  9.52ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.71ba/s]
[INFO|trainer.py:414] 2023-08-28 19:55:15,847 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:55:15,914 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:55:15,914 >>   Num examples = 7506
[INFO|trainer.py:1149] 2023-08-28 19:55:15,914 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:55:15,914 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:55:15,914 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:55:15,915 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:55:15,915 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<03:02,  3.20it/s]  0%|          | 2/585 [00:00<02:54,  3.34it/s]  1%|          | 3/585 [00:00<02:51,  3.39it/s]  1%|          | 4/585 [00:01<02:50,  3.41it/s]  1%|          | 5/585 [00:01<02:49,  3.42it/s]  1%|          | 6/585 [00:01<02:48,  3.43it/s]  1%|          | 7/585 [00:02<02:48,  3.43it/s]  1%|â–         | 8/585 [00:02<02:47,  3.44it/s]  2%|â–         | 9/585 [00:02<02:47,  3.44it/s]  2%|â–         | 10/585 [00:02<02:46,  3.44it/s]  2%|â–         | 11/585 [00:03<02:46,  3.44it/s]  2%|â–         | 12/585 [00:03<02:47,  3.43it/s]  2%|â–         | 13/585 [00:03<02:46,  3.43it/s]  2%|â–         | 14/585 [00:04<02:45,  3.44it/s]  3%|â–Ž         | 15/585 [00:04<02:45,  3.44it/s]  3%|â–Ž         | 16/585 [00:04<02:45,  3.44it/s]  3%|â–Ž         | 17/585 [00:04<02:44,  3.45it/s]  3%|â–Ž         | 18/585 [00:05<02:44,  3.45it/s]  3%|â–Ž         | 19/585 [00:05<02:44,  3.45it/s]  3%|â–Ž         | 20/585 [00:05<02:43,  3.45it/s]  4%|â–Ž         | 21/585 [00:06<02:43,  3.45it/s]  4%|â–         | 22/585 [00:06<02:42,  3.46it/s]  4%|â–         | 23/585 [00:06<02:47,  3.36it/s]  4%|â–         | 24/585 [00:07<02:45,  3.39it/s]  4%|â–         | 25/585 [00:07<02:44,  3.41it/s]  4%|â–         | 26/585 [00:07<02:43,  3.42it/s]  5%|â–         | 27/585 [00:07<02:42,  3.43it/s]  5%|â–         | 28/585 [00:08<02:42,  3.44it/s]  5%|â–         | 29/585 [00:08<02:41,  3.44it/s]  5%|â–Œ         | 30/585 [00:08<02:41,  3.44it/s]  5%|â–Œ         | 31/585 [00:09<02:40,  3.45it/s]  5%|â–Œ         | 32/585 [00:09<02:40,  3.45it/s]  6%|â–Œ         | 33/585 [00:09<02:39,  3.45it/s]  6%|â–Œ         | 34/585 [00:09<02:41,  3.41it/s]  6%|â–Œ         | 35/585 [00:10<02:40,  3.43it/s]  6%|â–Œ         | 36/585 [00:10<02:39,  3.44it/s]  6%|â–‹         | 37/585 [00:10<02:39,  3.44it/s]  6%|â–‹         | 38/585 [00:11<02:38,  3.44it/s]  7%|â–‹         | 39/585 [00:11<02:38,  3.45it/s]  7%|â–‹         | 40/585 [00:11<02:38,  3.45it/s]  7%|â–‹         | 41/585 [00:11<02:37,  3.45it/s]  7%|â–‹         | 42/585 [00:12<02:37,  3.45it/s]  7%|â–‹         | 43/585 [00:12<02:37,  3.45it/s]  8%|â–Š         | 44/585 [00:12<02:36,  3.45it/s]  8%|â–Š         | 45/585 [00:13<02:47,  3.22it/s]  8%|â–Š         | 46/585 [00:13<02:43,  3.29it/s]  8%|â–Š         | 47/585 [00:13<02:41,  3.34it/s]  8%|â–Š         | 48/585 [00:14<02:39,  3.37it/s]  8%|â–Š         | 49/585 [00:14<02:38,  3.39it/s]  9%|â–Š         | 50/585 [00:14<02:36,  3.41it/s]  9%|â–Š         | 51/585 [00:14<02:36,  3.42it/s]  9%|â–‰         | 52/585 [00:15<02:52,  3.08it/s]  9%|â–‰         | 53/585 [00:15<02:47,  3.18it/s]  9%|â–‰         | 54/585 [00:15<02:43,  3.26it/s]  9%|â–‰         | 55/585 [00:16<02:41,  3.29it/s] 10%|â–‰         | 56/585 [00:16<02:38,  3.33it/s] 10%|â–‰         | 57/585 [00:16<02:36,  3.37it/s] 10%|â–‰         | 58/585 [00:17<02:47,  3.14it/s] 10%|â–ˆ         | 59/585 [00:17<02:42,  3.23it/s] 10%|â–ˆ         | 60/585 [00:17<02:39,  3.29it/s] 10%|â–ˆ         | 61/585 [00:18<02:37,  3.34it/s] 11%|â–ˆ         | 62/585 [00:18<02:35,  3.37it/s] 11%|â–ˆ         | 63/585 [00:18<02:34,  3.39it/s] 11%|â–ˆ         | 64/585 [00:18<02:33,  3.40it/s] 11%|â–ˆ         | 65/585 [00:19<02:32,  3.42it/s] 11%|â–ˆâ–        | 66/585 [00:19<02:31,  3.42it/s] 11%|â–ˆâ–        | 67/585 [00:19<02:31,  3.43it/s] 12%|â–ˆâ–        | 68/585 [00:20<02:40,  3.22it/s] 12%|â–ˆâ–        | 69/585 [00:20<02:37,  3.28it/s] 12%|â–ˆâ–        | 70/585 [00:20<02:34,  3.33it/s] 12%|â–ˆâ–        | 71/585 [00:20<02:33,  3.36it/s] 12%|â–ˆâ–        | 72/585 [00:21<02:31,  3.38it/s] 12%|â–ˆâ–        | 73/585 [00:21<02:30,  3.40it/s] 13%|â–ˆâ–Ž        | 74/585 [00:21<02:30,  3.41it/s] 13%|â–ˆâ–Ž        | 75/585 [00:22<02:29,  3.41it/s] 13%|â–ˆâ–Ž        | 76/585 [00:22<02:29,  3.42it/s] 13%|â–ˆâ–Ž        | 77/585 [00:22<02:28,  3.42it/s] 13%|â–ˆâ–Ž        | 78/585 [00:23<02:28,  3.42it/s] 14%|â–ˆâ–Ž        | 79/585 [00:23<02:53,  2.92it/s] 14%|â–ˆâ–Ž        | 80/585 [00:23<02:45,  3.06it/s] 14%|â–ˆâ–        | 81/585 [00:24<02:39,  3.16it/s] 14%|â–ˆâ–        | 82/585 [00:24<02:35,  3.24it/s] 14%|â–ˆâ–        | 83/585 [00:24<02:32,  3.29it/s] 14%|â–ˆâ–        | 84/585 [00:24<02:30,  3.33it/s] 15%|â–ˆâ–        | 85/585 [00:25<02:28,  3.36it/s] 15%|â–ˆâ–        | 86/585 [00:25<02:27,  3.38it/s] 15%|â–ˆâ–        | 87/585 [00:25<02:26,  3.39it/s] 15%|â–ˆâ–Œ        | 88/585 [00:26<02:25,  3.40it/s] 15%|â–ˆâ–Œ        | 89/585 [00:26<02:54,  2.83it/s] 15%|â–ˆâ–Œ        | 90/585 [00:26<02:45,  2.99it/s] 16%|â–ˆâ–Œ        | 91/585 [00:27<02:38,  3.11it/s] 16%|â–ˆâ–Œ        | 92/585 [00:27<02:34,  3.20it/s] 16%|â–ˆâ–Œ        | 93/585 [00:27<02:30,  3.27it/s] 16%|â–ˆâ–Œ        | 94/585 [00:28<02:28,  3.32it/s] 16%|â–ˆâ–Œ        | 95/585 [00:28<02:26,  3.35it/s] 16%|â–ˆâ–‹        | 96/585 [00:28<02:24,  3.38it/s] 17%|â–ˆâ–‹        | 97/585 [00:28<02:23,  3.39it/s] 17%|â–ˆâ–‹        | 98/585 [00:29<02:23,  3.40it/s] 17%|â–ˆâ–‹        | 99/585 [00:29<02:24,  3.36it/s] 17%|â–ˆâ–‹        | 100/585 [00:29<02:23,  3.38it/s] 17%|â–ˆâ–‹        | 101/585 [00:30<02:22,  3.39it/s] 17%|â–ˆâ–‹        | 102/585 [00:30<02:22,  3.40it/s] 18%|â–ˆâ–Š        | 103/585 [00:30<02:21,  3.41it/s] 18%|â–ˆâ–Š        | 104/585 [00:30<02:20,  3.41it/s] 18%|â–ˆâ–Š        | 105/585 [00:31<02:20,  3.42it/s] 18%|â–ˆâ–Š        | 106/585 [00:31<02:19,  3.42it/s] 18%|â–ˆâ–Š        | 107/585 [00:31<02:19,  3.43it/s] 18%|â–ˆâ–Š        | 108/585 [00:32<02:19,  3.43it/s] 19%|â–ˆâ–Š        | 109/585 [00:32<02:18,  3.43it/s] 19%|â–ˆâ–‰        | 110/585 [00:33<03:39,  2.16it/s] 19%|â–ˆâ–‰        | 111/585 [00:33<03:14,  2.43it/s] 19%|â–ˆâ–‰        | 112/585 [00:33<02:57,  2.66it/s] 19%|â–ˆâ–‰        | 113/585 [00:34<02:45,  2.85it/s] 19%|â–ˆâ–‰        | 114/585 [00:34<02:36,  3.00it/s] 20%|â–ˆâ–‰        | 115/585 [00:34<02:30,  3.12it/s] 20%|â–ˆâ–‰        | 116/585 [00:35<02:26,  3.21it/s] 20%|â–ˆâ–ˆ        | 117/585 [00:35<02:23,  3.27it/s][INFO|trainer.py:2140] 2023-08-28 19:55:51,318 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:55:51,318 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:55:51,318 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 54.92it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 49.71it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.09it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.42it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.09it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 46.84it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.78it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.56it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.53it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.54it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.47it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 46.46it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.42it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.40it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.36it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.41it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.47it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.42it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.40it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.38it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.41it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.44it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.39it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.35it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.39it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:08, 35.38it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:03<00:07, 38.10it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:07, 40.23it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 41.94it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 43.13it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:06, 44.06it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:06, 44.74it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 45.22it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 45.58it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 45.82it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:04<00:05, 46.00it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.15it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.17it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.26it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:05, 46.35it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.30it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.27it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.34it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.33it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:05<00:04, 46.38it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.37it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.39it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.39it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.39it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.31it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.36it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.31it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 43.03it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:06<00:03, 43.99it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:06<00:03, 44.69it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 45.17it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 45.55it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 45.77it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 45.89it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.05it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.10it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.18it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.26it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:07<00:02, 46.32it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.34it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.41it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.36it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.35it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.33it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.36it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.32it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.31it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:08<00:01, 46.30it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.33it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.32it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.35it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.34it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.36it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.35it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.35it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:09<00:00, 34.45it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:09<00:00, 37.30it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 39.65it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 41.45it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 42.79it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 43.84it/s][A                                                 
                                                 [A 20%|â–ˆâ–ˆ        | 117/585 [00:45<02:23,  3.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 43.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:56:01,402 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 19:56:01,457 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:56:08,569 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:56:09,217 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:56:09,321 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 118/585 [01:05<1:11:51,  9.23s/it] 20%|â–ˆâ–ˆ        | 119/585 [01:05<51:07,  6.58s/it]   21%|â–ˆâ–ˆ        | 120/585 [01:06<36:23,  4.70s/it] 21%|â–ˆâ–ˆ        | 121/585 [01:06<26:05,  3.37s/it] 21%|â–ˆâ–ˆ        | 122/585 [01:06<18:53,  2.45s/it] 21%|â–ˆâ–ˆ        | 123/585 [01:06<13:52,  1.80s/it] 21%|â–ˆâ–ˆ        | 124/585 [01:07<10:21,  1.35s/it] 21%|â–ˆâ–ˆâ–       | 125/585 [01:07<07:53,  1.03s/it] 22%|â–ˆâ–ˆâ–       | 126/585 [01:07<06:10,  1.24it/s] 22%|â–ˆâ–ˆâ–       | 127/585 [01:08<04:58,  1.53it/s] 22%|â–ˆâ–ˆâ–       | 128/585 [01:08<04:08,  1.84it/s] 22%|â–ˆâ–ˆâ–       | 129/585 [01:08<03:34,  2.12it/s] 22%|â–ˆâ–ˆâ–       | 130/585 [01:09<03:09,  2.40it/s] 22%|â–ˆâ–ˆâ–       | 131/585 [01:09<02:51,  2.64it/s] 23%|â–ˆâ–ˆâ–Ž       | 132/585 [01:09<02:39,  2.84it/s] 23%|â–ˆâ–ˆâ–Ž       | 133/585 [01:09<02:30,  3.00it/s] 23%|â–ˆâ–ˆâ–Ž       | 134/585 [01:10<02:24,  3.12it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/585 [01:10<02:20,  3.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/585 [01:10<02:16,  3.28it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/585 [01:11<02:14,  3.33it/s] 24%|â–ˆâ–ˆâ–Ž       | 138/585 [01:11<02:12,  3.37it/s] 24%|â–ˆâ–ˆâ–       | 139/585 [01:11<02:11,  3.39it/s] 24%|â–ˆâ–ˆâ–       | 140/585 [01:11<02:15,  3.29it/s] 24%|â–ˆâ–ˆâ–       | 141/585 [01:12<02:13,  3.33it/s] 24%|â–ˆâ–ˆâ–       | 142/585 [01:12<02:11,  3.37it/s] 24%|â–ˆâ–ˆâ–       | 143/585 [01:12<02:10,  3.39it/s] 25%|â–ˆâ–ˆâ–       | 144/585 [01:13<02:09,  3.41it/s] 25%|â–ˆâ–ˆâ–       | 145/585 [01:13<02:08,  3.42it/s] 25%|â–ˆâ–ˆâ–       | 146/585 [01:13<02:07,  3.43it/s] 25%|â–ˆâ–ˆâ–Œ       | 147/585 [01:13<02:07,  3.43it/s] 25%|â–ˆâ–ˆâ–Œ       | 148/585 [01:14<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/585 [01:14<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 150/585 [01:14<02:06,  3.45it/s] 26%|â–ˆâ–ˆâ–Œ       | 151/585 [01:15<02:15,  3.19it/s] 26%|â–ˆâ–ˆâ–Œ       | 152/585 [01:15<02:48,  2.56it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/585 [01:16<02:35,  2.77it/s] 26%|â–ˆâ–ˆâ–‹       | 154/585 [01:16<02:26,  2.94it/s] 26%|â–ˆâ–ˆâ–‹       | 155/585 [01:16<02:19,  3.08it/s] 27%|â–ˆâ–ˆâ–‹       | 156/585 [01:16<02:14,  3.18it/s] 27%|â–ˆâ–ˆâ–‹       | 157/585 [01:17<02:11,  3.26it/s] 27%|â–ˆâ–ˆâ–‹       | 158/585 [01:17<02:08,  3.31it/s] 27%|â–ˆâ–ˆâ–‹       | 159/585 [01:17<02:07,  3.35it/s] 27%|â–ˆâ–ˆâ–‹       | 160/585 [01:18<02:05,  3.38it/s] 28%|â–ˆâ–ˆâ–Š       | 161/585 [01:18<02:08,  3.30it/s] 28%|â–ˆâ–ˆâ–Š       | 162/585 [01:18<02:06,  3.34it/s] 28%|â–ˆâ–ˆâ–Š       | 163/585 [01:18<02:05,  3.37it/s] 28%|â–ˆâ–ˆâ–Š       | 164/585 [01:19<02:03,  3.40it/s] 28%|â–ˆâ–ˆâ–Š       | 165/585 [01:19<02:03,  3.41it/s] 28%|â–ˆâ–ˆâ–Š       | 166/585 [01:19<02:02,  3.42it/s] 29%|â–ˆâ–ˆâ–Š       | 167/585 [01:20<02:01,  3.43it/s] 29%|â–ˆâ–ˆâ–Š       | 168/585 [01:20<02:01,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 169/585 [01:20<02:00,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 170/585 [01:21<02:00,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 171/585 [01:21<02:00,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 172/585 [01:21<02:01,  3.39it/s] 30%|â–ˆâ–ˆâ–‰       | 173/585 [01:21<02:01,  3.40it/s] 30%|â–ˆâ–ˆâ–‰       | 174/585 [01:22<02:00,  3.41it/s] 30%|â–ˆâ–ˆâ–‰       | 175/585 [01:22<01:59,  3.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 176/585 [01:22<01:59,  3.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 177/585 [01:23<01:58,  3.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 178/585 [01:23<01:58,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 179/585 [01:23<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 180/585 [01:23<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 181/585 [01:24<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 182/585 [01:24<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 183/585 [01:24<01:58,  3.38it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 184/585 [01:25<01:57,  3.40it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 185/585 [01:25<01:57,  3.42it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 186/585 [01:25<01:56,  3.42it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 187/585 [01:25<01:56,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 188/585 [01:26<01:55,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/585 [01:26<01:55,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/585 [01:26<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 191/585 [01:27<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 192/585 [01:27<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 193/585 [01:27<01:53,  3.45it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/585 [01:28<01:53,  3.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/585 [01:28<01:53,  3.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/585 [01:28<01:53,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/585 [01:28<01:52,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 198/585 [01:29<01:52,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 199/585 [01:29<01:52,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 200/585 [01:29<01:51,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/585 [01:30<01:51,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 202/585 [01:30<01:51,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 203/585 [01:30<01:50,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 204/585 [01:30<01:50,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 205/585 [01:31<01:58,  3.20it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 206/585 [01:31<01:56,  3.26it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 207/585 [01:31<01:54,  3.31it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 208/585 [01:32<01:52,  3.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/585 [01:32<01:51,  3.38it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/585 [01:32<01:50,  3.40it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/585 [01:33<01:49,  3.41it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/585 [01:33<01:48,  3.42it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 213/585 [01:33<01:48,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 214/585 [01:33<01:48,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 215/585 [01:34<01:47,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 216/585 [01:34<01:48,  3.41it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 217/585 [01:34<01:47,  3.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 218/585 [01:35<01:47,  3.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/585 [01:35<01:46,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 220/585 [01:35<01:46,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 221/585 [01:35<01:45,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 222/585 [01:36<01:45,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 223/585 [01:36<01:45,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 224/585 [01:36<01:45,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/585 [01:37<01:44,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 226/585 [01:37<01:44,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 227/585 [01:37<01:48,  3.31it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 228/585 [01:38<01:46,  3.35it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 229/585 [01:38<01:45,  3.37it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 230/585 [01:38<01:44,  3.39it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/585 [01:38<01:44,  3.40it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 232/585 [01:39<01:43,  3.41it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 233/585 [01:39<01:43,  3.42it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:39<01:42,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 19:56:55,756 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:56:55,756 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:56:55,756 >>   Batch size = 8
{'eval_loss': 0.963952362537384, 'eval_runtime': 9.9383, 'eval_samples_per_second': 350.06, 'eval_steps_per_second': 43.77, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.71it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.24it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.44it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.73it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.24it/s][A
  8%|â–Š         | 33/435 [00:00<00:12, 32.24it/s][A
  9%|â–Š         | 38/435 [00:00<00:11, 35.67it/s][A
 10%|â–‰         | 43/435 [00:01<00:10, 38.37it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:09, 40.53it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:09, 42.17it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 43.33it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 44.20it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:08, 44.84it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:08, 45.20it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 45.49it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 45.73it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:02<00:07, 45.87it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:02<00:07, 46.03it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.13it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.16it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.21it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.26it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.25it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.26it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.29it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.36it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:03<00:06, 46.34it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.34it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.33it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.36it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.38it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.30it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:06, 43.57it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 44.31it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 44.95it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:04<00:05, 45.36it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 45.69it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 45.87it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.03it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:05, 46.06it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.22it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.20it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.26it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.31it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:05<00:04, 46.33it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.33it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.32it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.40it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.35it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.42it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.41it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.42it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.28it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:06<00:03, 46.11it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:06<00:03, 46.27it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.35it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.33it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.32it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.32it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.40it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.23it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.30it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:07<00:02, 46.24it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:07<00:02, 46.24it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.30it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.34it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.29it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.35it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.37it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.30it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.29it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.39it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:08<00:01, 46.32it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.37it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.33it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.24it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.32it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.38it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.29it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.34it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.34it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:09<00:00, 46.29it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 46.37it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.36it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.34it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.32it/s][A                                                 
                                                 [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:49<01:42,  3.42it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.32it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:57:05,434 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 19:57:05,520 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:57:10,298 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:57:10,317 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:57:10,327 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 235/585 [02:05<46:41,  8.00s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 236/585 [02:06<33:11,  5.71s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 237/585 [02:06<23:40,  4.08s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/585 [02:06<17:01,  2.94s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/585 [02:06<12:23,  2.15s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 240/585 [02:07<09:08,  1.59s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/585 [02:07<06:52,  1.20s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242/585 [02:07<05:17,  1.08it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243/585 [02:08<04:11,  1.36it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/585 [02:08<03:25,  1.66it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/585 [02:08<02:52,  1.97it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/585 [02:09<02:30,  2.26it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/585 [02:09<02:15,  2.50it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/585 [02:09<02:03,  2.72it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 249/585 [02:09<01:55,  2.91it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 250/585 [02:10<01:49,  3.05it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 251/585 [02:10<01:45,  3.16it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 252/585 [02:10<01:42,  3.24it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/585 [02:11<01:40,  3.30it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 254/585 [02:11<01:39,  3.34it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/585 [02:11<01:37,  3.37it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/585 [02:11<01:36,  3.40it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/585 [02:12<01:36,  3.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/585 [02:12<01:38,  3.34it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/585 [02:12<01:36,  3.37it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/585 [02:13<01:35,  3.39it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/585 [02:13<01:35,  3.41it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 262/585 [02:13<01:34,  3.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/585 [02:13<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 264/585 [02:14<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 265/585 [02:14<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 266/585 [02:14<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 267/585 [02:15<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 268/585 [02:15<01:34,  3.36it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/585 [02:15<01:39,  3.18it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 270/585 [02:16<01:36,  3.25it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 271/585 [02:16<01:34,  3.31it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 272/585 [02:16<01:33,  3.35it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 273/585 [02:16<01:32,  3.38it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 274/585 [02:17<01:31,  3.40it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 275/585 [02:17<01:30,  3.41it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 276/585 [02:17<01:30,  3.42it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/585 [02:18<01:29,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 278/585 [02:18<01:29,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 279/585 [02:18<01:28,  3.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 280/585 [02:19<01:30,  3.38it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 281/585 [02:19<01:29,  3.40it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 282/585 [02:19<01:28,  3.41it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/585 [02:19<01:28,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 284/585 [02:20<01:27,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/585 [02:20<01:27,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 286/585 [02:20<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 287/585 [02:21<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 288/585 [02:21<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 289/585 [02:21<01:26,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 290/585 [02:21<01:25,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/585 [02:22<01:25,  3.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 292/585 [02:22<01:25,  3.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 293/585 [02:22<01:24,  3.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 294/585 [02:23<01:25,  3.41it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 295/585 [02:23<01:24,  3.42it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 296/585 [02:23<01:24,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 297/585 [02:23<01:23,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 298/585 [02:24<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/585 [02:24<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/585 [02:24<01:22,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/585 [02:25<01:22,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/585 [02:25<01:22,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/585 [02:25<01:21,  3.45it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/585 [02:26<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/585 [02:26<01:23,  3.37it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306/585 [02:26<01:22,  3.39it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/585 [02:26<01:21,  3.40it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 308/585 [02:27<01:21,  3.41it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 309/585 [02:27<01:20,  3.42it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 310/585 [02:27<01:20,  3.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 311/585 [02:28<01:19,  3.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 312/585 [02:28<01:19,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/585 [02:28<01:19,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 314/585 [02:28<01:18,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 315/585 [02:29<01:18,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/585 [02:29<01:20,  3.35it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/585 [02:29<01:19,  3.38it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/585 [02:30<01:18,  3.40it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/585 [02:30<01:18,  3.41it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/585 [02:30<01:17,  3.42it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/585 [02:30<01:17,  3.42it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 322/585 [02:31<01:16,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 323/585 [02:31<01:16,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 324/585 [02:31<01:16,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 325/585 [02:32<01:15,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 326/585 [02:32<01:15,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 327/585 [02:32<01:15,  3.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 328/585 [02:33<01:15,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/585 [02:33<01:14,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 330/585 [02:33<01:14,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 331/585 [02:33<01:14,  3.42it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 332/585 [02:34<01:13,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 333/585 [02:34<01:13,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 334/585 [02:34<01:13,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/585 [02:35<01:12,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 336/585 [02:35<01:12,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 337/585 [02:35<01:12,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 338/585 [02:35<01:15,  3.29it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 339/585 [02:36<01:13,  3.33it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 340/585 [02:36<01:12,  3.36it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 341/585 [02:36<01:12,  3.38it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 342/585 [02:37<01:11,  3.39it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/585 [02:37<01:11,  3.40it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 344/585 [02:37<01:10,  3.41it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 345/585 [02:38<01:10,  3.42it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 346/585 [02:38<01:09,  3.42it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 347/585 [02:38<01:09,  3.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 348/585 [02:38<01:09,  3.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 349/585 [02:39<01:16,  3.08it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 350/585 [02:39<01:13,  3.18it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:39<01:12,  3.25it/s][INFO|trainer.py:2140] 2023-08-28 19:57:55,861 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:57:55,861 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:57:55,861 >>   Batch size = 8
{'eval_loss': 0.9657106995582581, 'eval_runtime': 9.5839, 'eval_samples_per_second': 363.005, 'eval_steps_per_second': 45.389, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.85it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.02it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.38it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.67it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.12it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 46.96it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.72it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.67it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.56it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.41it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.30it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 46.33it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.34it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.37it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.38it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.38it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.36it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.35it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.36it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.35it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.36it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.27it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.31it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.33it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.29it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.36it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.37it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.41it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.41it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.34it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.32it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.40it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.37it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.29it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.33it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.26it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.34it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.41it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.35it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:05, 46.36it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.33it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.30it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.30it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.28it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.32it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.38it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 42.66it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 43.69it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 44.28it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:04, 44.96it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 45.33it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 45.68it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 45.81it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 45.95it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:06<00:03, 46.12it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.19it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.29it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.35it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.28it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.17it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.27it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.35it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.24it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.34it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.36it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.30it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.37it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.43it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.34it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.40it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.37it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.39it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.34it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.27it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.27it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.32it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.35it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.30it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.29it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.31it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.33it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.39it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 46.34it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.32it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.29it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.33it/s][A                                                 
                                                 [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:49<01:12,  3.25it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:58:05,318 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 19:58:05,337 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:58:12,513 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:58:12,551 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:58:12,558 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 352/585 [03:08<34:09,  8.80s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 353/585 [03:08<24:09,  6.25s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 354/585 [03:09<17:10,  4.46s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 355/585 [03:09<12:18,  3.21s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 356/585 [03:09<08:54,  2.33s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/585 [03:09<06:32,  1.72s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/585 [03:10<04:53,  1.29s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/585 [03:10<03:44,  1.01it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/585 [03:10<02:55,  1.28it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/585 [03:11<02:21,  1.58it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362/585 [03:11<01:58,  1.88it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 363/585 [03:11<01:41,  2.18it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 364/585 [03:11<01:30,  2.44it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 365/585 [03:12<01:22,  2.67it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 366/585 [03:12<01:16,  2.87it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 367/585 [03:12<01:12,  3.02it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 368/585 [03:13<01:09,  3.14it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 369/585 [03:13<01:06,  3.22it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 370/585 [03:13<01:05,  3.29it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 371/585 [03:14<01:04,  3.33it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/585 [03:14<01:03,  3.37it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 373/585 [03:14<01:02,  3.39it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/585 [03:14<01:01,  3.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375/585 [03:15<01:01,  3.40it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376/585 [03:15<01:07,  3.08it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/585 [03:16<01:14,  2.78it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/585 [03:16<01:10,  2.95it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/585 [03:16<01:06,  3.08it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/585 [03:16<01:04,  3.19it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 381/585 [03:17<01:02,  3.26it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 382/585 [03:17<01:01,  3.31it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 383/585 [03:17<01:00,  3.35it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 384/585 [03:18<00:59,  3.38it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 385/585 [03:18<01:01,  3.27it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 386/585 [03:18<00:59,  3.32it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 387/585 [03:18<00:58,  3.36it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 388/585 [03:19<00:58,  3.38it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 389/585 [03:19<00:57,  3.40it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 390/585 [03:19<00:57,  3.42it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 391/585 [03:20<00:56,  3.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 392/585 [03:20<00:56,  3.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 393/585 [03:20<00:55,  3.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 394/585 [03:21<00:55,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 395/585 [03:21<00:55,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 396/585 [03:21<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 397/585 [03:21<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 398/585 [03:22<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 399/585 [03:22<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/585 [03:22<00:53,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 401/585 [03:23<00:53,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/585 [03:23<00:53,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 403/585 [03:23<00:52,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 404/585 [03:23<00:52,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 405/585 [03:24<00:52,  3.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 406/585 [03:24<00:52,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 407/585 [03:24<00:51,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 408/585 [03:25<00:51,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 409/585 [03:25<00:51,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 410/585 [03:25<00:50,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 411/585 [03:25<00:50,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 412/585 [03:26<00:50,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 413/585 [03:26<00:49,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 414/585 [03:26<00:49,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 415/585 [03:27<00:49,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 416/585 [03:27<00:49,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417/585 [03:27<00:48,  3.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418/585 [03:27<00:48,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 419/585 [03:28<00:48,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 420/585 [03:28<00:47,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 421/585 [03:28<00:47,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 422/585 [03:29<00:47,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 423/585 [03:29<00:47,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/585 [03:29<00:46,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 425/585 [03:30<00:46,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 426/585 [03:30<00:46,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 427/585 [03:30<00:45,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 428/585 [03:30<00:45,  3.41it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 429/585 [03:31<00:45,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 430/585 [03:31<00:45,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 431/585 [03:31<00:44,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 432/585 [03:32<00:44,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 433/585 [03:32<00:44,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 434/585 [03:32<00:43,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 435/585 [03:32<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 436/585 [03:33<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 437/585 [03:33<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 438/585 [03:33<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 439/585 [03:34<00:42,  3.43it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 440/585 [03:34<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 441/585 [03:34<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 442/585 [03:34<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 443/585 [03:35<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 444/585 [03:35<00:40,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 445/585 [03:35<00:40,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 446/585 [03:36<00:40,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 447/585 [03:36<00:40,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 448/585 [03:36<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 449/585 [03:36<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 450/585 [03:37<00:39,  3.38it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 451/585 [03:37<00:39,  3.40it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 452/585 [03:37<00:38,  3.41it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 453/585 [03:38<00:38,  3.42it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 454/585 [03:38<00:38,  3.42it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 455/585 [03:38<00:37,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 456/585 [03:39<00:37,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 457/585 [03:39<00:37,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 458/585 [03:39<00:36,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 459/585 [03:39<00:36,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 460/585 [03:40<00:36,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 461/585 [03:40<00:36,  3.40it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 462/585 [03:40<00:36,  3.42it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 463/585 [03:41<00:35,  3.42it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 464/585 [03:41<00:35,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 465/585 [03:41<00:34,  3.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 466/585 [03:41<00:34,  3.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 467/585 [03:42<00:34,  3.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:42<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 19:58:58,517 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:58:58,517 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 19:58:58,517 >>   Batch size = 8
{'eval_loss': 0.9686427116394043, 'eval_runtime': 9.4416, 'eval_samples_per_second': 368.476, 'eval_steps_per_second': 46.073, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.17it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.50it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.50it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.85it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.42it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.00it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.88it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.55it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.60it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.60it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.57it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.63it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.49it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.48it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.46it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.55it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.51it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.52it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.51it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.56it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.50it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.57it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.51it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.57it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.63it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.52it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.52it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.57it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.52it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.57it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.67it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.52it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.54it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:06, 43.06it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 44.10it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 44.86it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 45.34it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 45.66it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 45.97it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:05, 46.22it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.27it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.31it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.33it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.28it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.41it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.51it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.53it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.59it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.54it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.47it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.43it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.48it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.44it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.53it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.49it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.46it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.48it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.56it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.54it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.56it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.49it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 44.48it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 45.10it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 45.57it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 45.82it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.07it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.29it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.32it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.41it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.28it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.31it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.41it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.41it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.41it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.44it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.51it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.54it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.58it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.42it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.39it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.42it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.48it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 46.51it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.53it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.44it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.56it/s][A                                                 
                                                 [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:52<00:34,  3.43it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:59:07,974 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 19:59:08,107 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:59:16,876 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:59:16,911 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:59:16,919 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 469/585 [04:16<19:53, 10.29s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 470/585 [04:16<13:59,  7.30s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 471/585 [04:16<09:52,  5.20s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 472/585 [04:17<07:01,  3.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 473/585 [04:17<05:01,  2.70s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 474/585 [04:17<03:39,  1.97s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 475/585 [04:17<02:41,  1.47s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 476/585 [04:18<02:01,  1.12s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 477/585 [04:18<01:33,  1.15it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 478/585 [04:18<01:14,  1.44it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 479/585 [04:19<01:00,  1.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 480/585 [04:19<00:51,  2.04it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 481/585 [04:19<00:44,  2.32it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 482/585 [04:19<00:40,  2.57it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 483/585 [04:20<00:36,  2.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 484/585 [04:20<00:34,  2.95it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 485/585 [04:20<00:32,  3.08it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 486/585 [04:21<00:31,  3.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 487/585 [04:21<00:30,  3.26it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 488/585 [04:21<00:29,  3.32it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 489/585 [04:22<00:28,  3.36it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 490/585 [04:22<00:28,  3.38it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 491/585 [04:22<00:27,  3.40it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 492/585 [04:22<00:27,  3.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 493/585 [04:23<00:26,  3.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 494/585 [04:23<00:26,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 495/585 [04:23<00:26,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 496/585 [04:24<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 497/585 [04:24<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 498/585 [04:24<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 499/585 [04:24<00:24,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [04:25<00:24,  3.44it/s]                                                  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [04:25<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 501/585 [04:25<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 502/585 [04:25<00:24,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 503/585 [04:26<00:23,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 504/585 [04:26<00:23,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 505/585 [04:26<00:23,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 506/585 [04:26<00:22,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 507/585 [04:27<00:22,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 508/585 [04:27<00:22,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 509/585 [04:27<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 510/585 [04:28<00:21,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 511/585 [04:28<00:21,  3.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 512/585 [04:28<00:21,  3.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 513/585 [04:28<00:21,  3.42it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 514/585 [04:29<00:20,  3.43it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 515/585 [04:29<00:20,  3.43it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 516/585 [04:29<00:20,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 517/585 [04:30<00:19,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 518/585 [04:30<00:19,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 519/585 [04:30<00:19,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 520/585 [04:31<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 521/585 [04:31<00:18,  3.45it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 522/585 [04:31<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 523/585 [04:31<00:17,  3.45it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 524/585 [04:32<00:18,  3.38it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 525/585 [04:32<00:17,  3.40it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 526/585 [04:32<00:17,  3.41it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 527/585 [04:33<00:16,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 528/585 [04:33<00:16,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 529/585 [04:33<00:16,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 530/585 [04:33<00:16,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 531/585 [04:34<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 532/585 [04:34<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 533/585 [04:34<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 534/585 [04:35<00:14,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 535/585 [04:35<00:14,  3.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 536/585 [04:35<00:14,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 537/585 [04:35<00:13,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 538/585 [04:36<00:13,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 539/585 [04:36<00:13,  3.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 540/585 [04:36<00:13,  3.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 541/585 [04:37<00:12,  3.45it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 542/585 [04:37<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 543/585 [04:37<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 544/585 [04:38<00:11,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 545/585 [04:38<00:11,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 546/585 [04:38<00:11,  3.40it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 547/585 [04:38<00:11,  3.41it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 548/585 [04:39<00:10,  3.42it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/585 [04:39<00:10,  3.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 550/585 [04:39<00:10,  3.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 551/585 [04:40<00:09,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 552/585 [04:40<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 553/585 [04:40<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 554/585 [04:40<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 555/585 [04:41<00:08,  3.45it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 556/585 [04:41<00:08,  3.45it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 557/585 [04:41<00:08,  3.35it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 558/585 [04:42<00:07,  3.38it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 559/585 [04:42<00:07,  3.40it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 560/585 [04:42<00:07,  3.41it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 561/585 [04:42<00:07,  3.42it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 562/585 [04:43<00:06,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 563/585 [04:43<00:06,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 564/585 [04:43<00:06,  3.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 565/585 [04:44<00:05,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 566/585 [04:44<00:05,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 567/585 [04:44<00:05,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 568/585 [04:45<00:04,  3.40it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 569/585 [04:45<00:04,  3.41it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 570/585 [04:45<00:04,  3.42it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 571/585 [04:45<00:04,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 572/585 [04:46<00:03,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 573/585 [04:46<00:03,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 574/585 [04:46<00:03,  3.44it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 575/585 [04:47<00:02,  3.44it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 576/585 [04:47<00:02,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 577/585 [04:47<00:02,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 578/585 [04:47<00:02,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 579/585 [04:48<00:01,  3.42it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 580/585 [04:48<00:01,  3.42it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 581/585 [04:48<00:01,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 582/585 [04:49<00:00,  3.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 583/585 [04:49<00:00,  3.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 584/585 [04:49<00:00,  3.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:49<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 20:00:05,902 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:00:05,902 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 20:00:05,902 >>   Batch size = 8
{'eval_loss': 0.9689542055130005, 'eval_runtime': 9.4166, 'eval_samples_per_second': 369.455, 'eval_steps_per_second': 46.195, 'epoch': 4.0}
{'loss': 0.8213, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.51it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.44it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.68it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.88it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.43it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.22it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.01it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.78it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.69it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:09, 39.45it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:09, 41.49it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 42.97it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:08, 43.95it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:08, 44.78it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 45.38it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 45.72it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 45.98it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:02<00:07, 45.95it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.10it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.29it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.42it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.47it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.53it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.52it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.61it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.55it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:03<00:06, 46.48it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.44it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.49it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 43.84it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:06, 44.65it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:06, 45.25it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 45.70it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 45.94it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.21it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.33it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.34it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.32it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.38it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.41it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.49it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.42it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.43it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.54it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.61it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.52it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.52it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.56it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.38it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.46it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.56it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.56it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.55it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.60it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:06<00:03, 46.55it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.51it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.55it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 43.17it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:03, 44.10it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 44.81it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 45.31it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 45.69it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 45.96it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:07<00:02, 46.21it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.25it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.28it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.34it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.32it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.46it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.46it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.45it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.53it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.52it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.52it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.57it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.47it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.42it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.34it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.45it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.56it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.54it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.57it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 46.61it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.50it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.42it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 41.49it/s][A                                                 
                                                 [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:59<00:00,  3.43it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 41.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:00:15,506 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 20:00:15,679 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:00:21,475 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:00:21,713 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:00:21,764 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:00:35,606 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:00:35,619 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117 (score: 0.963952362537384).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [05:23<00:00,  3.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [05:23<00:00,  1.81it/s]
[INFO|trainer.py:1894] 2023-08-28 20:00:39,803 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 20:00:39,816 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:00:44,275 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:00:44,327 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:00:44,337 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:00:44,740 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:44,740 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:44,740 >>   train_loss               =      0.816
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:44,740 >>   train_runtime            = 0:05:23.88
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:44,740 >>   train_samples            =       7506
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:44,740 >>   train_samples_per_second =    115.876
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:44,740 >>   train_steps_per_second   =      1.806
{'eval_loss': 0.9720550775527954, 'eval_runtime': 9.5024, 'eval_samples_per_second': 366.12, 'eval_steps_per_second': 45.778, 'epoch': 5.0}
{'train_runtime': 323.8807, 'train_samples_per_second': 115.876, 'train_steps_per_second': 1.806, 'train_loss': 0.8159735003088274, 'epoch': 5.0}
08/28/2023 20:00:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:00:44,858 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:00:44,858 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 20:00:44,858 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 57.23it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 50.73it/s]  4%|â–         | 18/435 [00:00<00:08, 48.83it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.21it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.80it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.40it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.23it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.07it/s] 11%|â–ˆ         | 48/435 [00:01<00:08, 46.79it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.61it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.68it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.74it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.70it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.51it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.71it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.73it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.68it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.74it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.62it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.58it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.65it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.60it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.35it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.49it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.54it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.55it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.62it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.57it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.62it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.62it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.59it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.66it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.72it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.71it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.73it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.70it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.68it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.71it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.54it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.60it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.58it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.58it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.68it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.70it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.60it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.59it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.57it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.59it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.63it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.65it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.62it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.66it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.56it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.55it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.61it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.63it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.64it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.80it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.75it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.82it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.90it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.89it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.85it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.88it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.81it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.86it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.88it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.96it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.95it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.98it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.81it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.89it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.83it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.88it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.94it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.94it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.85it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 47.00it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.97it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.94it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.83it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.77it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.87it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.81it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:00:54,179 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:54,179 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:54,179 >>   eval_loss               =      0.964
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:54,179 >>   eval_runtime            = 0:00:09.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:54,179 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:54,179 >>   eval_samples_per_second =    373.265
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:54,179 >>   eval_steps_per_second   =     46.672
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:00:54,179 >>   perplexity              =      2.622
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:02,198 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:02,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:02,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:02,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:02,224 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:01:02,955 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:01:02,956 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:01:03,549 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:01:04,619 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:01:04,619 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:07,795 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:07,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:07,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:07,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:01:07,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:01:08,423 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:01:08,424 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:01:09,046 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:01:09,198 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:01:09,198 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.27it/s]Extractor Predicting: 2it [00:01,  1.21it/s]Extractor Predicting: 3it [00:02,  1.15it/s]Extractor Predicting: 4it [00:03,  1.16it/s]Extractor Predicting: 5it [00:04,  1.16it/s]Extractor Predicting: 6it [00:05,  1.16it/s]Extractor Predicting: 7it [00:05,  1.17it/s]Extractor Predicting: 8it [00:06,  1.18it/s]Extractor Predicting: 9it [00:07,  1.18it/s]Extractor Predicting: 10it [00:08,  1.21it/s]Extractor Predicting: 11it [00:09,  1.19it/s]Extractor Predicting: 12it [00:10,  1.17it/s]Extractor Predicting: 13it [00:11,  1.16it/s]Extractor Predicting: 14it [00:11,  1.16it/s]Extractor Predicting: 15it [00:12,  1.19it/s]Extractor Predicting: 16it [00:13,  1.17it/s]Extractor Predicting: 17it [00:14,  1.20it/s]Extractor Predicting: 18it [00:15,  1.23it/s]Extractor Predicting: 19it [00:16,  1.18it/s]Extractor Predicting: 20it [00:16,  1.19it/s]Extractor Predicting: 21it [00:17,  1.20it/s]Extractor Predicting: 22it [00:18,  1.22it/s]Extractor Predicting: 23it [00:19,  1.21it/s]Extractor Predicting: 24it [00:20,  1.20it/s]Extractor Predicting: 25it [00:21,  1.18it/s]Extractor Predicting: 26it [00:21,  1.16it/s]Extractor Predicting: 27it [00:22,  1.14it/s]Extractor Predicting: 28it [00:23,  1.17it/s]Extractor Predicting: 29it [00:24,  1.17it/s]Extractor Predicting: 30it [00:25,  1.15it/s]Extractor Predicting: 31it [00:26,  1.16it/s]Extractor Predicting: 32it [00:27,  1.16it/s]Extractor Predicting: 33it [00:28,  1.17it/s]Extractor Predicting: 34it [00:28,  1.20it/s]Extractor Predicting: 35it [00:29,  1.20it/s]Extractor Predicting: 36it [00:30,  1.21it/s]Extractor Predicting: 37it [00:31,  1.22it/s]Extractor Predicting: 38it [00:32,  1.22it/s]Extractor Predicting: 39it [00:32,  1.23it/s]Extractor Predicting: 40it [00:33,  1.21it/s]Extractor Predicting: 41it [00:34,  1.22it/s]Extractor Predicting: 42it [00:35,  1.22it/s]Extractor Predicting: 43it [00:36,  1.22it/s]Extractor Predicting: 44it [00:36,  1.23it/s]Extractor Predicting: 45it [00:37,  1.22it/s]Extractor Predicting: 46it [00:38,  1.21it/s]Extractor Predicting: 47it [00:39,  1.20it/s]Extractor Predicting: 48it [00:40,  1.21it/s]Extractor Predicting: 49it [00:41,  1.22it/s]Extractor Predicting: 50it [00:41,  1.22it/s]Extractor Predicting: 51it [00:42,  1.23it/s]Extractor Predicting: 52it [00:43,  1.23it/s]Extractor Predicting: 53it [00:44,  1.23it/s]Extractor Predicting: 54it [00:45,  1.22it/s]Extractor Predicting: 55it [00:46,  1.19it/s]Extractor Predicting: 56it [00:46,  1.22it/s]Extractor Predicting: 57it [00:47,  1.20it/s]Extractor Predicting: 58it [00:48,  1.13it/s]Extractor Predicting: 59it [00:49,  1.18it/s]Extractor Predicting: 60it [00:50,  1.22it/s]Extractor Predicting: 61it [00:50,  1.23it/s]Extractor Predicting: 62it [00:51,  1.22it/s]Extractor Predicting: 63it [00:52,  1.21it/s]Extractor Predicting: 64it [00:53,  1.22it/s]Extractor Predicting: 65it [00:54,  1.23it/s]Extractor Predicting: 66it [00:55,  1.23it/s]Extractor Predicting: 67it [00:55,  1.23it/s]Extractor Predicting: 68it [00:56,  1.23it/s]Extractor Predicting: 69it [00:57,  1.27it/s]Extractor Predicting: 70it [00:58,  1.26it/s]Extractor Predicting: 71it [00:59,  1.27it/s]Extractor Predicting: 72it [00:59,  1.24it/s]Extractor Predicting: 73it [01:00,  1.25it/s]Extractor Predicting: 74it [01:01,  1.24it/s]Extractor Predicting: 75it [01:02,  1.22it/s]Extractor Predicting: 76it [01:03,  1.21it/s]Extractor Predicting: 77it [01:03,  1.23it/s]Extractor Predicting: 78it [01:04,  1.24it/s]Extractor Predicting: 79it [01:05,  1.27it/s]Extractor Predicting: 80it [01:06,  1.26it/s]Extractor Predicting: 81it [01:07,  1.20it/s]Extractor Predicting: 82it [01:08,  1.21it/s]Extractor Predicting: 83it [01:08,  1.23it/s]Extractor Predicting: 84it [01:09,  1.24it/s]Extractor Predicting: 85it [01:10,  1.25it/s]Extractor Predicting: 86it [01:11,  1.26it/s]Extractor Predicting: 87it [01:11,  1.28it/s]Extractor Predicting: 88it [01:12,  1.27it/s]Extractor Predicting: 89it [01:13,  1.25it/s]Extractor Predicting: 90it [01:14,  1.27it/s]Extractor Predicting: 91it [01:15,  1.29it/s]Extractor Predicting: 92it [01:15,  1.33it/s]Extractor Predicting: 93it [01:16,  1.30it/s]Extractor Predicting: 94it [01:17,  1.29it/s]Extractor Predicting: 95it [01:18,  1.30it/s]Extractor Predicting: 96it [01:18,  1.30it/s]Extractor Predicting: 97it [01:19,  1.29it/s]Extractor Predicting: 98it [01:20,  1.29it/s]Extractor Predicting: 99it [01:21,  1.27it/s]Extractor Predicting: 100it [01:22,  1.24it/s]Extractor Predicting: 101it [01:22,  1.23it/s]Extractor Predicting: 102it [01:23,  1.30it/s]Extractor Predicting: 103it [01:24,  1.31it/s]Extractor Predicting: 104it [01:25,  1.30it/s]Extractor Predicting: 105it [01:25,  1.30it/s]Extractor Predicting: 106it [01:26,  1.29it/s]Extractor Predicting: 107it [01:27,  1.29it/s]Extractor Predicting: 108it [01:28,  1.29it/s]Extractor Predicting: 109it [01:29,  1.29it/s]Extractor Predicting: 110it [01:29,  1.29it/s]Extractor Predicting: 111it [01:30,  1.29it/s]Extractor Predicting: 112it [01:31,  1.31it/s]Extractor Predicting: 113it [01:32,  1.34it/s]Extractor Predicting: 114it [01:32,  1.34it/s]Extractor Predicting: 115it [01:33,  1.35it/s]Extractor Predicting: 116it [01:34,  1.31it/s]Extractor Predicting: 117it [01:35,  1.28it/s]Extractor Predicting: 118it [01:35,  1.28it/s]Extractor Predicting: 119it [01:36,  1.25it/s]Extractor Predicting: 120it [01:37,  1.24it/s]Extractor Predicting: 121it [01:38,  1.23it/s]Extractor Predicting: 122it [01:39,  1.22it/s]Extractor Predicting: 123it [01:40,  1.23it/s]Extractor Predicting: 124it [01:40,  1.23it/s]Extractor Predicting: 125it [01:41,  1.25it/s]Extractor Predicting: 126it [01:42,  1.24it/s]Extractor Predicting: 127it [01:43,  1.26it/s]Extractor Predicting: 128it [01:44,  1.26it/s]Extractor Predicting: 129it [01:44,  1.24it/s]Extractor Predicting: 130it [01:45,  1.23it/s]Extractor Predicting: 131it [01:46,  1.25it/s]Extractor Predicting: 132it [01:47,  1.23it/s]Extractor Predicting: 133it [01:48,  1.14it/s]Extractor Predicting: 134it [01:49,  1.16it/s]Extractor Predicting: 135it [01:49,  1.19it/s]Extractor Predicting: 136it [01:50,  1.18it/s]Extractor Predicting: 137it [01:51,  1.20it/s]Extractor Predicting: 138it [01:52,  1.18it/s]Extractor Predicting: 139it [01:53,  1.19it/s]Extractor Predicting: 140it [01:54,  1.19it/s]Extractor Predicting: 141it [01:54,  1.21it/s]Extractor Predicting: 142it [01:55,  1.20it/s]Extractor Predicting: 143it [01:56,  1.21it/s]Extractor Predicting: 144it [01:56,  1.48it/s]Extractor Predicting: 144it [01:56,  1.23it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:16,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:16,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:16,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:16,858 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:16,859 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:03:17,153 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:03:17,154 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:03:17,427 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:03:18,466 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:03:18,466 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:19,805 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:19,807 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:19,807 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:19,807 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:03:19,807 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:03:20,120 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:03:20,140 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:03:20,412 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:03:20,560 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:03:20,560 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.12883435582822086,
  "recall": 0.012072434607645875,
  "score": 0.02207621550591327,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.24it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.27it/s]Extractor Predicting: 5it [00:03,  1.29it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.32it/s]Extractor Predicting: 8it [00:06,  1.35it/s]Extractor Predicting: 9it [00:06,  1.31it/s]Extractor Predicting: 10it [00:07,  1.32it/s]Extractor Predicting: 11it [00:08,  1.32it/s]Extractor Predicting: 12it [00:09,  1.30it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.25it/s]Extractor Predicting: 15it [00:11,  1.27it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:13,  1.27it/s]Extractor Predicting: 18it [00:13,  1.28it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.29it/s]Extractor Predicting: 21it [00:16,  1.30it/s]Extractor Predicting: 22it [00:17,  1.30it/s]Extractor Predicting: 23it [00:17,  1.31it/s]Extractor Predicting: 24it [00:18,  1.30it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.30it/s]Extractor Predicting: 27it [00:20,  1.28it/s]Extractor Predicting: 28it [00:21,  1.29it/s]Extractor Predicting: 29it [00:22,  1.30it/s]Extractor Predicting: 30it [00:23,  1.31it/s]Extractor Predicting: 31it [00:23,  1.29it/s]Extractor Predicting: 32it [00:24,  1.32it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:27,  1.22it/s]Extractor Predicting: 36it [00:27,  1.25it/s]Extractor Predicting: 37it [00:28,  1.28it/s]Extractor Predicting: 38it [00:29,  1.29it/s]Extractor Predicting: 39it [00:30,  1.29it/s]Extractor Predicting: 40it [00:31,  1.28it/s]Extractor Predicting: 41it [00:31,  1.29it/s]Extractor Predicting: 42it [00:32,  1.31it/s]Extractor Predicting: 43it [00:33,  1.28it/s]Extractor Predicting: 44it [00:34,  1.27it/s]Extractor Predicting: 45it [00:34,  1.28it/s]Extractor Predicting: 46it [00:35,  1.26it/s]Extractor Predicting: 47it [00:36,  1.28it/s]Extractor Predicting: 48it [00:37,  1.27it/s]Extractor Predicting: 49it [00:38,  1.30it/s]Extractor Predicting: 50it [00:38,  1.29it/s]Extractor Predicting: 51it [00:39,  1.28it/s]Extractor Predicting: 52it [00:40,  1.26it/s]Extractor Predicting: 53it [00:41,  1.27it/s]Extractor Predicting: 54it [00:41,  1.26it/s]Extractor Predicting: 55it [00:42,  1.30it/s]Extractor Predicting: 56it [00:43,  1.29it/s]Extractor Predicting: 57it [00:44,  1.26it/s]Extractor Predicting: 58it [00:45,  1.26it/s]Extractor Predicting: 59it [00:45,  1.26it/s]Extractor Predicting: 60it [00:46,  1.25it/s]Extractor Predicting: 61it [00:47,  1.24it/s]Extractor Predicting: 62it [00:48,  1.27it/s]Extractor Predicting: 63it [00:49,  1.27it/s]Extractor Predicting: 64it [00:49,  1.30it/s]Extractor Predicting: 65it [00:50,  1.27it/s]Extractor Predicting: 66it [00:51,  1.25it/s]Extractor Predicting: 67it [00:52,  1.25it/s]Extractor Predicting: 68it [00:53,  1.25it/s]Extractor Predicting: 69it [00:53,  1.23it/s]Extractor Predicting: 70it [00:54,  1.24it/s]Extractor Predicting: 71it [00:55,  1.24it/s]Extractor Predicting: 72it [00:56,  1.22it/s]Extractor Predicting: 73it [00:57,  1.23it/s]Extractor Predicting: 74it [00:57,  1.24it/s]Extractor Predicting: 75it [00:58,  1.25it/s]Extractor Predicting: 76it [00:59,  1.24it/s]Extractor Predicting: 77it [01:00,  1.28it/s]Extractor Predicting: 78it [01:01,  1.27it/s]Extractor Predicting: 79it [01:01,  1.30it/s]Extractor Predicting: 80it [01:02,  1.32it/s]Extractor Predicting: 81it [01:03,  1.31it/s]Extractor Predicting: 82it [01:04,  1.28it/s]Extractor Predicting: 83it [01:04,  1.27it/s]Extractor Predicting: 84it [01:05,  1.26it/s]Extractor Predicting: 85it [01:06,  1.23it/s]Extractor Predicting: 86it [01:07,  1.21it/s]Extractor Predicting: 87it [01:08,  1.22it/s]Extractor Predicting: 88it [01:09,  1.22it/s]Extractor Predicting: 89it [01:09,  1.20it/s]Extractor Predicting: 90it [01:10,  1.20it/s]Extractor Predicting: 91it [01:11,  1.19it/s]Extractor Predicting: 92it [01:12,  1.21it/s]Extractor Predicting: 93it [01:13,  1.22it/s]Extractor Predicting: 94it [01:14,  1.23it/s]Extractor Predicting: 95it [01:14,  1.24it/s]Extractor Predicting: 96it [01:15,  1.20it/s]Extractor Predicting: 97it [01:16,  1.19it/s]Extractor Predicting: 98it [01:17,  1.19it/s]Extractor Predicting: 99it [01:18,  1.21it/s]Extractor Predicting: 100it [01:19,  1.21it/s]Extractor Predicting: 101it [01:19,  1.24it/s]Extractor Predicting: 102it [01:20,  1.22it/s]Extractor Predicting: 103it [01:21,  1.20it/s]Extractor Predicting: 104it [01:22,  1.24it/s]Extractor Predicting: 105it [01:23,  1.23it/s]Extractor Predicting: 106it [01:23,  1.23it/s]Extractor Predicting: 107it [01:24,  1.25it/s]Extractor Predicting: 108it [01:25,  1.22it/s]Extractor Predicting: 109it [01:26,  1.22it/s]Extractor Predicting: 110it [01:27,  1.22it/s]Extractor Predicting: 111it [01:27,  1.23it/s]Extractor Predicting: 112it [01:28,  1.21it/s]Extractor Predicting: 113it [01:29,  1.23it/s]Extractor Predicting: 114it [01:30,  1.24it/s]Extractor Predicting: 115it [01:31,  1.20it/s]Extractor Predicting: 116it [01:32,  1.22it/s]Extractor Predicting: 117it [01:32,  1.22it/s]Extractor Predicting: 118it [01:33,  1.22it/s]Extractor Predicting: 119it [01:34,  1.22it/s]Extractor Predicting: 120it [01:35,  1.24it/s]Extractor Predicting: 121it [01:36,  1.26it/s]Extractor Predicting: 122it [01:36,  1.26it/s]Extractor Predicting: 123it [01:37,  1.26it/s]Extractor Predicting: 124it [01:38,  1.27it/s]Extractor Predicting: 125it [01:39,  1.29it/s]Extractor Predicting: 126it [01:40,  1.29it/s]Extractor Predicting: 127it [01:40,  1.28it/s]Extractor Predicting: 128it [01:41,  1.30it/s]Extractor Predicting: 129it [01:42,  1.31it/s]Extractor Predicting: 130it [01:43,  1.27it/s]Extractor Predicting: 131it [01:43,  1.31it/s]Extractor Predicting: 132it [01:44,  1.31it/s]Extractor Predicting: 133it [01:45,  1.33it/s]Extractor Predicting: 134it [01:46,  1.28it/s]Extractor Predicting: 135it [01:46,  1.31it/s]Extractor Predicting: 136it [01:47,  1.32it/s]Extractor Predicting: 137it [01:48,  1.33it/s]Extractor Predicting: 138it [01:49,  1.31it/s]Extractor Predicting: 139it [01:49,  1.30it/s]Extractor Predicting: 140it [01:50,  1.31it/s]Extractor Predicting: 141it [01:51,  1.18it/s]Extractor Predicting: 142it [01:52,  1.20it/s]Extractor Predicting: 143it [01:53,  1.24it/s]Extractor Predicting: 144it [01:54,  1.24it/s]Extractor Predicting: 145it [01:54,  1.29it/s]Extractor Predicting: 146it [01:55,  1.31it/s]Extractor Predicting: 147it [01:56,  1.32it/s]Extractor Predicting: 148it [01:56,  1.35it/s]Extractor Predicting: 149it [01:57,  1.35it/s]Extractor Predicting: 150it [01:58,  1.36it/s]Extractor Predicting: 151it [01:59,  1.35it/s]Extractor Predicting: 152it [01:59,  1.37it/s]Extractor Predicting: 153it [02:00,  1.35it/s]Extractor Predicting: 154it [02:01,  1.35it/s]Extractor Predicting: 155it [02:02,  1.39it/s]Extractor Predicting: 156it [02:02,  1.36it/s]Extractor Predicting: 157it [02:03,  1.43it/s]Extractor Predicting: 158it [02:04,  1.45it/s]Extractor Predicting: 159it [02:04,  1.41it/s]Extractor Predicting: 160it [02:05,  1.37it/s]Extractor Predicting: 161it [02:06,  1.36it/s]Extractor Predicting: 162it [02:07,  1.38it/s]Extractor Predicting: 163it [02:07,  1.39it/s]Extractor Predicting: 164it [02:08,  1.40it/s]Extractor Predicting: 165it [02:09,  1.40it/s]Extractor Predicting: 166it [02:10,  1.37it/s]Extractor Predicting: 167it [02:10,  1.39it/s]Extractor Predicting: 168it [02:11,  1.38it/s]Extractor Predicting: 169it [02:12,  1.43it/s]Extractor Predicting: 170it [02:12,  1.41it/s]Extractor Predicting: 171it [02:13,  1.41it/s]Extractor Predicting: 172it [02:14,  1.35it/s]Extractor Predicting: 173it [02:15,  1.33it/s]Extractor Predicting: 174it [02:15,  1.29it/s]Extractor Predicting: 175it [02:16,  1.25it/s]Extractor Predicting: 176it [02:17,  1.25it/s]Extractor Predicting: 177it [02:18,  1.25it/s]Extractor Predicting: 178it [02:19,  1.24it/s]Extractor Predicting: 179it [02:20,  1.23it/s]Extractor Predicting: 180it [02:20,  1.24it/s]Extractor Predicting: 181it [02:21,  1.24it/s]Extractor Predicting: 182it [02:22,  1.24it/s]Extractor Predicting: 183it [02:23,  1.23it/s]Extractor Predicting: 184it [02:24,  1.22it/s]Extractor Predicting: 185it [02:24,  1.21it/s]Extractor Predicting: 186it [02:25,  1.21it/s]Extractor Predicting: 187it [02:26,  1.21it/s]Extractor Predicting: 188it [02:27,  1.22it/s]Extractor Predicting: 189it [02:28,  1.22it/s]Extractor Predicting: 190it [02:29,  1.24it/s]Extractor Predicting: 191it [02:29,  1.19it/s]Extractor Predicting: 192it [02:30,  1.18it/s]Extractor Predicting: 193it [02:31,  1.19it/s]Extractor Predicting: 194it [02:32,  1.19it/s]Extractor Predicting: 195it [02:33,  1.20it/s]Extractor Predicting: 196it [02:34,  1.21it/s]Extractor Predicting: 197it [02:34,  1.21it/s]Extractor Predicting: 198it [02:35,  1.23it/s]Extractor Predicting: 199it [02:36,  1.24it/s]Extractor Predicting: 200it [02:37,  1.22it/s]Extractor Predicting: 201it [02:38,  1.20it/s]Extractor Predicting: 202it [02:38,  1.27it/s]Extractor Predicting: 203it [02:39,  1.24it/s]Extractor Predicting: 204it [02:40,  1.26it/s]Extractor Predicting: 205it [02:41,  1.25it/s]Extractor Predicting: 206it [02:42,  1.24it/s]Extractor Predicting: 207it [02:42,  1.23it/s]Extractor Predicting: 208it [02:43,  1.23it/s]Extractor Predicting: 209it [02:44,  1.16it/s]Extractor Predicting: 210it [02:45,  1.18it/s]Extractor Predicting: 211it [02:46,  1.18it/s]Extractor Predicting: 212it [02:47,  1.19it/s]Extractor Predicting: 213it [02:48,  1.19it/s]Extractor Predicting: 214it [02:48,  1.18it/s]Extractor Predicting: 215it [02:49,  1.21it/s]Extractor Predicting: 216it [02:50,  1.19it/s]Extractor Predicting: 217it [02:51,  1.19it/s]Extractor Predicting: 218it [02:52,  1.20it/s]Extractor Predicting: 219it [02:53,  1.19it/s]Extractor Predicting: 220it [02:53,  1.20it/s]Extractor Predicting: 221it [02:54,  1.18it/s]Extractor Predicting: 222it [02:55,  1.18it/s]Extractor Predicting: 223it [02:56,  1.21it/s]Extractor Predicting: 224it [02:57,  1.21it/s]Extractor Predicting: 225it [02:58,  1.21it/s]Extractor Predicting: 226it [02:58,  1.20it/s]Extractor Predicting: 227it [02:59,  1.24it/s]Extractor Predicting: 228it [03:00,  1.23it/s]Extractor Predicting: 229it [03:01,  1.21it/s]Extractor Predicting: 230it [03:02,  1.26it/s]Extractor Predicting: 231it [03:02,  1.27it/s]Extractor Predicting: 232it [03:03,  1.19it/s]Extractor Predicting: 233it [03:04,  1.20it/s]Extractor Predicting: 234it [03:05,  1.22it/s]Extractor Predicting: 235it [03:06,  1.23it/s]Extractor Predicting: 236it [03:07,  1.23it/s]Extractor Predicting: 237it [03:07,  1.18it/s]Extractor Predicting: 238it [03:08,  1.20it/s]Extractor Predicting: 239it [03:09,  1.20it/s]Extractor Predicting: 240it [03:10,  1.22it/s]Extractor Predicting: 241it [03:11,  1.18it/s]Extractor Predicting: 242it [03:12,  1.21it/s]Extractor Predicting: 243it [03:12,  1.21it/s]Extractor Predicting: 244it [03:13,  1.22it/s]Extractor Predicting: 245it [03:14,  1.25it/s]Extractor Predicting: 246it [03:15,  1.24it/s]Extractor Predicting: 247it [03:16,  1.24it/s]Extractor Predicting: 248it [03:16,  1.24it/s]Extractor Predicting: 249it [03:17,  1.23it/s]Extractor Predicting: 250it [03:18,  1.24it/s]Extractor Predicting: 251it [03:19,  1.25it/s]Extractor Predicting: 252it [03:20,  1.26it/s]Extractor Predicting: 253it [03:20,  1.24it/s]Extractor Predicting: 254it [03:21,  1.25it/s]Extractor Predicting: 255it [03:22,  1.22it/s]Extractor Predicting: 256it [03:23,  1.18it/s]Extractor Predicting: 257it [03:24,  1.19it/s]Extractor Predicting: 258it [03:25,  1.18it/s]Extractor Predicting: 259it [03:25,  1.20it/s]Extractor Predicting: 260it [03:26,  1.19it/s]Extractor Predicting: 261it [03:27,  1.22it/s]Extractor Predicting: 262it [03:28,  1.20it/s]Extractor Predicting: 263it [03:29,  1.19it/s]Extractor Predicting: 264it [03:30,  1.19it/s]Extractor Predicting: 265it [03:31,  1.15it/s]Extractor Predicting: 266it [03:31,  1.16it/s]Extractor Predicting: 267it [03:32,  1.17it/s]Extractor Predicting: 268it [03:33,  1.15it/s]Extractor Predicting: 269it [03:34,  1.17it/s]Extractor Predicting: 270it [03:35,  1.18it/s]Extractor Predicting: 271it [03:36,  1.16it/s]Extractor Predicting: 272it [03:37,  1.18it/s]Extractor Predicting: 273it [03:37,  1.18it/s]Extractor Predicting: 274it [03:38,  1.20it/s]Extractor Predicting: 275it [03:39,  1.22it/s]Extractor Predicting: 276it [03:40,  1.25it/s]Extractor Predicting: 277it [03:41,  1.25it/s]Extractor Predicting: 278it [03:41,  1.24it/s]Extractor Predicting: 279it [03:42,  1.24it/s]Extractor Predicting: 280it [03:43,  1.20it/s]Extractor Predicting: 281it [03:44,  1.20it/s]Extractor Predicting: 282it [03:45,  1.28it/s]Extractor Predicting: 282it [03:45,  1.25it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:14,740 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:14,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:14,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:14,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:14,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:07:15,437 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:07:15,438 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:07:15,697 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:07:16,760 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:07:16,760 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:18,157 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:18,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:18,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:18,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:07:18,168 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:07:18,514 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:07:18,515 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:07:18,788 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:07:18,947 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:07:18,947 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.39416058394160586,
  "recall": 0.09585798816568047,
  "score": 0.15421227986673014,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.20it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.19it/s]Extractor Predicting: 4it [00:03,  1.19it/s]Extractor Predicting: 5it [00:04,  1.23it/s]Extractor Predicting: 5it [00:04,  1.21it/s]
[INFO|configuration_utils.py:515] 2023-08-28 20:07:23,599 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:07:23,599 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:07:23,649 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:07:23,650 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 20:07:23,685 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:07:30,772 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 20:07:30,778 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 20:07:30,810 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:07:30,811 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:07:30,817 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:07:30,825 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:07:30,825 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:07:30,825 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:07:30,825 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:07:30,825 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:07:30,825 >> loading file outputs/wrapper/fewrel/unseen_10_seed_1/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5555555555555556,
  "recall": 0.041666666666666664,
  "score": 0.07751937984496124,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_1/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 20:07:31,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:32,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:33,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:34,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:35,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:36,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:37,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:38,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:39,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:40,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:41,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:41,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:43,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:44,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:44,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:46,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:47,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:48,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:49,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:50,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:51,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:52,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:52,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:54,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:55,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:56,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:57,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:07:58,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:28<06:35, 28.22s/it][WARNING|generation_utils.py:914] 2023-08-28 20:07:59,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:00,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:01,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:02,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:03,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:04,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:04,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:05,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:06,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:07,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:08,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:09,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:10,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:11,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:12,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:13,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:13,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:14,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:15,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:16,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:17,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:18,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:19,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:49<05:13, 24.15s/it][WARNING|generation_utils.py:914] 2023-08-28 20:08:20,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:21,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:22,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:23,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:24,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:25,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:26,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:27,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:28,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:29,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:29,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:30,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:31,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:32,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:33,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:34,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:35,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:36,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:37,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:38,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:39,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:40,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:41,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:42,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:43,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:13<04:48, 24.03s/it][WARNING|generation_utils.py:914] 2023-08-28 20:08:44,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:45,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:46,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:47,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:48,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:49,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:50,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:51,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:52,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:53,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:54,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:55,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:56,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:57,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:58,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:59,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:08:59,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:00,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:01,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:02,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:04,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:05,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:34<04:13, 23.01s/it][WARNING|generation_utils.py:914] 2023-08-28 20:09:05,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:06,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:07,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:08,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:10,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:11,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:11,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:12,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:13,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:14,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:16,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:17,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:17,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:18,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:19,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:20,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:21,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:22,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:23,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:24,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:25,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:26,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [01:56<03:45, 22.51s/it][WARNING|generation_utils.py:914] 2023-08-28 20:09:27,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:28,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:29,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:30,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:31,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:31,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:32,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:33,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:35,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:36,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:36,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:38,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:39,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:39,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:40,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:41,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:42,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:43,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:44,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:45,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:46,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:47,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:48,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:18<03:21, 22.41s/it][WARNING|generation_utils.py:914] 2023-08-28 20:09:49,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:50,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:51,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:52,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:53,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:54,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:55,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:56,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:57,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:58,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:09:59,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:00,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:01,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:02,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:03,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:04,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:05,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:06,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:07,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:08,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:09,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:10,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:40<02:57, 22.16s/it][WARNING|generation_utils.py:914] 2023-08-28 20:10:11,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:12,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:13,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:14,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:15,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:16,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:17,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:18,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:19,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:20,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:21,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:22,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:23,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:24,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:25,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:26,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:27,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:28,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:29,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:30,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:31,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:32,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:33,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [03:03<02:36, 22.42s/it][WARNING|generation_utils.py:914] 2023-08-28 20:10:34,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:35,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:36,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:37,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:38,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:39,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:40,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:41,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:42,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:43,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:44,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:45,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:45,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:46,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:47,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:48,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:49,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:50,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:52,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:53,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:54,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:24<02:12, 22.00s/it][WARNING|generation_utils.py:914] 2023-08-28 20:10:55,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:56,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:57,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:58,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:10:59,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:00,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:01,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:02,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:03,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:04,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:05,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:06,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:07,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:08,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:09,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:10,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:11,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:12,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:13,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:14,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:15,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:16,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [03:46<01:49, 21.92s/it][WARNING|generation_utils.py:914] 2023-08-28 20:11:17,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:18,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:19,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:20,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:21,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:22,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:23,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:25,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:26,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:27,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:27,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:29,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:30,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:31,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:32,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:33,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:34,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:35,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:36,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:37,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:38,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:39,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [04:09<01:29, 22.26s/it][WARNING|generation_utils.py:914] 2023-08-28 20:11:40,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:41,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:42,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:43,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:44,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:45,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:46,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:47,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:48,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:49,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:49,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:51,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:51,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:52,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:54,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:54,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:56,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:57,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:58,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:11:59,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:00,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:01,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:30<01:06, 22.06s/it][WARNING|generation_utils.py:914] 2023-08-28 20:12:01,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:02,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:03,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:04,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:05,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:05,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:06,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:07,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:08,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:09,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:10,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:11,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:12,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:14,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:14,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:15,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:16,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:17,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:18,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:19,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:20,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:21,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:21,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:22,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [04:52<00:44, 22.02s/it][WARNING|generation_utils.py:914] 2023-08-28 20:12:23,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:24,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:25,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:26,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:27,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:28,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:29,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:30,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:31,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:32,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:33,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:34,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:35,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:36,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:37,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:38,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:39,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:40,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:41,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:42,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:43,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:44,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:45,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:45,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [05:15<00:22, 22.35s/it][WARNING|generation_utils.py:914] 2023-08-28 20:12:46,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:47,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:48,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:50,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:51,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:52,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:53,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:54,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:55,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:56,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:57,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:58,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:12:59,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:00,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:02,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:03,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:04,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:05,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:06,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:07,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:08,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:09,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:10,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:13:11,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:41<00:00, 23.33s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:41<00:00, 22.76s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:19,757 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:19,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:19,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:19,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:19,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:13:20,535 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:13:20,536 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:13:21,224 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:13:22,334 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:13:22,334 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:25,602 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:25,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:25,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:25,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:13:25,618 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:13:26,326 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:13:26,327 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:13:26,946 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:13:27,142 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:13:27,142 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 59, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 105, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 176, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 223, 'raw': 320}
{'target': 600, 'success': 243, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 291, 'raw': 416}
{'target': 600, 'success': 315, 'raw': 448}
{'target': 600, 'success': 332, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 393, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 460, 'raw': 672}
{'target': 600, 'success': 480, 'raw': 704}
{'target': 600, 'success': 498, 'raw': 736}
{'target': 600, 'success': 522, 'raw': 768}
{'target': 600, 'success': 543, 'raw': 800}
{'target': 600, 'success': 562, 'raw': 832}
{'target': 600, 'success': 583, 'raw': 864}
{'target': 600, 'success': 603, 'raw': 896}
{'prompt': 'Relation : country .', 'success_rate': 0.6729910714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : followed by .', 'success_rate': 0.8315217391304348, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 274, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 320, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 373, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 418, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 467, 'raw': 608}
{'target': 600, 'success': 489, 'raw': 640}
{'target': 600, 'success': 515, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 595, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : genre .', 'success_rate': 0.7725, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8806818181818182, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8383152173913043, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 610, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9077380952380952, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : occupation . Context : Later in the year ( 1141â€“1231 ) he married Brigadier General Bessie de Bontus ( also known as the " Queen Elizabeth " ) , daughter of Robert de Bontus & Catherine Louise de Bontus ( 17th - 18th century ) . Head Entity : Brigadier General Bessie de Bontus ( also known as the " Queen Elizabeth " ) , Tail Entity : Catherine Louise de Bontus & Robert de Bontus .\n']
['Relation : occupation . Context : Later in the year ( 1141â€“1231 ) he married Brigadier General Bessie de Bontus ( also known as the " Queen Elizabeth " ) , daughter of Robert de Bontus & Catherine Louise de Bontus ( 17th - 18th century ) . Head Entity : Brigadier General Bessie de Bontus ( also known as the " Queen Elizabeth " ) , Tail Entity : Catherine Louise de Bontus & Robert de Bontus .\n', 'Relation : occupation . Context : After the death of King Charles IV of France ( c. 589 - 7 February 1202 ) , one man was succeeded as king by the emperor , Napoleon . Head Entity : Napoleon , Tail Entity : French .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 487, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.8059895833333334, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/1_ext.jsonl'}}
estimate vocab size: 13169
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13269, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.01it/s]Extractor Estimating: 2it [00:01,  1.10it/s]Extractor Estimating: 3it [00:02,  1.16it/s]Extractor Estimating: 4it [00:03,  1.22it/s]Extractor Estimating: 5it [00:04,  1.21it/s]Extractor Estimating: 6it [00:05,  1.24it/s]Extractor Estimating: 7it [00:05,  1.25it/s]Extractor Estimating: 8it [00:06,  1.32it/s]Extractor Estimating: 9it [00:07,  1.26it/s]Extractor Estimating: 10it [00:08,  1.29it/s]Extractor Estimating: 11it [00:09,  1.19it/s]Extractor Estimating: 12it [00:09,  1.20it/s]Extractor Estimating: 13it [00:10,  1.24it/s]Extractor Estimating: 14it [00:11,  1.25it/s]Extractor Estimating: 15it [00:12,  1.28it/s]Extractor Estimating: 16it [00:12,  1.30it/s]Extractor Estimating: 17it [00:13,  1.27it/s]Extractor Estimating: 18it [00:14,  1.25it/s]Extractor Estimating: 19it [00:15,  1.26it/s]Extractor Estimating: 20it [00:16,  1.27it/s]Extractor Estimating: 21it [00:17,  1.20it/s]Extractor Estimating: 22it [00:17,  1.24it/s]Extractor Estimating: 23it [00:18,  1.24it/s]Extractor Estimating: 24it [00:19,  1.21it/s]Extractor Estimating: 25it [00:20,  1.19it/s]Extractor Estimating: 26it [00:21,  1.14it/s]Extractor Estimating: 27it [00:22,  1.18it/s]Extractor Estimating: 28it [00:22,  1.21it/s]Extractor Estimating: 29it [00:23,  1.19it/s]Extractor Estimating: 30it [00:24,  1.19it/s]Extractor Estimating: 31it [00:25,  1.20it/s]Extractor Estimating: 32it [00:26,  1.24it/s]Extractor Estimating: 33it [00:26,  1.25it/s]Extractor Estimating: 34it [00:27,  1.27it/s]Extractor Estimating: 35it [00:28,  1.26it/s]Extractor Estimating: 36it [00:29,  1.28it/s]Extractor Estimating: 37it [00:30,  1.28it/s]Extractor Estimating: 38it [00:30,  1.28it/s]Extractor Estimating: 39it [00:31,  1.26it/s]Extractor Estimating: 40it [00:32,  1.29it/s]Extractor Estimating: 41it [00:33,  1.26it/s]Extractor Estimating: 42it [00:34,  1.23it/s]Extractor Estimating: 43it [00:34,  1.26it/s]Extractor Estimating: 44it [00:35,  1.29it/s]Extractor Estimating: 45it [00:36,  1.28it/s]Extractor Estimating: 46it [00:37,  1.29it/s]Extractor Estimating: 47it [00:37,  1.28it/s]Extractor Estimating: 48it [00:38,  1.27it/s]Extractor Estimating: 49it [00:39,  1.25it/s]Extractor Estimating: 50it [00:40,  1.20it/s]Extractor Estimating: 51it [00:41,  1.21it/s]Extractor Estimating: 52it [00:42,  1.22it/s]Extractor Estimating: 53it [00:42,  1.20it/s]Extractor Estimating: 54it [00:43,  1.20it/s]Extractor Estimating: 55it [00:44,  1.22it/s]Extractor Estimating: 56it [00:45,  1.18it/s]Extractor Estimating: 57it [00:46,  1.19it/s]Extractor Estimating: 58it [00:47,  1.23it/s]Extractor Estimating: 59it [00:47,  1.25it/s]Extractor Estimating: 60it [00:48,  1.25it/s]Extractor Estimating: 61it [00:49,  1.24it/s]Extractor Estimating: 62it [00:50,  1.21it/s]Extractor Estimating: 63it [00:51,  1.21it/s]Extractor Estimating: 64it [00:51,  1.20it/s]Extractor Estimating: 65it [00:52,  1.21it/s]Extractor Estimating: 66it [00:53,  1.23it/s]Extractor Estimating: 67it [00:54,  1.25it/s]Extractor Estimating: 68it [00:55,  1.23it/s]Extractor Estimating: 69it [00:56,  1.20it/s]Extractor Estimating: 70it [00:56,  1.24it/s]Extractor Estimating: 71it [00:57,  1.28it/s]Extractor Estimating: 72it [00:58,  1.26it/s]Extractor Estimating: 73it [00:59,  1.31it/s]Extractor Estimating: 74it [00:59,  1.29it/s]Extractor Estimating: 75it [01:00,  1.30it/s]Extractor Estimating: 76it [01:01,  1.28it/s]Extractor Estimating: 77it [01:02,  1.32it/s]Extractor Estimating: 78it [01:02,  1.32it/s]Extractor Estimating: 79it [01:03,  1.26it/s]Extractor Estimating: 80it [01:04,  1.29it/s]Extractor Estimating: 81it [01:05,  1.27it/s]Extractor Estimating: 82it [01:05,  1.32it/s]Extractor Estimating: 83it [01:06,  1.28it/s]Extractor Estimating: 84it [01:07,  1.25it/s]Extractor Estimating: 85it [01:08,  1.25it/s]Extractor Estimating: 86it [01:09,  1.18it/s]Extractor Estimating: 87it [01:10,  1.19it/s]Extractor Estimating: 88it [01:10,  1.22it/s]Extractor Estimating: 89it [01:11,  1.24it/s]Extractor Estimating: 90it [01:12,  1.28it/s]Extractor Estimating: 91it [01:13,  1.30it/s]Extractor Estimating: 92it [01:13,  1.31it/s]Extractor Estimating: 93it [01:14,  1.24it/s]Extractor Estimating: 94it [01:15,  1.25it/s]Extractor Estimating: 95it [01:16,  1.26it/s]Extractor Estimating: 96it [01:17,  1.20it/s]Extractor Estimating: 97it [01:18,  1.20it/s]Extractor Estimating: 98it [01:19,  1.21it/s]Extractor Estimating: 99it [01:19,  1.16it/s]Extractor Estimating: 100it [01:20,  1.20it/s]Extractor Estimating: 101it [01:21,  1.18it/s]Extractor Estimating: 102it [01:22,  1.19it/s]Extractor Estimating: 103it [01:23,  1.24it/s]Extractor Estimating: 104it [01:24,  1.21it/s]Extractor Estimating: 105it [01:24,  1.17it/s]Extractor Estimating: 106it [01:25,  1.22it/s]Extractor Estimating: 107it [01:26,  1.23it/s]Extractor Estimating: 108it [01:27,  1.22it/s]Extractor Estimating: 109it [01:28,  1.23it/s]Extractor Estimating: 110it [01:28,  1.24it/s]Extractor Estimating: 111it [01:29,  1.23it/s]Extractor Estimating: 112it [01:30,  1.22it/s]Extractor Estimating: 113it [01:31,  1.21it/s]Extractor Estimating: 114it [01:32,  1.23it/s]Extractor Estimating: 115it [01:33,  1.21it/s]Extractor Estimating: 116it [01:33,  1.22it/s]Extractor Estimating: 117it [01:34,  1.21it/s]Extractor Estimating: 118it [01:35,  1.24it/s]Extractor Estimating: 119it [01:36,  1.27it/s]Extractor Estimating: 120it [01:37,  1.26it/s]Extractor Estimating: 121it [01:37,  1.26it/s]Extractor Estimating: 122it [01:38,  1.28it/s]Extractor Estimating: 123it [01:39,  1.29it/s]Extractor Estimating: 124it [01:40,  1.30it/s]Extractor Estimating: 125it [01:40,  1.29it/s]Extractor Estimating: 126it [01:41,  1.32it/s]Extractor Estimating: 127it [01:42,  1.29it/s]Extractor Estimating: 128it [01:43,  1.29it/s]Extractor Estimating: 129it [01:43,  1.32it/s]Extractor Estimating: 130it [01:44,  1.38it/s]Extractor Estimating: 131it [01:45,  1.35it/s]Extractor Estimating: 132it [01:46,  1.38it/s]Extractor Estimating: 133it [01:47,  1.24it/s]Extractor Estimating: 134it [01:47,  1.28it/s]Extractor Estimating: 135it [01:48,  1.29it/s]Extractor Estimating: 136it [01:49,  1.29it/s]Extractor Estimating: 137it [01:50,  1.25it/s]Extractor Estimating: 138it [01:50,  1.26it/s]Extractor Estimating: 139it [01:51,  1.26it/s]Extractor Estimating: 140it [01:52,  1.29it/s]Extractor Estimating: 141it [01:53,  1.30it/s]Extractor Estimating: 142it [01:53,  1.35it/s]Extractor Estimating: 143it [01:54,  1.33it/s]Extractor Estimating: 144it [01:55,  1.23it/s]Extractor Estimating: 145it [01:56,  1.27it/s]Extractor Estimating: 146it [01:57,  1.27it/s]Extractor Estimating: 147it [01:57,  1.29it/s]Extractor Estimating: 148it [01:58,  1.30it/s]Extractor Estimating: 149it [01:59,  1.33it/s]Extractor Estimating: 150it [02:00,  1.26it/s]Extractor Estimating: 151it [02:00,  1.27it/s]Extractor Estimating: 152it [02:01,  1.30it/s]Extractor Estimating: 153it [02:02,  1.24it/s]Extractor Estimating: 154it [02:03,  1.28it/s]Extractor Estimating: 155it [02:04,  1.30it/s]Extractor Estimating: 156it [02:04,  1.23it/s]Extractor Estimating: 157it [02:05,  1.19it/s]Extractor Estimating: 158it [02:06,  1.23it/s]Extractor Estimating: 159it [02:07,  1.22it/s]Extractor Estimating: 160it [02:08,  1.17it/s]Extractor Estimating: 161it [02:09,  1.15it/s]Extractor Estimating: 162it [02:09,  1.23it/s]Extractor Estimating: 163it [02:10,  1.23it/s]Extractor Estimating: 164it [02:11,  1.19it/s]Extractor Estimating: 165it [02:12,  1.20it/s]Extractor Estimating: 166it [02:13,  1.24it/s]Extractor Estimating: 167it [02:14,  1.23it/s]Extractor Estimating: 168it [02:14,  1.22it/s]Extractor Estimating: 169it [02:15,  1.22it/s]Extractor Estimating: 170it [02:16,  1.25it/s]Extractor Estimating: 171it [02:17,  1.27it/s]Extractor Estimating: 172it [02:18,  1.15it/s]Extractor Estimating: 173it [02:19,  1.23it/s]Extractor Estimating: 174it [02:19,  1.25it/s]Extractor Estimating: 175it [02:20,  1.21it/s]Extractor Estimating: 176it [02:21,  1.20it/s]Extractor Estimating: 177it [02:22,  1.23it/s]Extractor Estimating: 178it [02:23,  1.25it/s]Extractor Estimating: 179it [02:23,  1.26it/s]Extractor Estimating: 180it [02:24,  1.26it/s]Extractor Estimating: 181it [02:25,  1.26it/s]Extractor Estimating: 182it [02:26,  1.21it/s]Extractor Estimating: 183it [02:27,  1.19it/s]Extractor Estimating: 184it [02:28,  1.19it/s]Extractor Estimating: 185it [02:28,  1.18it/s]Extractor Estimating: 186it [02:29,  1.22it/s]Extractor Estimating: 187it [02:30,  1.25it/s]Extractor Estimating: 188it [02:31,  1.23it/s]Extractor Estimating: 189it [02:32,  1.25it/s]Extractor Estimating: 190it [02:32,  1.21it/s]Extractor Estimating: 191it [02:33,  1.23it/s]Extractor Estimating: 192it [02:34,  1.24it/s]Extractor Estimating: 193it [02:35,  1.28it/s]Extractor Estimating: 194it [02:35,  1.29it/s]Extractor Estimating: 195it [02:36,  1.28it/s]Extractor Estimating: 196it [02:37,  1.27it/s]Extractor Estimating: 197it [02:38,  1.29it/s]Extractor Estimating: 198it [02:39,  1.28it/s]Extractor Estimating: 199it [02:39,  1.28it/s]Extractor Estimating: 200it [02:40,  1.28it/s]Extractor Estimating: 201it [02:41,  1.23it/s]Extractor Estimating: 202it [02:42,  1.23it/s]Extractor Estimating: 203it [02:43,  1.23it/s]Extractor Estimating: 204it [02:44,  1.21it/s]Extractor Estimating: 205it [02:44,  1.21it/s]Extractor Estimating: 206it [02:45,  1.21it/s]Extractor Estimating: 207it [02:46,  1.19it/s]Extractor Estimating: 208it [02:47,  1.20it/s]Extractor Estimating: 209it [02:48,  1.20it/s]Extractor Estimating: 210it [02:49,  1.19it/s]Extractor Estimating: 211it [02:49,  1.25it/s]Extractor Estimating: 212it [02:50,  1.27it/s]Extractor Estimating: 213it [02:51,  1.28it/s]Extractor Estimating: 214it [02:52,  1.25it/s]Extractor Estimating: 215it [02:53,  1.21it/s]Extractor Estimating: 216it [02:53,  1.19it/s]Extractor Estimating: 217it [02:54,  1.19it/s]Extractor Estimating: 218it [02:55,  1.14it/s]Extractor Estimating: 219it [02:56,  1.19it/s]Extractor Estimating: 220it [02:57,  1.18it/s]Extractor Estimating: 221it [02:58,  1.17it/s]Extractor Estimating: 222it [02:58,  1.20it/s]Extractor Estimating: 223it [02:59,  1.22it/s]Extractor Estimating: 224it [03:00,  1.24it/s]Extractor Estimating: 225it [03:01,  1.27it/s]Extractor Estimating: 226it [03:02,  1.25it/s]Extractor Estimating: 227it [03:02,  1.27it/s]Extractor Estimating: 228it [03:03,  1.31it/s]Extractor Estimating: 229it [03:04,  1.33it/s]Extractor Estimating: 230it [03:04,  1.36it/s]Extractor Estimating: 231it [03:05,  1.31it/s]Extractor Estimating: 232it [03:06,  1.33it/s]Extractor Estimating: 233it [03:07,  1.32it/s]Extractor Estimating: 234it [03:08,  1.30it/s]Extractor Estimating: 235it [03:08,  1.33it/s]Extractor Estimating: 236it [03:09,  1.32it/s]Extractor Estimating: 237it [03:10,  1.33it/s]Extractor Estimating: 238it [03:11,  1.32it/s]Extractor Estimating: 239it [03:11,  1.36it/s]Extractor Estimating: 240it [03:12,  1.32it/s]Extractor Estimating: 241it [03:13,  1.31it/s]Extractor Estimating: 242it [03:14,  1.30it/s]Extractor Estimating: 243it [03:14,  1.32it/s]Extractor Estimating: 244it [03:15,  1.37it/s]Extractor Estimating: 245it [03:16,  1.38it/s]Extractor Estimating: 246it [03:17,  1.34it/s]Extractor Estimating: 247it [03:17,  1.33it/s]Extractor Estimating: 248it [03:18,  1.34it/s]Extractor Estimating: 249it [03:19,  1.35it/s]Extractor Estimating: 250it [03:20,  1.25it/s]Extractor Estimating: 251it [03:21,  1.25it/s]Extractor Estimating: 252it [03:21,  1.25it/s]Extractor Estimating: 253it [03:22,  1.25it/s]Extractor Estimating: 254it [03:23,  1.20it/s]Extractor Estimating: 255it [03:24,  1.21it/s]Extractor Estimating: 256it [03:25,  1.24it/s]Extractor Estimating: 257it [03:25,  1.24it/s]Extractor Estimating: 258it [03:26,  1.19it/s]Extractor Estimating: 259it [03:27,  1.21it/s]Extractor Estimating: 260it [03:28,  1.21it/s]Extractor Estimating: 261it [03:29,  1.24it/s]Extractor Estimating: 262it [03:30,  1.18it/s]Extractor Estimating: 263it [03:30,  1.20it/s]Extractor Estimating: 264it [03:31,  1.25it/s]Extractor Estimating: 265it [03:32,  1.26it/s]Extractor Estimating: 266it [03:33,  1.27it/s]Extractor Estimating: 267it [03:33,  1.28it/s]Extractor Estimating: 268it [03:34,  1.24it/s]Extractor Estimating: 269it [03:35,  1.22it/s]Extractor Estimating: 270it [03:36,  1.22it/s]Extractor Estimating: 271it [03:37,  1.24it/s]Extractor Estimating: 272it [03:38,  1.24it/s]Extractor Estimating: 273it [03:38,  1.25it/s]Extractor Estimating: 274it [03:39,  1.26it/s]Extractor Estimating: 275it [03:40,  1.31it/s]Extractor Estimating: 276it [03:41,  1.32it/s]Extractor Estimating: 277it [03:41,  1.36it/s]Extractor Estimating: 278it [03:42,  1.34it/s]Extractor Estimating: 279it [03:43,  1.24it/s]Extractor Estimating: 280it [03:44,  1.22it/s]Extractor Estimating: 281it [03:45,  1.27it/s]Extractor Estimating: 282it [03:45,  1.30it/s]Extractor Estimating: 283it [03:46,  1.30it/s]Extractor Estimating: 284it [03:47,  1.30it/s]Extractor Estimating: 285it [03:48,  1.31it/s]Extractor Estimating: 286it [03:48,  1.27it/s]Extractor Estimating: 287it [03:49,  1.31it/s]Extractor Estimating: 288it [03:50,  1.30it/s]Extractor Estimating: 289it [03:51,  1.29it/s]Extractor Estimating: 290it [03:52,  1.27it/s]Extractor Estimating: 291it [03:52,  1.29it/s]Extractor Estimating: 292it [03:53,  1.28it/s]Extractor Estimating: 293it [03:54,  1.31it/s]Extractor Estimating: 294it [03:55,  1.29it/s]Extractor Estimating: 295it [03:55,  1.30it/s]Extractor Estimating: 296it [03:56,  1.27it/s]Extractor Estimating: 297it [03:57,  1.29it/s]Extractor Estimating: 298it [03:58,  1.33it/s]Extractor Estimating: 299it [03:58,  1.34it/s]Extractor Estimating: 300it [03:59,  1.36it/s]Extractor Estimating: 301it [04:00,  1.37it/s]Extractor Estimating: 302it [04:01,  1.36it/s]Extractor Estimating: 303it [04:01,  1.41it/s]Extractor Estimating: 304it [04:02,  1.36it/s]Extractor Estimating: 305it [04:03,  1.35it/s]Extractor Estimating: 306it [04:03,  1.36it/s]Extractor Estimating: 307it [04:04,  1.38it/s]Extractor Estimating: 308it [04:05,  1.38it/s]Extractor Estimating: 309it [04:06,  1.34it/s]Extractor Estimating: 310it [04:06,  1.37it/s]Extractor Estimating: 311it [04:07,  1.36it/s]Extractor Estimating: 312it [04:08,  1.28it/s]Extractor Estimating: 313it [04:09,  1.08it/s]Extractor Estimating: 314it [04:10,  1.19it/s]Extractor Estimating: 315it [04:11,  1.29it/s]Extractor Estimating: 316it [04:11,  1.25it/s]Extractor Estimating: 317it [04:12,  1.32it/s]Extractor Estimating: 318it [04:13,  1.35it/s]Extractor Estimating: 319it [04:13,  1.35it/s]Extractor Estimating: 320it [04:14,  1.32it/s]Extractor Estimating: 321it [04:15,  1.32it/s]Extractor Estimating: 322it [04:16,  1.32it/s]Extractor Estimating: 323it [04:16,  1.37it/s]Extractor Estimating: 324it [04:17,  1.35it/s]Extractor Estimating: 325it [04:18,  1.35it/s]Extractor Estimating: 326it [04:19,  1.32it/s]Extractor Estimating: 327it [04:20,  1.21it/s]Extractor Estimating: 328it [04:21,  1.19it/s]Extractor Estimating: 329it [04:21,  1.21it/s]Extractor Estimating: 330it [04:22,  1.25it/s]Extractor Estimating: 331it [04:23,  1.25it/s]Extractor Estimating: 332it [04:24,  1.22it/s]Extractor Estimating: 333it [04:25,  1.21it/s]Extractor Estimating: 334it [04:25,  1.23it/s]Extractor Estimating: 335it [04:26,  1.23it/s]Extractor Estimating: 336it [04:27,  1.20it/s]Extractor Estimating: 337it [04:28,  1.20it/s]Extractor Estimating: 338it [04:29,  1.27it/s]Extractor Estimating: 339it [04:30,  1.23it/s]Extractor Estimating: 340it [04:30,  1.24it/s]Extractor Estimating: 341it [04:31,  1.25it/s]Extractor Estimating: 342it [04:32,  1.25it/s]Extractor Estimating: 343it [04:33,  1.20it/s]Extractor Estimating: 344it [04:34,  1.23it/s]Extractor Estimating: 345it [04:34,  1.24it/s]Extractor Estimating: 346it [04:35,  1.22it/s]Extractor Estimating: 347it [04:36,  1.26it/s]Extractor Estimating: 348it [04:37,  1.26it/s]Extractor Estimating: 349it [04:38,  1.24it/s]Extractor Estimating: 350it [04:38,  1.27it/s]Extractor Estimating: 351it [04:39,  1.29it/s]Extractor Estimating: 352it [04:40,  1.23it/s]Extractor Estimating: 353it [04:41,  1.19it/s]Extractor Estimating: 354it [04:42,  1.24it/s]Extractor Estimating: 355it [04:42,  1.25it/s]Extractor Estimating: 356it [04:43,  1.25it/s]Extractor Estimating: 357it [04:44,  1.25it/s]Extractor Estimating: 358it [04:45,  1.26it/s]Extractor Estimating: 359it [04:45,  1.30it/s]Extractor Estimating: 360it [04:46,  1.27it/s]Extractor Estimating: 361it [04:47,  1.24it/s]Extractor Estimating: 362it [04:48,  1.25it/s]Extractor Estimating: 363it [04:49,  1.19it/s]Extractor Estimating: 364it [04:50,  1.20it/s]Extractor Estimating: 365it [04:51,  1.18it/s]Extractor Estimating: 366it [04:51,  1.23it/s]Extractor Estimating: 367it [04:52,  1.23it/s]Extractor Estimating: 368it [04:53,  1.23it/s]Extractor Estimating: 369it [04:54,  1.25it/s]Extractor Estimating: 370it [04:54,  1.27it/s]Extractor Estimating: 371it [04:55,  1.21it/s]Extractor Estimating: 372it [04:56,  1.25it/s]Extractor Estimating: 373it [04:57,  1.29it/s]Extractor Estimating: 374it [04:58,  1.29it/s]Extractor Estimating: 375it [04:58,  1.38it/s]Extractor Estimating: 375it [04:58,  1.26it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:39,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:39,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:39,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:39,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:39,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:18:40,640 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:18:40,641 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:18:41,223 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:18:42,307 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:18:42,307 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:45,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:45,697 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:45,697 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:45,697 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:18:45,697 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:18:46,394 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:18:46,395 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:18:46,979 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:18:47,149 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:18:47,149 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:09:58,021 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:09:58,027 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7490 mean pseudo reward: 0.922587211807108
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 25538
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25638, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25638, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.329, loss:693.2586
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.313, loss:656.6295
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.323, loss:676.9484
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.313, loss:652.7822
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.333, loss:643.4250
>> valid entity prec:0.5072, rec:0.4100, f1:0.4534
>> valid relation prec:0.0136, rec:0.0037, f1:0.0059
>> valid relation with NER prec:0.0136, rec:0.0037, f1:0.0059
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.984, loss:654.8705
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.309, loss:617.0685
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.305, loss:636.3942
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.337, loss:677.4758
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.318, loss:622.4509
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5057, rec:0.4646, f1:0.4843
>> valid relation prec:0.0263, rec:0.0095, f1:0.0139
>> valid relation with NER prec:0.0263, rec:0.0095, f1:0.0139
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.997, loss:656.2966
g_step 1200, step 261, avg_time 1.322, loss:673.0482
g_step 1300, step 48, avg_time 1.306, loss:628.6865
g_step 1400, step 148, avg_time 1.319, loss:612.5928
g_step 1500, step 248, avg_time 1.314, loss:627.2038
>> valid entity prec:0.4443, rec:0.4399, f1:0.4421
>> valid relation prec:0.0189, rec:0.0049, f1:0.0078
>> valid relation with NER prec:0.0189, rec:0.0049, f1:0.0078
g_step 1600, step 35, avg_time 2.980, loss:638.5059
g_step 1700, step 135, avg_time 1.323, loss:588.1292
g_step 1800, step 235, avg_time 1.325, loss:586.6013
g_step 1900, step 22, avg_time 1.328, loss:619.3295
g_step 2000, step 122, avg_time 1.301, loss:554.5609
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4790, rec:0.4447, f1:0.4612
>> valid relation prec:0.0184, rec:0.0060, f1:0.0091
>> valid relation with NER prec:0.0184, rec:0.0060, f1:0.0091
g_step 2100, step 222, avg_time 2.990, loss:575.9417
g_step 2200, step 9, avg_time 1.330, loss:590.1944
g_step 2300, step 109, avg_time 1.302, loss:531.2877
g_step 2400, step 209, avg_time 1.320, loss:552.1139
g_step 2500, step 309, avg_time 1.327, loss:563.0466
>> valid entity prec:0.4932, rec:0.4217, f1:0.4546
>> valid relation prec:0.0187, rec:0.0069, f1:0.0101
>> valid relation with NER prec:0.0187, rec:0.0069, f1:0.0101
g_step 2600, step 96, avg_time 2.975, loss:510.5155
g_step 2700, step 196, avg_time 1.310, loss:503.0169
g_step 2800, step 296, avg_time 1.342, loss:542.5706
g_step 2900, step 83, avg_time 1.311, loss:509.1936
g_step 3000, step 183, avg_time 1.311, loss:497.0080
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4786, rec:0.4669, f1:0.4727
>> valid relation prec:0.0314, rec:0.0115, f1:0.0168
>> valid relation with NER prec:0.0314, rec:0.0115, f1:0.0168
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 283, avg_time 2.997, loss:528.2203
g_step 3200, step 70, avg_time 1.319, loss:464.5216
g_step 3300, step 170, avg_time 1.329, loss:473.4858
g_step 3400, step 270, avg_time 1.315, loss:492.6544
g_step 3500, step 57, avg_time 1.317, loss:459.2089
>> valid entity prec:0.4693, rec:0.4686, f1:0.4689
>> valid relation prec:0.0298, rec:0.0103, f1:0.0154
>> valid relation with NER prec:0.0298, rec:0.0103, f1:0.0154
g_step 3600, step 157, avg_time 3.014, loss:456.1594
g_step 3700, step 257, avg_time 1.312, loss:480.9624
g_step 3800, step 44, avg_time 1.309, loss:448.3169
g_step 3900, step 144, avg_time 1.309, loss:451.7548
g_step 4000, step 244, avg_time 1.342, loss:454.2601
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4941, rec:0.4558, f1:0.4742
>> valid relation prec:0.0167, rec:0.0069, f1:0.0098
>> valid relation with NER prec:0.0167, rec:0.0069, f1:0.0098
g_step 4100, step 31, avg_time 2.973, loss:448.0176
g_step 4200, step 131, avg_time 1.336, loss:396.7490
g_step 4300, step 231, avg_time 1.311, loss:430.7223
g_step 4400, step 18, avg_time 1.321, loss:439.3388
g_step 4500, step 118, avg_time 1.344, loss:397.6747
>> valid entity prec:0.4772, rec:0.4332, f1:0.4542
>> valid relation prec:0.0279, rec:0.0103, f1:0.0151
>> valid relation with NER prec:0.0279, rec:0.0103, f1:0.0151
g_step 4600, step 218, avg_time 2.983, loss:431.9733
g_step 4700, step 5, avg_time 1.309, loss:408.9143
g_step 4800, step 105, avg_time 1.329, loss:391.4317
g_step 4900, step 205, avg_time 1.327, loss:401.5605
g_step 5000, step 305, avg_time 1.323, loss:430.1542
learning rate was adjusted to 0.0008
>> valid entity prec:0.4948, rec:0.4438, f1:0.4679
>> valid relation prec:0.0172, rec:0.0072, f1:0.0101
>> valid relation with NER prec:0.0172, rec:0.0072, f1:0.0101
g_step 5100, step 92, avg_time 2.998, loss:362.6076
g_step 5200, step 192, avg_time 1.314, loss:382.0651
g_step 5300, step 292, avg_time 1.314, loss:402.2449
g_step 5400, step 79, avg_time 1.324, loss:349.1930
g_step 5500, step 179, avg_time 1.330, loss:375.5497
>> valid entity prec:0.4882, rec:0.4442, f1:0.4652
>> valid relation prec:0.0231, rec:0.0098, f1:0.0137
>> valid relation with NER prec:0.0231, rec:0.0098, f1:0.0137
g_step 5600, step 279, avg_time 2.997, loss:388.6763
g_step 5700, step 66, avg_time 1.301, loss:352.2143
g_step 5800, step 166, avg_time 1.317, loss:360.7842
g_step 5900, step 266, avg_time 1.336, loss:385.7356
g_step 6000, step 53, avg_time 1.327, loss:346.8005
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4671, rec:0.4462, f1:0.4564
>> valid relation prec:0.0262, rec:0.0109, f1:0.0154
>> valid relation with NER prec:0.0262, rec:0.0109, f1:0.0154
g_step 6100, step 153, avg_time 2.986, loss:335.9561
g_step 6200, step 253, avg_time 1.315, loss:363.7811
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:09:58 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:09:58 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-09-58_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:09:59 - WARNING - datasets.builder -   Using custom data configuration default-b012f43907547589
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b012f43907547589/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:09:59,452 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:09:59,453 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:09:59,453 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:09:59,454 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:09:59,464 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:59,467 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:59,467 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:59,467 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:59,467 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:59,467 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:09:59,467 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:09:59,661 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:10:02,786 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:10:02,790 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b012f43907547589/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  2.94ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.71ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.00ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:00,  4.16ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.25ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.32ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.35ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.16ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.43ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  3.52ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  3.93ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.11ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.17ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.62ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  8.20ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  9.87ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  9.80ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00,  6.87ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00,  8.11ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.93ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  7.46ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  8.99ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.20ba/s]
[INFO|trainer.py:414] 2023-08-28 23:10:07,253 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:10:07,262 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:10:07,262 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 23:10:07,262 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:10:07,262 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:10:07,262 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:10:07,262 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:10:07,262 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:57,  3.28it/s]  0%|          | 2/585 [00:00<02:53,  3.37it/s]  1%|          | 3/585 [00:00<02:50,  3.40it/s]  1%|          | 4/585 [00:01<02:49,  3.42it/s]  1%|          | 5/585 [00:01<02:49,  3.43it/s]  1%|          | 6/585 [00:01<02:48,  3.43it/s]  1%|          | 7/585 [00:02<02:49,  3.41it/s]  1%|â–         | 8/585 [00:02<02:48,  3.43it/s]  2%|â–         | 9/585 [00:02<02:47,  3.43it/s]  2%|â–         | 10/585 [00:02<02:47,  3.43it/s]  2%|â–         | 11/585 [00:03<02:47,  3.44it/s]  2%|â–         | 12/585 [00:03<02:46,  3.44it/s]  2%|â–         | 13/585 [00:03<02:46,  3.45it/s]  2%|â–         | 14/585 [00:04<02:45,  3.45it/s]  3%|â–Ž         | 15/585 [00:04<02:45,  3.45it/s]  3%|â–Ž         | 16/585 [00:04<02:44,  3.45it/s]  3%|â–Ž         | 17/585 [00:04<02:44,  3.45it/s]  3%|â–Ž         | 18/585 [00:05<02:44,  3.45it/s]  3%|â–Ž         | 19/585 [00:05<02:43,  3.46it/s]  3%|â–Ž         | 20/585 [00:05<02:43,  3.45it/s]  4%|â–Ž         | 21/585 [00:06<02:43,  3.45it/s]  4%|â–         | 22/585 [00:06<02:43,  3.45it/s]  4%|â–         | 23/585 [00:06<02:42,  3.46it/s]  4%|â–         | 24/585 [00:06<02:42,  3.45it/s]  4%|â–         | 25/585 [00:07<02:42,  3.45it/s]  4%|â–         | 26/585 [00:07<02:41,  3.45it/s]  5%|â–         | 27/585 [00:07<02:41,  3.45it/s]  5%|â–         | 28/585 [00:08<02:41,  3.45it/s]  5%|â–         | 29/585 [00:08<02:41,  3.45it/s]  5%|â–Œ         | 30/585 [00:08<02:41,  3.45it/s]  5%|â–Œ         | 31/585 [00:09<02:40,  3.45it/s]  5%|â–Œ         | 32/585 [00:09<02:40,  3.45it/s]  6%|â–Œ         | 33/585 [00:09<02:39,  3.45it/s]  6%|â–Œ         | 34/585 [00:09<02:39,  3.45it/s]  6%|â–Œ         | 35/585 [00:10<02:39,  3.45it/s]  6%|â–Œ         | 36/585 [00:10<02:39,  3.45it/s]  6%|â–‹         | 37/585 [00:10<02:38,  3.45it/s]  6%|â–‹         | 38/585 [00:11<02:38,  3.45it/s]  7%|â–‹         | 39/585 [00:11<02:38,  3.45it/s]  7%|â–‹         | 40/585 [00:11<02:38,  3.44it/s]  7%|â–‹         | 41/585 [00:11<02:38,  3.44it/s]  7%|â–‹         | 42/585 [00:12<02:37,  3.44it/s]  7%|â–‹         | 43/585 [00:12<02:37,  3.44it/s]  8%|â–Š         | 44/585 [00:12<02:37,  3.44it/s]  8%|â–Š         | 45/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 46/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 47/585 [00:13<02:36,  3.45it/s]  8%|â–Š         | 48/585 [00:13<02:35,  3.45it/s]  8%|â–Š         | 49/585 [00:14<02:47,  3.20it/s]  9%|â–Š         | 50/585 [00:14<02:43,  3.27it/s]  9%|â–Š         | 51/585 [00:14<02:40,  3.33it/s]  9%|â–‰         | 52/585 [00:15<02:38,  3.36it/s]  9%|â–‰         | 53/585 [00:15<02:36,  3.39it/s]  9%|â–‰         | 54/585 [00:15<02:35,  3.41it/s]  9%|â–‰         | 55/585 [00:16<02:34,  3.42it/s] 10%|â–‰         | 56/585 [00:16<02:34,  3.43it/s] 10%|â–‰         | 57/585 [00:16<02:33,  3.43it/s] 10%|â–‰         | 58/585 [00:16<02:33,  3.44it/s] 10%|â–ˆ         | 59/585 [00:17<02:32,  3.44it/s] 10%|â–ˆ         | 60/585 [00:17<02:32,  3.44it/s] 10%|â–ˆ         | 61/585 [00:17<02:31,  3.45it/s] 11%|â–ˆ         | 62/585 [00:18<02:31,  3.45it/s] 11%|â–ˆ         | 63/585 [00:18<02:31,  3.45it/s] 11%|â–ˆ         | 64/585 [00:18<02:31,  3.45it/s] 11%|â–ˆ         | 65/585 [00:18<02:30,  3.45it/s] 11%|â–ˆâ–        | 66/585 [00:19<02:30,  3.45it/s] 11%|â–ˆâ–        | 67/585 [00:19<02:30,  3.45it/s] 12%|â–ˆâ–        | 68/585 [00:19<02:29,  3.45it/s] 12%|â–ˆâ–        | 69/585 [00:20<02:33,  3.36it/s] 12%|â–ˆâ–        | 70/585 [00:20<02:32,  3.39it/s] 12%|â–ˆâ–        | 71/585 [00:20<02:31,  3.40it/s] 12%|â–ˆâ–        | 72/585 [00:20<02:30,  3.42it/s] 12%|â–ˆâ–        | 73/585 [00:21<02:40,  3.19it/s] 13%|â–ˆâ–Ž        | 74/585 [00:21<02:36,  3.26it/s] 13%|â–ˆâ–Ž        | 75/585 [00:21<02:33,  3.32it/s] 13%|â–ˆâ–Ž        | 76/585 [00:22<02:31,  3.36it/s] 13%|â–ˆâ–Ž        | 77/585 [00:22<02:30,  3.39it/s] 13%|â–ˆâ–Ž        | 78/585 [00:22<02:28,  3.40it/s] 14%|â–ˆâ–Ž        | 79/585 [00:23<02:28,  3.42it/s] 14%|â–ˆâ–Ž        | 80/585 [00:23<02:27,  3.43it/s] 14%|â–ˆâ–        | 81/585 [00:23<02:26,  3.43it/s] 14%|â–ˆâ–        | 82/585 [00:23<02:26,  3.43it/s] 14%|â–ˆâ–        | 83/585 [00:24<02:25,  3.44it/s] 14%|â–ˆâ–        | 84/585 [00:24<02:25,  3.44it/s] 15%|â–ˆâ–        | 85/585 [00:24<02:25,  3.44it/s] 15%|â–ˆâ–        | 86/585 [00:25<02:25,  3.44it/s] 15%|â–ˆâ–        | 87/585 [00:25<02:24,  3.44it/s] 15%|â–ˆâ–Œ        | 88/585 [00:25<02:24,  3.44it/s] 15%|â–ˆâ–Œ        | 89/585 [00:26<02:24,  3.44it/s] 15%|â–ˆâ–Œ        | 90/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 91/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 92/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 93/585 [00:27<02:22,  3.44it/s] 16%|â–ˆâ–Œ        | 94/585 [00:27<02:22,  3.44it/s] 16%|â–ˆâ–Œ        | 95/585 [00:27<02:22,  3.44it/s] 16%|â–ˆâ–‹        | 96/585 [00:28<02:22,  3.44it/s] 17%|â–ˆâ–‹        | 97/585 [00:28<02:21,  3.44it/s] 17%|â–ˆâ–‹        | 98/585 [00:28<02:21,  3.44it/s] 17%|â–ˆâ–‹        | 99/585 [00:28<02:21,  3.44it/s] 17%|â–ˆâ–‹        | 100/585 [00:29<02:20,  3.44it/s] 17%|â–ˆâ–‹        | 101/585 [00:29<02:20,  3.44it/s] 17%|â–ˆâ–‹        | 102/585 [00:29<02:20,  3.44it/s] 18%|â–ˆâ–Š        | 103/585 [00:30<02:20,  3.44it/s] 18%|â–ˆâ–Š        | 104/585 [00:30<02:19,  3.44it/s] 18%|â–ˆâ–Š        | 105/585 [00:30<02:19,  3.44it/s] 18%|â–ˆâ–Š        | 106/585 [00:30<02:19,  3.44it/s] 18%|â–ˆâ–Š        | 107/585 [00:31<02:18,  3.44it/s] 18%|â–ˆâ–Š        | 108/585 [00:31<02:18,  3.44it/s] 19%|â–ˆâ–Š        | 109/585 [00:31<02:18,  3.44it/s] 19%|â–ˆâ–‰        | 110/585 [00:32<02:18,  3.44it/s] 19%|â–ˆâ–‰        | 111/585 [00:32<02:17,  3.44it/s] 19%|â–ˆâ–‰        | 112/585 [00:32<02:17,  3.44it/s] 19%|â–ˆâ–‰        | 113/585 [00:32<02:17,  3.44it/s] 19%|â–ˆâ–‰        | 114/585 [00:33<02:16,  3.44it/s] 20%|â–ˆâ–‰        | 115/585 [00:33<02:16,  3.44it/s] 20%|â–ˆâ–‰        | 116/585 [00:33<02:16,  3.44it/s] 20%|â–ˆâ–ˆ        | 117/585 [00:34<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 23:10:41,457 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:10:41,457 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:10:41,457 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.16it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.32it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.61it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.96it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.51it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.26it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.00it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.73it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.55it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.58it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.67it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.69it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.61it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.66it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.68it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.72it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.59it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.43it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.54it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.60it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.57it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.58it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.57it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.57it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.68it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.66it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.57it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.53it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.58it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.58it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.59it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.64it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.56it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.56it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.64it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.58it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.59it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.52it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.52it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.57it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.64it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.58it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.41it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.57it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.48it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.53it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.56it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.46it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.53it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.50it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.57it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.59it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.61it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.70it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.58it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.56it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.61it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.50it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.54it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.53it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.49it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.60it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.66it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.58it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.65it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.58it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.53it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.53it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.49it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.48it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.56it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.51it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.48it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.55it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.56it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.62it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.64it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.62it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.61it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.52it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.57it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.56it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.56it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.56it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.64it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.64it/s][A                                                 
                                                 [A 20%|â–ˆâ–ˆ        | 117/585 [00:43<02:16,  3.44it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.64it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:10:50,875 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 23:10:50,901 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:10:55,022 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:10:55,055 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:10:55,100 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 118/585 [00:55<51:47,  6.65s/it] 20%|â–ˆâ–ˆ        | 119/585 [00:55<36:51,  4.75s/it] 21%|â–ˆâ–ˆ        | 120/585 [00:56<26:25,  3.41s/it] 21%|â–ˆâ–ˆ        | 121/585 [00:56<19:07,  2.47s/it] 21%|â–ˆâ–ˆ        | 122/585 [00:56<14:02,  1.82s/it] 21%|â–ˆâ–ˆ        | 123/585 [00:57<10:28,  1.36s/it] 21%|â–ˆâ–ˆ        | 124/585 [00:57<07:59,  1.04s/it] 21%|â–ˆâ–ˆâ–       | 125/585 [00:57<06:14,  1.23it/s] 22%|â–ˆâ–ˆâ–       | 126/585 [00:57<05:01,  1.52it/s] 22%|â–ˆâ–ˆâ–       | 127/585 [00:58<04:10,  1.83it/s] 22%|â–ˆâ–ˆâ–       | 128/585 [00:58<03:34,  2.13it/s] 22%|â–ˆâ–ˆâ–       | 129/585 [00:58<03:09,  2.40it/s] 22%|â–ˆâ–ˆâ–       | 130/585 [00:59<02:52,  2.64it/s] 22%|â–ˆâ–ˆâ–       | 131/585 [00:59<02:39,  2.84it/s] 23%|â–ˆâ–ˆâ–Ž       | 132/585 [00:59<02:30,  3.00it/s] 23%|â–ˆâ–ˆâ–Ž       | 133/585 [01:00<02:30,  3.00it/s] 23%|â–ˆâ–ˆâ–Ž       | 134/585 [01:00<02:24,  3.12it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/585 [01:00<02:20,  3.21it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/585 [01:00<02:16,  3.28it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/585 [01:01<02:14,  3.33it/s] 24%|â–ˆâ–ˆâ–Ž       | 138/585 [01:01<02:13,  3.36it/s] 24%|â–ˆâ–ˆâ–       | 139/585 [01:01<02:11,  3.38it/s] 24%|â–ˆâ–ˆâ–       | 140/585 [01:02<02:10,  3.41it/s] 24%|â–ˆâ–ˆâ–       | 141/585 [01:02<02:09,  3.42it/s] 24%|â–ˆâ–ˆâ–       | 142/585 [01:02<02:09,  3.43it/s] 24%|â–ˆâ–ˆâ–       | 143/585 [01:02<02:08,  3.43it/s] 25%|â–ˆâ–ˆâ–       | 144/585 [01:03<02:21,  3.12it/s] 25%|â–ˆâ–ˆâ–       | 145/585 [01:03<02:17,  3.21it/s] 25%|â–ˆâ–ˆâ–       | 146/585 [01:03<02:13,  3.28it/s] 25%|â–ˆâ–ˆâ–Œ       | 147/585 [01:04<02:11,  3.33it/s] 25%|â–ˆâ–ˆâ–Œ       | 148/585 [01:04<02:09,  3.36it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/585 [01:04<02:08,  3.38it/s] 26%|â–ˆâ–ˆâ–Œ       | 150/585 [01:05<02:07,  3.40it/s] 26%|â–ˆâ–ˆâ–Œ       | 151/585 [01:05<02:07,  3.41it/s] 26%|â–ˆâ–ˆâ–Œ       | 152/585 [01:05<02:06,  3.42it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/585 [01:05<02:06,  3.43it/s] 26%|â–ˆâ–ˆâ–‹       | 154/585 [01:06<02:10,  3.31it/s] 26%|â–ˆâ–ˆâ–‹       | 155/585 [01:06<02:08,  3.35it/s] 27%|â–ˆâ–ˆâ–‹       | 156/585 [01:06<02:07,  3.37it/s] 27%|â–ˆâ–ˆâ–‹       | 157/585 [01:07<02:05,  3.40it/s] 27%|â–ˆâ–ˆâ–‹       | 158/585 [01:07<02:05,  3.41it/s] 27%|â–ˆâ–ˆâ–‹       | 159/585 [01:07<02:04,  3.42it/s] 27%|â–ˆâ–ˆâ–‹       | 160/585 [01:08<02:04,  3.43it/s] 28%|â–ˆâ–ˆâ–Š       | 161/585 [01:08<02:03,  3.43it/s] 28%|â–ˆâ–ˆâ–Š       | 162/585 [01:08<02:03,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 163/585 [01:08<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 164/585 [01:09<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 165/585 [01:09<02:03,  3.40it/s] 28%|â–ˆâ–ˆâ–Š       | 166/585 [01:09<02:02,  3.41it/s] 29%|â–ˆâ–ˆâ–Š       | 167/585 [01:10<02:02,  3.42it/s] 29%|â–ˆâ–ˆâ–Š       | 168/585 [01:10<02:01,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 169/585 [01:10<02:01,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 170/585 [01:10<02:00,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 171/585 [01:11<02:00,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 172/585 [01:11<02:00,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 173/585 [01:11<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 174/585 [01:12<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 175/585 [01:12<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 176/585 [01:12<01:59,  3.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 177/585 [01:12<01:58,  3.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 178/585 [01:13<01:58,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 179/585 [01:13<01:58,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 180/585 [01:13<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 181/585 [01:14<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 182/585 [01:14<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 183/585 [01:14<01:56,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 184/585 [01:15<01:56,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 185/585 [01:15<01:56,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 186/585 [01:15<01:55,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 187/585 [01:15<01:56,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 188/585 [01:16<01:55,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/585 [01:16<01:55,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/585 [01:16<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 191/585 [01:17<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 192/585 [01:17<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 193/585 [01:17<01:53,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/585 [01:17<01:53,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/585 [01:18<01:53,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/585 [01:18<01:53,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/585 [01:18<01:52,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 198/585 [01:19<01:55,  3.35it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 199/585 [01:19<01:54,  3.38it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 200/585 [01:19<01:53,  3.40it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/585 [01:19<01:52,  3.41it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 202/585 [01:20<01:52,  3.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 203/585 [01:20<01:51,  3.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 204/585 [01:20<01:51,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 205/585 [01:21<01:50,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 206/585 [01:21<02:02,  3.10it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 207/585 [01:21<01:58,  3.19it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 208/585 [01:22<01:56,  3.25it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/585 [01:22<01:53,  3.31it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/585 [01:22<01:52,  3.35it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/585 [01:22<01:50,  3.38it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/585 [01:23<01:49,  3.40it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 213/585 [01:23<01:48,  3.41it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 214/585 [01:23<01:48,  3.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 215/585 [01:24<01:47,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 216/585 [01:24<01:47,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 217/585 [01:24<01:46,  3.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 218/585 [01:25<01:46,  3.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/585 [01:25<01:46,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 220/585 [01:25<01:45,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 221/585 [01:25<01:45,  3.45it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 222/585 [01:26<01:45,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 223/585 [01:26<01:45,  3.45it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 224/585 [01:26<01:44,  3.45it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/585 [01:27<01:44,  3.45it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 226/585 [01:27<01:45,  3.40it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 227/585 [01:27<01:44,  3.42it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 228/585 [01:27<01:44,  3.42it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 229/585 [01:28<01:43,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 230/585 [01:28<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/585 [01:28<01:42,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 232/585 [01:29<01:42,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 233/585 [01:29<01:42,  3.45it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:29<01:41,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 23:11:36,988 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:11:36,988 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:11:36,988 >>   Batch size = 8
{'eval_loss': 0.958117663860321, 'eval_runtime': 9.3443, 'eval_samples_per_second': 372.313, 'eval_steps_per_second': 46.553, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.53it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.41it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.58it/s][A
  5%|â–Œ         | 23/435 [00:00<00:09, 42.17it/s][A
  6%|â–‹         | 28/435 [00:00<00:09, 43.51it/s][A
  8%|â–Š         | 33/435 [00:00<00:09, 44.48it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 45.22it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 45.69it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.01it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.28it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.40it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 46.41it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.40it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.45it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.50it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.55it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.59it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:02<00:07, 46.69it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.75it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.68it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.62it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.67it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.59it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.63it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.56it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.60it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.67it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.71it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.74it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.75it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.68it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.62it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.60it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.63it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.65it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.66it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.65it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.70it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.74it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.67it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.57it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.56it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.66it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.59it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.60it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.65it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.70it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.76it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.75it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.60it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.67it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.67it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.64it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.60it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.68it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.68it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.73it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.68it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.62it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 45.03it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 45.48it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 45.89it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.14it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.29it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.44it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.37it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.48it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.33it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.40it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.50it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.42it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.58it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.58it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.63it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.68it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.70it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.57it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.53it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.54it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.60it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.60it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.46it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.53it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.58it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.67it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.65it/s][A                                                 
                                                 [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:39<01:41,  3.44it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:11:46,400 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 23:11:46,525 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:11:49,942 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:11:49,962 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:11:49,969 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 235/585 [01:53<42:25,  7.27s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 236/585 [01:53<30:07,  5.18s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 237/585 [01:53<21:32,  3.71s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/585 [01:54<15:32,  2.69s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/585 [01:54<11:20,  1.97s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 240/585 [01:54<08:25,  1.46s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/585 [01:54<06:22,  1.11s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242/585 [01:55<04:56,  1.16it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243/585 [01:55<03:56,  1.44it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/585 [01:55<03:14,  1.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/585 [01:56<02:45,  2.05it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/585 [01:56<02:25,  2.33it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/585 [01:56<02:10,  2.58it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/585 [01:57<02:00,  2.79it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 249/585 [01:57<01:53,  2.96it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 250/585 [01:57<01:48,  3.10it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 251/585 [01:57<01:44,  3.20it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 252/585 [01:58<01:51,  2.99it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/585 [01:58<01:46,  3.12it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 254/585 [01:58<01:43,  3.21it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/585 [01:59<01:40,  3.28it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/585 [01:59<01:38,  3.33it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/585 [01:59<01:37,  3.37it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/585 [02:00<01:36,  3.39it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/585 [02:00<01:35,  3.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/585 [02:00<01:35,  3.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/585 [02:00<01:34,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 262/585 [02:01<01:33,  3.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/585 [02:01<01:34,  3.41it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 264/585 [02:01<01:33,  3.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 265/585 [02:02<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 266/585 [02:02<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 267/585 [02:02<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 268/585 [02:02<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/585 [02:03<01:31,  3.45it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 270/585 [02:03<01:31,  3.45it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 271/585 [02:03<01:31,  3.45it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 272/585 [02:04<01:30,  3.45it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 273/585 [02:04<01:30,  3.45it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 274/585 [02:04<01:30,  3.44it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 275/585 [02:04<01:30,  3.44it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 276/585 [02:05<01:29,  3.45it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/585 [02:05<01:29,  3.45it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 278/585 [02:05<01:29,  3.45it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 279/585 [02:06<01:28,  3.45it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 280/585 [02:06<01:28,  3.45it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 281/585 [02:06<01:28,  3.45it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 282/585 [02:06<01:27,  3.45it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/585 [02:07<01:27,  3.45it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 284/585 [02:07<01:27,  3.45it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/585 [02:07<01:27,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 286/585 [02:08<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 287/585 [02:08<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 288/585 [02:08<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 289/585 [02:09<01:25,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 290/585 [02:09<01:25,  3.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/585 [02:09<01:25,  3.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 292/585 [02:09<01:24,  3.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 293/585 [02:10<01:24,  3.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 294/585 [02:10<01:24,  3.45it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 295/585 [02:10<01:24,  3.45it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 296/585 [02:11<01:24,  3.42it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 297/585 [02:11<01:24,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 298/585 [02:11<01:23,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/585 [02:11<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/585 [02:12<01:23,  3.42it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/585 [02:12<01:22,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/585 [02:12<01:22,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/585 [02:13<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/585 [02:13<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/585 [02:13<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306/585 [02:13<01:20,  3.45it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/585 [02:14<01:21,  3.42it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 308/585 [02:14<01:20,  3.42it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 309/585 [02:14<01:20,  3.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 310/585 [02:15<01:19,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 311/585 [02:15<01:19,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 312/585 [02:15<01:19,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/585 [02:15<01:18,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 314/585 [02:16<01:18,  3.45it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 315/585 [02:16<01:18,  3.45it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/585 [02:16<01:18,  3.45it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/585 [02:17<01:17,  3.45it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/585 [02:17<01:18,  3.38it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/585 [02:17<01:18,  3.40it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/585 [02:18<01:17,  3.42it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/585 [02:18<01:17,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 322/585 [02:18<01:16,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 323/585 [02:18<01:16,  3.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 324/585 [02:19<01:15,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 325/585 [02:19<01:15,  3.45it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 326/585 [02:19<01:15,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 327/585 [02:20<01:14,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 328/585 [02:20<01:14,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/585 [02:20<01:14,  3.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 330/585 [02:20<01:14,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 331/585 [02:21<01:13,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 332/585 [02:21<01:14,  3.40it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 333/585 [02:21<01:13,  3.42it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 334/585 [02:22<01:13,  3.42it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/585 [02:22<01:12,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 336/585 [02:22<01:12,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 337/585 [02:22<01:12,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 338/585 [02:23<01:11,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 339/585 [02:23<01:11,  3.45it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 340/585 [02:23<01:11,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 341/585 [02:24<01:11,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 342/585 [02:24<01:10,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/585 [02:24<01:10,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 344/585 [02:25<01:09,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 345/585 [02:25<01:09,  3.45it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 346/585 [02:25<01:09,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 347/585 [02:25<01:09,  3.45it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 348/585 [02:26<01:08,  3.45it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 349/585 [02:26<01:08,  3.45it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 350/585 [02:26<01:08,  3.45it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:27<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 23:12:34,367 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:12:34,367 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:12:34,367 >>   Batch size = 8
{'eval_loss': 0.9644041657447815, 'eval_runtime': 9.3851, 'eval_samples_per_second': 370.695, 'eval_steps_per_second': 46.35, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.14it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.37it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.69it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.94it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.56it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.31it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.14it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.83it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.81it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.80it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.79it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.72it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.71it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.67it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.69it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.71it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.62it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.58it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.40it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.69it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.67it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.68it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.58it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.71it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.72it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.60it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.61it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.63it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.67it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.75it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.64it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.72it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.75it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.79it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.69it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.63it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.57it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.66it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.74it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.72it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.35it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.50it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.56it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.53it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.40it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.51it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.48it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.56it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.67it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.72it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.64it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.67it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.66it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.58it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.65it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.59it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.59it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.69it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.69it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.73it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.69it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.62it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.50it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.56it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.62it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.69it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.62it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.64it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 45.36it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 45.77it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.10it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.20it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.33it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.44it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.54it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.59it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.45it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.51it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.49it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.58it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.65it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.65it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.61it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.70it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.62it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.52it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.52it/s][A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:36<01:07,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:12:43,721 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 23:12:43,736 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:12:48,683 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:12:49,040 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:12:49,147 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 352/585 [02:56<35:35,  9.16s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 353/585 [02:57<25:08,  6.50s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 354/585 [02:57<17:51,  4.64s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 355/585 [02:57<12:46,  3.33s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 356/585 [02:58<09:14,  2.42s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/585 [02:58<06:46,  1.78s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/585 [02:58<05:02,  1.33s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/585 [02:58<03:50,  1.02s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/585 [02:59<03:00,  1.25it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/585 [02:59<02:25,  1.54it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362/585 [02:59<02:01,  1.84it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 363/585 [03:00<01:43,  2.14it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 364/585 [03:00<01:31,  2.41it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 365/585 [03:00<01:22,  2.65it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 366/585 [03:00<01:16,  2.85it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 367/585 [03:01<01:12,  3.00it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 368/585 [03:01<01:09,  3.12it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 369/585 [03:01<01:07,  3.21it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 370/585 [03:02<01:05,  3.27it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 371/585 [03:02<01:04,  3.32it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/585 [03:02<01:03,  3.33it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 373/585 [03:03<01:03,  3.36it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/585 [03:03<01:02,  3.39it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375/585 [03:03<01:01,  3.40it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376/585 [03:03<01:01,  3.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/585 [03:04<01:00,  3.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/585 [03:04<01:00,  3.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/585 [03:04<00:59,  3.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/585 [03:05<00:59,  3.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 381/585 [03:05<00:59,  3.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 382/585 [03:05<00:58,  3.45it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 383/585 [03:05<00:59,  3.42it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 384/585 [03:06<00:58,  3.43it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 385/585 [03:06<00:58,  3.43it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 386/585 [03:06<00:57,  3.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 387/585 [03:07<00:57,  3.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 388/585 [03:07<00:57,  3.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 389/585 [03:07<00:56,  3.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 390/585 [03:07<00:56,  3.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 391/585 [03:08<00:56,  3.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 392/585 [03:08<00:55,  3.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 393/585 [03:08<00:55,  3.45it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 394/585 [03:09<00:56,  3.40it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 395/585 [03:09<00:55,  3.42it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 396/585 [03:09<00:55,  3.42it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 397/585 [03:10<00:54,  3.43it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 398/585 [03:10<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 399/585 [03:10<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/585 [03:10<00:53,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 401/585 [03:11<00:53,  3.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/585 [03:11<00:53,  3.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 403/585 [03:11<00:52,  3.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 404/585 [03:12<00:52,  3.45it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 405/585 [03:12<00:53,  3.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 406/585 [03:12<00:52,  3.41it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 407/585 [03:12<00:51,  3.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 408/585 [03:13<00:51,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 409/585 [03:13<00:51,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 410/585 [03:13<00:50,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 411/585 [03:14<00:50,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 412/585 [03:14<00:50,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 413/585 [03:14<00:49,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 414/585 [03:14<00:49,  3.45it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 415/585 [03:15<00:49,  3.45it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 416/585 [03:15<00:50,  3.38it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417/585 [03:15<00:49,  3.40it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418/585 [03:16<00:48,  3.41it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 419/585 [03:16<00:48,  3.42it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 420/585 [03:16<00:48,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 421/585 [03:17<00:47,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 422/585 [03:17<00:47,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 423/585 [03:17<00:47,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/585 [03:17<00:46,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 425/585 [03:18<00:46,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 426/585 [03:18<00:46,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 427/585 [03:18<00:47,  3.32it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 428/585 [03:19<00:46,  3.35it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 429/585 [03:19<00:46,  3.38it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 430/585 [03:19<00:45,  3.40it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 431/585 [03:19<00:45,  3.41it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 432/585 [03:20<00:44,  3.42it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 433/585 [03:20<00:44,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 434/585 [03:20<00:44,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 435/585 [03:21<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 436/585 [03:21<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 437/585 [03:21<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 438/585 [03:21<00:42,  3.43it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 439/585 [03:22<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 440/585 [03:22<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 441/585 [03:22<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 442/585 [03:23<00:41,  3.45it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 443/585 [03:23<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 444/585 [03:23<00:40,  3.45it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 445/585 [03:24<00:40,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 446/585 [03:24<00:40,  3.45it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 447/585 [03:24<00:40,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 448/585 [03:24<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 449/585 [03:25<00:39,  3.40it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 450/585 [03:25<00:39,  3.42it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 451/585 [03:25<00:39,  3.42it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 452/585 [03:26<00:38,  3.43it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 453/585 [03:26<00:38,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 454/585 [03:26<00:38,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 455/585 [03:26<00:37,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 456/585 [03:27<00:37,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 457/585 [03:27<00:37,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 458/585 [03:27<00:36,  3.45it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 459/585 [03:28<00:36,  3.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 460/585 [03:28<00:36,  3.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 461/585 [03:28<00:35,  3.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 462/585 [03:28<00:35,  3.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 463/585 [03:29<00:35,  3.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 464/585 [03:29<00:35,  3.45it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 465/585 [03:29<00:34,  3.45it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 466/585 [03:30<00:34,  3.45it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 467/585 [03:30<00:34,  3.42it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:30<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 23:13:38,029 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:13:38,030 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:13:38,030 >>   Batch size = 8
{'eval_loss': 0.9716283082962036, 'eval_runtime': 9.3409, 'eval_samples_per_second': 372.448, 'eval_steps_per_second': 46.569, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.48it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.28it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.59it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.90it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.49it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.25it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.11it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.87it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.72it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.72it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.73it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.76it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.69it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.67it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.74it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.76it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.71it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.64it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.64it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.68it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.72it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.68it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.70it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.68it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.66it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.65it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.71it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.57it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.56it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.55it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.50it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.54it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.56it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.63it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.61it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.64it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.61it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.61it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.58it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.65it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.59it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.63it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.71it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.71it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.65it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.67it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.70it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.58it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.63it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.31it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:04, 40.06it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:04, 41.87it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 43.17it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 44.23it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 44.93it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 45.42it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 45.73it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.05it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.13it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.20it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.30it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.38it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.44it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.48it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.52it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.55it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.50it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.47it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.41it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.48it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.48it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.46it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.50it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.58it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.62it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.66it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.62it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.63it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 42.71it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 43.89it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 44.65it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 45.21it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:09<00:00, 45.69it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 45.93it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.16it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.38it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.38it/s][A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:40<00:34,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:13:47,715 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 23:13:48,275 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:13:55,516 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:13:55,555 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:13:55,568 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 469/585 [03:55<14:58,  7.74s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 470/585 [03:56<10:33,  5.51s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 471/585 [03:56<07:29,  3.94s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 472/585 [03:56<05:21,  2.85s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 473/585 [03:57<03:52,  2.08s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 474/585 [03:57<02:51,  1.54s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 475/585 [03:57<02:08,  1.17s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 476/585 [03:57<01:40,  1.08it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 477/585 [03:58<01:19,  1.36it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 478/585 [03:58<01:04,  1.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 479/585 [03:58<00:53,  1.97it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 480/585 [03:59<00:46,  2.26it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 481/585 [03:59<00:41,  2.52it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 482/585 [03:59<00:37,  2.74it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 483/585 [03:59<00:34,  2.92it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 484/585 [04:00<00:32,  3.06it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 485/585 [04:00<00:31,  3.17it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 486/585 [04:00<00:30,  3.25it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 487/585 [04:01<00:30,  3.26it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 488/585 [04:01<00:29,  3.32it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 489/585 [04:01<00:28,  3.35it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 490/585 [04:02<00:28,  3.38it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 491/585 [04:02<00:27,  3.40it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 492/585 [04:02<00:27,  3.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 493/585 [04:02<00:26,  3.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 494/585 [04:03<00:26,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 495/585 [04:03<00:26,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 496/585 [04:03<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 497/585 [04:04<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 498/585 [04:04<00:25,  3.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 499/585 [04:04<00:25,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [04:04<00:24,  3.43it/s]                                                  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [04:04<00:24,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 501/585 [04:05<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 502/585 [04:05<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 503/585 [04:05<00:23,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 504/585 [04:06<00:23,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 505/585 [04:06<00:23,  3.45it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 506/585 [04:06<00:22,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 507/585 [04:06<00:22,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 508/585 [04:07<00:22,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 509/585 [04:07<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 510/585 [04:07<00:21,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 511/585 [04:08<00:21,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 512/585 [04:08<00:21,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 513/585 [04:08<00:20,  3.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 514/585 [04:08<00:20,  3.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 515/585 [04:09<00:20,  3.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 516/585 [04:09<00:20,  3.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 517/585 [04:09<00:19,  3.45it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 518/585 [04:10<00:19,  3.45it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 519/585 [04:10<00:19,  3.45it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 520/585 [04:10<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 521/585 [04:11<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 522/585 [04:11<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 523/585 [04:11<00:18,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 524/585 [04:11<00:17,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 525/585 [04:12<00:17,  3.45it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 526/585 [04:12<00:17,  3.45it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 527/585 [04:12<00:16,  3.45it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 528/585 [04:13<00:16,  3.45it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 529/585 [04:13<00:16,  3.45it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 530/585 [04:13<00:15,  3.45it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 531/585 [04:13<00:15,  3.43it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 532/585 [04:14<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 533/585 [04:14<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 534/585 [04:14<00:14,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 535/585 [04:15<00:14,  3.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 536/585 [04:15<00:14,  3.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 537/585 [04:15<00:13,  3.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 538/585 [04:15<00:13,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 539/585 [04:16<00:13,  3.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 540/585 [04:16<00:13,  3.45it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 541/585 [04:16<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 542/585 [04:17<00:12,  3.43it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 543/585 [04:17<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 544/585 [04:17<00:11,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 545/585 [04:17<00:11,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 546/585 [04:18<00:11,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 547/585 [04:18<00:11,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 548/585 [04:18<00:10,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/585 [04:19<00:10,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 550/585 [04:19<00:10,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 551/585 [04:19<00:09,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 552/585 [04:20<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 553/585 [04:20<00:09,  3.35it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 554/585 [04:20<00:09,  3.38it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 555/585 [04:20<00:08,  3.40it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 556/585 [04:21<00:08,  3.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 557/585 [04:21<00:08,  3.12it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 558/585 [04:21<00:08,  3.21it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 559/585 [04:22<00:07,  3.28it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 560/585 [04:22<00:07,  3.33it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 561/585 [04:22<00:07,  3.36it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 562/585 [04:23<00:06,  3.39it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 563/585 [04:23<00:06,  3.39it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 564/585 [04:23<00:06,  3.40it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 565/585 [04:23<00:05,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 566/585 [04:24<00:05,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 567/585 [04:24<00:05,  3.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 568/585 [04:24<00:04,  3.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 569/585 [04:25<00:04,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 570/585 [04:25<00:04,  3.44it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 571/585 [04:25<00:04,  3.44it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 572/585 [04:25<00:03,  3.44it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 573/585 [04:26<00:03,  3.44it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 574/585 [04:26<00:03,  3.40it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 575/585 [04:26<00:02,  3.42it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 576/585 [04:27<00:02,  3.42it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 577/585 [04:27<00:02,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 578/585 [04:27<00:02,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 579/585 [04:27<00:01,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 580/585 [04:28<00:01,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 581/585 [04:28<00:01,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 582/585 [04:28<00:00,  3.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 583/585 [04:29<00:00,  3.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 584/585 [04:29<00:00,  3.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:29<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 23:14:37,008 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:14:37,008 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:14:37,008 >>   Batch size = 8
{'eval_loss': 0.9791812896728516, 'eval_runtime': 9.4266, 'eval_samples_per_second': 369.063, 'eval_steps_per_second': 46.146, 'epoch': 4.0}
{'loss': 0.7402, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.00it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.37it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.71it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.73it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.36it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.12it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.98it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.89it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.69it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.70it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.67it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.69it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.64it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.67it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.63it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.60it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.68it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.74it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.61it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.56it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.63it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.60it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.65it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.73it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.57it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.63it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.65it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.65it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.68it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.74it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.61it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.62it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.67it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.70it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.68it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.71it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.66it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.66it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.70it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.63it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.64it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.62it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.66it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.59it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.64it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.68it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.66it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.65it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.59it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.60it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.60it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.70it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.63it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.62it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.60it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.55it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.61it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.64it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.57it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.57it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.64it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.54it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.56it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.58it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.59it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.60it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.63it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.50it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.53it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.56it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.62it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.58it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.60it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.49it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.53it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.59it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.60it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.55it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.60it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.40it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.42it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.50it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.42it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.44it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.54it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.54it/s][A                                                 
                                                 [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:39<00:00,  3.44it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:14:46,354 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 23:14:46,373 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:14:50,147 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:14:50,248 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:14:50,277 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:14:57,956 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:14:57,964 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117 (score: 0.958117663860321).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:54<00:00,  3.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:54<00:00,  1.99it/s]
[INFO|trainer.py:1894] 2023-08-28 23:15:01,574 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 23:15:01,612 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:15:05,489 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:15:05,523 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:15:05,547 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:15:05,871 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:05,872 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:05,872 >>   train_loss               =     0.7356
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:05,872 >>   train_runtime            = 0:04:54.28
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:05,872 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:05,872 >>   train_samples_per_second =    127.426
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:05,872 >>   train_steps_per_second   =      1.988
{'eval_loss': 0.9810537099838257, 'eval_runtime': 9.3348, 'eval_samples_per_second': 372.69, 'eval_steps_per_second': 46.6, 'epoch': 5.0}
{'train_runtime': 294.2874, 'train_samples_per_second': 127.426, 'train_steps_per_second': 1.988, 'train_loss': 0.7356429466834435, 'epoch': 5.0}
08/28/2023 23:15:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:15:05,918 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:15:05,918 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-28 23:15:05,918 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 56.60it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 50.34it/s]  4%|â–         | 18/435 [00:00<00:08, 48.59it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 47.92it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.59it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.17it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.07it/s] 10%|â–‰         | 43/435 [00:00<00:08, 46.99it/s] 11%|â–ˆ         | 48/435 [00:01<00:08, 46.71it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.56it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.47it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.62it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.67it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.70it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.63it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.69it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.63it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.54it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.55it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.61it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.57it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.65it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.69it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.72it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.64it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.65it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.60it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.51it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.51it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.51it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.53it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.58it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.63it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.66it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.53it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.59it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.35it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.50it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.52it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.57it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.50it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.61it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.65it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.60it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.67it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.62it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.50it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.57it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.55it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.57it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.59it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.48it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.56it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.70it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.72it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.79it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.85it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.75it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.85it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.91it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.81it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.87it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.84it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.91it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.98it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.85it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.88it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.93it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.88it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.94it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.87it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.79it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.80it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.91it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.85it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.93it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.89it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.81it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.85it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.92it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.84it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.88it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.95it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.93it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.78it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:15:15,241 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:15,241 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:15,241 >>   eval_loss               =     0.9581
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:15,241 >>   eval_runtime            = 0:00:09.32
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:15,241 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:15,241 >>   eval_samples_per_second =    373.167
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:15,241 >>   eval_steps_per_second   =     46.659
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:15:15,241 >>   perplexity              =     2.6068
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:22,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:22,815 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:22,815 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:22,815 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:22,815 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:15:23,862 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:15:23,863 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:15:24,672 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:15:25,686 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:15:25,687 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:30,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:30,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:30,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:30,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:15:30,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:15:32,251 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:15:32,252 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:15:33,185 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:15:33,336 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:15:33,336 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.22it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.20it/s]Extractor Predicting: 4it [00:03,  1.19it/s]Extractor Predicting: 5it [00:04,  1.18it/s]Extractor Predicting: 6it [00:05,  1.18it/s]Extractor Predicting: 7it [00:05,  1.17it/s]Extractor Predicting: 8it [00:06,  1.19it/s]Extractor Predicting: 9it [00:07,  1.18it/s]Extractor Predicting: 10it [00:08,  1.22it/s]Extractor Predicting: 11it [00:09,  1.20it/s]Extractor Predicting: 12it [00:10,  1.18it/s]Extractor Predicting: 13it [00:10,  1.17it/s]Extractor Predicting: 14it [00:11,  1.16it/s]Extractor Predicting: 15it [00:12,  1.20it/s]Extractor Predicting: 16it [00:13,  1.18it/s]Extractor Predicting: 17it [00:14,  1.20it/s]Extractor Predicting: 18it [00:15,  1.24it/s]Extractor Predicting: 19it [00:15,  1.21it/s]Extractor Predicting: 20it [00:16,  1.21it/s]Extractor Predicting: 21it [00:17,  1.21it/s]Extractor Predicting: 22it [00:18,  1.23it/s]Extractor Predicting: 23it [00:19,  1.22it/s]Extractor Predicting: 24it [00:20,  1.20it/s]Extractor Predicting: 25it [00:20,  1.18it/s]Extractor Predicting: 26it [00:21,  1.17it/s]Extractor Predicting: 27it [00:22,  1.17it/s]Extractor Predicting: 28it [00:23,  1.20it/s]Extractor Predicting: 29it [00:24,  1.18it/s]Extractor Predicting: 30it [00:25,  1.16it/s]Extractor Predicting: 31it [00:26,  1.17it/s]Extractor Predicting: 32it [00:26,  1.17it/s]Extractor Predicting: 33it [00:27,  1.17it/s]Extractor Predicting: 34it [00:28,  1.20it/s]Extractor Predicting: 35it [00:29,  1.20it/s]Extractor Predicting: 36it [00:30,  1.21it/s]Extractor Predicting: 37it [00:31,  1.21it/s]Extractor Predicting: 38it [00:31,  1.22it/s]Extractor Predicting: 39it [00:32,  1.23it/s]Extractor Predicting: 40it [00:33,  1.21it/s]Extractor Predicting: 41it [00:34,  1.21it/s]Extractor Predicting: 42it [00:35,  1.21it/s]Extractor Predicting: 43it [00:35,  1.24it/s]Extractor Predicting: 44it [00:36,  1.24it/s]Extractor Predicting: 45it [00:37,  1.23it/s]Extractor Predicting: 46it [00:38,  1.22it/s]Extractor Predicting: 47it [00:39,  1.21it/s]Extractor Predicting: 48it [00:40,  1.16it/s]Extractor Predicting: 49it [00:40,  1.17it/s]Extractor Predicting: 50it [00:41,  1.19it/s]Extractor Predicting: 51it [00:42,  1.21it/s]Extractor Predicting: 52it [00:43,  1.21it/s]Extractor Predicting: 53it [00:44,  1.22it/s]Extractor Predicting: 54it [00:45,  1.21it/s]Extractor Predicting: 55it [00:45,  1.19it/s]Extractor Predicting: 56it [00:46,  1.21it/s]Extractor Predicting: 57it [00:47,  1.20it/s]Extractor Predicting: 58it [00:48,  1.20it/s]Extractor Predicting: 59it [00:49,  1.23it/s]Extractor Predicting: 60it [00:49,  1.26it/s]Extractor Predicting: 61it [00:50,  1.27it/s]Extractor Predicting: 62it [00:51,  1.25it/s]Extractor Predicting: 63it [00:52,  1.22it/s]Extractor Predicting: 64it [00:53,  1.23it/s]Extractor Predicting: 65it [00:53,  1.24it/s]Extractor Predicting: 66it [00:54,  1.24it/s]Extractor Predicting: 67it [00:55,  1.24it/s]Extractor Predicting: 68it [00:56,  1.24it/s]Extractor Predicting: 69it [00:57,  1.28it/s]Extractor Predicting: 70it [00:57,  1.27it/s]Extractor Predicting: 71it [00:58,  1.28it/s]Extractor Predicting: 72it [00:59,  1.25it/s]Extractor Predicting: 73it [01:00,  1.26it/s]Extractor Predicting: 74it [01:01,  1.25it/s]Extractor Predicting: 75it [01:01,  1.22it/s]Extractor Predicting: 76it [01:02,  1.21it/s]Extractor Predicting: 77it [01:03,  1.23it/s]Extractor Predicting: 78it [01:04,  1.25it/s]Extractor Predicting: 79it [01:05,  1.27it/s]Extractor Predicting: 80it [01:05,  1.26it/s]Extractor Predicting: 81it [01:06,  1.25it/s]Extractor Predicting: 82it [01:07,  1.24it/s]Extractor Predicting: 83it [01:08,  1.25it/s]Extractor Predicting: 84it [01:09,  1.26it/s]Extractor Predicting: 85it [01:09,  1.27it/s]Extractor Predicting: 86it [01:10,  1.27it/s]Extractor Predicting: 87it [01:11,  1.29it/s]Extractor Predicting: 88it [01:12,  1.28it/s]Extractor Predicting: 89it [01:13,  1.28it/s]Extractor Predicting: 90it [01:13,  1.29it/s]Extractor Predicting: 91it [01:14,  1.31it/s]Extractor Predicting: 92it [01:15,  1.34it/s]Extractor Predicting: 93it [01:16,  1.32it/s]Extractor Predicting: 94it [01:16,  1.30it/s]Extractor Predicting: 95it [01:17,  1.31it/s]Extractor Predicting: 96it [01:18,  1.30it/s]Extractor Predicting: 97it [01:19,  1.30it/s]Extractor Predicting: 98it [01:19,  1.29it/s]Extractor Predicting: 99it [01:20,  1.27it/s]Extractor Predicting: 100it [01:21,  1.24it/s]Extractor Predicting: 101it [01:22,  1.24it/s]Extractor Predicting: 102it [01:23,  1.30it/s]Extractor Predicting: 103it [01:23,  1.31it/s]Extractor Predicting: 104it [01:24,  1.30it/s]Extractor Predicting: 105it [01:25,  1.30it/s]Extractor Predicting: 106it [01:26,  1.29it/s]Extractor Predicting: 107it [01:26,  1.29it/s]Extractor Predicting: 108it [01:27,  1.28it/s]Extractor Predicting: 109it [01:28,  1.29it/s]Extractor Predicting: 110it [01:29,  1.30it/s]Extractor Predicting: 111it [01:30,  1.29it/s]Extractor Predicting: 112it [01:30,  1.30it/s]Extractor Predicting: 113it [01:31,  1.33it/s]Extractor Predicting: 114it [01:32,  1.34it/s]Extractor Predicting: 115it [01:32,  1.35it/s]Extractor Predicting: 116it [01:33,  1.30it/s]Extractor Predicting: 117it [01:34,  1.27it/s]Extractor Predicting: 118it [01:35,  1.28it/s]Extractor Predicting: 119it [01:36,  1.25it/s]Extractor Predicting: 120it [01:37,  1.26it/s]Extractor Predicting: 121it [01:37,  1.25it/s]Extractor Predicting: 122it [01:38,  1.23it/s]Extractor Predicting: 123it [01:39,  1.24it/s]Extractor Predicting: 124it [01:40,  1.25it/s]Extractor Predicting: 125it [01:41,  1.18it/s]Extractor Predicting: 126it [01:42,  1.18it/s]Extractor Predicting: 127it [01:42,  1.22it/s]Extractor Predicting: 128it [01:43,  1.23it/s]Extractor Predicting: 129it [01:44,  1.22it/s]Extractor Predicting: 130it [01:45,  1.22it/s]Extractor Predicting: 131it [01:46,  1.24it/s]Extractor Predicting: 132it [01:46,  1.23it/s]Extractor Predicting: 133it [01:47,  1.22it/s]Extractor Predicting: 134it [01:48,  1.21it/s]Extractor Predicting: 135it [01:49,  1.23it/s]Extractor Predicting: 136it [01:50,  1.21it/s]Extractor Predicting: 137it [01:51,  1.22it/s]Extractor Predicting: 138it [01:51,  1.20it/s]Extractor Predicting: 139it [01:52,  1.21it/s]Extractor Predicting: 140it [01:53,  1.20it/s]Extractor Predicting: 141it [01:54,  1.22it/s]Extractor Predicting: 142it [01:55,  1.21it/s]Extractor Predicting: 143it [01:55,  1.22it/s]Extractor Predicting: 144it [01:56,  1.50it/s]Extractor Predicting: 144it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:37,761 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:37,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:37,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:37,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:37,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:17:38,068 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:17:38,069 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:17:38,345 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:17:39,383 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:17:39,383 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:41,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:41,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:41,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:41,935 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:17:41,936 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:17:42,254 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:17:42,256 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:17:42,530 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:17:42,683 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:17:42,683 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.15421686746987953,
  "recall": 0.018396090830698476,
  "score": 0.03287108371854134,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.24it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.26it/s]Extractor Predicting: 5it [00:03,  1.28it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.34it/s]Extractor Predicting: 9it [00:06,  1.31it/s]Extractor Predicting: 10it [00:07,  1.31it/s]Extractor Predicting: 11it [00:08,  1.32it/s]Extractor Predicting: 12it [00:09,  1.30it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:13,  1.28it/s]Extractor Predicting: 18it [00:14,  1.24it/s]Extractor Predicting: 19it [00:14,  1.28it/s]Extractor Predicting: 20it [00:15,  1.26it/s]Extractor Predicting: 21it [00:16,  1.28it/s]Extractor Predicting: 22it [00:17,  1.30it/s]Extractor Predicting: 23it [00:17,  1.30it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.29it/s]Extractor Predicting: 27it [00:20,  1.28it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.28it/s]Extractor Predicting: 30it [00:23,  1.30it/s]Extractor Predicting: 31it [00:24,  1.28it/s]Extractor Predicting: 32it [00:24,  1.31it/s]Extractor Predicting: 33it [00:25,  1.30it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:27,  1.28it/s]Extractor Predicting: 36it [00:27,  1.30it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:29,  1.33it/s]Extractor Predicting: 39it [00:30,  1.31it/s]Extractor Predicting: 40it [00:30,  1.29it/s]Extractor Predicting: 41it [00:31,  1.29it/s]Extractor Predicting: 42it [00:32,  1.32it/s]Extractor Predicting: 43it [00:33,  1.29it/s]Extractor Predicting: 44it [00:34,  1.28it/s]Extractor Predicting: 45it [00:34,  1.28it/s]Extractor Predicting: 46it [00:35,  1.26it/s]Extractor Predicting: 47it [00:36,  1.28it/s]Extractor Predicting: 48it [00:37,  1.27it/s]Extractor Predicting: 49it [00:37,  1.30it/s]Extractor Predicting: 50it [00:38,  1.29it/s]Extractor Predicting: 51it [00:39,  1.28it/s]Extractor Predicting: 52it [00:40,  1.26it/s]Extractor Predicting: 53it [00:41,  1.27it/s]Extractor Predicting: 54it [00:41,  1.26it/s]Extractor Predicting: 55it [00:42,  1.29it/s]Extractor Predicting: 56it [00:43,  1.28it/s]Extractor Predicting: 57it [00:44,  1.26it/s]Extractor Predicting: 58it [00:45,  1.26it/s]Extractor Predicting: 59it [00:45,  1.26it/s]Extractor Predicting: 60it [00:46,  1.25it/s]Extractor Predicting: 61it [00:47,  1.24it/s]Extractor Predicting: 62it [00:48,  1.27it/s]Extractor Predicting: 63it [00:49,  1.27it/s]Extractor Predicting: 64it [00:49,  1.29it/s]Extractor Predicting: 65it [00:50,  1.27it/s]Extractor Predicting: 66it [00:51,  1.24it/s]Extractor Predicting: 67it [00:52,  1.25it/s]Extractor Predicting: 68it [00:53,  1.24it/s]Extractor Predicting: 69it [00:53,  1.23it/s]Extractor Predicting: 70it [00:54,  1.24it/s]Extractor Predicting: 71it [00:55,  1.24it/s]Extractor Predicting: 72it [00:56,  1.22it/s]Extractor Predicting: 73it [00:57,  1.24it/s]Extractor Predicting: 74it [00:57,  1.24it/s]Extractor Predicting: 75it [00:58,  1.26it/s]Extractor Predicting: 76it [00:59,  1.24it/s]Extractor Predicting: 77it [01:00,  1.28it/s]Extractor Predicting: 78it [01:01,  1.27it/s]Extractor Predicting: 79it [01:01,  1.30it/s]Extractor Predicting: 80it [01:02,  1.32it/s]Extractor Predicting: 81it [01:03,  1.31it/s]Extractor Predicting: 82it [01:04,  1.30it/s]Extractor Predicting: 83it [01:04,  1.28it/s]Extractor Predicting: 84it [01:05,  1.27it/s]Extractor Predicting: 85it [01:06,  1.24it/s]Extractor Predicting: 86it [01:07,  1.21it/s]Extractor Predicting: 87it [01:08,  1.22it/s]Extractor Predicting: 88it [01:09,  1.22it/s]Extractor Predicting: 89it [01:09,  1.20it/s]Extractor Predicting: 90it [01:10,  1.19it/s]Extractor Predicting: 91it [01:11,  1.18it/s]Extractor Predicting: 92it [01:12,  1.21it/s]Extractor Predicting: 93it [01:13,  1.22it/s]Extractor Predicting: 94it [01:14,  1.23it/s]Extractor Predicting: 95it [01:14,  1.24it/s]Extractor Predicting: 96it [01:15,  1.21it/s]Extractor Predicting: 97it [01:16,  1.20it/s]Extractor Predicting: 98it [01:17,  1.19it/s]Extractor Predicting: 99it [01:18,  1.21it/s]Extractor Predicting: 100it [01:18,  1.22it/s]Extractor Predicting: 101it [01:19,  1.25it/s]Extractor Predicting: 102it [01:20,  1.22it/s]Extractor Predicting: 103it [01:21,  1.20it/s]Extractor Predicting: 104it [01:22,  1.23it/s]Extractor Predicting: 105it [01:23,  1.22it/s]Extractor Predicting: 106it [01:23,  1.22it/s]Extractor Predicting: 107it [01:24,  1.24it/s]Extractor Predicting: 108it [01:25,  1.23it/s]Extractor Predicting: 109it [01:26,  1.23it/s]Extractor Predicting: 110it [01:27,  1.21it/s]Extractor Predicting: 111it [01:27,  1.23it/s]Extractor Predicting: 112it [01:28,  1.22it/s]Extractor Predicting: 113it [01:29,  1.23it/s]Extractor Predicting: 114it [01:30,  1.24it/s]Extractor Predicting: 115it [01:31,  1.20it/s]Extractor Predicting: 116it [01:32,  1.22it/s]Extractor Predicting: 117it [01:32,  1.22it/s]Extractor Predicting: 118it [01:33,  1.21it/s]Extractor Predicting: 119it [01:34,  1.22it/s]Extractor Predicting: 120it [01:35,  1.24it/s]Extractor Predicting: 121it [01:36,  1.25it/s]Extractor Predicting: 122it [01:36,  1.26it/s]Extractor Predicting: 123it [01:37,  1.25it/s]Extractor Predicting: 124it [01:38,  1.27it/s]Extractor Predicting: 125it [01:39,  1.29it/s]Extractor Predicting: 126it [01:39,  1.28it/s]Extractor Predicting: 127it [01:40,  1.27it/s]Extractor Predicting: 128it [01:41,  1.18it/s]Extractor Predicting: 129it [01:42,  1.22it/s]Extractor Predicting: 130it [01:43,  1.22it/s]Extractor Predicting: 131it [01:44,  1.27it/s]Extractor Predicting: 132it [01:44,  1.28it/s]Extractor Predicting: 133it [01:45,  1.31it/s]Extractor Predicting: 134it [01:46,  1.27it/s]Extractor Predicting: 135it [01:47,  1.30it/s]Extractor Predicting: 136it [01:47,  1.30it/s]Extractor Predicting: 137it [01:48,  1.32it/s]Extractor Predicting: 138it [01:49,  1.30it/s]Extractor Predicting: 139it [01:50,  1.29it/s]Extractor Predicting: 140it [01:50,  1.30it/s]Extractor Predicting: 141it [01:51,  1.27it/s]Extractor Predicting: 142it [01:52,  1.27it/s]Extractor Predicting: 143it [01:53,  1.28it/s]Extractor Predicting: 144it [01:54,  1.27it/s]Extractor Predicting: 145it [01:54,  1.31it/s]Extractor Predicting: 146it [01:55,  1.33it/s]Extractor Predicting: 147it [01:56,  1.33it/s]Extractor Predicting: 148it [01:56,  1.36it/s]Extractor Predicting: 149it [01:57,  1.34it/s]Extractor Predicting: 150it [01:58,  1.36it/s]Extractor Predicting: 151it [01:59,  1.36it/s]Extractor Predicting: 152it [01:59,  1.38it/s]Extractor Predicting: 153it [02:00,  1.36it/s]Extractor Predicting: 154it [02:01,  1.36it/s]Extractor Predicting: 155it [02:02,  1.39it/s]Extractor Predicting: 156it [02:02,  1.38it/s]Extractor Predicting: 157it [02:03,  1.44it/s]Extractor Predicting: 158it [02:04,  1.45it/s]Extractor Predicting: 159it [02:04,  1.42it/s]Extractor Predicting: 160it [02:05,  1.37it/s]Extractor Predicting: 161it [02:06,  1.36it/s]Extractor Predicting: 162it [02:07,  1.37it/s]Extractor Predicting: 163it [02:07,  1.40it/s]Extractor Predicting: 164it [02:08,  1.40it/s]Extractor Predicting: 165it [02:09,  1.40it/s]Extractor Predicting: 166it [02:09,  1.37it/s]Extractor Predicting: 167it [02:10,  1.39it/s]Extractor Predicting: 168it [02:11,  1.37it/s]Extractor Predicting: 169it [02:12,  1.42it/s]Extractor Predicting: 170it [02:12,  1.41it/s]Extractor Predicting: 171it [02:13,  1.40it/s]Extractor Predicting: 172it [02:14,  1.34it/s]Extractor Predicting: 173it [02:15,  1.32it/s]Extractor Predicting: 174it [02:15,  1.29it/s]Extractor Predicting: 175it [02:16,  1.24it/s]Extractor Predicting: 176it [02:17,  1.25it/s]Extractor Predicting: 177it [02:18,  1.25it/s]Extractor Predicting: 178it [02:19,  1.23it/s]Extractor Predicting: 179it [02:20,  1.23it/s]Extractor Predicting: 180it [02:20,  1.24it/s]Extractor Predicting: 181it [02:21,  1.24it/s]Extractor Predicting: 182it [02:22,  1.23it/s]Extractor Predicting: 183it [02:23,  1.23it/s]Extractor Predicting: 184it [02:24,  1.22it/s]Extractor Predicting: 185it [02:24,  1.21it/s]Extractor Predicting: 186it [02:25,  1.21it/s]Extractor Predicting: 187it [02:26,  1.21it/s]Extractor Predicting: 188it [02:27,  1.22it/s]Extractor Predicting: 189it [02:28,  1.22it/s]Extractor Predicting: 190it [02:29,  1.23it/s]Extractor Predicting: 191it [02:29,  1.20it/s]Extractor Predicting: 192it [02:30,  1.19it/s]Extractor Predicting: 193it [02:31,  1.20it/s]Extractor Predicting: 194it [02:32,  1.20it/s]Extractor Predicting: 195it [02:33,  1.20it/s]Extractor Predicting: 196it [02:34,  1.21it/s]Extractor Predicting: 197it [02:34,  1.21it/s]Extractor Predicting: 198it [02:35,  1.23it/s]Extractor Predicting: 199it [02:36,  1.24it/s]Extractor Predicting: 200it [02:37,  1.22it/s]Extractor Predicting: 201it [02:38,  1.20it/s]Extractor Predicting: 202it [02:38,  1.26it/s]Extractor Predicting: 203it [02:39,  1.24it/s]Extractor Predicting: 204it [02:40,  1.26it/s]Extractor Predicting: 205it [02:41,  1.25it/s]Extractor Predicting: 206it [02:42,  1.24it/s]Extractor Predicting: 207it [02:42,  1.23it/s]Extractor Predicting: 208it [02:43,  1.23it/s]Extractor Predicting: 209it [02:44,  1.19it/s]Extractor Predicting: 210it [02:45,  1.20it/s]Extractor Predicting: 211it [02:46,  1.19it/s]Extractor Predicting: 212it [02:47,  1.20it/s]Extractor Predicting: 213it [02:48,  1.20it/s]Extractor Predicting: 214it [02:48,  1.19it/s]Extractor Predicting: 215it [02:49,  1.21it/s]Extractor Predicting: 216it [02:50,  1.19it/s]Extractor Predicting: 217it [02:51,  1.20it/s]Extractor Predicting: 218it [02:52,  1.21it/s]Extractor Predicting: 219it [02:53,  1.20it/s]Extractor Predicting: 220it [02:53,  1.21it/s]Extractor Predicting: 221it [02:54,  1.18it/s]Extractor Predicting: 222it [02:55,  1.18it/s]Extractor Predicting: 223it [02:56,  1.20it/s]Extractor Predicting: 224it [02:57,  1.21it/s]Extractor Predicting: 225it [02:58,  1.12it/s]Extractor Predicting: 226it [02:59,  1.14it/s]Extractor Predicting: 227it [02:59,  1.19it/s]Extractor Predicting: 228it [03:00,  1.19it/s]Extractor Predicting: 229it [03:01,  1.20it/s]Extractor Predicting: 230it [03:02,  1.25it/s]Extractor Predicting: 231it [03:02,  1.26it/s]Extractor Predicting: 232it [03:03,  1.28it/s]Extractor Predicting: 233it [03:04,  1.26it/s]Extractor Predicting: 234it [03:05,  1.27it/s]Extractor Predicting: 235it [03:06,  1.26it/s]Extractor Predicting: 236it [03:06,  1.24it/s]Extractor Predicting: 237it [03:07,  1.22it/s]Extractor Predicting: 238it [03:08,  1.23it/s]Extractor Predicting: 239it [03:09,  1.22it/s]Extractor Predicting: 240it [03:10,  1.23it/s]Extractor Predicting: 241it [03:11,  1.17it/s]Extractor Predicting: 242it [03:11,  1.20it/s]Extractor Predicting: 243it [03:12,  1.20it/s]Extractor Predicting: 244it [03:13,  1.21it/s]Extractor Predicting: 245it [03:14,  1.24it/s]Extractor Predicting: 246it [03:15,  1.23it/s]Extractor Predicting: 247it [03:16,  1.24it/s]Extractor Predicting: 248it [03:16,  1.24it/s]Extractor Predicting: 249it [03:17,  1.22it/s]Extractor Predicting: 250it [03:18,  1.24it/s]Extractor Predicting: 251it [03:19,  1.25it/s]Extractor Predicting: 252it [03:20,  1.25it/s]Extractor Predicting: 253it [03:20,  1.23it/s]Extractor Predicting: 254it [03:21,  1.25it/s]Extractor Predicting: 255it [03:22,  1.21it/s]Extractor Predicting: 256it [03:23,  1.18it/s]Extractor Predicting: 257it [03:24,  1.18it/s]Extractor Predicting: 258it [03:25,  1.18it/s]Extractor Predicting: 259it [03:25,  1.20it/s]Extractor Predicting: 260it [03:26,  1.19it/s]Extractor Predicting: 261it [03:27,  1.21it/s]Extractor Predicting: 262it [03:28,  1.20it/s]Extractor Predicting: 263it [03:29,  1.19it/s]Extractor Predicting: 264it [03:30,  1.19it/s]Extractor Predicting: 265it [03:31,  1.15it/s]Extractor Predicting: 266it [03:31,  1.15it/s]Extractor Predicting: 267it [03:32,  1.16it/s]Extractor Predicting: 268it [03:33,  1.15it/s]Extractor Predicting: 269it [03:34,  1.18it/s]Extractor Predicting: 270it [03:35,  1.19it/s]Extractor Predicting: 271it [03:36,  1.17it/s]Extractor Predicting: 272it [03:37,  1.18it/s]Extractor Predicting: 273it [03:37,  1.19it/s]Extractor Predicting: 274it [03:38,  1.21it/s]Extractor Predicting: 275it [03:39,  1.22it/s]Extractor Predicting: 276it [03:40,  1.25it/s]Extractor Predicting: 277it [03:40,  1.25it/s]Extractor Predicting: 278it [03:41,  1.24it/s]Extractor Predicting: 279it [03:42,  1.24it/s]Extractor Predicting: 280it [03:43,  1.20it/s]Extractor Predicting: 281it [03:44,  1.20it/s]Extractor Predicting: 282it [03:45,  1.28it/s]Extractor Predicting: 282it [03:45,  1.25it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:35,145 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:35,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:35,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:35,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:35,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:21:35,560 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:21:35,561 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:21:35,821 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:21:36,876 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:21:36,876 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:39,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:39,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:39,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:39,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:21:39,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:21:39,367 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:21:39,368 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:21:39,632 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:21:39,776 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:21:39,777 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3462177888611804,
  "recall": 0.12322485207100592,
  "score": 0.1817586733580624,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.21it/s]Extractor Predicting: 2it [00:01,  1.20it/s]Extractor Predicting: 3it [00:02,  1.19it/s]Extractor Predicting: 4it [00:03,  1.19it/s]Extractor Predicting: 5it [00:04,  1.23it/s]Extractor Predicting: 5it [00:04,  1.21it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:21:44,495 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:21:44,495 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:21:44,521 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:21:44,522 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:21:44,535 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:21:48,769 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:21:48,780 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:21:48,825 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:21:48,825 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_1/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:21:48,840 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:48,862 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:48,863 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:48,863 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:48,863 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:48,863 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:21:48,863 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.375,
  "recall": 0.05,
  "score": 0.08823529411764708,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:21:49,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:50,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:51,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:52,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:53,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:54,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:54,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:56,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:57,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:21:59,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:00,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:01,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:02,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:03,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:04,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:05,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:06,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:07,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:08,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:09,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:10,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:11,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:12,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:13,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:14,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:15,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:26<06:15, 26.81s/it][WARNING|generation_utils.py:914] 2023-08-28 23:22:15,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:16,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:17,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:18,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:19,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:20,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:21,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:22,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:23,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:24,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:25,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:26,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:27,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:28,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:29,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:30,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:31,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:32,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:33,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:34,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:34,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:35,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:47<05:03, 23.31s/it][WARNING|generation_utils.py:914] 2023-08-28 23:22:36,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:37,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:38,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:39,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:40,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:42,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:43,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:43,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:44,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:45,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:46,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:47,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:48,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:50,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:51,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:52,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:53,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:53,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:54,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:55,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:56,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:57,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:22:58,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:10<04:38, 23.19s/it][WARNING|generation_utils.py:914] 2023-08-28 23:22:59,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:00,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:01,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:02,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:03,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:04,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:05,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:06,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:07,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:08,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:09,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:10,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:11,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:12,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:13,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:14,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:15,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:16,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:17,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:18,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:19,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:20,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:21,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:33<04:14, 23.13s/it][WARNING|generation_utils.py:914] 2023-08-28 23:23:22,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:23,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:24,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:25,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:26,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:27,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:29,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:30,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:31,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:32,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:32,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:33,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:34,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:35,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:36,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:37,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:38,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:39,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:40,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:42,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:43,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:44,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [01:56<03:48, 22.88s/it][WARNING|generation_utils.py:914] 2023-08-28 23:23:45,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:46,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:47,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:48,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:49,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:50,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:51,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:52,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:52,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:53,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:54,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:55,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:56,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:58,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:59,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:23:59,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:00,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:02,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:02,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:03,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:04,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:05,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:06,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:18<03:25, 22.81s/it][WARNING|generation_utils.py:914] 2023-08-28 23:24:07,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:08,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:09,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:10,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:11,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:12,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:13,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:14,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:15,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:15,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:16,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:17,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:18,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:19,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:20,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:21,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:22,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:23,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:24,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:24,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:25,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:37<02:52, 21.55s/it][WARNING|generation_utils.py:914] 2023-08-28 23:24:26,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:28,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:29,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:30,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:31,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:32,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:33,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:34,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:35,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:36,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:37,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:38,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:39,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:40,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:41,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:43,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:44,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:45,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:45,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:46,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:47,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:48,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [03:00<02:34, 22.01s/it][WARNING|generation_utils.py:914] 2023-08-28 23:24:49,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:51,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:52,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:53,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:54,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:55,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:56,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:57,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:58,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:58,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:24:59,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:00,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:01,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:02,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:03,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:04,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:05,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:06,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:07,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:08,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:09,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:21<02:08, 21.48s/it][WARNING|generation_utils.py:914] 2023-08-28 23:25:10,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:11,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:12,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:13,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:14,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:15,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:16,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:17,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:18,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:19,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:21,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:21,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:23,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:23,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:24,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:25,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:26,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:27,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:28,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:29,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:30,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [03:42<01:47, 21.45s/it][WARNING|generation_utils.py:914] 2023-08-28 23:25:31,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:32,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:33,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:34,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:35,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:36,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:37,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:38,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:39,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:40,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:41,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:42,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:43,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:44,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:45,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:46,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:47,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:48,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:49,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:50,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:51,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:52,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [04:04<01:26, 21.66s/it][WARNING|generation_utils.py:914] 2023-08-28 23:25:53,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:54,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:55,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:56,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:57,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:58,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:25:59,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:00,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:01,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:02,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:03,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:04,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:05,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:06,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:07,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:08,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:09,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:10,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:11,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:11,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:12,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:14,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:26<01:04, 21.56s/it][WARNING|generation_utils.py:914] 2023-08-28 23:26:15,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:15,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:16,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:17,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:18,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:19,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:20,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:21,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:22,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:23,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:24,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:25,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:26,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:27,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:28,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:29,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:29,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:30,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:31,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:32,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:33,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:34,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [04:46<00:42, 21.14s/it][WARNING|generation_utils.py:914] 2023-08-28 23:26:35,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:36,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:37,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:38,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:39,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:40,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:41,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:42,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:43,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:44,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:45,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:46,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:47,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:47,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:48,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:49,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:50,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:51,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:52,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:53,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:54,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:55,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [05:07<00:21, 21.10s/it][WARNING|generation_utils.py:914] 2023-08-28 23:26:56,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:57,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:58,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:26:59,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:00,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:01,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:02,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:04,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:05,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:06,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:07,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:08,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:09,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:10,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:11,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:12,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:13,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:14,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:15,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:17,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:18,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:19,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:21,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:22,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:27:23,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:35<00:00, 23.24s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [05:35<00:00, 22.36s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:30,088 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:30,094 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:30,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:30,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:30,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:27:30,399 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:27:30,400 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:27:30,665 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:27:31,750 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:27:31,750 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:33,912 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:33,915 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:33,915 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:33,915 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:27:33,915 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:27:34,248 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:27:34,250 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:27:34,516 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:27:34,680 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:27:34,680 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 140, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 308, 'raw': 416}
{'target': 600, 'success': 330, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 377, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 426, 'raw': 576}
{'target': 600, 'success': 448, 'raw': 608}
{'target': 600, 'success': 476, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 527, 'raw': 704}
{'target': 600, 'success': 550, 'raw': 736}
{'target': 600, 'success': 576, 'raw': 768}
{'target': 600, 'success': 598, 'raw': 800}
{'target': 600, 'success': 626, 'raw': 832}
{'prompt': 'Relation : country .', 'success_rate': 0.7524038461538461, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : followed by .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 487, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : genre .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8288043478260869, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8934659090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.8973214285714286, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8707386363636364, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : located on terrain feature . Context : The city is located in the southeastern part of the state of Oregon , just north of Roseville and southwest of the downtown area . Head Entity : Roseville , Tail Entity : western part of state of Oregon .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8778409090909091, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : spouse .', 'success_rate': 0.77, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/2_ext.jsonl'}}
estimate vocab size: 11836
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11936, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.08it/s]Extractor Estimating: 2it [00:01,  1.14it/s]Extractor Estimating: 3it [00:02,  1.11it/s]Extractor Estimating: 4it [00:03,  1.19it/s]Extractor Estimating: 5it [00:04,  1.21it/s]Extractor Estimating: 6it [00:05,  1.20it/s]Extractor Estimating: 7it [00:05,  1.21it/s]Extractor Estimating: 8it [00:06,  1.21it/s]Extractor Estimating: 9it [00:07,  1.24it/s]Extractor Estimating: 10it [00:08,  1.19it/s]Extractor Estimating: 11it [00:09,  1.19it/s]Extractor Estimating: 12it [00:10,  1.22it/s]Extractor Estimating: 13it [00:11,  1.14it/s]Extractor Estimating: 14it [00:11,  1.16it/s]Extractor Estimating: 15it [00:12,  1.16it/s]Extractor Estimating: 16it [00:13,  1.17it/s]Extractor Estimating: 17it [00:14,  1.11it/s]Extractor Estimating: 18it [00:15,  1.14it/s]Extractor Estimating: 19it [00:16,  1.21it/s]Extractor Estimating: 20it [00:16,  1.24it/s]Extractor Estimating: 21it [00:17,  1.21it/s]Extractor Estimating: 22it [00:18,  1.26it/s]Extractor Estimating: 23it [00:19,  1.24it/s]Extractor Estimating: 24it [00:20,  1.23it/s]Extractor Estimating: 25it [00:20,  1.22it/s]Extractor Estimating: 26it [00:21,  1.20it/s]Extractor Estimating: 27it [00:22,  1.20it/s]Extractor Estimating: 28it [00:23,  1.21it/s]Extractor Estimating: 29it [00:24,  1.23it/s]Extractor Estimating: 30it [00:25,  1.23it/s]Extractor Estimating: 31it [00:25,  1.20it/s]Extractor Estimating: 32it [00:26,  1.18it/s]Extractor Estimating: 33it [00:27,  1.20it/s]Extractor Estimating: 34it [00:28,  1.15it/s]Extractor Estimating: 35it [00:29,  1.20it/s]Extractor Estimating: 36it [00:30,  1.16it/s]Extractor Estimating: 37it [00:31,  1.19it/s]Extractor Estimating: 38it [00:31,  1.16it/s]Extractor Estimating: 39it [00:32,  1.12it/s]Extractor Estimating: 40it [00:33,  1.09it/s]Extractor Estimating: 41it [00:34,  1.12it/s]Extractor Estimating: 42it [00:35,  1.13it/s]Extractor Estimating: 43it [00:36,  1.13it/s]Extractor Estimating: 44it [00:37,  1.14it/s]Extractor Estimating: 45it [00:38,  1.19it/s]Extractor Estimating: 46it [00:38,  1.20it/s]Extractor Estimating: 47it [00:39,  1.19it/s]Extractor Estimating: 48it [00:40,  1.11it/s]Extractor Estimating: 49it [00:41,  1.13it/s]Extractor Estimating: 50it [00:42,  1.16it/s]Extractor Estimating: 51it [00:43,  1.17it/s]Extractor Estimating: 52it [00:44,  1.21it/s]Extractor Estimating: 53it [00:44,  1.26it/s]Extractor Estimating: 54it [00:45,  1.24it/s]Extractor Estimating: 55it [00:46,  1.22it/s]Extractor Estimating: 56it [00:47,  1.28it/s]Extractor Estimating: 57it [00:47,  1.27it/s]Extractor Estimating: 58it [00:48,  1.26it/s]Extractor Estimating: 59it [00:49,  1.25it/s]Extractor Estimating: 60it [00:50,  1.27it/s]Extractor Estimating: 61it [00:51,  1.27it/s]Extractor Estimating: 62it [00:52,  1.20it/s]Extractor Estimating: 63it [00:52,  1.16it/s]Extractor Estimating: 64it [00:53,  1.14it/s]Extractor Estimating: 65it [00:54,  1.17it/s]Extractor Estimating: 66it [00:55,  1.21it/s]Extractor Estimating: 67it [00:56,  1.22it/s]Extractor Estimating: 68it [00:57,  1.22it/s]Extractor Estimating: 69it [00:57,  1.23it/s]Extractor Estimating: 70it [00:58,  1.24it/s]Extractor Estimating: 71it [00:59,  1.27it/s]Extractor Estimating: 72it [01:00,  1.23it/s]Extractor Estimating: 73it [01:01,  1.19it/s]Extractor Estimating: 74it [01:02,  1.19it/s]Extractor Estimating: 75it [01:02,  1.16it/s]Extractor Estimating: 76it [01:03,  1.17it/s]Extractor Estimating: 77it [01:04,  1.18it/s]Extractor Estimating: 78it [01:05,  1.19it/s]Extractor Estimating: 79it [01:06,  1.19it/s]Extractor Estimating: 80it [01:07,  1.14it/s]Extractor Estimating: 81it [01:08,  1.18it/s]Extractor Estimating: 82it [01:08,  1.19it/s]Extractor Estimating: 83it [01:09,  1.24it/s]Extractor Estimating: 84it [01:10,  1.23it/s]Extractor Estimating: 85it [01:11,  1.23it/s]Extractor Estimating: 86it [01:11,  1.29it/s]Extractor Estimating: 87it [01:12,  1.29it/s]Extractor Estimating: 88it [01:13,  1.24it/s]Extractor Estimating: 89it [01:14,  1.21it/s]Extractor Estimating: 90it [01:15,  1.25it/s]Extractor Estimating: 91it [01:16,  1.22it/s]Extractor Estimating: 92it [01:16,  1.23it/s]Extractor Estimating: 93it [01:17,  1.19it/s]Extractor Estimating: 94it [01:18,  1.22it/s]Extractor Estimating: 95it [01:19,  1.21it/s]Extractor Estimating: 96it [01:20,  1.22it/s]Extractor Estimating: 97it [01:21,  1.17it/s]Extractor Estimating: 98it [01:21,  1.17it/s]Extractor Estimating: 99it [01:22,  1.12it/s]Extractor Estimating: 100it [01:23,  1.21it/s]Extractor Estimating: 101it [01:24,  1.22it/s]Extractor Estimating: 102it [01:25,  1.22it/s]Extractor Estimating: 103it [01:26,  1.23it/s]Extractor Estimating: 104it [01:26,  1.22it/s]Extractor Estimating: 105it [01:27,  1.23it/s]Extractor Estimating: 106it [01:28,  1.24it/s]Extractor Estimating: 107it [01:29,  1.24it/s]Extractor Estimating: 108it [01:30,  1.21it/s]Extractor Estimating: 109it [01:30,  1.20it/s]Extractor Estimating: 110it [01:31,  1.23it/s]Extractor Estimating: 111it [01:32,  1.24it/s]Extractor Estimating: 112it [01:33,  1.25it/s]Extractor Estimating: 113it [01:34,  1.18it/s]Extractor Estimating: 114it [01:35,  1.20it/s]Extractor Estimating: 115it [01:35,  1.23it/s]Extractor Estimating: 116it [01:36,  1.23it/s]Extractor Estimating: 117it [01:37,  1.24it/s]Extractor Estimating: 118it [01:38,  1.24it/s]Extractor Estimating: 119it [01:38,  1.30it/s]Extractor Estimating: 120it [01:39,  1.27it/s]Extractor Estimating: 121it [01:40,  1.24it/s]Extractor Estimating: 122it [01:41,  1.26it/s]Extractor Estimating: 123it [01:42,  1.31it/s]Extractor Estimating: 124it [01:42,  1.32it/s]Extractor Estimating: 125it [01:43,  1.30it/s]Extractor Estimating: 126it [01:44,  1.31it/s]Extractor Estimating: 127it [01:45,  1.31it/s]Extractor Estimating: 128it [01:45,  1.33it/s]Extractor Estimating: 129it [01:46,  1.34it/s]Extractor Estimating: 130it [01:47,  1.33it/s]Extractor Estimating: 131it [01:48,  1.33it/s]Extractor Estimating: 132it [01:48,  1.33it/s]Extractor Estimating: 133it [01:49,  1.31it/s]Extractor Estimating: 134it [01:50,  1.26it/s]Extractor Estimating: 135it [01:51,  1.28it/s]Extractor Estimating: 136it [01:52,  1.30it/s]Extractor Estimating: 137it [01:52,  1.28it/s]Extractor Estimating: 138it [01:53,  1.26it/s]Extractor Estimating: 139it [01:54,  1.23it/s]Extractor Estimating: 140it [01:55,  1.28it/s]Extractor Estimating: 141it [01:55,  1.30it/s]Extractor Estimating: 142it [01:56,  1.32it/s]Extractor Estimating: 143it [01:57,  1.30it/s]Extractor Estimating: 144it [01:58,  1.25it/s]Extractor Estimating: 145it [01:59,  1.26it/s]Extractor Estimating: 146it [01:59,  1.30it/s]Extractor Estimating: 147it [02:00,  1.28it/s]Extractor Estimating: 148it [02:01,  1.31it/s]Extractor Estimating: 149it [02:02,  1.31it/s]Extractor Estimating: 150it [02:02,  1.27it/s]Extractor Estimating: 151it [02:03,  1.25it/s]Extractor Estimating: 152it [02:04,  1.26it/s]Extractor Estimating: 153it [02:05,  1.30it/s]Extractor Estimating: 154it [02:06,  1.28it/s]Extractor Estimating: 155it [02:06,  1.31it/s]Extractor Estimating: 156it [02:07,  1.29it/s]Extractor Estimating: 157it [02:08,  1.29it/s]Extractor Estimating: 158it [02:09,  1.28it/s]Extractor Estimating: 159it [02:10,  1.26it/s]Extractor Estimating: 160it [02:10,  1.27it/s]Extractor Estimating: 161it [02:11,  1.28it/s]Extractor Estimating: 162it [02:12,  1.25it/s]Extractor Estimating: 163it [02:13,  1.30it/s]Extractor Estimating: 164it [02:13,  1.31it/s]Extractor Estimating: 165it [02:14,  1.27it/s]Extractor Estimating: 166it [02:15,  1.30it/s]Extractor Estimating: 167it [02:16,  1.34it/s]Extractor Estimating: 168it [02:17,  1.27it/s]Extractor Estimating: 169it [02:17,  1.25it/s]Extractor Estimating: 170it [02:18,  1.29it/s]Extractor Estimating: 171it [02:19,  1.26it/s]Extractor Estimating: 172it [02:20,  1.29it/s]Extractor Estimating: 173it [02:20,  1.33it/s]Extractor Estimating: 174it [02:21,  1.26it/s]Extractor Estimating: 175it [02:22,  1.22it/s]Extractor Estimating: 176it [02:23,  1.23it/s]Extractor Estimating: 177it [02:24,  1.25it/s]Extractor Estimating: 178it [02:25,  1.22it/s]Extractor Estimating: 179it [02:25,  1.23it/s]Extractor Estimating: 180it [02:26,  1.23it/s]Extractor Estimating: 181it [02:27,  1.20it/s]Extractor Estimating: 182it [02:28,  1.22it/s]Extractor Estimating: 183it [02:29,  1.20it/s]Extractor Estimating: 184it [02:30,  1.18it/s]Extractor Estimating: 185it [02:30,  1.21it/s]Extractor Estimating: 186it [02:31,  1.24it/s]Extractor Estimating: 187it [02:32,  1.20it/s]Extractor Estimating: 188it [02:33,  1.18it/s]Extractor Estimating: 189it [02:34,  1.10it/s]Extractor Estimating: 190it [02:35,  1.14it/s]Extractor Estimating: 191it [02:36,  1.13it/s]Extractor Estimating: 192it [02:36,  1.15it/s]Extractor Estimating: 193it [02:37,  1.14it/s]Extractor Estimating: 194it [02:38,  1.18it/s]Extractor Estimating: 195it [02:39,  1.20it/s]Extractor Estimating: 196it [02:40,  1.21it/s]Extractor Estimating: 197it [02:41,  1.21it/s]Extractor Estimating: 198it [02:41,  1.25it/s]Extractor Estimating: 199it [02:42,  1.21it/s]Extractor Estimating: 200it [02:43,  1.23it/s]Extractor Estimating: 201it [02:44,  1.14it/s]Extractor Estimating: 202it [02:45,  1.09it/s]Extractor Estimating: 203it [02:46,  1.13it/s]Extractor Estimating: 204it [02:47,  1.15it/s]Extractor Estimating: 205it [02:47,  1.17it/s]Extractor Estimating: 206it [02:48,  1.22it/s]Extractor Estimating: 207it [02:49,  1.23it/s]Extractor Estimating: 208it [02:50,  1.23it/s]Extractor Estimating: 209it [02:51,  1.21it/s]Extractor Estimating: 210it [02:52,  1.21it/s]Extractor Estimating: 211it [02:52,  1.22it/s]Extractor Estimating: 212it [02:53,  1.25it/s]Extractor Estimating: 213it [02:54,  1.27it/s]Extractor Estimating: 214it [02:55,  1.28it/s]Extractor Estimating: 215it [02:55,  1.24it/s]Extractor Estimating: 216it [02:56,  1.25it/s]Extractor Estimating: 217it [02:57,  1.24it/s]Extractor Estimating: 218it [02:58,  1.28it/s]Extractor Estimating: 219it [02:59,  1.26it/s]Extractor Estimating: 220it [02:59,  1.28it/s]Extractor Estimating: 221it [03:00,  1.25it/s]Extractor Estimating: 222it [03:01,  1.22it/s]Extractor Estimating: 223it [03:02,  1.23it/s]Extractor Estimating: 224it [03:03,  1.23it/s]Extractor Estimating: 225it [03:04,  1.19it/s]Extractor Estimating: 226it [03:04,  1.22it/s]Extractor Estimating: 227it [03:05,  1.23it/s]Extractor Estimating: 228it [03:06,  1.20it/s]Extractor Estimating: 229it [03:07,  1.25it/s]Extractor Estimating: 230it [03:08,  1.19it/s]Extractor Estimating: 231it [03:09,  1.21it/s]Extractor Estimating: 232it [03:09,  1.22it/s]Extractor Estimating: 233it [03:10,  1.23it/s]Extractor Estimating: 234it [03:11,  1.27it/s]Extractor Estimating: 235it [03:12,  1.23it/s]Extractor Estimating: 236it [03:13,  1.20it/s]Extractor Estimating: 237it [03:13,  1.22it/s]Extractor Estimating: 238it [03:14,  1.22it/s]Extractor Estimating: 239it [03:15,  1.20it/s]Extractor Estimating: 240it [03:16,  1.21it/s]Extractor Estimating: 241it [03:17,  1.28it/s]Extractor Estimating: 242it [03:17,  1.32it/s]Extractor Estimating: 243it [03:18,  1.32it/s]Extractor Estimating: 244it [03:19,  1.29it/s]Extractor Estimating: 245it [03:20,  1.29it/s]Extractor Estimating: 246it [03:20,  1.31it/s]Extractor Estimating: 247it [03:21,  1.31it/s]Extractor Estimating: 248it [03:22,  1.21it/s]Extractor Estimating: 249it [03:23,  1.23it/s]Extractor Estimating: 250it [03:24,  1.20it/s]Extractor Estimating: 251it [03:25,  1.21it/s]Extractor Estimating: 252it [03:25,  1.21it/s]Extractor Estimating: 253it [03:26,  1.20it/s]Extractor Estimating: 254it [03:27,  1.23it/s]Extractor Estimating: 255it [03:28,  1.20it/s]Extractor Estimating: 256it [03:29,  1.23it/s]Extractor Estimating: 257it [03:30,  1.16it/s]Extractor Estimating: 258it [03:30,  1.16it/s]Extractor Estimating: 259it [03:31,  1.18it/s]Extractor Estimating: 260it [03:32,  1.20it/s]Extractor Estimating: 261it [03:33,  1.24it/s]Extractor Estimating: 262it [03:34,  1.21it/s]Extractor Estimating: 263it [03:34,  1.24it/s]Extractor Estimating: 264it [03:35,  1.24it/s]Extractor Estimating: 265it [03:36,  1.22it/s]Extractor Estimating: 266it [03:37,  1.20it/s]Extractor Estimating: 267it [03:38,  1.22it/s]Extractor Estimating: 268it [03:39,  1.23it/s]Extractor Estimating: 269it [03:40,  1.17it/s]Extractor Estimating: 270it [03:40,  1.19it/s]Extractor Estimating: 271it [03:41,  1.21it/s]Extractor Estimating: 272it [03:42,  1.18it/s]Extractor Estimating: 273it [03:43,  1.19it/s]Extractor Estimating: 274it [03:44,  1.18it/s]Extractor Estimating: 275it [03:44,  1.23it/s]Extractor Estimating: 276it [03:45,  1.28it/s]Extractor Estimating: 277it [03:46,  1.27it/s]Extractor Estimating: 278it [03:47,  1.30it/s]Extractor Estimating: 279it [03:48,  1.25it/s]Extractor Estimating: 280it [03:48,  1.27it/s]Extractor Estimating: 281it [03:49,  1.30it/s]Extractor Estimating: 282it [03:50,  1.31it/s]Extractor Estimating: 283it [03:50,  1.35it/s]Extractor Estimating: 284it [03:51,  1.33it/s]Extractor Estimating: 285it [03:52,  1.26it/s]Extractor Estimating: 286it [03:53,  1.30it/s]Extractor Estimating: 287it [03:54,  1.30it/s]Extractor Estimating: 288it [03:55,  1.25it/s]Extractor Estimating: 289it [03:55,  1.28it/s]Extractor Estimating: 290it [03:56,  1.28it/s]Extractor Estimating: 291it [03:57,  1.28it/s]Extractor Estimating: 292it [03:58,  1.26it/s]Extractor Estimating: 293it [03:58,  1.33it/s]Extractor Estimating: 294it [03:59,  1.33it/s]Extractor Estimating: 295it [04:00,  1.36it/s]Extractor Estimating: 296it [04:01,  1.32it/s]Extractor Estimating: 297it [04:01,  1.33it/s]Extractor Estimating: 298it [04:02,  1.31it/s]Extractor Estimating: 299it [04:03,  1.33it/s]Extractor Estimating: 300it [04:04,  1.30it/s]Extractor Estimating: 301it [04:04,  1.41it/s]Extractor Estimating: 302it [04:05,  1.40it/s]Extractor Estimating: 303it [04:06,  1.40it/s]Extractor Estimating: 304it [04:06,  1.40it/s]Extractor Estimating: 305it [04:07,  1.43it/s]Extractor Estimating: 306it [04:08,  1.38it/s]Extractor Estimating: 307it [04:09,  1.37it/s]Extractor Estimating: 308it [04:09,  1.38it/s]Extractor Estimating: 309it [04:10,  1.36it/s]Extractor Estimating: 310it [04:11,  1.31it/s]Extractor Estimating: 311it [04:12,  1.29it/s]Extractor Estimating: 312it [04:13,  1.23it/s]Extractor Estimating: 313it [04:13,  1.24it/s]Extractor Estimating: 314it [04:14,  1.30it/s]Extractor Estimating: 315it [04:15,  1.28it/s]Extractor Estimating: 316it [04:16,  1.27it/s]Extractor Estimating: 317it [04:16,  1.31it/s]Extractor Estimating: 318it [04:17,  1.32it/s]Extractor Estimating: 319it [04:18,  1.35it/s]Extractor Estimating: 320it [04:18,  1.37it/s]Extractor Estimating: 321it [04:19,  1.35it/s]Extractor Estimating: 322it [04:20,  1.38it/s]Extractor Estimating: 323it [04:21,  1.35it/s]Extractor Estimating: 324it [04:21,  1.39it/s]Extractor Estimating: 325it [04:22,  1.40it/s]Extractor Estimating: 326it [04:23,  1.36it/s]Extractor Estimating: 327it [04:24,  1.32it/s]Extractor Estimating: 328it [04:25,  1.25it/s]Extractor Estimating: 329it [04:25,  1.24it/s]Extractor Estimating: 330it [04:26,  1.22it/s]Extractor Estimating: 331it [04:27,  1.16it/s]Extractor Estimating: 332it [04:28,  1.20it/s]Extractor Estimating: 333it [04:29,  1.19it/s]Extractor Estimating: 334it [04:30,  1.22it/s]Extractor Estimating: 335it [04:30,  1.24it/s]Extractor Estimating: 336it [04:31,  1.23it/s]Extractor Estimating: 337it [04:32,  1.22it/s]Extractor Estimating: 338it [04:33,  1.25it/s]Extractor Estimating: 339it [04:34,  1.20it/s]Extractor Estimating: 340it [04:34,  1.22it/s]Extractor Estimating: 341it [04:35,  1.22it/s]Extractor Estimating: 342it [04:36,  1.26it/s]Extractor Estimating: 343it [04:37,  1.26it/s]Extractor Estimating: 344it [04:38,  1.26it/s]Extractor Estimating: 345it [04:38,  1.23it/s]Extractor Estimating: 346it [04:39,  1.22it/s]Extractor Estimating: 347it [04:40,  1.20it/s]Extractor Estimating: 348it [04:41,  1.24it/s]Extractor Estimating: 349it [04:42,  1.26it/s]Extractor Estimating: 350it [04:42,  1.25it/s]Extractor Estimating: 351it [04:43,  1.26it/s]Extractor Estimating: 352it [04:44,  1.26it/s]Extractor Estimating: 353it [04:45,  1.16it/s]Extractor Estimating: 354it [04:46,  1.19it/s]Extractor Estimating: 355it [04:47,  1.21it/s]Extractor Estimating: 356it [04:48,  1.19it/s]Extractor Estimating: 357it [04:48,  1.16it/s]Extractor Estimating: 358it [04:49,  1.19it/s]Extractor Estimating: 359it [04:50,  1.23it/s]Extractor Estimating: 360it [04:51,  1.26it/s]Extractor Estimating: 361it [04:52,  1.24it/s]Extractor Estimating: 362it [04:52,  1.25it/s]Extractor Estimating: 363it [04:53,  1.20it/s]Extractor Estimating: 364it [04:54,  1.24it/s]Extractor Estimating: 365it [04:55,  1.21it/s]Extractor Estimating: 366it [04:56,  1.21it/s]Extractor Estimating: 367it [04:56,  1.25it/s]Extractor Estimating: 368it [04:57,  1.28it/s]Extractor Estimating: 369it [04:58,  1.29it/s]Extractor Estimating: 370it [04:59,  1.20it/s]Extractor Estimating: 371it [05:00,  1.16it/s]Extractor Estimating: 372it [05:01,  1.22it/s]Extractor Estimating: 373it [05:01,  1.24it/s]Extractor Estimating: 374it [05:02,  1.23it/s]Extractor Estimating: 375it [05:03,  1.35it/s]Extractor Estimating: 375it [05:03,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:51,911 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:51,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:51,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:51,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:51,924 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:32:52,574 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:32:52,575 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:32:53,145 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:32:54,231 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:32:54,231 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:57,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:57,160 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:57,160 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:57,160 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:32:57,160 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:32:57,804 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:32:57,805 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:32:58,392 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:32:58,557 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:32:58,557 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 02:28:37,058 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 02:28:37,085 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7488 mean pseudo reward: 0.9391793472053976
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 23204
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23304, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23304, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.359, loss:747.9749
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.384, loss:690.3479
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.364, loss:700.2640
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.386, loss:673.8560
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.368, loss:679.2866
>> valid entity prec:0.5014, rec:0.4536, f1:0.4763
>> valid relation prec:0.0263, rec:0.0080, f1:0.0123
>> valid relation with NER prec:0.0263, rec:0.0080, f1:0.0123
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 3.044, loss:690.8614
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.360, loss:658.7061
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.378, loss:655.5691
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.373, loss:702.9303
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.380, loss:672.2896
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4765, rec:0.4101, f1:0.4408
>> valid relation prec:0.0183, rec:0.0052, f1:0.0081
>> valid relation with NER prec:0.0183, rec:0.0052, f1:0.0081
g_step 1100, step 164, avg_time 3.034, loss:690.4537
g_step 1200, step 264, avg_time 1.370, loss:695.8457
g_step 1300, step 52, avg_time 1.361, loss:652.2294
g_step 1400, step 152, avg_time 1.375, loss:638.2028
g_step 1500, step 252, avg_time 1.347, loss:647.3559
>> valid entity prec:0.4984, rec:0.4454, f1:0.4704
>> valid relation prec:0.0059, rec:0.0017, f1:0.0027
>> valid relation with NER prec:0.0059, rec:0.0017, f1:0.0027
g_step 1600, step 40, avg_time 3.045, loss:657.4145
g_step 1700, step 140, avg_time 1.377, loss:611.3394
g_step 1800, step 240, avg_time 1.354, loss:628.8300
g_step 1900, step 28, avg_time 1.364, loss:617.3879
g_step 2000, step 128, avg_time 1.371, loss:594.3952
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5089, rec:0.4818, f1:0.4950
>> valid relation prec:0.0201, rec:0.0057, f1:0.0089
>> valid relation with NER prec:0.0201, rec:0.0057, f1:0.0089
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 3.043, loss:612.2483
g_step 2200, step 16, avg_time 1.386, loss:617.7424
g_step 2300, step 116, avg_time 1.350, loss:561.7980
g_step 2400, step 216, avg_time 1.380, loss:570.2155
g_step 2500, step 4, avg_time 1.365, loss:584.0884
>> valid entity prec:0.4754, rec:0.4299, f1:0.4515
>> valid relation prec:0.0198, rec:0.0060, f1:0.0092
>> valid relation with NER prec:0.0198, rec:0.0060, f1:0.0092
g_step 2600, step 104, avg_time 3.033, loss:535.2015
g_step 2700, step 204, avg_time 1.357, loss:538.9943
g_step 2800, step 304, avg_time 1.373, loss:565.9248
g_step 2900, step 92, avg_time 1.379, loss:505.8814
g_step 3000, step 192, avg_time 1.360, loss:519.5265
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4946, rec:0.4377, f1:0.4644
>> valid relation prec:0.0192, rec:0.0069, f1:0.0101
>> valid relation with NER prec:0.0192, rec:0.0069, f1:0.0101
g_step 3100, step 292, avg_time 3.028, loss:538.6590
g_step 3200, step 80, avg_time 1.361, loss:499.0293
g_step 3300, step 180, avg_time 1.365, loss:509.5518
g_step 3400, step 280, avg_time 1.374, loss:528.3868
g_step 3500, step 68, avg_time 1.363, loss:483.5227
>> valid entity prec:0.5235, rec:0.3983, f1:0.4524
>> valid relation prec:0.0246, rec:0.0080, f1:0.0121
>> valid relation with NER prec:0.0246, rec:0.0080, f1:0.0121
g_step 3600, step 168, avg_time 3.018, loss:480.9710
g_step 3700, step 268, avg_time 1.352, loss:495.4287
g_step 3800, step 56, avg_time 1.370, loss:487.8451
g_step 3900, step 156, avg_time 1.365, loss:450.0582
g_step 4000, step 256, avg_time 1.361, loss:483.4488
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4950, rec:0.4548, f1:0.4741
>> valid relation prec:0.0287, rec:0.0109, f1:0.0158
>> valid relation with NER prec:0.0287, rec:0.0109, f1:0.0158
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 44, avg_time 3.042, loss:451.3251
g_step 4200, step 144, avg_time 1.367, loss:439.2684
g_step 4300, step 244, avg_time 1.349, loss:463.3360
g_step 4400, step 32, avg_time 1.381, loss:456.8998
g_step 4500, step 132, avg_time 1.354, loss:421.2492
>> valid entity prec:0.4745, rec:0.4751, f1:0.4748
>> valid relation prec:0.0216, rec:0.0103, f1:0.0140
>> valid relation with NER prec:0.0216, rec:0.0103, f1:0.0140
g_step 4600, step 232, avg_time 3.056, loss:449.8652
g_step 4700, step 20, avg_time 1.353, loss:430.2275
g_step 4800, step 120, avg_time 1.356, loss:396.5802
g_step 4900, step 220, avg_time 1.375, loss:437.0236
g_step 5000, step 8, avg_time 1.371, loss:423.2914
learning rate was adjusted to 0.0008
>> valid entity prec:0.4876, rec:0.4981, f1:0.4928
>> valid relation prec:0.0311, rec:0.0164, f1:0.0215
>> valid relation with NER prec:0.0311, rec:0.0164, f1:0.0215
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5100, step 108, avg_time 3.048, loss:385.8346
g_step 5200, step 208, avg_time 1.371, loss:410.1567
g_step 5300, step 308, avg_time 1.368, loss:413.2767
g_step 5400, step 96, avg_time 1.351, loss:360.7822
g_step 5500, step 196, avg_time 1.359, loss:387.9299
>> valid entity prec:0.5088, rec:0.4036, f1:0.4502
>> valid relation prec:0.0302, rec:0.0109, f1:0.0160
>> valid relation with NER prec:0.0302, rec:0.0109, f1:0.0160
g_step 5600, step 296, avg_time 3.049, loss:409.6157
g_step 5700, step 84, avg_time 1.362, loss:374.7422
g_step 5800, step 184, avg_time 1.352, loss:367.9401
g_step 5900, step 284, avg_time 1.383, loss:424.9618
g_step 6000, step 72, avg_time 1.370, loss:373.2780
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4703, rec:0.4447, f1:0.4571
>> valid relation prec:0.0290, rec:0.0129, f1:0.0179
>> valid relation with NER prec:0.0290, rec:0.0129, f1:0.0179
g_step 6100, step 172, avg_time 3.030, loss:345.5550
g_step 6200, step 272, avg_time 1.383, loss:381.0295
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:28:37 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:28:37 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-28-37_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:28:37 - WARNING - datasets.builder -   Using custom data configuration default-6804291b0309e13e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6804291b0309e13e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:28:38,307 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:28:38,308 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:28:38,309 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:28:38,310 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:28:38,317 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:38,320 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:38,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:38,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:38,321 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:38,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:28:38,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:28:38,448 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:28:41,579 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:28:41,584 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6804291b0309e13e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  3.26ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.06ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  3.62ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  3.91ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.11ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.25ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.33ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.14ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.28ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  3.87ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.12ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.23ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.33ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.80ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  8.23ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  9.77ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00, 10.25ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00, 10.37ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.82ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  8.44ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  9.59ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.96ba/s]
[INFO|trainer.py:414] 2023-08-29 02:28:45,794 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:28:45,808 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:28:45,808 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 02:28:45,809 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:28:45,809 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:28:45,809 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:28:45,809 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:28:45,809 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<03:00,  3.23it/s]  0%|          | 2/585 [00:00<02:54,  3.35it/s]  1%|          | 3/585 [00:00<02:51,  3.39it/s]  1%|          | 4/585 [00:01<02:50,  3.41it/s]  1%|          | 5/585 [00:01<02:49,  3.42it/s]  1%|          | 6/585 [00:01<02:48,  3.43it/s]  1%|          | 7/585 [00:02<02:48,  3.43it/s]  1%|â–         | 8/585 [00:02<02:48,  3.43it/s]  2%|â–         | 9/585 [00:02<02:47,  3.43it/s]  2%|â–         | 10/585 [00:02<02:47,  3.44it/s]  2%|â–         | 11/585 [00:03<02:47,  3.44it/s]  2%|â–         | 12/585 [00:03<02:46,  3.44it/s]  2%|â–         | 13/585 [00:03<02:46,  3.44it/s]  2%|â–         | 14/585 [00:04<02:45,  3.45it/s]  3%|â–Ž         | 15/585 [00:04<02:45,  3.45it/s]  3%|â–Ž         | 16/585 [00:04<02:44,  3.45it/s]  3%|â–Ž         | 17/585 [00:04<02:44,  3.45it/s]  3%|â–Ž         | 18/585 [00:05<02:44,  3.45it/s]  3%|â–Ž         | 19/585 [00:05<02:44,  3.44it/s]  3%|â–Ž         | 20/585 [00:05<02:44,  3.44it/s]  4%|â–Ž         | 21/585 [00:06<02:43,  3.45it/s]  4%|â–         | 22/585 [00:06<02:43,  3.45it/s]  4%|â–         | 23/585 [00:06<02:43,  3.45it/s]  4%|â–         | 24/585 [00:06<02:42,  3.45it/s]  4%|â–         | 25/585 [00:07<02:42,  3.45it/s]  4%|â–         | 26/585 [00:07<02:42,  3.45it/s]  5%|â–         | 27/585 [00:07<02:41,  3.45it/s]  5%|â–         | 28/585 [00:08<02:41,  3.45it/s]  5%|â–         | 29/585 [00:08<02:41,  3.45it/s]  5%|â–Œ         | 30/585 [00:08<02:41,  3.43it/s]  5%|â–Œ         | 31/585 [00:09<02:41,  3.43it/s]  5%|â–Œ         | 32/585 [00:09<02:40,  3.44it/s]  6%|â–Œ         | 33/585 [00:09<02:40,  3.44it/s]  6%|â–Œ         | 34/585 [00:09<02:40,  3.44it/s]  6%|â–Œ         | 35/585 [00:10<02:39,  3.44it/s]  6%|â–Œ         | 36/585 [00:10<02:39,  3.44it/s]  6%|â–‹         | 37/585 [00:10<02:39,  3.44it/s]  6%|â–‹         | 38/585 [00:11<02:38,  3.44it/s]  7%|â–‹         | 39/585 [00:11<02:38,  3.45it/s]  7%|â–‹         | 40/585 [00:11<02:38,  3.45it/s]  7%|â–‹         | 41/585 [00:11<02:38,  3.43it/s]  7%|â–‹         | 42/585 [00:12<02:38,  3.43it/s]  7%|â–‹         | 43/585 [00:12<02:37,  3.44it/s]  8%|â–Š         | 44/585 [00:12<02:37,  3.44it/s]  8%|â–Š         | 45/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 46/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 47/585 [00:13<02:36,  3.45it/s]  8%|â–Š         | 48/585 [00:13<02:35,  3.45it/s]  8%|â–Š         | 49/585 [00:14<02:35,  3.45it/s]  9%|â–Š         | 50/585 [00:14<02:35,  3.45it/s]  9%|â–Š         | 51/585 [00:14<02:34,  3.45it/s]  9%|â–‰         | 52/585 [00:15<02:35,  3.44it/s]  9%|â–‰         | 53/585 [00:15<02:34,  3.44it/s]  9%|â–‰         | 54/585 [00:15<02:34,  3.44it/s]  9%|â–‰         | 55/585 [00:15<02:34,  3.44it/s] 10%|â–‰         | 56/585 [00:16<02:33,  3.44it/s] 10%|â–‰         | 57/585 [00:16<02:33,  3.44it/s] 10%|â–‰         | 58/585 [00:16<02:33,  3.44it/s] 10%|â–ˆ         | 59/585 [00:17<02:32,  3.44it/s] 10%|â–ˆ         | 60/585 [00:17<02:32,  3.44it/s] 10%|â–ˆ         | 61/585 [00:17<02:32,  3.44it/s] 11%|â–ˆ         | 62/585 [00:18<02:31,  3.44it/s] 11%|â–ˆ         | 63/585 [00:18<02:31,  3.43it/s] 11%|â–ˆ         | 64/585 [00:18<02:31,  3.44it/s] 11%|â–ˆ         | 65/585 [00:18<02:31,  3.44it/s] 11%|â–ˆâ–        | 66/585 [00:19<02:30,  3.44it/s] 11%|â–ˆâ–        | 67/585 [00:19<02:30,  3.44it/s] 12%|â–ˆâ–        | 68/585 [00:19<02:30,  3.44it/s] 12%|â–ˆâ–        | 69/585 [00:20<02:29,  3.44it/s] 12%|â–ˆâ–        | 70/585 [00:20<02:29,  3.44it/s] 12%|â–ˆâ–        | 71/585 [00:20<02:29,  3.44it/s] 12%|â–ˆâ–        | 72/585 [00:20<02:28,  3.44it/s] 12%|â–ˆâ–        | 73/585 [00:21<02:28,  3.45it/s] 13%|â–ˆâ–Ž        | 74/585 [00:21<02:28,  3.43it/s] 13%|â–ˆâ–Ž        | 75/585 [00:21<02:28,  3.44it/s] 13%|â–ˆâ–Ž        | 76/585 [00:22<02:28,  3.44it/s] 13%|â–ˆâ–Ž        | 77/585 [00:22<02:27,  3.44it/s] 13%|â–ˆâ–Ž        | 78/585 [00:22<02:27,  3.44it/s] 14%|â–ˆâ–Ž        | 79/585 [00:22<02:27,  3.44it/s] 14%|â–ˆâ–Ž        | 80/585 [00:23<02:26,  3.44it/s] 14%|â–ˆâ–        | 81/585 [00:23<02:26,  3.44it/s] 14%|â–ˆâ–        | 82/585 [00:23<02:26,  3.44it/s] 14%|â–ˆâ–        | 83/585 [00:24<02:25,  3.44it/s] 14%|â–ˆâ–        | 84/585 [00:24<02:25,  3.44it/s] 15%|â–ˆâ–        | 85/585 [00:24<02:25,  3.44it/s] 15%|â–ˆâ–        | 86/585 [00:25<02:24,  3.44it/s] 15%|â–ˆâ–        | 87/585 [00:25<02:24,  3.44it/s] 15%|â–ˆâ–Œ        | 88/585 [00:25<02:24,  3.44it/s] 15%|â–ˆâ–Œ        | 89/585 [00:25<02:24,  3.44it/s] 15%|â–ˆâ–Œ        | 90/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 91/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 92/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 93/585 [00:27<02:22,  3.44it/s] 16%|â–ˆâ–Œ        | 94/585 [00:27<02:22,  3.44it/s] 16%|â–ˆâ–Œ        | 95/585 [00:27<02:22,  3.44it/s] 16%|â–ˆâ–‹        | 96/585 [00:27<02:22,  3.44it/s] 17%|â–ˆâ–‹        | 97/585 [00:28<02:21,  3.44it/s] 17%|â–ˆâ–‹        | 98/585 [00:28<02:21,  3.43it/s] 17%|â–ˆâ–‹        | 99/585 [00:28<02:21,  3.43it/s] 17%|â–ˆâ–‹        | 100/585 [00:29<02:21,  3.44it/s] 17%|â–ˆâ–‹        | 101/585 [00:29<02:20,  3.44it/s] 17%|â–ˆâ–‹        | 102/585 [00:29<02:20,  3.44it/s] 18%|â–ˆâ–Š        | 103/585 [00:29<02:20,  3.44it/s] 18%|â–ˆâ–Š        | 104/585 [00:30<02:19,  3.44it/s] 18%|â–ˆâ–Š        | 105/585 [00:30<02:19,  3.44it/s] 18%|â–ˆâ–Š        | 106/585 [00:30<02:19,  3.44it/s] 18%|â–ˆâ–Š        | 107/585 [00:31<02:19,  3.43it/s] 18%|â–ˆâ–Š        | 108/585 [00:31<02:18,  3.44it/s] 19%|â–ˆâ–Š        | 109/585 [00:31<02:18,  3.42it/s] 19%|â–ˆâ–‰        | 110/585 [00:31<02:18,  3.43it/s] 19%|â–ˆâ–‰        | 111/585 [00:32<02:18,  3.43it/s] 19%|â–ˆâ–‰        | 112/585 [00:32<02:17,  3.43it/s] 19%|â–ˆâ–‰        | 113/585 [00:32<02:17,  3.44it/s] 19%|â–ˆâ–‰        | 114/585 [00:33<02:17,  3.44it/s] 20%|â–ˆâ–‰        | 115/585 [00:33<02:16,  3.44it/s] 20%|â–ˆâ–‰        | 116/585 [00:33<02:16,  3.44it/s] 20%|â–ˆâ–ˆ        | 117/585 [00:34<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 02:29:19,885 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:29:19,886 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 02:29:19,886 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.11it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.27it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.68it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.98it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.53it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.21it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.97it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.55it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.50it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.62it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.64it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.58it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.70it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.67it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.67it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.60it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.46it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.46it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.46it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.51it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.58it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.61it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.64it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.57it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.61it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.53it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.47it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.52it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.51it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.53it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.58it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.56it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.63it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.50it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.47it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.51it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.53it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.53it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.53it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.49it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.55it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.53it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.52it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.48it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.51it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.52it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.54it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.56it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.58it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.54it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.49it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.56it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.51it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.49it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.56it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.52it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.56it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.61it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.53it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.55it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.49it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.53it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.54it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.57it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.56it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.51it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.51it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.60it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.58it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.52it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.45it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.46it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.54it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.58it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.50it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.52it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.39it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.50it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.56it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.45it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.50it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.45it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.43it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.48it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.42it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.42it/s][A 20%|â–ˆâ–ˆ        | 117/585 [00:43<02:16,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:29:29,257 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 02:29:29,276 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:29:31,331 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:29:31,346 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:29:31,354 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 118/585 [00:50<39:15,  5.04s/it] 20%|â–ˆâ–ˆ        | 119/585 [00:50<28:06,  3.62s/it] 21%|â–ˆâ–ˆ        | 120/585 [00:50<20:18,  2.62s/it] 21%|â–ˆâ–ˆ        | 121/585 [00:51<14:51,  1.92s/it] 21%|â–ˆâ–ˆ        | 122/585 [00:51<11:07,  1.44s/it] 21%|â–ˆâ–ˆ        | 123/585 [00:51<08:26,  1.10s/it] 21%|â–ˆâ–ˆ        | 124/585 [00:51<06:33,  1.17it/s] 21%|â–ˆâ–ˆâ–       | 125/585 [00:52<05:15,  1.46it/s] 22%|â–ˆâ–ˆâ–       | 126/585 [00:52<04:20,  1.76it/s] 22%|â–ˆâ–ˆâ–       | 127/585 [00:52<03:41,  2.07it/s] 22%|â–ˆâ–ˆâ–       | 128/585 [00:53<03:14,  2.35it/s] 22%|â–ˆâ–ˆâ–       | 129/585 [00:53<02:55,  2.60it/s] 22%|â–ˆâ–ˆâ–       | 130/585 [00:53<02:42,  2.80it/s] 22%|â–ˆâ–ˆâ–       | 131/585 [00:53<02:32,  2.97it/s] 23%|â–ˆâ–ˆâ–Ž       | 132/585 [00:54<02:26,  3.10it/s] 23%|â–ˆâ–ˆâ–Ž       | 133/585 [00:54<02:21,  3.19it/s] 23%|â–ˆâ–ˆâ–Ž       | 134/585 [00:54<02:18,  3.27it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/585 [00:55<02:15,  3.32it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/585 [00:55<02:13,  3.35it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/585 [00:55<02:12,  3.38it/s] 24%|â–ˆâ–ˆâ–Ž       | 138/585 [00:55<02:11,  3.39it/s] 24%|â–ˆâ–ˆâ–       | 139/585 [00:56<02:10,  3.41it/s] 24%|â–ˆâ–ˆâ–       | 140/585 [00:56<02:10,  3.42it/s] 24%|â–ˆâ–ˆâ–       | 141/585 [00:56<02:09,  3.43it/s] 24%|â–ˆâ–ˆâ–       | 142/585 [00:57<02:09,  3.43it/s] 24%|â–ˆâ–ˆâ–       | 143/585 [00:57<02:08,  3.44it/s] 25%|â–ˆâ–ˆâ–       | 144/585 [00:57<02:08,  3.44it/s] 25%|â–ˆâ–ˆâ–       | 145/585 [00:58<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–       | 146/585 [00:58<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 147/585 [00:58<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 148/585 [00:58<02:06,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/585 [00:59<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 150/585 [00:59<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 151/585 [00:59<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 152/585 [01:00<02:05,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/585 [01:00<02:05,  3.44it/s] 26%|â–ˆâ–ˆâ–‹       | 154/585 [01:00<02:05,  3.45it/s] 26%|â–ˆâ–ˆâ–‹       | 155/585 [01:00<02:04,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 156/585 [01:01<02:04,  3.45it/s] 27%|â–ˆâ–ˆâ–‹       | 157/585 [01:01<02:04,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 158/585 [01:01<02:03,  3.45it/s] 27%|â–ˆâ–ˆâ–‹       | 159/585 [01:02<02:03,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 160/585 [01:02<02:03,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 161/585 [01:02<02:03,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 162/585 [01:02<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 163/585 [01:03<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 164/585 [01:03<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 165/585 [01:03<02:01,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 166/585 [01:04<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–Š       | 167/585 [01:04<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–Š       | 168/585 [01:04<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 169/585 [01:05<02:00,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 170/585 [01:05<02:00,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 171/585 [01:05<02:00,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 172/585 [01:05<02:00,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 173/585 [01:06<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 174/585 [01:06<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 175/585 [01:06<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 176/585 [01:07<01:58,  3.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 177/585 [01:07<01:58,  3.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 178/585 [01:07<01:58,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 179/585 [01:07<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 180/585 [01:08<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 181/585 [01:08<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 182/585 [01:08<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 183/585 [01:09<01:56,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 184/585 [01:09<01:56,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 185/585 [01:09<01:56,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 186/585 [01:09<01:55,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 187/585 [01:10<01:55,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 188/585 [01:10<01:55,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/585 [01:10<01:55,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/585 [01:11<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 191/585 [01:11<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 192/585 [01:11<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 193/585 [01:11<01:54,  3.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/585 [01:12<01:53,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/585 [01:12<01:53,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/585 [01:12<01:53,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/585 [01:13<01:52,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 198/585 [01:13<01:52,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 199/585 [01:13<01:52,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 200/585 [01:14<01:51,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/585 [01:14<01:51,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 202/585 [01:14<01:51,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 203/585 [01:14<01:51,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 204/585 [01:15<01:51,  3.42it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 205/585 [01:15<01:50,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 206/585 [01:15<01:50,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 207/585 [01:16<01:49,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 208/585 [01:16<01:49,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/585 [01:16<01:49,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/585 [01:16<01:48,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/585 [01:17<01:48,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/585 [01:17<01:48,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 213/585 [01:17<01:48,  3.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 214/585 [01:18<01:47,  3.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 215/585 [01:18<01:48,  3.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 216/585 [01:18<01:47,  3.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 217/585 [01:18<01:47,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 218/585 [01:19<01:46,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/585 [01:19<01:46,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 220/585 [01:19<01:46,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 221/585 [01:20<01:46,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 222/585 [01:20<01:45,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 223/585 [01:20<01:45,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 224/585 [01:20<01:44,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/585 [01:21<01:44,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 226/585 [01:21<01:44,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 227/585 [01:21<01:44,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 228/585 [01:22<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 229/585 [01:22<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 230/585 [01:22<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/585 [01:23<01:42,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 232/585 [01:23<01:42,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 233/585 [01:23<01:42,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:23<01:41,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 02:30:09,767 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:30:09,767 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 02:30:09,767 >>   Batch size = 8
{'eval_loss': 0.9662191867828369, 'eval_runtime': 9.3534, 'eval_samples_per_second': 371.952, 'eval_steps_per_second': 46.507, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.00it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.25it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.62it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.90it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.51it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.26it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.98it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.68it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.69it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.61it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.42it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.53it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.51it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.55it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.62it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.56it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.59it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.56it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.55it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.51it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.56it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.51it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.59it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.59it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.52it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.55it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.51it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.52it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.51it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.51it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.48it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.56it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.52it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.56it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.59it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.52it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.59it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.59it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.48it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.56it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.54it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.56it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.57it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.54it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.55it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.56it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.58it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.48it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.51it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.59it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.48it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.51it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.56it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.51it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.54it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.62it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.58it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.58it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.51it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.52it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.52it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.53it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.50it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.53it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.55it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.57it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.55it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.50it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.57it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.57it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.32it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.61it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.62it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.56it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.59it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.56it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.56it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.55it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.52it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.54it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.61it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.56it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.56it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.58it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.56it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.58it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.58it/s][A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:33<01:41,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:30:19,143 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 02:30:19,171 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:30:21,210 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:30:21,222 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:30:21,232 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 235/585 [01:41<32:07,  5.51s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 236/585 [01:41<22:57,  3.95s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 237/585 [01:42<16:31,  2.85s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/585 [01:42<12:02,  2.08s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/585 [01:42<08:54,  1.54s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 240/585 [01:43<06:42,  1.17s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/585 [01:43<05:11,  1.11it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242/585 [01:43<04:07,  1.39it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243/585 [01:43<03:22,  1.69it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/585 [01:44<02:50,  2.00it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/585 [01:44<02:28,  2.28it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/585 [01:44<02:13,  2.54it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/585 [01:45<02:02,  2.75it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/585 [01:45<01:55,  2.93it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 249/585 [01:45<01:49,  3.06it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 250/585 [01:45<01:45,  3.17it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 251/585 [01:46<01:42,  3.25it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 252/585 [01:46<01:40,  3.31it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/585 [01:46<01:39,  3.34it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 254/585 [01:47<01:38,  3.38it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/585 [01:47<01:37,  3.39it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/585 [01:47<01:36,  3.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/585 [01:47<01:35,  3.42it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/585 [01:48<01:35,  3.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/585 [01:48<01:35,  3.42it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/585 [01:48<01:34,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/585 [01:49<01:34,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 262/585 [01:49<01:33,  3.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/585 [01:49<01:33,  3.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 264/585 [01:50<01:33,  3.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 265/585 [01:50<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 266/585 [01:50<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 267/585 [01:50<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 268/585 [01:51<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/585 [01:51<01:34,  3.34it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 270/585 [01:51<01:33,  3.37it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 271/585 [01:52<01:32,  3.39it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 272/585 [01:52<01:31,  3.41it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 273/585 [01:52<01:31,  3.42it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 274/585 [01:52<01:30,  3.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 275/585 [01:53<01:30,  3.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 276/585 [01:53<01:29,  3.44it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/585 [01:53<01:29,  3.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 278/585 [01:54<01:29,  3.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 279/585 [01:54<01:28,  3.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 280/585 [01:54<01:28,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 281/585 [01:54<01:28,  3.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 282/585 [01:55<01:28,  3.44it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/585 [01:55<01:27,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 284/585 [01:55<01:27,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/585 [01:56<01:27,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 286/585 [01:56<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 287/585 [01:56<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 288/585 [01:57<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 289/585 [01:57<01:25,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 290/585 [01:57<01:25,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/585 [01:57<01:25,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 292/585 [01:58<01:25,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 293/585 [01:58<01:24,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 294/585 [01:58<01:24,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 295/585 [01:59<01:24,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 296/585 [01:59<01:23,  3.45it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 297/585 [01:59<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 298/585 [01:59<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/585 [02:00<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/585 [02:00<01:22,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/585 [02:00<01:22,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/585 [02:01<01:22,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/585 [02:01<01:22,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/585 [02:01<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/585 [02:01<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306/585 [02:02<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/585 [02:02<01:20,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 308/585 [02:02<01:20,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 309/585 [02:03<01:20,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 310/585 [02:03<01:19,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 311/585 [02:03<01:19,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 312/585 [02:03<01:19,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/585 [02:04<01:19,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 314/585 [02:04<01:18,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 315/585 [02:04<01:18,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/585 [02:05<01:18,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/585 [02:05<01:17,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/585 [02:05<01:17,  3.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/585 [02:06<01:17,  3.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/585 [02:06<01:17,  3.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/585 [02:06<01:16,  3.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 322/585 [02:06<01:16,  3.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 323/585 [02:07<01:16,  3.44it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 324/585 [02:07<01:15,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 325/585 [02:07<01:15,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 326/585 [02:08<01:15,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 327/585 [02:08<01:14,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 328/585 [02:08<01:14,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/585 [02:08<01:14,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 330/585 [02:09<01:14,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 331/585 [02:09<01:13,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 332/585 [02:09<01:13,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 333/585 [02:10<01:13,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 334/585 [02:10<01:12,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/585 [02:10<01:12,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 336/585 [02:10<01:12,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 337/585 [02:11<01:12,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 338/585 [02:11<01:11,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 339/585 [02:11<01:11,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 340/585 [02:12<01:11,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 341/585 [02:12<01:10,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 342/585 [02:12<01:10,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/585 [02:13<01:10,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 344/585 [02:13<01:10,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 345/585 [02:13<01:09,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 346/585 [02:13<01:09,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 347/585 [02:14<01:09,  3.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 348/585 [02:14<01:08,  3.44it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 349/585 [02:14<01:08,  3.44it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 350/585 [02:15<01:08,  3.44it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:15<01:07,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 02:31:01,202 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:31:01,202 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 02:31:01,202 >>   Batch size = 8
{'eval_loss': 0.9780818223953247, 'eval_runtime': 9.3499, 'eval_samples_per_second': 372.091, 'eval_steps_per_second': 46.525, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.18it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.43it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.55it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.85it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.50it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.31it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.07it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.70it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.59it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.61it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.67it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.66it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.64it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.65it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.70it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.72it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.64it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.46it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.47it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.53it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.53it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.57it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.64it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.56it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.61it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.60it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.51it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.56it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.51it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.56it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.58it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.59it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.62it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.56it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.56it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.52it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.54it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.56it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.54it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.53it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.50it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.57it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.65it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.50it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.55it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.59it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.52it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.62it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.57it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.32it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.49it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.52it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.48it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.52it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.52it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.57it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.58it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.56it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.53it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.54it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.55it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.60it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.63it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.47it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.58it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.52it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.64it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.59it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.59it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.56it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.60it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.58it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.48it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.58it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.59it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.54it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.60it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.56it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.53it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.51it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.44it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.45it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.52it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.50it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.50it/s][A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:24<01:07,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:31:10,569 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 02:31:10,594 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:31:12,942 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:31:12,960 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:31:12,977 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 352/585 [02:32<20:21,  5.24s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 353/585 [02:32<14:31,  3.76s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 354/585 [02:32<10:27,  2.72s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 355/585 [02:33<07:37,  1.99s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 356/585 [02:33<05:38,  1.48s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/585 [02:33<04:15,  1.12s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/585 [02:33<03:18,  1.15it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/585 [02:34<02:37,  1.43it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/585 [02:34<02:09,  1.74it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/585 [02:34<01:49,  2.04it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362/585 [02:35<01:35,  2.33it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 363/585 [02:35<01:26,  2.58it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 364/585 [02:35<01:19,  2.78it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 365/585 [02:35<01:14,  2.95it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 366/585 [02:36<01:10,  3.08it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 367/585 [02:36<01:08,  3.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 368/585 [02:36<01:06,  3.26it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 369/585 [02:37<01:05,  3.31it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 370/585 [02:37<01:04,  3.35it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 371/585 [02:37<01:03,  3.38it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/585 [02:37<01:02,  3.40it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 373/585 [02:38<01:02,  3.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/585 [02:38<01:01,  3.42it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375/585 [02:38<01:01,  3.42it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376/585 [02:39<01:00,  3.43it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/585 [02:39<01:00,  3.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/585 [02:39<01:00,  3.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/585 [02:39<00:59,  3.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/585 [02:40<00:59,  3.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 381/585 [02:40<00:59,  3.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 382/585 [02:40<00:58,  3.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 383/585 [02:41<00:58,  3.45it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 384/585 [02:41<00:58,  3.45it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 385/585 [02:41<00:58,  3.45it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 386/585 [02:42<00:58,  3.37it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 387/585 [02:42<00:58,  3.40it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 388/585 [02:42<00:57,  3.41it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 389/585 [02:42<00:57,  3.42it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 390/585 [02:43<00:56,  3.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 391/585 [02:43<00:56,  3.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 392/585 [02:43<00:56,  3.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 393/585 [02:44<00:55,  3.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 394/585 [02:44<00:55,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 395/585 [02:44<00:55,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 396/585 [02:44<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 397/585 [02:45<00:54,  3.43it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 398/585 [02:45<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 399/585 [02:45<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/585 [02:46<00:53,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 401/585 [02:46<00:53,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/585 [02:46<00:53,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 403/585 [02:46<00:52,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 404/585 [02:47<00:52,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 405/585 [02:47<00:52,  3.44it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 406/585 [02:47<00:51,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 407/585 [02:48<00:51,  3.44it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 408/585 [02:48<00:51,  3.41it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 409/585 [02:48<00:51,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 410/585 [02:49<00:51,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 411/585 [02:49<00:50,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 412/585 [02:49<00:50,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 413/585 [02:49<00:50,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 414/585 [02:50<00:49,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 415/585 [02:50<00:49,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 416/585 [02:50<00:49,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417/585 [02:51<00:48,  3.44it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418/585 [02:51<00:48,  3.44it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 419/585 [02:51<00:49,  3.34it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 420/585 [02:51<00:48,  3.37it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 421/585 [02:52<00:48,  3.39it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 422/585 [02:52<00:47,  3.41it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 423/585 [02:52<00:47,  3.42it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/585 [02:53<00:46,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 425/585 [02:53<00:46,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 426/585 [02:53<00:46,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 427/585 [02:53<00:45,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 428/585 [02:54<00:45,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 429/585 [02:54<00:45,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 430/585 [02:54<00:45,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 431/585 [02:55<00:44,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 432/585 [02:55<00:44,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 433/585 [02:55<00:44,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 434/585 [02:56<00:43,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 435/585 [02:56<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 436/585 [02:56<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 437/585 [02:56<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 438/585 [02:57<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 439/585 [02:57<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 440/585 [02:57<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 441/585 [02:58<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 442/585 [02:58<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 443/585 [02:58<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 444/585 [02:58<00:41,  3.43it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 445/585 [02:59<00:40,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 446/585 [02:59<00:40,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 447/585 [02:59<00:40,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 448/585 [03:00<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 449/585 [03:00<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 450/585 [03:00<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 451/585 [03:00<00:38,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 452/585 [03:01<00:38,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 453/585 [03:01<00:38,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 454/585 [03:01<00:38,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 455/585 [03:02<00:37,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 456/585 [03:02<00:37,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 457/585 [03:02<00:37,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 458/585 [03:02<00:36,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 459/585 [03:03<00:36,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 460/585 [03:03<00:36,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 461/585 [03:03<00:35,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 462/585 [03:04<00:35,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 463/585 [03:04<00:35,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 464/585 [03:04<00:35,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 465/585 [03:05<00:34,  3.44it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 466/585 [03:05<00:34,  3.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 467/585 [03:05<00:34,  3.43it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:05<00:34,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 02:31:51,754 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:31:51,754 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 02:31:51,754 >>   Batch size = 8
{'eval_loss': 0.989254891872406, 'eval_runtime': 9.3473, 'eval_samples_per_second': 372.193, 'eval_steps_per_second': 46.538, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.10it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.35it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.50it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.86it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.28it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.06it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.98it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.62it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.59it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.61it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.67it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.65it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.52it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.65it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.63it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.67it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.63it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.55it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.57it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.50it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.55it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.62it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.56it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.63it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.68it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.60it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.61it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.63it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.52it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.39it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.50it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.54it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.56it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.61it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.61it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.67it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.63it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.65it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.60it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.47it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.44it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.52it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.60it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.58it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.62it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.60it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.59it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.61it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.61it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.50it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.50it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.56it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.59it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.60it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.64it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.50it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.56it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.59it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.53it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.49it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.50it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.55it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.60it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.62it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.59it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.59it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.63it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.58it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.56it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.50it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.59it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.55it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.58it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.62it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.56it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.57it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.56it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.56it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.55it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.52it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.51it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.60it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.55it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.57it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.55it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.55it/s][A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:15<00:34,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:32:01,116 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 02:32:01,134 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:32:03,436 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:32:03,451 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:32:03,482 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 469/585 [03:22<10:03,  5.20s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 470/585 [03:22<07:09,  3.73s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 471/585 [03:23<05:07,  2.70s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 472/585 [03:23<03:43,  1.98s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 473/585 [03:23<02:44,  1.47s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 474/585 [03:24<02:03,  1.12s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 475/585 [03:24<01:35,  1.15it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 476/585 [03:24<01:15,  1.44it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 477/585 [03:24<01:01,  1.74it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 478/585 [03:25<00:52,  2.05it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 479/585 [03:25<00:45,  2.33it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 480/585 [03:25<00:40,  2.58it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 481/585 [03:26<00:37,  2.79it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 482/585 [03:26<00:34,  2.95it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 483/585 [03:26<00:33,  3.09it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 484/585 [03:26<00:31,  3.19it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 485/585 [03:27<00:30,  3.26it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 486/585 [03:27<00:29,  3.31it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 487/585 [03:27<00:29,  3.35it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 488/585 [03:28<00:28,  3.38it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 489/585 [03:28<00:28,  3.40it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 490/585 [03:28<00:27,  3.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 491/585 [03:28<00:27,  3.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 492/585 [03:29<00:27,  3.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 493/585 [03:29<00:26,  3.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 494/585 [03:29<00:26,  3.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 495/585 [03:30<00:26,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 496/585 [03:30<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 497/585 [03:30<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 498/585 [03:30<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 499/585 [03:31<00:24,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [03:31<00:24,  3.44it/s]                                                  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [03:31<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 501/585 [03:31<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 502/585 [03:32<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 503/585 [03:32<00:23,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 504/585 [03:32<00:23,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 505/585 [03:33<00:23,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 506/585 [03:33<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 507/585 [03:33<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 508/585 [03:33<00:22,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 509/585 [03:34<00:22,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 510/585 [03:34<00:21,  3.45it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 511/585 [03:34<00:21,  3.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 512/585 [03:35<00:21,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 513/585 [03:35<00:20,  3.45it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 514/585 [03:35<00:20,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 515/585 [03:35<00:20,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 516/585 [03:36<00:20,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 517/585 [03:36<00:19,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 518/585 [03:36<00:19,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 519/585 [03:37<00:19,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 520/585 [03:37<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 521/585 [03:37<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 522/585 [03:37<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 523/585 [03:38<00:18,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 524/585 [03:38<00:17,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 525/585 [03:38<00:17,  3.42it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 526/585 [03:39<00:17,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 527/585 [03:39<00:16,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 528/585 [03:39<00:16,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 529/585 [03:39<00:16,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 530/585 [03:40<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 531/585 [03:40<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 532/585 [03:40<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 533/585 [03:41<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 534/585 [03:41<00:14,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 535/585 [03:41<00:14,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 536/585 [03:42<00:14,  3.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 537/585 [03:42<00:13,  3.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 538/585 [03:42<00:13,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 539/585 [03:42<00:13,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 540/585 [03:43<00:13,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 541/585 [03:43<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 542/585 [03:43<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 543/585 [03:44<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 544/585 [03:44<00:11,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 545/585 [03:44<00:11,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 546/585 [03:44<00:11,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 547/585 [03:45<00:11,  3.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 548/585 [03:45<00:10,  3.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/585 [03:45<00:10,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 550/585 [03:46<00:10,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 551/585 [03:46<00:09,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 552/585 [03:46<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 553/585 [03:46<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 554/585 [03:47<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 555/585 [03:47<00:08,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 556/585 [03:47<00:08,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 557/585 [03:48<00:08,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 558/585 [03:48<00:07,  3.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 559/585 [03:48<00:07,  3.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 560/585 [03:49<00:07,  3.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 561/585 [03:49<00:06,  3.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 562/585 [03:49<00:06,  3.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 563/585 [03:49<00:06,  3.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 564/585 [03:50<00:06,  3.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 565/585 [03:50<00:05,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 566/585 [03:50<00:05,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 567/585 [03:51<00:05,  3.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 568/585 [03:51<00:04,  3.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 569/585 [03:51<00:04,  3.34it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 570/585 [03:51<00:04,  3.37it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 571/585 [03:52<00:04,  3.39it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 572/585 [03:52<00:03,  3.41it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 573/585 [03:52<00:03,  3.42it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 574/585 [03:53<00:03,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 575/585 [03:53<00:02,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 576/585 [03:53<00:02,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 577/585 [03:53<00:02,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 578/585 [03:54<00:02,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 579/585 [03:54<00:01,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 580/585 [03:54<00:01,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 581/585 [03:55<00:01,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 582/585 [03:55<00:00,  3.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 583/585 [03:55<00:00,  3.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 584/585 [03:56<00:00,  3.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [03:56<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 02:32:42,126 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:32:42,126 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 02:32:42,126 >>   Batch size = 8
{'eval_loss': 0.9965202212333679, 'eval_runtime': 9.347, 'eval_samples_per_second': 372.205, 'eval_steps_per_second': 46.539, 'epoch': 4.0}
{'loss': 0.6628, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.90it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.53it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.66it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.91it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.55it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.27it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.95it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.74it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.58it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.62it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.72it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.68it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.68it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.73it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.71it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.67it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.50it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.50it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.51it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.53it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.59it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.62it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.66it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.72it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.70it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.56it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.43it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.46it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.50it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.53it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.53it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.59it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.66it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.68it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.69it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.61it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.51it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.56it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.55it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.57it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.56it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.55it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.63it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.67it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.65it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.55it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.51it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.57it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.59it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.53it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.57it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.56it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.62it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.69it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.62it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.52it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.51it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.55it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.30it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.43it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.48it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.59it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.54it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.59it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.62it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.54it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.56it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.54it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.52it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.56it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.54it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.62it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.60it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.56it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.57it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.54it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.58it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.48it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.53it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.57it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.60it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.59it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.64it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.60it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.54it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.52it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.52it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:05<00:00,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:32:51,481 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 02:32:51,495 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:32:53,762 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:32:53,792 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:32:53,802 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:32:58,379 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:32:58,381 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117 (score: 0.9662191867828369).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:14<00:00,  3.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:14<00:00,  2.30it/s]
[INFO|trainer.py:1894] 2023-08-29 02:33:00,021 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 02:33:00,040 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:33:02,325 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:33:02,352 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:33:02,363 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:33:02,549 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:02,549 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:02,549 >>   train_loss               =     0.6582
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:02,549 >>   train_runtime            = 0:04:14.20
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:02,549 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:02,549 >>   train_samples_per_second =    147.517
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:02,549 >>   train_steps_per_second   =      2.301
{'eval_loss': 0.9989596009254456, 'eval_runtime': 9.3406, 'eval_samples_per_second': 372.46, 'eval_steps_per_second': 46.571, 'epoch': 5.0}
{'train_runtime': 254.2074, 'train_samples_per_second': 147.517, 'train_steps_per_second': 2.301, 'train_loss': 0.65821044139373, 'epoch': 5.0}
08/29/2023 02:33:02 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:33:02,589 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:33:02,589 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 02:33:02,589 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 57.89it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 50.99it/s]  4%|â–         | 18/435 [00:00<00:08, 49.03it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.41it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.86it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.63it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.47it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.25it/s] 11%|â–ˆ         | 48/435 [00:00<00:08, 47.09it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.92it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.94it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.94it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 47.00it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.95it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.96it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 47.01it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.86it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.87it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.81it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.74it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.81it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.75it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.82it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.81it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.85it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.89it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.77it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.75it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.74it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.78it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.71it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.77it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.89it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.85it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.82it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.85it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.68it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.78it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.79it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.74it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.77it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.75it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.77it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.86it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.80it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.74it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.64it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.71it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:03, 46.76it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.78it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.75it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.77it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.84it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.84it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.79it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.78it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.63it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.71it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.72it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.76it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.76it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.73it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.71it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.81it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.81it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.71it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.68it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.70it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.76it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.74it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.75it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.60it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.67it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.69it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.77it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.65it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.70it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.71it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.74it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.78it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.70it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.70it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.71it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.78it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.66it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.88it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:33:11,895 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:11,895 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:11,895 >>   eval_loss               =     0.9662
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:11,895 >>   eval_runtime            = 0:00:09.30
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:11,895 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:11,895 >>   eval_samples_per_second =    373.838
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:11,895 >>   eval_steps_per_second   =     46.743
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:33:11,895 >>   perplexity              =      2.628
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:18,694 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:18,696 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:18,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:18,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:18,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:33:19,328 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:33:19,329 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:33:19,906 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:33:20,918 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:33:20,918 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:23,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:23,764 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:23,764 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:23,764 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:33:23,764 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:33:24,398 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:33:24,399 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:33:24,972 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:33:25,134 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:33:25,134 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.22it/s]Extractor Predicting: 2it [00:01,  1.19it/s]Extractor Predicting: 3it [00:02,  1.19it/s]Extractor Predicting: 4it [00:03,  1.19it/s]Extractor Predicting: 5it [00:04,  1.18it/s]Extractor Predicting: 6it [00:05,  1.16it/s]Extractor Predicting: 7it [00:05,  1.16it/s]Extractor Predicting: 8it [00:06,  1.18it/s]Extractor Predicting: 9it [00:07,  1.17it/s]Extractor Predicting: 10it [00:08,  1.21it/s]Extractor Predicting: 11it [00:09,  1.19it/s]Extractor Predicting: 12it [00:10,  1.17it/s]Extractor Predicting: 13it [00:11,  1.16it/s]Extractor Predicting: 14it [00:11,  1.15it/s]Extractor Predicting: 15it [00:12,  1.19it/s]Extractor Predicting: 16it [00:13,  1.17it/s]Extractor Predicting: 17it [00:14,  1.20it/s]Extractor Predicting: 18it [00:15,  1.23it/s]Extractor Predicting: 19it [00:16,  1.20it/s]Extractor Predicting: 20it [00:16,  1.20it/s]Extractor Predicting: 21it [00:17,  1.21it/s]Extractor Predicting: 22it [00:18,  1.22it/s]Extractor Predicting: 23it [00:19,  1.22it/s]Extractor Predicting: 24it [00:20,  1.20it/s]Extractor Predicting: 25it [00:21,  1.18it/s]Extractor Predicting: 26it [00:21,  1.16it/s]Extractor Predicting: 27it [00:22,  1.16it/s]Extractor Predicting: 28it [00:23,  1.19it/s]Extractor Predicting: 29it [00:24,  1.18it/s]Extractor Predicting: 30it [00:25,  1.15it/s]Extractor Predicting: 31it [00:26,  1.16it/s]Extractor Predicting: 32it [00:27,  1.17it/s]Extractor Predicting: 33it [00:27,  1.17it/s]Extractor Predicting: 34it [00:28,  1.20it/s]Extractor Predicting: 35it [00:29,  1.20it/s]Extractor Predicting: 36it [00:30,  1.21it/s]Extractor Predicting: 37it [00:31,  1.21it/s]Extractor Predicting: 38it [00:31,  1.22it/s]Extractor Predicting: 39it [00:32,  1.23it/s]Extractor Predicting: 40it [00:33,  1.21it/s]Extractor Predicting: 41it [00:34,  1.22it/s]Extractor Predicting: 42it [00:35,  1.22it/s]Extractor Predicting: 43it [00:36,  1.23it/s]Extractor Predicting: 44it [00:36,  1.25it/s]Extractor Predicting: 45it [00:37,  1.23it/s]Extractor Predicting: 46it [00:38,  1.22it/s]Extractor Predicting: 47it [00:39,  1.21it/s]Extractor Predicting: 48it [00:40,  1.22it/s]Extractor Predicting: 49it [00:40,  1.21it/s]Extractor Predicting: 50it [00:41,  1.22it/s]Extractor Predicting: 51it [00:42,  1.23it/s]Extractor Predicting: 52it [00:43,  1.23it/s]Extractor Predicting: 53it [00:44,  1.23it/s]Extractor Predicting: 54it [00:45,  1.22it/s]Extractor Predicting: 55it [00:45,  1.19it/s]Extractor Predicting: 56it [00:46,  1.22it/s]Extractor Predicting: 57it [00:47,  1.20it/s]Extractor Predicting: 58it [00:48,  1.13it/s]Extractor Predicting: 59it [00:49,  1.18it/s]Extractor Predicting: 60it [00:50,  1.22it/s]Extractor Predicting: 61it [00:50,  1.24it/s]Extractor Predicting: 62it [00:51,  1.22it/s]Extractor Predicting: 63it [00:52,  1.21it/s]Extractor Predicting: 64it [00:53,  1.22it/s]Extractor Predicting: 65it [00:54,  1.23it/s]Extractor Predicting: 66it [00:55,  1.23it/s]Extractor Predicting: 67it [00:55,  1.23it/s]Extractor Predicting: 68it [00:56,  1.23it/s]Extractor Predicting: 69it [00:57,  1.27it/s]Extractor Predicting: 70it [00:58,  1.27it/s]Extractor Predicting: 71it [00:58,  1.28it/s]Extractor Predicting: 72it [00:59,  1.25it/s]Extractor Predicting: 73it [01:00,  1.26it/s]Extractor Predicting: 74it [01:01,  1.24it/s]Extractor Predicting: 75it [01:02,  1.22it/s]Extractor Predicting: 76it [01:03,  1.21it/s]Extractor Predicting: 77it [01:03,  1.23it/s]Extractor Predicting: 78it [01:04,  1.25it/s]Extractor Predicting: 79it [01:05,  1.27it/s]Extractor Predicting: 80it [01:06,  1.26it/s]Extractor Predicting: 81it [01:06,  1.25it/s]Extractor Predicting: 82it [01:07,  1.24it/s]Extractor Predicting: 83it [01:08,  1.25it/s]Extractor Predicting: 84it [01:09,  1.26it/s]Extractor Predicting: 85it [01:10,  1.26it/s]Extractor Predicting: 86it [01:10,  1.27it/s]Extractor Predicting: 87it [01:11,  1.29it/s]Extractor Predicting: 88it [01:12,  1.28it/s]Extractor Predicting: 89it [01:13,  1.28it/s]Extractor Predicting: 90it [01:14,  1.29it/s]Extractor Predicting: 91it [01:14,  1.31it/s]Extractor Predicting: 92it [01:15,  1.34it/s]Extractor Predicting: 93it [01:16,  1.31it/s]Extractor Predicting: 94it [01:17,  1.30it/s]Extractor Predicting: 95it [01:17,  1.31it/s]Extractor Predicting: 96it [01:18,  1.30it/s]Extractor Predicting: 97it [01:19,  1.30it/s]Extractor Predicting: 98it [01:20,  1.29it/s]Extractor Predicting: 99it [01:20,  1.27it/s]Extractor Predicting: 100it [01:21,  1.24it/s]Extractor Predicting: 101it [01:22,  1.24it/s]Extractor Predicting: 102it [01:23,  1.30it/s]Extractor Predicting: 103it [01:24,  1.31it/s]Extractor Predicting: 104it [01:24,  1.30it/s]Extractor Predicting: 105it [01:25,  1.30it/s]Extractor Predicting: 106it [01:26,  1.29it/s]Extractor Predicting: 107it [01:27,  1.29it/s]Extractor Predicting: 108it [01:27,  1.29it/s]Extractor Predicting: 109it [01:28,  1.29it/s]Extractor Predicting: 110it [01:29,  1.29it/s]Extractor Predicting: 111it [01:30,  1.29it/s]Extractor Predicting: 112it [01:31,  1.30it/s]Extractor Predicting: 113it [01:31,  1.33it/s]Extractor Predicting: 114it [01:32,  1.33it/s]Extractor Predicting: 115it [01:33,  1.34it/s]Extractor Predicting: 116it [01:34,  1.31it/s]Extractor Predicting: 117it [01:34,  1.27it/s]Extractor Predicting: 118it [01:35,  1.28it/s]Extractor Predicting: 119it [01:36,  1.24it/s]Extractor Predicting: 120it [01:37,  1.26it/s]Extractor Predicting: 121it [01:38,  1.25it/s]Extractor Predicting: 122it [01:38,  1.23it/s]Extractor Predicting: 123it [01:39,  1.24it/s]Extractor Predicting: 124it [01:40,  1.25it/s]Extractor Predicting: 125it [01:41,  1.27it/s]Extractor Predicting: 126it [01:42,  1.24it/s]Extractor Predicting: 127it [01:42,  1.27it/s]Extractor Predicting: 128it [01:43,  1.26it/s]Extractor Predicting: 129it [01:44,  1.25it/s]Extractor Predicting: 130it [01:45,  1.23it/s]Extractor Predicting: 131it [01:46,  1.25it/s]Extractor Predicting: 132it [01:46,  1.24it/s]Extractor Predicting: 133it [01:47,  1.22it/s]Extractor Predicting: 134it [01:48,  1.21it/s]Extractor Predicting: 135it [01:49,  1.23it/s]Extractor Predicting: 136it [01:50,  1.22it/s]Extractor Predicting: 137it [01:51,  1.22it/s]Extractor Predicting: 138it [01:51,  1.20it/s]Extractor Predicting: 139it [01:52,  1.14it/s]Extractor Predicting: 140it [01:53,  1.15it/s]Extractor Predicting: 141it [01:54,  1.18it/s]Extractor Predicting: 142it [01:55,  1.18it/s]Extractor Predicting: 143it [01:56,  1.19it/s]Extractor Predicting: 144it [01:56,  1.47it/s]Extractor Predicting: 144it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:29,198 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:29,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:29,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:29,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:29,207 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:35:29,893 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:35:29,894 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:35:30,466 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:35:31,501 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:35:31,501 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:34,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:34,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:34,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:34,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:35:34,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:35:35,035 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:35:35,036 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:35:35,602 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:35:35,762 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:35:35,762 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.14883720930232558,
  "recall": 0.027594136246047715,
  "score": 0.04655674102812803,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.24it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.26it/s]Extractor Predicting: 5it [00:03,  1.28it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.34it/s]Extractor Predicting: 9it [00:06,  1.30it/s]Extractor Predicting: 10it [00:07,  1.31it/s]Extractor Predicting: 11it [00:08,  1.31it/s]Extractor Predicting: 12it [00:09,  1.29it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.30it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:12,  1.30it/s]Extractor Predicting: 17it [00:13,  1.28it/s]Extractor Predicting: 18it [00:13,  1.30it/s]Extractor Predicting: 19it [00:14,  1.33it/s]Extractor Predicting: 20it [00:15,  1.29it/s]Extractor Predicting: 21it [00:16,  1.31it/s]Extractor Predicting: 22it [00:16,  1.32it/s]Extractor Predicting: 23it [00:17,  1.32it/s]Extractor Predicting: 24it [00:18,  1.30it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.31it/s]Extractor Predicting: 27it [00:20,  1.28it/s]Extractor Predicting: 28it [00:21,  1.29it/s]Extractor Predicting: 29it [00:22,  1.30it/s]Extractor Predicting: 30it [00:23,  1.31it/s]Extractor Predicting: 31it [00:23,  1.29it/s]Extractor Predicting: 32it [00:24,  1.32it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:26,  1.29it/s]Extractor Predicting: 36it [00:27,  1.30it/s]Extractor Predicting: 37it [00:28,  1.32it/s]Extractor Predicting: 38it [00:29,  1.32it/s]Extractor Predicting: 39it [00:29,  1.31it/s]Extractor Predicting: 40it [00:30,  1.29it/s]Extractor Predicting: 41it [00:31,  1.29it/s]Extractor Predicting: 42it [00:32,  1.33it/s]Extractor Predicting: 43it [00:33,  1.21it/s]Extractor Predicting: 44it [00:34,  1.22it/s]Extractor Predicting: 45it [00:34,  1.24it/s]Extractor Predicting: 46it [00:35,  1.23it/s]Extractor Predicting: 47it [00:36,  1.26it/s]Extractor Predicting: 48it [00:37,  1.25it/s]Extractor Predicting: 49it [00:37,  1.28it/s]Extractor Predicting: 50it [00:38,  1.27it/s]Extractor Predicting: 51it [00:39,  1.26it/s]Extractor Predicting: 52it [00:40,  1.25it/s]Extractor Predicting: 53it [00:41,  1.26it/s]Extractor Predicting: 54it [00:41,  1.25it/s]Extractor Predicting: 55it [00:42,  1.28it/s]Extractor Predicting: 56it [00:43,  1.27it/s]Extractor Predicting: 57it [00:44,  1.25it/s]Extractor Predicting: 58it [00:45,  1.25it/s]Extractor Predicting: 59it [00:45,  1.25it/s]Extractor Predicting: 60it [00:46,  1.24it/s]Extractor Predicting: 61it [00:47,  1.24it/s]Extractor Predicting: 62it [00:48,  1.26it/s]Extractor Predicting: 63it [00:49,  1.26it/s]Extractor Predicting: 64it [00:49,  1.29it/s]Extractor Predicting: 65it [00:50,  1.26it/s]Extractor Predicting: 66it [00:51,  1.24it/s]Extractor Predicting: 67it [00:52,  1.24it/s]Extractor Predicting: 68it [00:53,  1.24it/s]Extractor Predicting: 69it [00:53,  1.23it/s]Extractor Predicting: 70it [00:54,  1.24it/s]Extractor Predicting: 71it [00:55,  1.24it/s]Extractor Predicting: 72it [00:56,  1.22it/s]Extractor Predicting: 73it [00:57,  1.24it/s]Extractor Predicting: 74it [00:57,  1.25it/s]Extractor Predicting: 75it [00:58,  1.26it/s]Extractor Predicting: 76it [00:59,  1.24it/s]Extractor Predicting: 77it [01:00,  1.28it/s]Extractor Predicting: 78it [01:01,  1.27it/s]Extractor Predicting: 79it [01:01,  1.30it/s]Extractor Predicting: 80it [01:02,  1.32it/s]Extractor Predicting: 81it [01:03,  1.31it/s]Extractor Predicting: 82it [01:04,  1.31it/s]Extractor Predicting: 83it [01:04,  1.28it/s]Extractor Predicting: 84it [01:05,  1.27it/s]Extractor Predicting: 85it [01:06,  1.24it/s]Extractor Predicting: 86it [01:07,  1.21it/s]Extractor Predicting: 87it [01:08,  1.22it/s]Extractor Predicting: 88it [01:09,  1.22it/s]Extractor Predicting: 89it [01:09,  1.21it/s]Extractor Predicting: 90it [01:10,  1.20it/s]Extractor Predicting: 91it [01:11,  1.19it/s]Extractor Predicting: 92it [01:12,  1.22it/s]Extractor Predicting: 93it [01:13,  1.23it/s]Extractor Predicting: 94it [01:14,  1.24it/s]Extractor Predicting: 95it [01:14,  1.25it/s]Extractor Predicting: 96it [01:15,  1.21it/s]Extractor Predicting: 97it [01:16,  1.20it/s]Extractor Predicting: 98it [01:17,  1.19it/s]Extractor Predicting: 99it [01:18,  1.22it/s]Extractor Predicting: 100it [01:18,  1.22it/s]Extractor Predicting: 101it [01:19,  1.25it/s]Extractor Predicting: 102it [01:20,  1.23it/s]Extractor Predicting: 103it [01:21,  1.21it/s]Extractor Predicting: 104it [01:22,  1.24it/s]Extractor Predicting: 105it [01:23,  1.23it/s]Extractor Predicting: 106it [01:23,  1.23it/s]Extractor Predicting: 107it [01:24,  1.25it/s]Extractor Predicting: 108it [01:25,  1.23it/s]Extractor Predicting: 109it [01:26,  1.23it/s]Extractor Predicting: 110it [01:27,  1.22it/s]Extractor Predicting: 111it [01:27,  1.24it/s]Extractor Predicting: 112it [01:28,  1.22it/s]Extractor Predicting: 113it [01:29,  1.23it/s]Extractor Predicting: 114it [01:30,  1.24it/s]Extractor Predicting: 115it [01:31,  1.20it/s]Extractor Predicting: 116it [01:31,  1.23it/s]Extractor Predicting: 117it [01:32,  1.22it/s]Extractor Predicting: 118it [01:33,  1.22it/s]Extractor Predicting: 119it [01:34,  1.22it/s]Extractor Predicting: 120it [01:35,  1.24it/s]Extractor Predicting: 121it [01:36,  1.25it/s]Extractor Predicting: 122it [01:36,  1.26it/s]Extractor Predicting: 123it [01:37,  1.25it/s]Extractor Predicting: 124it [01:38,  1.27it/s]Extractor Predicting: 125it [01:39,  1.29it/s]Extractor Predicting: 126it [01:39,  1.28it/s]Extractor Predicting: 127it [01:40,  1.27it/s]Extractor Predicting: 128it [01:41,  1.30it/s]Extractor Predicting: 129it [01:42,  1.30it/s]Extractor Predicting: 130it [01:43,  1.27it/s]Extractor Predicting: 131it [01:43,  1.31it/s]Extractor Predicting: 132it [01:44,  1.31it/s]Extractor Predicting: 133it [01:45,  1.33it/s]Extractor Predicting: 134it [01:46,  1.28it/s]Extractor Predicting: 135it [01:46,  1.31it/s]Extractor Predicting: 136it [01:47,  1.32it/s]Extractor Predicting: 137it [01:48,  1.33it/s]Extractor Predicting: 138it [01:49,  1.31it/s]Extractor Predicting: 139it [01:49,  1.30it/s]Extractor Predicting: 140it [01:50,  1.31it/s]Extractor Predicting: 141it [01:51,  1.28it/s]Extractor Predicting: 142it [01:52,  1.28it/s]Extractor Predicting: 143it [01:52,  1.29it/s]Extractor Predicting: 144it [01:53,  1.28it/s]Extractor Predicting: 145it [01:54,  1.32it/s]Extractor Predicting: 146it [01:55,  1.34it/s]Extractor Predicting: 147it [01:55,  1.34it/s]Extractor Predicting: 148it [01:56,  1.36it/s]Extractor Predicting: 149it [01:57,  1.35it/s]Extractor Predicting: 150it [01:58,  1.36it/s]Extractor Predicting: 151it [01:58,  1.37it/s]Extractor Predicting: 152it [01:59,  1.38it/s]Extractor Predicting: 153it [02:00,  1.36it/s]Extractor Predicting: 154it [02:01,  1.36it/s]Extractor Predicting: 155it [02:01,  1.28it/s]Extractor Predicting: 156it [02:02,  1.30it/s]Extractor Predicting: 157it [02:03,  1.37it/s]Extractor Predicting: 158it [02:03,  1.41it/s]Extractor Predicting: 159it [02:04,  1.38it/s]Extractor Predicting: 160it [02:05,  1.35it/s]Extractor Predicting: 161it [02:06,  1.34it/s]Extractor Predicting: 162it [02:06,  1.36it/s]Extractor Predicting: 163it [02:07,  1.39it/s]Extractor Predicting: 164it [02:08,  1.38it/s]Extractor Predicting: 165it [02:09,  1.39it/s]Extractor Predicting: 166it [02:09,  1.36it/s]Extractor Predicting: 167it [02:10,  1.39it/s]Extractor Predicting: 168it [02:11,  1.37it/s]Extractor Predicting: 169it [02:11,  1.42it/s]Extractor Predicting: 170it [02:12,  1.41it/s]Extractor Predicting: 171it [02:13,  1.41it/s]Extractor Predicting: 172it [02:14,  1.35it/s]Extractor Predicting: 173it [02:14,  1.32it/s]Extractor Predicting: 174it [02:15,  1.29it/s]Extractor Predicting: 175it [02:16,  1.24it/s]Extractor Predicting: 176it [02:17,  1.25it/s]Extractor Predicting: 177it [02:18,  1.25it/s]Extractor Predicting: 178it [02:19,  1.24it/s]Extractor Predicting: 179it [02:19,  1.24it/s]Extractor Predicting: 180it [02:20,  1.24it/s]Extractor Predicting: 181it [02:21,  1.24it/s]Extractor Predicting: 182it [02:22,  1.24it/s]Extractor Predicting: 183it [02:23,  1.24it/s]Extractor Predicting: 184it [02:23,  1.23it/s]Extractor Predicting: 185it [02:24,  1.21it/s]Extractor Predicting: 186it [02:25,  1.21it/s]Extractor Predicting: 187it [02:26,  1.21it/s]Extractor Predicting: 188it [02:27,  1.22it/s]Extractor Predicting: 189it [02:28,  1.22it/s]Extractor Predicting: 190it [02:28,  1.24it/s]Extractor Predicting: 191it [02:29,  1.21it/s]Extractor Predicting: 192it [02:30,  1.19it/s]Extractor Predicting: 193it [02:31,  1.20it/s]Extractor Predicting: 194it [02:32,  1.19it/s]Extractor Predicting: 195it [02:33,  1.21it/s]Extractor Predicting: 196it [02:33,  1.21it/s]Extractor Predicting: 197it [02:34,  1.21it/s]Extractor Predicting: 198it [02:35,  1.23it/s]Extractor Predicting: 199it [02:36,  1.24it/s]Extractor Predicting: 200it [02:37,  1.22it/s]Extractor Predicting: 201it [02:37,  1.20it/s]Extractor Predicting: 202it [02:38,  1.27it/s]Extractor Predicting: 203it [02:39,  1.25it/s]Extractor Predicting: 204it [02:40,  1.27it/s]Extractor Predicting: 205it [02:41,  1.26it/s]Extractor Predicting: 206it [02:41,  1.25it/s]Extractor Predicting: 207it [02:42,  1.23it/s]Extractor Predicting: 208it [02:43,  1.23it/s]Extractor Predicting: 209it [02:44,  1.19it/s]Extractor Predicting: 210it [02:45,  1.20it/s]Extractor Predicting: 211it [02:46,  1.20it/s]Extractor Predicting: 212it [02:46,  1.20it/s]Extractor Predicting: 213it [02:47,  1.20it/s]Extractor Predicting: 214it [02:48,  1.19it/s]Extractor Predicting: 215it [02:49,  1.22it/s]Extractor Predicting: 216it [02:50,  1.20it/s]Extractor Predicting: 217it [02:51,  1.21it/s]Extractor Predicting: 218it [02:51,  1.21it/s]Extractor Predicting: 219it [02:52,  1.20it/s]Extractor Predicting: 220it [02:53,  1.21it/s]Extractor Predicting: 221it [02:54,  1.19it/s]Extractor Predicting: 222it [02:55,  1.18it/s]Extractor Predicting: 223it [02:56,  1.21it/s]Extractor Predicting: 224it [02:56,  1.21it/s]Extractor Predicting: 225it [02:57,  1.22it/s]Extractor Predicting: 226it [02:58,  1.21it/s]Extractor Predicting: 227it [02:59,  1.25it/s]Extractor Predicting: 228it [03:00,  1.24it/s]Extractor Predicting: 229it [03:00,  1.24it/s]Extractor Predicting: 230it [03:01,  1.28it/s]Extractor Predicting: 231it [03:02,  1.28it/s]Extractor Predicting: 232it [03:03,  1.30it/s]Extractor Predicting: 233it [03:03,  1.28it/s]Extractor Predicting: 234it [03:04,  1.28it/s]Extractor Predicting: 235it [03:05,  1.27it/s]Extractor Predicting: 236it [03:06,  1.25it/s]Extractor Predicting: 237it [03:07,  1.23it/s]Extractor Predicting: 238it [03:07,  1.24it/s]Extractor Predicting: 239it [03:08,  1.23it/s]Extractor Predicting: 240it [03:09,  1.23it/s]Extractor Predicting: 241it [03:10,  1.19it/s]Extractor Predicting: 242it [03:11,  1.22it/s]Extractor Predicting: 243it [03:12,  1.21it/s]Extractor Predicting: 244it [03:13,  1.13it/s]Extractor Predicting: 245it [03:13,  1.17it/s]Extractor Predicting: 246it [03:14,  1.18it/s]Extractor Predicting: 247it [03:15,  1.20it/s]Extractor Predicting: 248it [03:16,  1.21it/s]Extractor Predicting: 249it [03:17,  1.20it/s]Extractor Predicting: 250it [03:18,  1.23it/s]Extractor Predicting: 251it [03:18,  1.23it/s]Extractor Predicting: 252it [03:19,  1.24it/s]Extractor Predicting: 253it [03:20,  1.22it/s]Extractor Predicting: 254it [03:21,  1.24it/s]Extractor Predicting: 255it [03:22,  1.21it/s]Extractor Predicting: 256it [03:23,  1.18it/s]Extractor Predicting: 257it [03:23,  1.19it/s]Extractor Predicting: 258it [03:24,  1.18it/s]Extractor Predicting: 259it [03:25,  1.20it/s]Extractor Predicting: 260it [03:26,  1.19it/s]Extractor Predicting: 261it [03:27,  1.22it/s]Extractor Predicting: 262it [03:27,  1.20it/s]Extractor Predicting: 263it [03:28,  1.19it/s]Extractor Predicting: 264it [03:29,  1.19it/s]Extractor Predicting: 265it [03:30,  1.15it/s]Extractor Predicting: 266it [03:31,  1.16it/s]Extractor Predicting: 267it [03:32,  1.17it/s]Extractor Predicting: 268it [03:33,  1.15it/s]Extractor Predicting: 269it [03:34,  1.18it/s]Extractor Predicting: 270it [03:34,  1.19it/s]Extractor Predicting: 271it [03:35,  1.17it/s]Extractor Predicting: 272it [03:36,  1.18it/s]Extractor Predicting: 273it [03:37,  1.19it/s]Extractor Predicting: 274it [03:38,  1.21it/s]Extractor Predicting: 275it [03:38,  1.23it/s]Extractor Predicting: 276it [03:39,  1.26it/s]Extractor Predicting: 277it [03:40,  1.25it/s]Extractor Predicting: 278it [03:41,  1.25it/s]Extractor Predicting: 279it [03:42,  1.24it/s]Extractor Predicting: 280it [03:43,  1.20it/s]Extractor Predicting: 281it [03:43,  1.20it/s]Extractor Predicting: 282it [03:44,  1.28it/s]Extractor Predicting: 282it [03:44,  1.26it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:28,788 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:28,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:28,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:28,790 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:28,791 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:39:29,408 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:39:29,409 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:39:29,994 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:39:31,047 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:39:31,049 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:34,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:34,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:34,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:34,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:39:34,562 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:39:35,214 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:39:35,215 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:39:35,779 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:39:35,967 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:39:35,968 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.34004559270516715,
  "recall": 0.13239644970414202,
  "score": 0.190587734241908,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.17it/s]Extractor Predicting: 2it [00:01,  1.11it/s]Extractor Predicting: 3it [00:02,  1.14it/s]Extractor Predicting: 4it [00:03,  1.16it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 5it [00:04,  1.17it/s]
[INFO|configuration_utils.py:515] 2023-08-29 02:39:40,808 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:39:40,809 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:39:40,814 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:39:40,814 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 02:39:40,820 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:39:43,860 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 02:39:43,861 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 02:39:43,871 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:39:43,871 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:39:43,878 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:39:43,880 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:39:43,880 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:39:43,880 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:39:43,880 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:39:43,881 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:39:43,881 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4166666666666667,
  "recall": 0.0625,
  "score": 0.10869565217391304,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 02:39:44,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:44,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:45,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:46,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:47,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:48,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:49,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:50,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:51,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:52,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:52,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:53,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:54,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:55,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:56,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:57,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:58,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:58,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:39:59,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:00,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:01,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:02,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:03,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:04,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:21<04:57, 21.27s/it][WARNING|generation_utils.py:914] 2023-08-29 02:40:05,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:06,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:07,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:09,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:10,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:11,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:12,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:13,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:14,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:15,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:16,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:16,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:18,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:18,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:19,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:20,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:21,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:22,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:23,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:24,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:41<04:27, 20.56s/it][WARNING|generation_utils.py:914] 2023-08-29 02:40:25,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:26,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:27,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:28,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:29,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:31,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:31,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:32,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:33,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:34,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:35,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:36,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:37,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:38,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:39,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:39,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:41,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:42,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:43,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:43,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:44,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:45,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [01:02<04:10, 20.86s/it][WARNING|generation_utils.py:914] 2023-08-29 02:40:46,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:47,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:48,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:49,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:50,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:51,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:52,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:53,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:54,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:55,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:56,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:57,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:57,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:58,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:40:59,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:00,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:01,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:02,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:03,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:04,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:05,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:06,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:23<03:49, 20.90s/it][WARNING|generation_utils.py:914] 2023-08-29 02:41:07,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:08,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:09,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:10,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:11,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:12,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:12,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:13,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:14,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:15,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:16,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:17,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:18,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:19,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:20,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:20,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:21,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:22,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:23,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:24,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:25,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [01:42<03:21, 20.16s/it][WARNING|generation_utils.py:914] 2023-08-29 02:41:26,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:27,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:28,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:29,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:30,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:31,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:32,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:33,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:34,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:35,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:36,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:37,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:38,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:39,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:40,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:41,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:42,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:43,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:43,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:44,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:46,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [02:02<03:02, 20.28s/it][WARNING|generation_utils.py:914] 2023-08-29 02:41:47,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:47,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:48,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:49,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:50,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:51,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:51,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:52,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:53,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:54,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:55,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:56,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:57,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:58,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:59,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:41:59,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:00,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:01,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:02,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:03,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:04,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:21<02:37, 19.63s/it][WARNING|generation_utils.py:914] 2023-08-29 02:42:05,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:06,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:07,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:07,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:09,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:10,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:11,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:12,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:13,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:14,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:15,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:16,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:17,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:18,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:19,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:20,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:21,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:22,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:22,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:23,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:24,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [02:41<02:19, 19.93s/it][WARNING|generation_utils.py:914] 2023-08-29 02:42:25,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:26,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:27,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:28,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:29,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:30,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:31,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:32,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:33,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:34,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:35,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:36,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:36,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:37,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:38,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:39,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:40,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:41,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:42,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:43,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:44,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [03:01<01:58, 19.76s/it][WARNING|generation_utils.py:914] 2023-08-29 02:42:45,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:46,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:47,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:48,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:49,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:50,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:50,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:51,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:52,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:53,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:54,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:55,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:56,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:57,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:58,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:42:59,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:00,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:01,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:02,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:03,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:04,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [03:21<01:39, 19.89s/it][WARNING|generation_utils.py:914] 2023-08-29 02:43:05,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:06,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:07,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:08,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:09,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:09,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:10,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:11,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:12,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:13,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:14,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:15,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:15,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:16,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:17,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:18,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:19,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:20,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:21,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:22,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:23,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:24,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [03:41<01:19, 19.98s/it][WARNING|generation_utils.py:914] 2023-08-29 02:43:25,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:26,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:27,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:28,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:28,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:29,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:30,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:31,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:32,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:33,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:33,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:34,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:35,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:36,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:37,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:38,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:39,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:40,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:40,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:41,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:42,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:43,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [04:00<00:58, 19.57s/it][WARNING|generation_utils.py:914] 2023-08-29 02:43:44,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:45,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:46,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:46,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:47,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:48,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:49,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:50,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:51,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:52,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:53,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:53,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:54,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:55,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:56,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:57,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:57,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:58,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:43:59,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:00,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:01,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [04:18<00:38, 19.12s/it][WARNING|generation_utils.py:914] 2023-08-29 02:44:02,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:03,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:04,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:04,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:05,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:06,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:07,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:08,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:09,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:09,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:10,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:11,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:12,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:13,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:14,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:14,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:15,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:16,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:17,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:18,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:19,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [04:35<00:18, 18.70s/it][WARNING|generation_utils.py:914] 2023-08-29 02:44:20,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:21,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:22,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:22,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:23,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:24,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:25,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:27,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:28,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:29,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:30,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:31,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:32,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:33,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:34,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:35,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:36,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:37,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:38,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:38,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:39,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:40,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:44:41,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:58<00:00, 19.91s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:58<00:00, 19.91s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:49,256 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:49,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:49,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:49,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:49,262 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:44:49,568 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:44:49,569 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:44:49,837 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:44:50,909 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:44:50,910 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:53,041 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:53,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:53,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:53,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:44:53,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:44:53,789 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:44:53,790 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:44:54,050 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:44:54,217 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:44:54,217 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : Later in 2008 , he appeared in a comedy called " The Simpsons " as the character Bubba Bubba , in which Bart starts eating the milk . Head Entity : The Simpsons , Tail Entity : American .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 277, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 402, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 479, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 532, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : country .', 'success_rate': 0.7877604166666666, 'errors': {'', '(\'Bad Boyz\', \'country\', \'\', \'He joined the band " Bad Boyz " in 1980 .\')', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9047619047619048, 'errors': {'', 'too many values to unpack (expected 2)', '(\'The Faint Fart Band\', \'followed by\', \'\', \'She was also the lead singer of The Faint Fart Band on their debut debut album , " Faint Fart Band " .\')'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : genre .', 'success_rate': 0.8678977272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 550, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9017857142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8973214285714286, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 579, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8988095238095238, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : instrument .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : occupation .', 'success_rate': 0.8636363636363636, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : participating team .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.8943452380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Safari', 'platform', '', 'On Windows , the browser is run on Windows XP using the iTunes Store app and the Safari browser on iOS .')"}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/3_ext.jsonl'}}
estimate vocab size: 10004
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10104, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.22it/s]Extractor Estimating: 2it [00:01,  1.09it/s]Extractor Estimating: 3it [00:02,  1.12it/s]Extractor Estimating: 4it [00:03,  1.17it/s]Extractor Estimating: 5it [00:04,  1.20it/s]Extractor Estimating: 6it [00:04,  1.27it/s]Extractor Estimating: 7it [00:05,  1.30it/s]Extractor Estimating: 8it [00:06,  1.31it/s]Extractor Estimating: 9it [00:07,  1.36it/s]Extractor Estimating: 10it [00:07,  1.36it/s]Extractor Estimating: 11it [00:08,  1.37it/s]Extractor Estimating: 12it [00:09,  1.35it/s]Extractor Estimating: 13it [00:10,  1.34it/s]Extractor Estimating: 14it [00:10,  1.31it/s]Extractor Estimating: 15it [00:11,  1.32it/s]Extractor Estimating: 16it [00:12,  1.35it/s]Extractor Estimating: 17it [00:13,  1.30it/s]Extractor Estimating: 18it [00:13,  1.36it/s]Extractor Estimating: 19it [00:14,  1.40it/s]Extractor Estimating: 20it [00:15,  1.36it/s]Extractor Estimating: 21it [00:16,  1.19it/s]Extractor Estimating: 22it [00:17,  1.21it/s]Extractor Estimating: 23it [00:17,  1.26it/s]Extractor Estimating: 24it [00:18,  1.25it/s]Extractor Estimating: 25it [00:19,  1.25it/s]Extractor Estimating: 26it [00:20,  1.27it/s]Extractor Estimating: 27it [00:21,  1.20it/s]Extractor Estimating: 28it [00:22,  1.09it/s]Extractor Estimating: 29it [00:23,  1.12it/s]Extractor Estimating: 30it [00:23,  1.19it/s]Extractor Estimating: 31it [00:24,  1.23it/s]Extractor Estimating: 32it [00:25,  1.21it/s]Extractor Estimating: 33it [00:26,  1.24it/s]Extractor Estimating: 34it [00:27,  1.23it/s]Extractor Estimating: 35it [00:27,  1.25it/s]Extractor Estimating: 36it [00:28,  1.26it/s]Extractor Estimating: 37it [00:29,  1.25it/s]Extractor Estimating: 38it [00:30,  1.27it/s]Extractor Estimating: 39it [00:31,  1.25it/s]Extractor Estimating: 40it [00:31,  1.27it/s]Extractor Estimating: 41it [00:32,  1.22it/s]Extractor Estimating: 42it [00:33,  1.20it/s]Extractor Estimating: 43it [00:34,  1.20it/s]Extractor Estimating: 44it [00:35,  1.25it/s]Extractor Estimating: 45it [00:35,  1.24it/s]Extractor Estimating: 46it [00:36,  1.24it/s]Extractor Estimating: 47it [00:37,  1.24it/s]Extractor Estimating: 48it [00:38,  1.26it/s]Extractor Estimating: 49it [00:39,  1.23it/s]Extractor Estimating: 50it [00:39,  1.27it/s]Extractor Estimating: 51it [00:40,  1.30it/s]Extractor Estimating: 52it [00:41,  1.31it/s]Extractor Estimating: 53it [00:42,  1.33it/s]Extractor Estimating: 54it [00:42,  1.29it/s]Extractor Estimating: 55it [00:43,  1.36it/s]Extractor Estimating: 56it [00:44,  1.34it/s]Extractor Estimating: 57it [00:45,  1.31it/s]Extractor Estimating: 58it [00:45,  1.27it/s]Extractor Estimating: 59it [00:46,  1.25it/s]Extractor Estimating: 60it [00:47,  1.25it/s]Extractor Estimating: 61it [00:48,  1.22it/s]Extractor Estimating: 62it [00:49,  1.26it/s]Extractor Estimating: 63it [00:49,  1.29it/s]Extractor Estimating: 64it [00:50,  1.30it/s]Extractor Estimating: 65it [00:51,  1.30it/s]Extractor Estimating: 66it [00:52,  1.28it/s]Extractor Estimating: 67it [00:53,  1.29it/s]Extractor Estimating: 68it [00:53,  1.31it/s]Extractor Estimating: 69it [00:54,  1.26it/s]Extractor Estimating: 70it [00:55,  1.28it/s]Extractor Estimating: 71it [00:56,  1.28it/s]Extractor Estimating: 72it [00:56,  1.29it/s]Extractor Estimating: 73it [00:57,  1.30it/s]Extractor Estimating: 74it [00:58,  1.27it/s]Extractor Estimating: 75it [00:59,  1.28it/s]Extractor Estimating: 76it [01:00,  1.25it/s]Extractor Estimating: 77it [01:00,  1.30it/s]Extractor Estimating: 78it [01:01,  1.26it/s]Extractor Estimating: 79it [01:02,  1.26it/s]Extractor Estimating: 80it [01:03,  1.28it/s]Extractor Estimating: 81it [01:03,  1.30it/s]Extractor Estimating: 82it [01:04,  1.28it/s]Extractor Estimating: 83it [01:05,  1.32it/s]Extractor Estimating: 84it [01:06,  1.30it/s]Extractor Estimating: 85it [01:06,  1.35it/s]Extractor Estimating: 86it [01:07,  1.35it/s]Extractor Estimating: 87it [01:08,  1.38it/s]Extractor Estimating: 88it [01:09,  1.34it/s]Extractor Estimating: 89it [01:09,  1.34it/s]Extractor Estimating: 90it [01:10,  1.27it/s]Extractor Estimating: 91it [01:11,  1.26it/s]Extractor Estimating: 92it [01:12,  1.29it/s]Extractor Estimating: 93it [01:13,  1.30it/s]Extractor Estimating: 94it [01:13,  1.27it/s]Extractor Estimating: 95it [01:14,  1.27it/s]Extractor Estimating: 96it [01:15,  1.22it/s]Extractor Estimating: 97it [01:16,  1.27it/s]Extractor Estimating: 98it [01:17,  1.20it/s]Extractor Estimating: 99it [01:18,  1.22it/s]Extractor Estimating: 100it [01:18,  1.22it/s]Extractor Estimating: 101it [01:19,  1.27it/s]Extractor Estimating: 102it [01:20,  1.28it/s]Extractor Estimating: 103it [01:21,  1.29it/s]Extractor Estimating: 104it [01:21,  1.32it/s]Extractor Estimating: 105it [01:22,  1.32it/s]Extractor Estimating: 106it [01:23,  1.34it/s]Extractor Estimating: 107it [01:24,  1.35it/s]Extractor Estimating: 108it [01:24,  1.37it/s]Extractor Estimating: 109it [01:25,  1.37it/s]Extractor Estimating: 110it [01:26,  1.38it/s]Extractor Estimating: 111it [01:26,  1.40it/s]Extractor Estimating: 112it [01:27,  1.38it/s]Extractor Estimating: 113it [01:28,  1.36it/s]Extractor Estimating: 114it [01:29,  1.35it/s]Extractor Estimating: 115it [01:29,  1.34it/s]Extractor Estimating: 116it [01:30,  1.36it/s]Extractor Estimating: 117it [01:31,  1.38it/s]Extractor Estimating: 118it [01:32,  1.36it/s]Extractor Estimating: 119it [01:32,  1.33it/s]Extractor Estimating: 120it [01:33,  1.34it/s]Extractor Estimating: 121it [01:34,  1.34it/s]Extractor Estimating: 122it [01:35,  1.31it/s]Extractor Estimating: 123it [01:35,  1.34it/s]Extractor Estimating: 124it [01:36,  1.24it/s]Extractor Estimating: 125it [01:37,  1.31it/s]Extractor Estimating: 126it [01:38,  1.31it/s]Extractor Estimating: 127it [01:38,  1.33it/s]Extractor Estimating: 128it [01:39,  1.30it/s]Extractor Estimating: 129it [01:40,  1.30it/s]Extractor Estimating: 130it [01:41,  1.34it/s]Extractor Estimating: 131it [01:41,  1.35it/s]Extractor Estimating: 132it [01:42,  1.32it/s]Extractor Estimating: 133it [01:43,  1.35it/s]Extractor Estimating: 134it [01:44,  1.35it/s]Extractor Estimating: 135it [01:44,  1.38it/s]Extractor Estimating: 136it [01:45,  1.39it/s]Extractor Estimating: 137it [01:46,  1.38it/s]Extractor Estimating: 138it [01:47,  1.36it/s]Extractor Estimating: 139it [01:47,  1.34it/s]Extractor Estimating: 140it [01:48,  1.33it/s]Extractor Estimating: 141it [01:49,  1.35it/s]Extractor Estimating: 142it [01:50,  1.38it/s]Extractor Estimating: 143it [01:50,  1.33it/s]Extractor Estimating: 144it [01:51,  1.33it/s]Extractor Estimating: 145it [01:52,  1.36it/s]Extractor Estimating: 146it [01:52,  1.38it/s]Extractor Estimating: 147it [01:53,  1.38it/s]Extractor Estimating: 148it [01:54,  1.40it/s]Extractor Estimating: 149it [01:55,  1.44it/s]Extractor Estimating: 150it [01:55,  1.38it/s]Extractor Estimating: 151it [01:56,  1.37it/s]Extractor Estimating: 152it [01:57,  1.38it/s]Extractor Estimating: 153it [01:58,  1.37it/s]Extractor Estimating: 154it [01:58,  1.42it/s]Extractor Estimating: 155it [01:59,  1.44it/s]Extractor Estimating: 156it [02:00,  1.43it/s]Extractor Estimating: 157it [02:00,  1.44it/s]Extractor Estimating: 158it [02:01,  1.39it/s]Extractor Estimating: 159it [02:02,  1.37it/s]Extractor Estimating: 160it [02:03,  1.33it/s]Extractor Estimating: 161it [02:03,  1.34it/s]Extractor Estimating: 162it [02:04,  1.36it/s]Extractor Estimating: 163it [02:05,  1.37it/s]Extractor Estimating: 164it [02:05,  1.37it/s]Extractor Estimating: 165it [02:06,  1.36it/s]Extractor Estimating: 166it [02:07,  1.37it/s]Extractor Estimating: 167it [02:08,  1.34it/s]Extractor Estimating: 168it [02:08,  1.34it/s]Extractor Estimating: 169it [02:09,  1.36it/s]Extractor Estimating: 170it [02:10,  1.39it/s]Extractor Estimating: 171it [02:11,  1.38it/s]Extractor Estimating: 172it [02:11,  1.32it/s]Extractor Estimating: 173it [02:12,  1.30it/s]Extractor Estimating: 174it [02:13,  1.35it/s]Extractor Estimating: 175it [02:14,  1.32it/s]Extractor Estimating: 176it [02:14,  1.35it/s]Extractor Estimating: 177it [02:15,  1.27it/s]Extractor Estimating: 178it [02:16,  1.29it/s]Extractor Estimating: 179it [02:17,  1.30it/s]Extractor Estimating: 180it [02:18,  1.30it/s]Extractor Estimating: 181it [02:18,  1.27it/s]Extractor Estimating: 182it [02:19,  1.30it/s]Extractor Estimating: 183it [02:20,  1.27it/s]Extractor Estimating: 184it [02:21,  1.31it/s]Extractor Estimating: 185it [02:21,  1.30it/s]Extractor Estimating: 186it [02:22,  1.23it/s]Extractor Estimating: 187it [02:23,  1.27it/s]Extractor Estimating: 188it [02:24,  1.32it/s]Extractor Estimating: 189it [02:25,  1.33it/s]Extractor Estimating: 190it [02:25,  1.35it/s]Extractor Estimating: 191it [02:26,  1.33it/s]Extractor Estimating: 192it [02:27,  1.36it/s]Extractor Estimating: 193it [02:27,  1.38it/s]Extractor Estimating: 194it [02:28,  1.37it/s]Extractor Estimating: 195it [02:29,  1.33it/s]Extractor Estimating: 196it [02:30,  1.36it/s]Extractor Estimating: 197it [02:30,  1.34it/s]Extractor Estimating: 198it [02:31,  1.36it/s]Extractor Estimating: 199it [02:32,  1.35it/s]Extractor Estimating: 200it [02:33,  1.31it/s]Extractor Estimating: 201it [02:33,  1.29it/s]Extractor Estimating: 202it [02:34,  1.27it/s]Extractor Estimating: 203it [02:35,  1.25it/s]Extractor Estimating: 204it [02:36,  1.28it/s]Extractor Estimating: 205it [02:37,  1.26it/s]Extractor Estimating: 206it [02:38,  1.22it/s]Extractor Estimating: 207it [02:38,  1.24it/s]Extractor Estimating: 208it [02:39,  1.29it/s]Extractor Estimating: 209it [02:40,  1.31it/s]Extractor Estimating: 210it [02:41,  1.32it/s]Extractor Estimating: 211it [02:41,  1.31it/s]Extractor Estimating: 212it [02:42,  1.28it/s]Extractor Estimating: 213it [02:43,  1.27it/s]Extractor Estimating: 214it [02:44,  1.25it/s]Extractor Estimating: 215it [02:45,  1.28it/s]Extractor Estimating: 216it [02:45,  1.30it/s]Extractor Estimating: 217it [02:46,  1.28it/s]Extractor Estimating: 218it [02:47,  1.32it/s]Extractor Estimating: 219it [02:48,  1.17it/s]Extractor Estimating: 220it [02:49,  1.24it/s]Extractor Estimating: 221it [02:49,  1.25it/s]Extractor Estimating: 222it [02:50,  1.22it/s]Extractor Estimating: 223it [02:51,  1.24it/s]Extractor Estimating: 224it [02:52,  1.27it/s]Extractor Estimating: 225it [02:52,  1.30it/s]Extractor Estimating: 226it [02:53,  1.33it/s]Extractor Estimating: 227it [02:54,  1.31it/s]Extractor Estimating: 228it [02:55,  1.30it/s]Extractor Estimating: 229it [02:55,  1.32it/s]Extractor Estimating: 230it [02:56,  1.32it/s]Extractor Estimating: 231it [02:57,  1.36it/s]Extractor Estimating: 232it [02:58,  1.39it/s]Extractor Estimating: 233it [02:58,  1.38it/s]Extractor Estimating: 234it [02:59,  1.36it/s]Extractor Estimating: 235it [03:00,  1.36it/s]Extractor Estimating: 236it [03:01,  1.39it/s]Extractor Estimating: 237it [03:01,  1.34it/s]Extractor Estimating: 238it [03:02,  1.33it/s]Extractor Estimating: 239it [03:03,  1.35it/s]Extractor Estimating: 240it [03:03,  1.37it/s]Extractor Estimating: 241it [03:04,  1.30it/s]Extractor Estimating: 242it [03:05,  1.31it/s]Extractor Estimating: 243it [03:06,  1.30it/s]Extractor Estimating: 244it [03:07,  1.30it/s]Extractor Estimating: 245it [03:07,  1.36it/s]Extractor Estimating: 246it [03:08,  1.37it/s]Extractor Estimating: 247it [03:09,  1.37it/s]Extractor Estimating: 248it [03:10,  1.36it/s]Extractor Estimating: 249it [03:10,  1.36it/s]Extractor Estimating: 250it [03:11,  1.37it/s]Extractor Estimating: 251it [03:12,  1.31it/s]Extractor Estimating: 252it [03:13,  1.29it/s]Extractor Estimating: 253it [03:13,  1.30it/s]Extractor Estimating: 254it [03:14,  1.32it/s]Extractor Estimating: 255it [03:15,  1.36it/s]Extractor Estimating: 256it [03:16,  1.35it/s]Extractor Estimating: 257it [03:16,  1.33it/s]Extractor Estimating: 258it [03:17,  1.36it/s]Extractor Estimating: 259it [03:18,  1.37it/s]Extractor Estimating: 260it [03:18,  1.39it/s]Extractor Estimating: 261it [03:19,  1.35it/s]Extractor Estimating: 262it [03:20,  1.36it/s]Extractor Estimating: 263it [03:21,  1.33it/s]Extractor Estimating: 264it [03:21,  1.34it/s]Extractor Estimating: 265it [03:22,  1.35it/s]Extractor Estimating: 266it [03:23,  1.35it/s]Extractor Estimating: 267it [03:24,  1.36it/s]Extractor Estimating: 268it [03:24,  1.32it/s]Extractor Estimating: 269it [03:25,  1.30it/s]Extractor Estimating: 270it [03:26,  1.27it/s]Extractor Estimating: 271it [03:27,  1.25it/s]Extractor Estimating: 272it [03:28,  1.28it/s]Extractor Estimating: 273it [03:28,  1.30it/s]Extractor Estimating: 274it [03:29,  1.29it/s]Extractor Estimating: 275it [03:30,  1.26it/s]Extractor Estimating: 276it [03:31,  1.30it/s]Extractor Estimating: 277it [03:31,  1.33it/s]Extractor Estimating: 278it [03:32,  1.39it/s]Extractor Estimating: 279it [03:33,  1.39it/s]Extractor Estimating: 280it [03:33,  1.41it/s]Extractor Estimating: 281it [03:34,  1.38it/s]Extractor Estimating: 282it [03:35,  1.40it/s]Extractor Estimating: 283it [03:36,  1.34it/s]Extractor Estimating: 284it [03:36,  1.39it/s]Extractor Estimating: 285it [03:37,  1.41it/s]Extractor Estimating: 286it [03:38,  1.39it/s]Extractor Estimating: 287it [03:39,  1.31it/s]Extractor Estimating: 288it [03:39,  1.34it/s]Extractor Estimating: 289it [03:40,  1.37it/s]Extractor Estimating: 290it [03:41,  1.35it/s]Extractor Estimating: 291it [03:42,  1.38it/s]Extractor Estimating: 292it [03:42,  1.34it/s]Extractor Estimating: 293it [03:43,  1.36it/s]Extractor Estimating: 294it [03:44,  1.44it/s]Extractor Estimating: 295it [03:44,  1.42it/s]Extractor Estimating: 296it [03:45,  1.40it/s]Extractor Estimating: 297it [03:46,  1.42it/s]Extractor Estimating: 298it [03:47,  1.40it/s]Extractor Estimating: 299it [03:47,  1.37it/s]Extractor Estimating: 300it [03:48,  1.35it/s]Extractor Estimating: 301it [03:49,  1.40it/s]Extractor Estimating: 302it [03:49,  1.43it/s]Extractor Estimating: 303it [03:50,  1.48it/s]Extractor Estimating: 304it [03:51,  1.56it/s]Extractor Estimating: 305it [03:51,  1.42it/s]Extractor Estimating: 306it [03:52,  1.39it/s]Extractor Estimating: 307it [03:53,  1.41it/s]Extractor Estimating: 308it [03:54,  1.42it/s]Extractor Estimating: 309it [03:54,  1.43it/s]Extractor Estimating: 310it [03:55,  1.46it/s]Extractor Estimating: 311it [03:56,  1.50it/s]Extractor Estimating: 312it [03:56,  1.42it/s]Extractor Estimating: 313it [03:57,  1.44it/s]Extractor Estimating: 314it [03:58,  1.50it/s]Extractor Estimating: 315it [03:58,  1.48it/s]Extractor Estimating: 316it [03:59,  1.48it/s]Extractor Estimating: 317it [04:00,  1.47it/s]Extractor Estimating: 318it [04:00,  1.51it/s]Extractor Estimating: 319it [04:01,  1.52it/s]Extractor Estimating: 320it [04:02,  1.42it/s]Extractor Estimating: 321it [04:02,  1.47it/s]Extractor Estimating: 322it [04:03,  1.44it/s]Extractor Estimating: 323it [04:04,  1.43it/s]Extractor Estimating: 324it [04:05,  1.42it/s]Extractor Estimating: 325it [04:05,  1.43it/s]Extractor Estimating: 326it [04:06,  1.39it/s]Extractor Estimating: 327it [04:07,  1.37it/s]Extractor Estimating: 328it [04:08,  1.33it/s]Extractor Estimating: 329it [04:08,  1.32it/s]Extractor Estimating: 330it [04:09,  1.33it/s]Extractor Estimating: 331it [04:10,  1.33it/s]Extractor Estimating: 332it [04:11,  1.30it/s]Extractor Estimating: 333it [04:11,  1.32it/s]Extractor Estimating: 334it [04:12,  1.31it/s]Extractor Estimating: 335it [04:13,  1.34it/s]Extractor Estimating: 336it [04:14,  1.35it/s]Extractor Estimating: 337it [04:14,  1.38it/s]Extractor Estimating: 338it [04:15,  1.33it/s]Extractor Estimating: 339it [04:16,  1.34it/s]Extractor Estimating: 340it [04:17,  1.35it/s]Extractor Estimating: 341it [04:17,  1.31it/s]Extractor Estimating: 342it [04:18,  1.31it/s]Extractor Estimating: 343it [04:19,  1.33it/s]Extractor Estimating: 344it [04:20,  1.33it/s]Extractor Estimating: 345it [04:20,  1.32it/s]Extractor Estimating: 346it [04:21,  1.32it/s]Extractor Estimating: 347it [04:22,  1.34it/s]Extractor Estimating: 348it [04:22,  1.38it/s]Extractor Estimating: 349it [04:23,  1.38it/s]Extractor Estimating: 350it [04:24,  1.36it/s]Extractor Estimating: 351it [04:25,  1.33it/s]Extractor Estimating: 352it [04:25,  1.36it/s]Extractor Estimating: 353it [04:26,  1.37it/s]Extractor Estimating: 354it [04:27,  1.32it/s]Extractor Estimating: 355it [04:28,  1.29it/s]Extractor Estimating: 356it [04:29,  1.28it/s]Extractor Estimating: 357it [04:29,  1.33it/s]Extractor Estimating: 358it [04:30,  1.36it/s]Extractor Estimating: 359it [04:31,  1.32it/s]Extractor Estimating: 360it [04:32,  1.32it/s]Extractor Estimating: 361it [04:32,  1.32it/s]Extractor Estimating: 362it [04:33,  1.32it/s]Extractor Estimating: 363it [04:34,  1.32it/s]Extractor Estimating: 364it [04:35,  1.30it/s]Extractor Estimating: 365it [04:35,  1.40it/s]Extractor Estimating: 366it [04:36,  1.43it/s]Extractor Estimating: 367it [04:37,  1.36it/s]Extractor Estimating: 368it [04:37,  1.38it/s]Extractor Estimating: 369it [04:38,  1.35it/s]Extractor Estimating: 370it [04:39,  1.39it/s]Extractor Estimating: 371it [04:40,  1.39it/s]Extractor Estimating: 372it [04:40,  1.39it/s]Extractor Estimating: 373it [04:41,  1.38it/s]Extractor Estimating: 374it [04:42,  1.36it/s]Extractor Estimating: 375it [04:42,  1.50it/s]Extractor Estimating: 375it [04:42,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:50,318 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:50,354 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:50,355 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:50,355 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:50,355 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:49:50,908 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:49:50,909 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:49:51,228 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:49:52,307 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:49:52,307 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:54,443 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:54,451 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:54,451 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:54,451 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:49:54,451 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:49:54,810 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:49:54,811 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:49:55,485 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:49:55,648 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:49:55,649 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 05:43:09,380 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 05:43:09,384 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7491 mean pseudo reward: 0.9348090318592591
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 20233
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20333, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20333, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.329, loss:701.2467
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.332, loss:654.5376
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.408, loss:600.4490
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 1.330, loss:593.9010
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 1.342, loss:570.7010
>> valid entity prec:0.4927, rec:0.4613, f1:0.4765
>> valid relation prec:0.0370, rec:0.0118, f1:0.0179
>> valid relation with NER prec:0.0370, rec:0.0118, f1:0.0179
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 3.009, loss:589.3400
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 1.321, loss:555.8386
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 1.352, loss:571.1121
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 1.343, loss:598.0326
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 1.340, loss:576.0582
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4590, rec:0.4227, f1:0.4401
>> valid relation prec:0.0425, rec:0.0126, f1:0.0195
>> valid relation with NER prec:0.0425, rec:0.0126, f1:0.0195
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1100, step 161, avg_time 3.010, loss:585.1419
g_step 1200, step 261, avg_time 1.354, loss:615.9713
g_step 1300, step 48, avg_time 1.325, loss:550.6650
g_step 1400, step 148, avg_time 1.356, loss:566.3332
g_step 1500, step 248, avg_time 1.347, loss:558.2573
>> valid entity prec:0.4786, rec:0.4500, f1:0.4639
>> valid relation prec:0.0302, rec:0.0089, f1:0.0138
>> valid relation with NER prec:0.0302, rec:0.0089, f1:0.0138
g_step 1600, step 35, avg_time 2.990, loss:529.7825
g_step 1700, step 135, avg_time 1.341, loss:535.5693
g_step 1800, step 235, avg_time 1.351, loss:522.3730
g_step 1900, step 22, avg_time 1.334, loss:535.6122
g_step 2000, step 122, avg_time 1.337, loss:499.7520
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4958, rec:0.4194, f1:0.4544
>> valid relation prec:0.0288, rec:0.0089, f1:0.0136
>> valid relation with NER prec:0.0288, rec:0.0089, f1:0.0136
g_step 2100, step 222, avg_time 3.030, loss:512.1882
g_step 2200, step 9, avg_time 1.333, loss:505.2214
g_step 2300, step 109, avg_time 1.347, loss:474.2816
g_step 2400, step 209, avg_time 1.329, loss:491.4564
g_step 2500, step 309, avg_time 1.345, loss:494.4656
>> valid entity prec:0.4750, rec:0.4441, f1:0.4590
>> valid relation prec:0.0380, rec:0.0152, f1:0.0217
>> valid relation with NER prec:0.0380, rec:0.0152, f1:0.0217
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 96, avg_time 2.981, loss:442.6406
g_step 2700, step 196, avg_time 1.358, loss:452.0379
g_step 2800, step 296, avg_time 1.351, loss:495.9036
g_step 2900, step 83, avg_time 1.359, loss:432.2932
g_step 3000, step 183, avg_time 1.346, loss:458.1071
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4705, rec:0.4575, f1:0.4639
>> valid relation prec:0.0290, rec:0.0086, f1:0.0133
>> valid relation with NER prec:0.0290, rec:0.0086, f1:0.0133
g_step 3100, step 283, avg_time 3.014, loss:459.5828
g_step 3200, step 70, avg_time 1.318, loss:417.1984
g_step 3300, step 170, avg_time 1.353, loss:425.8148
g_step 3400, step 270, avg_time 1.331, loss:442.6882
g_step 3500, step 57, avg_time 1.357, loss:400.3376
>> valid entity prec:0.4663, rec:0.3688, f1:0.4118
>> valid relation prec:0.0379, rec:0.0138, f1:0.0202
>> valid relation with NER prec:0.0379, rec:0.0138, f1:0.0202
g_step 3600, step 157, avg_time 2.999, loss:395.7438
g_step 3700, step 257, avg_time 1.334, loss:426.2757
g_step 3800, step 44, avg_time 1.325, loss:388.9733
g_step 3900, step 144, avg_time 1.340, loss:400.0605
g_step 4000, step 244, avg_time 1.333, loss:392.4607
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4739, rec:0.4272, f1:0.4493
>> valid relation prec:0.0305, rec:0.0118, f1:0.0170
>> valid relation with NER prec:0.0305, rec:0.0118, f1:0.0170
g_step 4100, step 31, avg_time 2.999, loss:412.5618
g_step 4200, step 131, avg_time 1.335, loss:375.5567
g_step 4300, step 231, avg_time 1.337, loss:383.9363
g_step 4400, step 18, avg_time 1.342, loss:369.0883
g_step 4500, step 118, avg_time 1.346, loss:367.5586
>> valid entity prec:0.4792, rec:0.4075, f1:0.4405
>> valid relation prec:0.0513, rec:0.0198, f1:0.0286
>> valid relation with NER prec:0.0513, rec:0.0198, f1:0.0286
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 218, avg_time 2.993, loss:359.9630
g_step 4700, step 5, avg_time 1.339, loss:378.8718
g_step 4800, step 105, avg_time 1.331, loss:344.5668
g_step 4900, step 205, avg_time 1.345, loss:353.8197
g_step 5000, step 305, avg_time 1.340, loss:370.8982
learning rate was adjusted to 0.0008
>> valid entity prec:0.4730, rec:0.4360, f1:0.4538
>> valid relation prec:0.0302, rec:0.0112, f1:0.0164
>> valid relation with NER prec:0.0302, rec:0.0112, f1:0.0164
g_step 5100, step 92, avg_time 3.014, loss:327.5954
g_step 5200, step 192, avg_time 1.333, loss:342.0541
g_step 5300, step 292, avg_time 1.345, loss:363.8357
g_step 5400, step 79, avg_time 1.328, loss:326.6799
g_step 5500, step 179, avg_time 1.345, loss:341.2134
>> valid entity prec:0.4632, rec:0.4685, f1:0.4658
>> valid relation prec:0.0354, rec:0.0175, f1:0.0235
>> valid relation with NER prec:0.0354, rec:0.0175, f1:0.0235
g_step 5600, step 279, avg_time 3.011, loss:344.3205
g_step 5700, step 66, avg_time 1.346, loss:312.2253
g_step 5800, step 166, avg_time 1.330, loss:312.7550
g_step 5900, step 266, avg_time 1.341, loss:329.4412
g_step 6000, step 53, avg_time 1.315, loss:322.6832
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4643, rec:0.4432, f1:0.4535
>> valid relation prec:0.0529, rec:0.0262, f1:0.0350
>> valid relation with NER prec:0.0529, rec:0.0262, f1:0.0350
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 153, avg_time 3.018, loss:318.6209
g_step 6200, step 253, avg_time 1.353, loss:322.6076
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 05:43:09 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 05:43:09 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_05-43-09_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 05:43:10 - WARNING - datasets.builder -   Using custom data configuration default-89039db93283890d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-89039db93283890d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 05:43:10,613 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:43:10,615 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:43:10,615 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:43:10,616 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:43:10,627 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:43:10,630 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:43:10,631 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:43:10,631 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:43:10,631 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:43:10,631 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:43:10,631 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 05:43:10,792 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:43:13,892 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 05:43:13,895 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-89039db93283890d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  3.30ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  4.13ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  3.58ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:00,  4.02ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.29ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.45ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.59ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.49ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.57ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  4.08ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  4.30ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  4.39ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.53ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.00ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  8.88ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00,  9.96ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00, 10.14ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00, 10.26ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.84ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  9.06ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00, 10.35ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.59ba/s]
[INFO|trainer.py:414] 2023-08-29 05:43:17,904 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 05:43:17,922 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 05:43:17,922 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-29 05:43:17,922 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 05:43:17,922 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 05:43:17,922 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 05:43:17,922 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 05:43:17,923 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<03:00,  3.23it/s]  0%|          | 2/585 [00:00<02:53,  3.36it/s]  1%|          | 3/585 [00:00<02:51,  3.40it/s]  1%|          | 4/585 [00:01<02:49,  3.42it/s]  1%|          | 5/585 [00:01<02:49,  3.43it/s]  1%|          | 6/585 [00:01<02:48,  3.44it/s]  1%|          | 7/585 [00:02<02:47,  3.44it/s]  1%|â–         | 8/585 [00:02<02:47,  3.44it/s]  2%|â–         | 9/585 [00:02<02:47,  3.43it/s]  2%|â–         | 10/585 [00:02<02:47,  3.44it/s]  2%|â–         | 11/585 [00:03<02:46,  3.44it/s]  2%|â–         | 12/585 [00:03<02:46,  3.45it/s]  2%|â–         | 13/585 [00:03<02:45,  3.45it/s]  2%|â–         | 14/585 [00:04<02:45,  3.45it/s]  3%|â–Ž         | 15/585 [00:04<02:45,  3.45it/s]  3%|â–Ž         | 16/585 [00:04<02:45,  3.45it/s]  3%|â–Ž         | 17/585 [00:04<02:44,  3.45it/s]  3%|â–Ž         | 18/585 [00:05<02:44,  3.45it/s]  3%|â–Ž         | 19/585 [00:05<02:44,  3.45it/s]  3%|â–Ž         | 20/585 [00:05<02:44,  3.44it/s]  4%|â–Ž         | 21/585 [00:06<02:43,  3.44it/s]  4%|â–         | 22/585 [00:06<02:43,  3.45it/s]  4%|â–         | 23/585 [00:06<02:43,  3.45it/s]  4%|â–         | 24/585 [00:06<02:42,  3.45it/s]  4%|â–         | 25/585 [00:07<02:42,  3.45it/s]  4%|â–         | 26/585 [00:07<02:42,  3.45it/s]  5%|â–         | 27/585 [00:07<02:41,  3.45it/s]  5%|â–         | 28/585 [00:08<02:41,  3.45it/s]  5%|â–         | 29/585 [00:08<02:41,  3.45it/s]  5%|â–Œ         | 30/585 [00:08<02:41,  3.45it/s]  5%|â–Œ         | 31/585 [00:09<02:41,  3.44it/s]  5%|â–Œ         | 32/585 [00:09<02:40,  3.44it/s]  6%|â–Œ         | 33/585 [00:09<02:40,  3.45it/s]  6%|â–Œ         | 34/585 [00:09<02:39,  3.44it/s]  6%|â–Œ         | 35/585 [00:10<02:39,  3.45it/s]  6%|â–Œ         | 36/585 [00:10<02:39,  3.45it/s]  6%|â–‹         | 37/585 [00:10<02:38,  3.45it/s]  6%|â–‹         | 38/585 [00:11<02:38,  3.45it/s]  7%|â–‹         | 39/585 [00:11<02:38,  3.45it/s]  7%|â–‹         | 40/585 [00:11<02:37,  3.45it/s]  7%|â–‹         | 41/585 [00:11<02:37,  3.45it/s]  7%|â–‹         | 42/585 [00:12<02:38,  3.43it/s]  7%|â–‹         | 43/585 [00:12<02:37,  3.43it/s]  8%|â–Š         | 44/585 [00:12<02:37,  3.44it/s]  8%|â–Š         | 45/585 [00:13<02:37,  3.44it/s]  8%|â–Š         | 46/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 47/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 48/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 49/585 [00:14<02:36,  3.44it/s]  9%|â–Š         | 50/585 [00:14<02:35,  3.44it/s]  9%|â–Š         | 51/585 [00:14<02:35,  3.44it/s]  9%|â–‰         | 52/585 [00:15<02:34,  3.44it/s]  9%|â–‰         | 53/585 [00:15<02:34,  3.44it/s]  9%|â–‰         | 54/585 [00:15<02:34,  3.44it/s]  9%|â–‰         | 55/585 [00:15<02:34,  3.44it/s] 10%|â–‰         | 56/585 [00:16<02:34,  3.43it/s] 10%|â–‰         | 57/585 [00:16<02:33,  3.44it/s] 10%|â–‰         | 58/585 [00:16<02:33,  3.44it/s] 10%|â–ˆ         | 59/585 [00:17<02:32,  3.44it/s] 10%|â–ˆ         | 60/585 [00:17<02:32,  3.44it/s] 10%|â–ˆ         | 61/585 [00:17<02:32,  3.44it/s] 11%|â–ˆ         | 62/585 [00:18<02:31,  3.44it/s] 11%|â–ˆ         | 63/585 [00:18<02:31,  3.44it/s] 11%|â–ˆ         | 64/585 [00:18<02:31,  3.43it/s] 11%|â–ˆ         | 65/585 [00:18<02:31,  3.43it/s] 11%|â–ˆâ–        | 66/585 [00:19<02:31,  3.44it/s] 11%|â–ˆâ–        | 67/585 [00:19<02:30,  3.44it/s] 12%|â–ˆâ–        | 68/585 [00:19<02:30,  3.44it/s] 12%|â–ˆâ–        | 69/585 [00:20<02:30,  3.43it/s] 12%|â–ˆâ–        | 70/585 [00:20<02:30,  3.43it/s] 12%|â–ˆâ–        | 71/585 [00:20<02:29,  3.43it/s] 12%|â–ˆâ–        | 72/585 [00:20<02:29,  3.43it/s] 12%|â–ˆâ–        | 73/585 [00:21<02:29,  3.43it/s] 13%|â–ˆâ–Ž        | 74/585 [00:21<02:28,  3.43it/s] 13%|â–ˆâ–Ž        | 75/585 [00:21<02:28,  3.43it/s] 13%|â–ˆâ–Ž        | 76/585 [00:22<02:28,  3.43it/s] 13%|â–ˆâ–Ž        | 77/585 [00:22<02:27,  3.43it/s] 13%|â–ˆâ–Ž        | 78/585 [00:22<02:27,  3.43it/s] 14%|â–ˆâ–Ž        | 79/585 [00:22<02:27,  3.43it/s] 14%|â–ˆâ–Ž        | 80/585 [00:23<02:26,  3.44it/s] 14%|â–ˆâ–        | 81/585 [00:23<02:26,  3.44it/s] 14%|â–ˆâ–        | 82/585 [00:23<02:27,  3.41it/s] 14%|â–ˆâ–        | 83/585 [00:24<02:26,  3.42it/s] 14%|â–ˆâ–        | 84/585 [00:24<02:26,  3.43it/s] 15%|â–ˆâ–        | 85/585 [00:24<02:25,  3.43it/s] 15%|â–ˆâ–        | 86/585 [00:25<02:25,  3.43it/s] 15%|â–ˆâ–        | 87/585 [00:25<02:25,  3.43it/s] 15%|â–ˆâ–Œ        | 88/585 [00:25<02:24,  3.43it/s] 15%|â–ˆâ–Œ        | 89/585 [00:25<02:24,  3.43it/s] 15%|â–ˆâ–Œ        | 90/585 [00:26<02:24,  3.44it/s] 16%|â–ˆâ–Œ        | 91/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 92/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 93/585 [00:27<02:23,  3.42it/s] 16%|â–ˆâ–Œ        | 94/585 [00:27<02:23,  3.43it/s] 16%|â–ˆâ–Œ        | 95/585 [00:27<02:22,  3.43it/s] 16%|â–ˆâ–‹        | 96/585 [00:27<02:22,  3.43it/s] 17%|â–ˆâ–‹        | 97/585 [00:28<02:22,  3.43it/s] 17%|â–ˆâ–‹        | 98/585 [00:28<02:21,  3.43it/s] 17%|â–ˆâ–‹        | 99/585 [00:28<02:21,  3.43it/s] 17%|â–ˆâ–‹        | 100/585 [00:29<02:21,  3.43it/s] 17%|â–ˆâ–‹        | 101/585 [00:29<02:20,  3.44it/s] 17%|â–ˆâ–‹        | 102/585 [00:29<02:20,  3.44it/s] 18%|â–ˆâ–Š        | 103/585 [00:29<02:20,  3.44it/s] 18%|â–ˆâ–Š        | 104/585 [00:30<02:20,  3.41it/s] 18%|â–ˆâ–Š        | 105/585 [00:30<02:20,  3.42it/s] 18%|â–ˆâ–Š        | 106/585 [00:30<02:19,  3.42it/s] 18%|â–ˆâ–Š        | 107/585 [00:31<02:19,  3.43it/s] 18%|â–ˆâ–Š        | 108/585 [00:31<02:19,  3.43it/s] 19%|â–ˆâ–Š        | 109/585 [00:31<02:18,  3.43it/s] 19%|â–ˆâ–‰        | 110/585 [00:32<02:18,  3.44it/s] 19%|â–ˆâ–‰        | 111/585 [00:32<02:17,  3.44it/s] 19%|â–ˆâ–‰        | 112/585 [00:32<02:17,  3.44it/s] 19%|â–ˆâ–‰        | 113/585 [00:32<02:17,  3.44it/s] 19%|â–ˆâ–‰        | 114/585 [00:33<02:17,  3.44it/s] 20%|â–ˆâ–‰        | 115/585 [00:33<02:17,  3.43it/s] 20%|â–ˆâ–‰        | 116/585 [00:33<02:16,  3.43it/s] 20%|â–ˆâ–ˆ        | 117/585 [00:34<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:43:52,036 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:43:52,036 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 05:43:52,036 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.04it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.23it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.58it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.89it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.52it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.25it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.06it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.74it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.59it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.62it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.71it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.56it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.67it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.70it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.75it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.70it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.61it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.67it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.55it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.52it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.63it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.56it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.62it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.68it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.63it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.59it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.58it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.64it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.59it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.56it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.67it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.56it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.62it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.72it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.58it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.59it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.60it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.59it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.62it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.64it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.59it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.55it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.54it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.58it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.62it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.56it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.56it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.57it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.58it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.58it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.63it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.49it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.59it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.61it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.51it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.61it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.59it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.59it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.62it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.58it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.60it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.56it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.59it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.57it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.46it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.61it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.52it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.61it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.66it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.57it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.61it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.65it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.59it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.55it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.61it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.54it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.47it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.54it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.55it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.60it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.62it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.67it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.58it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.53it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.52it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.43it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.43it/s][A 20%|â–ˆâ–ˆ        | 117/585 [00:43<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:44:01,389 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 05:44:01,406 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:44:04,487 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:44:04,509 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:44:04,524 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 118/585 [00:53<48:02,  6.17s/it] 20%|â–ˆâ–ˆ        | 119/585 [00:54<34:14,  4.41s/it] 21%|â–ˆâ–ˆ        | 120/585 [00:54<24:35,  3.17s/it] 21%|â–ˆâ–ˆ        | 121/585 [00:54<17:51,  2.31s/it] 21%|â–ˆâ–ˆ        | 122/585 [00:55<13:08,  1.70s/it] 21%|â–ˆâ–ˆ        | 123/585 [00:55<09:50,  1.28s/it] 21%|â–ˆâ–ˆ        | 124/585 [00:55<07:32,  1.02it/s] 21%|â–ˆâ–ˆâ–       | 125/585 [00:55<05:56,  1.29it/s] 22%|â–ˆâ–ˆâ–       | 126/585 [00:56<04:48,  1.59it/s] 22%|â–ˆâ–ˆâ–       | 127/585 [00:56<04:01,  1.89it/s] 22%|â–ˆâ–ˆâ–       | 128/585 [00:56<03:28,  2.19it/s] 22%|â–ˆâ–ˆâ–       | 129/585 [00:57<03:05,  2.46it/s] 22%|â–ˆâ–ˆâ–       | 130/585 [00:57<02:49,  2.69it/s] 22%|â–ˆâ–ˆâ–       | 131/585 [00:57<02:38,  2.87it/s] 23%|â–ˆâ–ˆâ–Ž       | 132/585 [00:58<02:29,  3.02it/s] 23%|â–ˆâ–ˆâ–Ž       | 133/585 [00:58<02:24,  3.14it/s] 23%|â–ˆâ–ˆâ–Ž       | 134/585 [00:58<02:19,  3.22it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/585 [00:58<02:16,  3.29it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/585 [00:59<02:14,  3.33it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/585 [00:59<02:13,  3.37it/s] 24%|â–ˆâ–ˆâ–Ž       | 138/585 [00:59<02:12,  3.39it/s] 24%|â–ˆâ–ˆâ–       | 139/585 [01:00<02:11,  3.40it/s] 24%|â–ˆâ–ˆâ–       | 140/585 [01:00<02:10,  3.41it/s] 24%|â–ˆâ–ˆâ–       | 141/585 [01:00<02:09,  3.42it/s] 24%|â–ˆâ–ˆâ–       | 142/585 [01:00<02:09,  3.42it/s] 24%|â–ˆâ–ˆâ–       | 143/585 [01:01<02:09,  3.42it/s] 25%|â–ˆâ–ˆâ–       | 144/585 [01:01<02:08,  3.43it/s] 25%|â–ˆâ–ˆâ–       | 145/585 [01:01<02:08,  3.43it/s] 25%|â–ˆâ–ˆâ–       | 146/585 [01:02<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 147/585 [01:02<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 148/585 [01:02<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/585 [01:02<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 150/585 [01:03<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 151/585 [01:03<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 152/585 [01:03<02:05,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/585 [01:04<02:05,  3.43it/s] 26%|â–ˆâ–ˆâ–‹       | 154/585 [01:04<02:05,  3.43it/s] 26%|â–ˆâ–ˆâ–‹       | 155/585 [01:04<02:05,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 156/585 [01:05<02:04,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 157/585 [01:05<02:04,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 158/585 [01:05<02:04,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 159/585 [01:05<02:03,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 160/585 [01:06<02:03,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 161/585 [01:06<02:03,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 162/585 [01:06<02:03,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 163/585 [01:07<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 164/585 [01:07<02:02,  3.43it/s] 28%|â–ˆâ–ˆâ–Š       | 165/585 [01:07<02:02,  3.43it/s] 28%|â–ˆâ–ˆâ–Š       | 166/585 [01:07<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–Š       | 167/585 [01:08<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–Š       | 168/585 [01:08<02:01,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 169/585 [01:08<02:01,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 170/585 [01:09<02:00,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 171/585 [01:09<02:00,  3.43it/s] 29%|â–ˆâ–ˆâ–‰       | 172/585 [01:09<02:00,  3.43it/s] 30%|â–ˆâ–ˆâ–‰       | 173/585 [01:09<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 174/585 [01:10<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 175/585 [01:10<01:59,  3.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 176/585 [01:10<01:59,  3.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 177/585 [01:11<01:58,  3.43it/s] 30%|â–ˆâ–ˆâ–ˆ       | 178/585 [01:11<01:58,  3.43it/s] 31%|â–ˆâ–ˆâ–ˆ       | 179/585 [01:11<01:58,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 180/585 [01:11<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 181/585 [01:12<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 182/585 [01:12<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 183/585 [01:12<01:56,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 184/585 [01:13<01:56,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 185/585 [01:13<01:56,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 186/585 [01:13<01:56,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 187/585 [01:14<01:55,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 188/585 [01:14<01:55,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/585 [01:14<01:55,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/585 [01:14<01:55,  3.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 191/585 [01:15<01:54,  3.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 192/585 [01:15<01:54,  3.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 193/585 [01:15<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/585 [01:16<01:53,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/585 [01:16<01:53,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/585 [01:16<01:53,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/585 [01:16<01:53,  3.42it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 198/585 [01:17<01:52,  3.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 199/585 [01:17<01:52,  3.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 200/585 [01:17<01:52,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/585 [01:18<01:51,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 202/585 [01:18<01:51,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 203/585 [01:18<01:51,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 204/585 [01:18<01:50,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 205/585 [01:19<01:50,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 206/585 [01:19<01:50,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 207/585 [01:19<01:49,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 208/585 [01:20<01:50,  3.42it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/585 [01:20<01:49,  3.43it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/585 [01:20<01:49,  3.43it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/585 [01:21<01:48,  3.43it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/585 [01:21<01:48,  3.43it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 213/585 [01:21<01:48,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 214/585 [01:21<01:47,  3.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 215/585 [01:22<01:47,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 216/585 [01:22<01:47,  3.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 217/585 [01:22<01:47,  3.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 218/585 [01:23<01:46,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/585 [01:23<01:46,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 220/585 [01:23<01:46,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 221/585 [01:23<01:46,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 222/585 [01:24<01:45,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 223/585 [01:24<01:45,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 224/585 [01:24<01:45,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/585 [01:25<01:44,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 226/585 [01:25<01:44,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 227/585 [01:25<01:44,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 228/585 [01:25<01:43,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 229/585 [01:26<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 230/585 [01:26<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/585 [01:26<01:43,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 232/585 [01:27<01:42,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 233/585 [01:27<01:42,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:27<01:42,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 05:44:45,694 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:44:45,694 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 05:44:45,694 >>   Batch size = 8
{'eval_loss': 0.9975741505622864, 'eval_runtime': 9.3397, 'eval_samples_per_second': 372.494, 'eval_steps_per_second': 46.575, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.69it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.02it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.48it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.71it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.38it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.10it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.86it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.77it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.69it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.63it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.58it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.56it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.53it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.47it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.50it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.55it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.54it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.48it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.45it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.53it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.53it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.48it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.49it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.49it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.50it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.55it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.51it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.52it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.38it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.39it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.49it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.49it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.54it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.52it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.46it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.44it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.51it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.52it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.48it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.50it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.40it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.43it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.46it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.47it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.43it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.47it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.41it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.50it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.50it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.45it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.48it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.50it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.44it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.49it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.54it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.50it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.49it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.51it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.49it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.45it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.47it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.51it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.51it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.49it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.44it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.54it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.47it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.55it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.58it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.47it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.43it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.47it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.47it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.54it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.53it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.50it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.44it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.53it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.29it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.28it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.41it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.38it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.44it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.49it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.42it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.50it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.50it/s][A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:37<01:42,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:44:55,069 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 05:44:55,136 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:44:57,716 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:44:57,735 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:44:57,753 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 235/585 [01:46<33:35,  5.76s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 236/585 [01:46<24:03,  4.14s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 237/585 [01:46<17:21,  2.99s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/585 [01:47<12:37,  2.18s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/585 [01:47<09:18,  1.61s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 240/585 [01:47<07:00,  1.22s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/585 [01:48<05:23,  1.06it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242/585 [01:48<04:15,  1.34it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243/585 [01:48<03:28,  1.64it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/585 [01:48<02:55,  1.95it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/585 [01:49<02:31,  2.24it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/585 [01:49<02:19,  2.44it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/585 [01:49<02:06,  2.67it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/585 [01:50<01:57,  2.86it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 249/585 [01:50<01:51,  3.01it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 250/585 [01:50<01:47,  3.13it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 251/585 [01:51<01:43,  3.22it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 252/585 [01:51<01:41,  3.28it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/585 [01:51<01:39,  3.33it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 254/585 [01:51<01:38,  3.36it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/585 [01:52<01:37,  3.38it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/585 [01:52<01:36,  3.40it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/585 [01:52<01:36,  3.40it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/585 [01:53<01:35,  3.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/585 [01:53<01:35,  3.42it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/585 [01:53<01:34,  3.42it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/585 [01:53<01:34,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 262/585 [01:54<01:34,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/585 [01:54<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 264/585 [01:54<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 265/585 [01:55<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 266/585 [01:55<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 267/585 [01:55<01:32,  3.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 268/585 [01:55<01:32,  3.41it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/585 [01:56<01:32,  3.42it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 270/585 [01:56<01:31,  3.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 271/585 [01:56<01:31,  3.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 272/585 [01:57<01:31,  3.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 273/585 [01:57<01:30,  3.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 274/585 [01:57<01:30,  3.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 275/585 [01:58<01:30,  3.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 276/585 [01:58<01:29,  3.43it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/585 [01:58<01:29,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 278/585 [01:58<01:29,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 279/585 [01:59<01:29,  3.42it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 280/585 [01:59<01:29,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 281/585 [01:59<01:28,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 282/585 [02:00<01:28,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/585 [02:00<01:28,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 284/585 [02:00<01:27,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/585 [02:00<01:27,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 286/585 [02:01<01:27,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 287/585 [02:01<01:26,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 288/585 [02:01<01:26,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 289/585 [02:02<01:26,  3.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 290/585 [02:02<01:26,  3.42it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/585 [02:02<01:25,  3.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 292/585 [02:02<01:25,  3.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 293/585 [02:03<01:25,  3.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 294/585 [02:03<01:24,  3.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 295/585 [02:03<01:24,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 296/585 [02:04<01:24,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 297/585 [02:04<01:23,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 298/585 [02:04<01:23,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/585 [02:05<01:23,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/585 [02:05<01:23,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/585 [02:05<01:23,  3.42it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/585 [02:05<01:22,  3.42it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/585 [02:06<01:22,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/585 [02:06<01:22,  3.42it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/585 [02:06<01:21,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306/585 [02:07<01:21,  3.42it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/585 [02:07<01:21,  3.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 308/585 [02:07<01:20,  3.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 309/585 [02:07<01:20,  3.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 310/585 [02:08<01:20,  3.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 311/585 [02:08<01:19,  3.43it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 312/585 [02:08<01:19,  3.41it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/585 [02:09<01:19,  3.42it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 314/585 [02:09<01:19,  3.42it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 315/585 [02:09<01:18,  3.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/585 [02:09<01:18,  3.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/585 [02:10<01:18,  3.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/585 [02:10<01:17,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/585 [02:10<01:17,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/585 [02:11<01:17,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/585 [02:11<01:16,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 322/585 [02:11<01:16,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 323/585 [02:12<01:16,  3.41it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 324/585 [02:12<01:16,  3.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 325/585 [02:12<01:15,  3.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 326/585 [02:12<01:15,  3.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 327/585 [02:13<01:15,  3.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 328/585 [02:13<01:15,  3.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/585 [02:13<01:14,  3.42it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 330/585 [02:14<01:14,  3.42it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 331/585 [02:14<01:14,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 332/585 [02:14<01:13,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 333/585 [02:14<01:13,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 334/585 [02:15<01:13,  3.41it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/585 [02:15<01:13,  3.42it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 336/585 [02:15<01:12,  3.42it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 337/585 [02:16<01:12,  3.42it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 338/585 [02:16<01:12,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 339/585 [02:16<01:11,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 340/585 [02:16<01:11,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 341/585 [02:17<01:11,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 342/585 [02:17<01:10,  3.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/585 [02:17<01:10,  3.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 344/585 [02:18<01:10,  3.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 345/585 [02:18<01:10,  3.42it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 346/585 [02:18<01:09,  3.42it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 347/585 [02:19<01:09,  3.42it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 348/585 [02:19<01:09,  3.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 349/585 [02:19<01:08,  3.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 350/585 [02:19<01:08,  3.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:20<01:08,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:45:38,165 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:45:38,165 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 05:45:38,165 >>   Batch size = 8
{'eval_loss': 0.9999771118164062, 'eval_runtime': 9.3608, 'eval_samples_per_second': 371.656, 'eval_steps_per_second': 46.47, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.81it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.28it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.44it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.72it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.27it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.09it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.86it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.80it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.64it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.57it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.58it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.56it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.50it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.52it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.53it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.51it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.52it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.52it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.48it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.56it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.49it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.38it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.51it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.49it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.52it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.55it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.58it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.40it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.48it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.52it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.55it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.49it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.37it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.51it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.51it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.48it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.50it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.44it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.50it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.49it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.45it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.45it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.47it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.52it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.50it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.49it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.45it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.51it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.50it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.55it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.49it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.47it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.49it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.50it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.51it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.51it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.43it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.51it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.52it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.48it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.52it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.52it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.50it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.51it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.47it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.48it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.49it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.49it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.42it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.41it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.43it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.49it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.49it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.49it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.45it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.52it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.47it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.49it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.55it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.47it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.46it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.53it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.46it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.52it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.47it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.47it/s][A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:29<01:08,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:45:47,538 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 05:45:47,627 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:45:49,890 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:45:49,905 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:45:49,915 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 352/585 [02:37<20:25,  5.26s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 353/585 [02:37<14:34,  3.77s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 354/585 [02:37<10:29,  2.73s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 355/585 [02:37<07:38,  2.00s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 356/585 [02:38<05:39,  1.48s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/585 [02:38<04:16,  1.13s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/585 [02:38<03:18,  1.14it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/585 [02:39<02:38,  1.43it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/585 [02:39<02:09,  1.73it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/585 [02:39<01:49,  2.04it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362/585 [02:39<01:36,  2.32it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 363/585 [02:40<01:26,  2.57it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 364/585 [02:40<01:19,  2.77it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 365/585 [02:40<01:14,  2.94it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 366/585 [02:41<01:11,  3.08it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 367/585 [02:41<01:08,  3.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 368/585 [02:41<01:06,  3.25it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 369/585 [02:41<01:05,  3.31it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 370/585 [02:42<01:04,  3.35it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 371/585 [02:42<01:03,  3.38it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/585 [02:42<01:02,  3.39it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 373/585 [02:43<01:02,  3.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/585 [02:43<01:01,  3.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375/585 [02:43<01:04,  3.28it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376/585 [02:44<01:02,  3.32it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/585 [02:44<01:02,  3.35it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/585 [02:44<01:01,  3.38it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/585 [02:44<01:00,  3.39it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/585 [02:45<01:00,  3.41it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 381/585 [02:45<00:59,  3.41it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 382/585 [02:45<00:59,  3.41it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 383/585 [02:46<00:59,  3.42it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 384/585 [02:46<00:58,  3.42it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 385/585 [02:46<01:00,  3.30it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 386/585 [02:47<01:21,  2.43it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 387/585 [02:47<01:14,  2.66it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 388/585 [02:47<01:09,  2.85it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 389/585 [02:48<01:05,  3.01it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 390/585 [02:48<01:02,  3.12it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 391/585 [02:48<01:00,  3.21it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 392/585 [02:49<00:59,  3.27it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 393/585 [02:49<00:58,  3.31it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 394/585 [02:49<00:57,  3.34it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 395/585 [02:50<00:56,  3.36it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 396/585 [02:50<00:55,  3.38it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 397/585 [02:50<00:55,  3.39it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 398/585 [02:50<00:54,  3.40it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 399/585 [02:51<00:54,  3.41it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/585 [02:51<00:54,  3.41it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 401/585 [02:51<00:53,  3.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/585 [02:52<00:53,  3.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 403/585 [02:52<00:53,  3.43it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 404/585 [02:52<00:52,  3.43it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 405/585 [02:52<00:52,  3.43it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 406/585 [02:53<00:52,  3.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 407/585 [02:53<00:52,  3.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 408/585 [02:53<00:51,  3.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 409/585 [02:54<00:51,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 410/585 [02:54<00:51,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 411/585 [02:54<00:50,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 412/585 [02:54<00:50,  3.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 413/585 [02:55<00:50,  3.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 414/585 [02:55<00:49,  3.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 415/585 [02:55<00:49,  3.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 416/585 [02:56<00:49,  3.43it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417/585 [02:56<00:49,  3.42it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418/585 [02:56<00:48,  3.42it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 419/585 [02:57<00:48,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 420/585 [02:57<00:48,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 421/585 [02:57<00:47,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 422/585 [02:57<00:47,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 423/585 [02:58<00:47,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/585 [02:58<00:46,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 425/585 [02:58<00:46,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 426/585 [02:59<00:46,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 427/585 [02:59<00:46,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 428/585 [02:59<00:45,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 429/585 [02:59<00:45,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 430/585 [03:00<00:45,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 431/585 [03:00<00:44,  3.42it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 432/585 [03:00<00:44,  3.42it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 433/585 [03:01<00:44,  3.42it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 434/585 [03:01<00:44,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 435/585 [03:01<00:43,  3.42it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 436/585 [03:01<00:43,  3.43it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 437/585 [03:02<00:43,  3.43it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 438/585 [03:02<00:42,  3.42it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 439/585 [03:02<00:42,  3.43it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 440/585 [03:03<00:42,  3.42it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 441/585 [03:03<00:42,  3.42it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 442/585 [03:03<00:41,  3.41it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 443/585 [03:04<00:41,  3.42it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 444/585 [03:04<00:41,  3.42it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 445/585 [03:04<00:40,  3.42it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 446/585 [03:04<00:40,  3.42it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 447/585 [03:05<00:40,  3.43it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 448/585 [03:05<00:39,  3.43it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 449/585 [03:05<00:39,  3.43it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 450/585 [03:06<00:39,  3.43it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 451/585 [03:06<00:39,  3.42it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 452/585 [03:06<00:38,  3.43it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 453/585 [03:06<00:38,  3.42it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 454/585 [03:07<00:38,  3.42it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 455/585 [03:07<00:37,  3.42it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 456/585 [03:07<00:37,  3.42it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 457/585 [03:08<00:37,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 458/585 [03:08<00:37,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 459/585 [03:08<00:36,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 460/585 [03:08<00:36,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 461/585 [03:09<00:36,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 462/585 [03:09<00:35,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 463/585 [03:09<00:35,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 464/585 [03:10<00:35,  3.41it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 465/585 [03:10<00:35,  3.42it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 466/585 [03:10<00:34,  3.42it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 467/585 [03:11<00:34,  3.42it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:11<00:34,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 05:46:29,311 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:46:29,311 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 05:46:29,311 >>   Batch size = 8
{'eval_loss': 1.0119918584823608, 'eval_runtime': 9.3592, 'eval_samples_per_second': 371.721, 'eval_steps_per_second': 46.479, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.16it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.30it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.49it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.75it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.31it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.01it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.82it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.74it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.63it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.64it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.61it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.53it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.48it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.44it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.50it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.54it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.52it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.47it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.48it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.51it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.52it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.57it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.43it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.45it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.51it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.50it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.53it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.52it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.22it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.43it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.46it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.53it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.48it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.43it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.46it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.52it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.51it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.46it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.50it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.46it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.53it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.53it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.43it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.47it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.54it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.51it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.51it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.52it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.43it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.48it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.45it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.43it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.35it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.34it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.29it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.31it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.32it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.37it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.49it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.47it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.52it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.43it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.43it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.47it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.51it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.51it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.51it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.44it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.49it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.53it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.44it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.45it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.22it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.50it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.57it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.54it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.52it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.41it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.49it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.53it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.51it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.47it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.45it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.49it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.53it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.44it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.44it/s][A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:20<00:34,  3.42it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:46:38,684 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 05:46:38,704 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:46:40,848 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:46:40,863 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:46:40,875 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 469/585 [03:27<10:01,  5.19s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 470/585 [03:28<07:07,  3.72s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 471/585 [03:28<05:06,  2.69s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 472/585 [03:28<03:42,  1.97s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 473/585 [03:29<02:44,  1.47s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 474/585 [03:29<02:03,  1.11s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 475/585 [03:29<01:35,  1.15it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 476/585 [03:29<01:15,  1.44it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 477/585 [03:30<01:01,  1.75it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 478/585 [03:30<00:52,  2.05it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 479/585 [03:30<00:45,  2.33it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 480/585 [03:31<00:40,  2.58it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 481/585 [03:31<00:37,  2.78it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 482/585 [03:31<00:34,  2.95it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 483/585 [03:32<00:33,  3.08it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 484/585 [03:32<00:31,  3.18it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 485/585 [03:32<00:30,  3.25it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 486/585 [03:32<00:29,  3.31it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 487/585 [03:33<00:29,  3.35it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 488/585 [03:33<00:28,  3.37it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 489/585 [03:33<00:28,  3.39it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 490/585 [03:34<00:27,  3.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 491/585 [03:34<00:27,  3.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 492/585 [03:34<00:27,  3.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 493/585 [03:34<00:26,  3.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 494/585 [03:35<00:26,  3.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 495/585 [03:35<00:26,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 496/585 [03:35<00:25,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 497/585 [03:36<00:25,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 498/585 [03:36<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 499/585 [03:36<00:25,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [03:36<00:24,  3.43it/s]                                                  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [03:36<00:24,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 501/585 [03:37<00:24,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 502/585 [03:37<00:24,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 503/585 [03:37<00:23,  3.42it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 504/585 [03:38<00:23,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 505/585 [03:38<00:23,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 506/585 [03:38<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 507/585 [03:38<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 508/585 [03:39<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 509/585 [03:39<00:22,  3.43it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 510/585 [03:39<00:21,  3.43it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 511/585 [03:40<00:21,  3.43it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 512/585 [03:40<00:21,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 513/585 [03:40<00:20,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 514/585 [03:41<00:20,  3.43it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 515/585 [03:41<00:20,  3.43it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 516/585 [03:41<00:20,  3.43it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 517/585 [03:41<00:19,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 518/585 [03:42<00:19,  3.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 519/585 [03:42<00:19,  3.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 520/585 [03:42<00:18,  3.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 521/585 [03:43<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 522/585 [03:43<00:18,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 523/585 [03:43<00:18,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 524/585 [03:43<00:17,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 525/585 [03:44<00:17,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 526/585 [03:44<00:17,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 527/585 [03:44<00:16,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 528/585 [03:45<00:16,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 529/585 [03:45<00:16,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 530/585 [03:45<00:16,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 531/585 [03:45<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 532/585 [03:46<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 533/585 [03:46<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 534/585 [03:46<00:14,  3.42it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 535/585 [03:47<00:14,  3.39it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 536/585 [03:47<00:14,  3.32it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 537/585 [03:47<00:14,  3.35it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 538/585 [03:48<00:13,  3.37it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 539/585 [03:48<00:13,  3.39it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 540/585 [03:48<00:13,  3.40it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 541/585 [03:48<00:12,  3.41it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 542/585 [03:49<00:12,  3.41it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 543/585 [03:49<00:12,  3.42it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 544/585 [03:49<00:11,  3.42it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 545/585 [03:50<00:11,  3.42it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 546/585 [03:50<00:11,  3.42it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 547/585 [03:50<00:11,  3.41it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 548/585 [03:50<00:10,  3.41it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/585 [03:51<00:10,  3.42it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 550/585 [03:51<00:10,  3.42it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 551/585 [03:51<00:09,  3.42it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 552/585 [03:52<00:09,  3.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 553/585 [03:52<00:09,  3.43it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 554/585 [03:52<00:09,  3.43it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 555/585 [03:53<00:08,  3.43it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 556/585 [03:53<00:08,  3.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 557/585 [03:53<00:08,  3.42it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 558/585 [03:53<00:07,  3.42it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 559/585 [03:54<00:07,  3.42it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 560/585 [03:54<00:07,  3.42it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 561/585 [03:54<00:07,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 562/585 [03:55<00:06,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 563/585 [03:55<00:06,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 564/585 [03:55<00:06,  3.43it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 565/585 [03:55<00:05,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 566/585 [03:56<00:05,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 567/585 [03:56<00:05,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 568/585 [03:56<00:04,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 569/585 [03:57<00:04,  3.42it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 570/585 [03:57<00:04,  3.42it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 571/585 [03:57<00:04,  3.42it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 572/585 [03:57<00:03,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 573/585 [03:58<00:03,  3.42it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 574/585 [03:58<00:03,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 575/585 [03:58<00:02,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 576/585 [03:59<00:02,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 577/585 [03:59<00:02,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 578/585 [03:59<00:02,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 579/585 [04:00<00:01,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 580/585 [04:00<00:01,  3.43it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 581/585 [04:00<00:01,  3.42it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 582/585 [04:00<00:00,  3.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 583/585 [04:01<00:00,  3.42it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 584/585 [04:01<00:00,  3.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:01<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:47:19,719 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:47:19,719 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 05:47:19,719 >>   Batch size = 8
{'eval_loss': 1.0192461013793945, 'eval_runtime': 9.3623, 'eval_samples_per_second': 371.597, 'eval_steps_per_second': 46.463, 'epoch': 4.0}
{'loss': 0.5391, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.69it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.12it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.41it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.65it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.32it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 46.96it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.74it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.70it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.58it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.51it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.50it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:08, 46.50it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.44it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.54it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.42it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.39it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.34it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.43it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.49it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.37it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.37it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.44it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.43it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.52it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.53it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.43it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.47it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.53it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.47it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.44it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.49it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.43it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.47it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.47it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.39it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.47it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.46it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.42it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.41it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.46it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.45it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.48it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.51it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.44it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.46it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.46it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.48it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.53it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.37it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.37it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.34it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.44it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.34it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.42it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.43it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.34it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.38it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.41it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.37it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.41it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.44it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.36it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.40it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.46it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.51it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.53it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.45it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.47it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.50it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.42it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.45it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.42it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.47it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.49it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.49it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.45it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.53it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.55it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.52it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.30it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.47it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.43it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.49it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.49it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.44it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.50it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.50it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:11<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:47:29,105 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 05:47:29,121 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:47:31,505 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:47:31,520 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:47:31,528 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 05:47:36,031 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 05:47:36,034 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117 (score: 0.9975741505622864).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:19<00:00,  3.43it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:19<00:00,  2.25it/s]
[INFO|trainer.py:1894] 2023-08-29 05:47:37,630 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 05:47:37,642 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:47:39,807 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:47:39,827 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:47:39,846 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:47:40,027 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:40,027 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:40,027 >>   train_loss               =     0.5351
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:40,027 >>   train_runtime            = 0:04:19.70
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:40,027 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:40,027 >>   train_samples_per_second =    144.376
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:40,027 >>   train_steps_per_second   =      2.253
{'eval_loss': 1.0253827571868896, 'eval_runtime': 9.3673, 'eval_samples_per_second': 371.397, 'eval_steps_per_second': 46.438, 'epoch': 5.0}
{'train_runtime': 259.7046, 'train_samples_per_second': 144.376, 'train_steps_per_second': 2.253, 'train_loss': 0.5351439581976997, 'epoch': 5.0}
08/29/2023 05:47:40 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 05:47:40,057 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:47:40,057 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 05:47:40,057 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 57.98it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 51.00it/s]  4%|â–         | 18/435 [00:00<00:08, 49.05it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.42it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.99it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.57it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.40it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.30it/s] 11%|â–ˆ         | 48/435 [00:00<00:08, 46.89it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.91it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.88it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.74it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.93it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.96it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.89it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.93it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.94it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.83it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.76it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.71it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.75it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.81it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.81it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.83it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.63it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.66it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.70it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.76it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.69it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.70it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.75it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.70it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.83it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.81it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.83it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.92it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.75it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.71it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.75it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.74it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.69it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.85it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.78it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.80it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.85it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.75it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.79it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.82it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.70it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.70it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.84it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.81it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.80it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.88it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.74it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.82it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.72it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.61it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.66it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.80it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.75it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.84it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.78it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.82it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.79it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.61it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.68it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.74it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.72it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.78it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.80it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.74it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.74it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.70it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.72it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.69it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.62it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.71it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.73it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.71it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.72it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.73it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.64it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.64it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.86it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:47:49,367 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:49,368 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:49,368 >>   eval_loss               =     0.9976
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:49,368 >>   eval_runtime            = 0:00:09.31
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:49,368 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:49,368 >>   eval_samples_per_second =    373.682
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:49,368 >>   eval_steps_per_second   =     46.724
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:47:49,368 >>   perplexity              =     2.7117
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:55,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:55,317 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:55,317 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:55,317 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:55,317 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:47:55,622 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:47:55,623 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:47:56,320 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:47:57,333 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:47:57,334 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:59,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:59,459 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:59,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:59,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:47:59,460 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:47:59,799 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:47:59,800 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:48:00,477 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:48:00,624 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:48:00,624 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.19it/s]Extractor Predicting: 2it [00:01,  1.10it/s]Extractor Predicting: 3it [00:02,  1.14it/s]Extractor Predicting: 4it [00:03,  1.16it/s]Extractor Predicting: 5it [00:04,  1.15it/s]Extractor Predicting: 6it [00:05,  1.15it/s]Extractor Predicting: 7it [00:06,  1.15it/s]Extractor Predicting: 8it [00:06,  1.16it/s]Extractor Predicting: 9it [00:07,  1.16it/s]Extractor Predicting: 10it [00:08,  1.19it/s]Extractor Predicting: 11it [00:09,  1.17it/s]Extractor Predicting: 12it [00:10,  1.16it/s]Extractor Predicting: 13it [00:11,  1.14it/s]Extractor Predicting: 14it [00:12,  1.14it/s]Extractor Predicting: 15it [00:12,  1.18it/s]Extractor Predicting: 16it [00:13,  1.16it/s]Extractor Predicting: 17it [00:14,  1.19it/s]Extractor Predicting: 18it [00:15,  1.22it/s]Extractor Predicting: 19it [00:16,  1.19it/s]Extractor Predicting: 20it [00:17,  1.20it/s]Extractor Predicting: 21it [00:17,  1.20it/s]Extractor Predicting: 22it [00:18,  1.22it/s]Extractor Predicting: 23it [00:19,  1.21it/s]Extractor Predicting: 24it [00:20,  1.20it/s]Extractor Predicting: 25it [00:21,  1.18it/s]Extractor Predicting: 26it [00:22,  1.16it/s]Extractor Predicting: 27it [00:23,  1.16it/s]Extractor Predicting: 28it [00:23,  1.19it/s]Extractor Predicting: 29it [00:24,  1.18it/s]Extractor Predicting: 30it [00:25,  1.16it/s]Extractor Predicting: 31it [00:26,  1.16it/s]Extractor Predicting: 32it [00:27,  1.17it/s]Extractor Predicting: 33it [00:28,  1.17it/s]Extractor Predicting: 34it [00:28,  1.20it/s]Extractor Predicting: 35it [00:29,  1.21it/s]Extractor Predicting: 36it [00:30,  1.21it/s]Extractor Predicting: 37it [00:31,  1.21it/s]Extractor Predicting: 38it [00:32,  1.22it/s]Extractor Predicting: 39it [00:32,  1.23it/s]Extractor Predicting: 40it [00:33,  1.21it/s]Extractor Predicting: 41it [00:34,  1.22it/s]Extractor Predicting: 42it [00:35,  1.22it/s]Extractor Predicting: 43it [00:36,  1.23it/s]Extractor Predicting: 44it [00:37,  1.25it/s]Extractor Predicting: 45it [00:37,  1.23it/s]Extractor Predicting: 46it [00:38,  1.22it/s]Extractor Predicting: 47it [00:39,  1.21it/s]Extractor Predicting: 48it [00:40,  1.22it/s]Extractor Predicting: 49it [00:41,  1.22it/s]Extractor Predicting: 50it [00:41,  1.23it/s]Extractor Predicting: 51it [00:42,  1.23it/s]Extractor Predicting: 52it [00:43,  1.23it/s]Extractor Predicting: 53it [00:44,  1.24it/s]Extractor Predicting: 54it [00:45,  1.22it/s]Extractor Predicting: 55it [00:46,  1.19it/s]Extractor Predicting: 56it [00:46,  1.22it/s]Extractor Predicting: 57it [00:47,  1.20it/s]Extractor Predicting: 58it [00:48,  1.20it/s]Extractor Predicting: 59it [00:49,  1.23it/s]Extractor Predicting: 60it [00:50,  1.26it/s]Extractor Predicting: 61it [00:50,  1.28it/s]Extractor Predicting: 62it [00:51,  1.25it/s]Extractor Predicting: 63it [00:52,  1.23it/s]Extractor Predicting: 64it [00:53,  1.24it/s]Extractor Predicting: 65it [00:54,  1.25it/s]Extractor Predicting: 66it [00:54,  1.24it/s]Extractor Predicting: 67it [00:55,  1.25it/s]Extractor Predicting: 68it [00:56,  1.24it/s]Extractor Predicting: 69it [00:57,  1.28it/s]Extractor Predicting: 70it [00:58,  1.28it/s]Extractor Predicting: 71it [00:58,  1.29it/s]Extractor Predicting: 72it [00:59,  1.26it/s]Extractor Predicting: 73it [01:00,  1.26it/s]Extractor Predicting: 74it [01:01,  1.25it/s]Extractor Predicting: 75it [01:02,  1.22it/s]Extractor Predicting: 76it [01:02,  1.22it/s]Extractor Predicting: 77it [01:03,  1.24it/s]Extractor Predicting: 78it [01:04,  1.25it/s]Extractor Predicting: 79it [01:05,  1.27it/s]Extractor Predicting: 80it [01:06,  1.26it/s]Extractor Predicting: 81it [01:07,  1.17it/s]Extractor Predicting: 82it [01:07,  1.18it/s]Extractor Predicting: 83it [01:08,  1.21it/s]Extractor Predicting: 84it [01:09,  1.22it/s]Extractor Predicting: 85it [01:10,  1.23it/s]Extractor Predicting: 86it [01:11,  1.25it/s]Extractor Predicting: 87it [01:11,  1.27it/s]Extractor Predicting: 88it [01:12,  1.26it/s]Extractor Predicting: 89it [01:13,  1.26it/s]Extractor Predicting: 90it [01:14,  1.28it/s]Extractor Predicting: 91it [01:14,  1.29it/s]Extractor Predicting: 92it [01:15,  1.33it/s]Extractor Predicting: 93it [01:16,  1.30it/s]Extractor Predicting: 94it [01:17,  1.28it/s]Extractor Predicting: 95it [01:17,  1.30it/s]Extractor Predicting: 96it [01:18,  1.29it/s]Extractor Predicting: 97it [01:19,  1.29it/s]Extractor Predicting: 98it [01:20,  1.28it/s]Extractor Predicting: 99it [01:21,  1.26it/s]Extractor Predicting: 100it [01:22,  1.23it/s]Extractor Predicting: 101it [01:22,  1.23it/s]Extractor Predicting: 102it [01:23,  1.29it/s]Extractor Predicting: 103it [01:24,  1.30it/s]Extractor Predicting: 104it [01:25,  1.28it/s]Extractor Predicting: 105it [01:25,  1.28it/s]Extractor Predicting: 106it [01:26,  1.28it/s]Extractor Predicting: 107it [01:27,  1.28it/s]Extractor Predicting: 108it [01:28,  1.27it/s]Extractor Predicting: 109it [01:29,  1.28it/s]Extractor Predicting: 110it [01:29,  1.29it/s]Extractor Predicting: 111it [01:30,  1.29it/s]Extractor Predicting: 112it [01:31,  1.30it/s]Extractor Predicting: 113it [01:32,  1.33it/s]Extractor Predicting: 114it [01:32,  1.34it/s]Extractor Predicting: 115it [01:33,  1.34it/s]Extractor Predicting: 116it [01:34,  1.31it/s]Extractor Predicting: 117it [01:35,  1.28it/s]Extractor Predicting: 118it [01:35,  1.28it/s]Extractor Predicting: 119it [01:36,  1.25it/s]Extractor Predicting: 120it [01:37,  1.26it/s]Extractor Predicting: 121it [01:38,  1.25it/s]Extractor Predicting: 122it [01:39,  1.23it/s]Extractor Predicting: 123it [01:39,  1.24it/s]Extractor Predicting: 124it [01:40,  1.25it/s]Extractor Predicting: 125it [01:41,  1.27it/s]Extractor Predicting: 126it [01:42,  1.24it/s]Extractor Predicting: 127it [01:43,  1.27it/s]Extractor Predicting: 128it [01:43,  1.27it/s]Extractor Predicting: 129it [01:44,  1.25it/s]Extractor Predicting: 130it [01:45,  1.23it/s]Extractor Predicting: 131it [01:46,  1.25it/s]Extractor Predicting: 132it [01:47,  1.24it/s]Extractor Predicting: 133it [01:47,  1.22it/s]Extractor Predicting: 134it [01:48,  1.21it/s]Extractor Predicting: 135it [01:49,  1.23it/s]Extractor Predicting: 136it [01:50,  1.22it/s]Extractor Predicting: 137it [01:51,  1.22it/s]Extractor Predicting: 138it [01:52,  1.20it/s]Extractor Predicting: 139it [01:52,  1.21it/s]Extractor Predicting: 140it [01:53,  1.20it/s]Extractor Predicting: 141it [01:54,  1.22it/s]Extractor Predicting: 142it [01:55,  1.21it/s]Extractor Predicting: 143it [01:56,  1.22it/s]Extractor Predicting: 144it [01:56,  1.50it/s]Extractor Predicting: 144it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:05,431 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:05,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:05,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:05,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:05,437 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:50:06,031 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:50:06,033 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:50:06,616 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:50:07,623 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:50:07,623 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:10,450 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:10,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:10,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:10,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:50:10,454 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:50:11,084 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:50:11,085 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:50:11,649 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:50:11,801 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:50:11,801 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2080745341614907,
  "recall": 0.01925840758838747,
  "score": 0.03525388055774797,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.23it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.25it/s]Extractor Predicting: 5it [00:03,  1.27it/s]Extractor Predicting: 6it [00:04,  1.28it/s]Extractor Predicting: 7it [00:05,  1.31it/s]Extractor Predicting: 8it [00:06,  1.34it/s]Extractor Predicting: 9it [00:07,  1.30it/s]Extractor Predicting: 10it [00:07,  1.30it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.28it/s]Extractor Predicting: 13it [00:10,  1.27it/s]Extractor Predicting: 14it [00:10,  1.28it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:13,  1.27it/s]Extractor Predicting: 18it [00:14,  1.29it/s]Extractor Predicting: 19it [00:14,  1.31it/s]Extractor Predicting: 20it [00:15,  1.27it/s]Extractor Predicting: 21it [00:16,  1.29it/s]Extractor Predicting: 22it [00:17,  1.30it/s]Extractor Predicting: 23it [00:17,  1.30it/s]Extractor Predicting: 24it [00:18,  1.28it/s]Extractor Predicting: 25it [00:19,  1.27it/s]Extractor Predicting: 26it [00:20,  1.28it/s]Extractor Predicting: 27it [00:21,  1.27it/s]Extractor Predicting: 28it [00:21,  1.27it/s]Extractor Predicting: 29it [00:22,  1.28it/s]Extractor Predicting: 30it [00:23,  1.30it/s]Extractor Predicting: 31it [00:24,  1.27it/s]Extractor Predicting: 32it [00:24,  1.31it/s]Extractor Predicting: 33it [00:25,  1.30it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:27,  1.28it/s]Extractor Predicting: 36it [00:28,  1.29it/s]Extractor Predicting: 37it [00:28,  1.31it/s]Extractor Predicting: 38it [00:29,  1.31it/s]Extractor Predicting: 39it [00:30,  1.30it/s]Extractor Predicting: 40it [00:31,  1.28it/s]Extractor Predicting: 41it [00:31,  1.29it/s]Extractor Predicting: 42it [00:32,  1.32it/s]Extractor Predicting: 43it [00:33,  1.28it/s]Extractor Predicting: 44it [00:34,  1.27it/s]Extractor Predicting: 45it [00:34,  1.28it/s]Extractor Predicting: 46it [00:35,  1.26it/s]Extractor Predicting: 47it [00:36,  1.28it/s]Extractor Predicting: 48it [00:37,  1.26it/s]Extractor Predicting: 49it [00:38,  1.30it/s]Extractor Predicting: 50it [00:38,  1.28it/s]Extractor Predicting: 51it [00:39,  1.28it/s]Extractor Predicting: 52it [00:40,  1.26it/s]Extractor Predicting: 53it [00:41,  1.26it/s]Extractor Predicting: 54it [00:42,  1.26it/s]Extractor Predicting: 55it [00:42,  1.29it/s]Extractor Predicting: 56it [00:43,  1.28it/s]Extractor Predicting: 57it [00:44,  1.26it/s]Extractor Predicting: 58it [00:45,  1.25it/s]Extractor Predicting: 59it [00:46,  1.26it/s]Extractor Predicting: 60it [00:46,  1.24it/s]Extractor Predicting: 61it [00:47,  1.24it/s]Extractor Predicting: 62it [00:48,  1.27it/s]Extractor Predicting: 63it [00:49,  1.27it/s]Extractor Predicting: 64it [00:49,  1.30it/s]Extractor Predicting: 65it [00:50,  1.27it/s]Extractor Predicting: 66it [00:51,  1.25it/s]Extractor Predicting: 67it [00:52,  1.24it/s]Extractor Predicting: 68it [00:53,  1.24it/s]Extractor Predicting: 69it [00:54,  1.23it/s]Extractor Predicting: 70it [00:54,  1.24it/s]Extractor Predicting: 71it [00:55,  1.24it/s]Extractor Predicting: 72it [00:56,  1.22it/s]Extractor Predicting: 73it [00:57,  1.24it/s]Extractor Predicting: 74it [00:58,  1.24it/s]Extractor Predicting: 75it [00:58,  1.26it/s]Extractor Predicting: 76it [00:59,  1.24it/s]Extractor Predicting: 77it [01:00,  1.28it/s]Extractor Predicting: 78it [01:01,  1.27it/s]Extractor Predicting: 79it [01:01,  1.30it/s]Extractor Predicting: 80it [01:02,  1.32it/s]Extractor Predicting: 81it [01:03,  1.31it/s]Extractor Predicting: 82it [01:04,  1.31it/s]Extractor Predicting: 83it [01:05,  1.28it/s]Extractor Predicting: 84it [01:05,  1.27it/s]Extractor Predicting: 85it [01:06,  1.24it/s]Extractor Predicting: 86it [01:07,  1.21it/s]Extractor Predicting: 87it [01:08,  1.22it/s]Extractor Predicting: 88it [01:09,  1.22it/s]Extractor Predicting: 89it [01:10,  1.21it/s]Extractor Predicting: 90it [01:10,  1.21it/s]Extractor Predicting: 91it [01:11,  1.19it/s]Extractor Predicting: 92it [01:12,  1.22it/s]Extractor Predicting: 93it [01:13,  1.23it/s]Extractor Predicting: 94it [01:14,  1.24it/s]Extractor Predicting: 95it [01:14,  1.25it/s]Extractor Predicting: 96it [01:15,  1.22it/s]Extractor Predicting: 97it [01:16,  1.20it/s]Extractor Predicting: 98it [01:17,  1.20it/s]Extractor Predicting: 99it [01:18,  1.22it/s]Extractor Predicting: 100it [01:19,  1.13it/s]Extractor Predicting: 101it [01:20,  1.18it/s]Extractor Predicting: 102it [01:20,  1.17it/s]Extractor Predicting: 103it [01:21,  1.17it/s]Extractor Predicting: 104it [01:22,  1.21it/s]Extractor Predicting: 105it [01:23,  1.20it/s]Extractor Predicting: 106it [01:24,  1.21it/s]Extractor Predicting: 107it [01:24,  1.23it/s]Extractor Predicting: 108it [01:25,  1.21it/s]Extractor Predicting: 109it [01:26,  1.21it/s]Extractor Predicting: 110it [01:27,  1.20it/s]Extractor Predicting: 111it [01:28,  1.22it/s]Extractor Predicting: 112it [01:29,  1.20it/s]Extractor Predicting: 113it [01:29,  1.22it/s]Extractor Predicting: 114it [01:30,  1.22it/s]Extractor Predicting: 115it [01:31,  1.19it/s]Extractor Predicting: 116it [01:32,  1.21it/s]Extractor Predicting: 117it [01:33,  1.20it/s]Extractor Predicting: 118it [01:34,  1.20it/s]Extractor Predicting: 119it [01:34,  1.20it/s]Extractor Predicting: 120it [01:35,  1.23it/s]Extractor Predicting: 121it [01:36,  1.24it/s]Extractor Predicting: 122it [01:37,  1.24it/s]Extractor Predicting: 123it [01:38,  1.24it/s]Extractor Predicting: 124it [01:38,  1.25it/s]Extractor Predicting: 125it [01:39,  1.27it/s]Extractor Predicting: 126it [01:40,  1.26it/s]Extractor Predicting: 127it [01:41,  1.25it/s]Extractor Predicting: 128it [01:42,  1.28it/s]Extractor Predicting: 129it [01:42,  1.28it/s]Extractor Predicting: 130it [01:43,  1.25it/s]Extractor Predicting: 131it [01:44,  1.29it/s]Extractor Predicting: 132it [01:45,  1.29it/s]Extractor Predicting: 133it [01:45,  1.31it/s]Extractor Predicting: 134it [01:46,  1.27it/s]Extractor Predicting: 135it [01:47,  1.29it/s]Extractor Predicting: 136it [01:48,  1.30it/s]Extractor Predicting: 137it [01:48,  1.31it/s]Extractor Predicting: 138it [01:49,  1.30it/s]Extractor Predicting: 139it [01:50,  1.29it/s]Extractor Predicting: 140it [01:51,  1.30it/s]Extractor Predicting: 141it [01:52,  1.27it/s]Extractor Predicting: 142it [01:52,  1.27it/s]Extractor Predicting: 143it [01:53,  1.28it/s]Extractor Predicting: 144it [01:54,  1.27it/s]Extractor Predicting: 145it [01:55,  1.31it/s]Extractor Predicting: 146it [01:55,  1.33it/s]Extractor Predicting: 147it [01:56,  1.34it/s]Extractor Predicting: 148it [01:57,  1.36it/s]Extractor Predicting: 149it [01:58,  1.35it/s]Extractor Predicting: 150it [01:58,  1.36it/s]Extractor Predicting: 151it [01:59,  1.37it/s]Extractor Predicting: 152it [02:00,  1.38it/s]Extractor Predicting: 153it [02:01,  1.35it/s]Extractor Predicting: 154it [02:01,  1.36it/s]Extractor Predicting: 155it [02:02,  1.39it/s]Extractor Predicting: 156it [02:03,  1.38it/s]Extractor Predicting: 157it [02:03,  1.44it/s]Extractor Predicting: 158it [02:04,  1.46it/s]Extractor Predicting: 159it [02:05,  1.42it/s]Extractor Predicting: 160it [02:05,  1.37it/s]Extractor Predicting: 161it [02:06,  1.36it/s]Extractor Predicting: 162it [02:07,  1.38it/s]Extractor Predicting: 163it [02:08,  1.40it/s]Extractor Predicting: 164it [02:08,  1.40it/s]Extractor Predicting: 165it [02:09,  1.40it/s]Extractor Predicting: 166it [02:10,  1.37it/s]Extractor Predicting: 167it [02:11,  1.39it/s]Extractor Predicting: 168it [02:11,  1.38it/s]Extractor Predicting: 169it [02:12,  1.43it/s]Extractor Predicting: 170it [02:13,  1.41it/s]Extractor Predicting: 171it [02:13,  1.41it/s]Extractor Predicting: 172it [02:14,  1.35it/s]Extractor Predicting: 173it [02:15,  1.33it/s]Extractor Predicting: 174it [02:16,  1.30it/s]Extractor Predicting: 175it [02:17,  1.25it/s]Extractor Predicting: 176it [02:17,  1.25it/s]Extractor Predicting: 177it [02:18,  1.25it/s]Extractor Predicting: 178it [02:19,  1.24it/s]Extractor Predicting: 179it [02:20,  1.24it/s]Extractor Predicting: 180it [02:21,  1.25it/s]Extractor Predicting: 181it [02:21,  1.25it/s]Extractor Predicting: 182it [02:22,  1.24it/s]Extractor Predicting: 183it [02:23,  1.24it/s]Extractor Predicting: 184it [02:24,  1.23it/s]Extractor Predicting: 185it [02:25,  1.21it/s]Extractor Predicting: 186it [02:26,  1.21it/s]Extractor Predicting: 187it [02:26,  1.22it/s]Extractor Predicting: 188it [02:27,  1.22it/s]Extractor Predicting: 189it [02:28,  1.22it/s]Extractor Predicting: 190it [02:29,  1.24it/s]Extractor Predicting: 191it [02:30,  1.21it/s]Extractor Predicting: 192it [02:31,  1.19it/s]Extractor Predicting: 193it [02:31,  1.20it/s]Extractor Predicting: 194it [02:32,  1.20it/s]Extractor Predicting: 195it [02:33,  1.21it/s]Extractor Predicting: 196it [02:34,  1.22it/s]Extractor Predicting: 197it [02:35,  1.21it/s]Extractor Predicting: 198it [02:35,  1.23it/s]Extractor Predicting: 199it [02:36,  1.25it/s]Extractor Predicting: 200it [02:37,  1.23it/s]Extractor Predicting: 201it [02:38,  1.21it/s]Extractor Predicting: 202it [02:39,  1.27it/s]Extractor Predicting: 203it [02:39,  1.25it/s]Extractor Predicting: 204it [02:40,  1.26it/s]Extractor Predicting: 205it [02:41,  1.25it/s]Extractor Predicting: 206it [02:42,  1.25it/s]Extractor Predicting: 207it [02:43,  1.23it/s]Extractor Predicting: 208it [02:43,  1.23it/s]Extractor Predicting: 209it [02:44,  1.18it/s]Extractor Predicting: 210it [02:45,  1.11it/s]Extractor Predicting: 211it [02:46,  1.13it/s]Extractor Predicting: 212it [02:47,  1.14it/s]Extractor Predicting: 213it [02:48,  1.16it/s]Extractor Predicting: 214it [02:49,  1.16it/s]Extractor Predicting: 215it [02:50,  1.19it/s]Extractor Predicting: 216it [02:50,  1.17it/s]Extractor Predicting: 217it [02:51,  1.18it/s]Extractor Predicting: 218it [02:52,  1.19it/s]Extractor Predicting: 219it [02:53,  1.18it/s]Extractor Predicting: 220it [02:54,  1.19it/s]Extractor Predicting: 221it [02:55,  1.17it/s]Extractor Predicting: 222it [02:56,  1.16it/s]Extractor Predicting: 223it [02:56,  1.19it/s]Extractor Predicting: 224it [02:57,  1.20it/s]Extractor Predicting: 225it [02:58,  1.20it/s]Extractor Predicting: 226it [02:59,  1.19it/s]Extractor Predicting: 227it [03:00,  1.23it/s]Extractor Predicting: 228it [03:00,  1.22it/s]Extractor Predicting: 229it [03:01,  1.22it/s]Extractor Predicting: 230it [03:02,  1.26it/s]Extractor Predicting: 231it [03:03,  1.26it/s]Extractor Predicting: 232it [03:04,  1.29it/s]Extractor Predicting: 233it [03:04,  1.27it/s]Extractor Predicting: 234it [03:05,  1.27it/s]Extractor Predicting: 235it [03:06,  1.26it/s]Extractor Predicting: 236it [03:07,  1.24it/s]Extractor Predicting: 237it [03:08,  1.22it/s]Extractor Predicting: 238it [03:08,  1.23it/s]Extractor Predicting: 239it [03:09,  1.22it/s]Extractor Predicting: 240it [03:10,  1.23it/s]Extractor Predicting: 241it [03:11,  1.19it/s]Extractor Predicting: 242it [03:12,  1.21it/s]Extractor Predicting: 243it [03:13,  1.21it/s]Extractor Predicting: 244it [03:13,  1.23it/s]Extractor Predicting: 245it [03:14,  1.25it/s]Extractor Predicting: 246it [03:15,  1.24it/s]Extractor Predicting: 247it [03:16,  1.24it/s]Extractor Predicting: 248it [03:17,  1.24it/s]Extractor Predicting: 249it [03:17,  1.23it/s]Extractor Predicting: 250it [03:18,  1.25it/s]Extractor Predicting: 251it [03:19,  1.25it/s]Extractor Predicting: 252it [03:20,  1.26it/s]Extractor Predicting: 253it [03:21,  1.24it/s]Extractor Predicting: 254it [03:21,  1.25it/s]Extractor Predicting: 255it [03:22,  1.21it/s]Extractor Predicting: 256it [03:23,  1.18it/s]Extractor Predicting: 257it [03:24,  1.19it/s]Extractor Predicting: 258it [03:25,  1.19it/s]Extractor Predicting: 259it [03:26,  1.20it/s]Extractor Predicting: 260it [03:27,  1.19it/s]Extractor Predicting: 261it [03:27,  1.22it/s]Extractor Predicting: 262it [03:28,  1.20it/s]Extractor Predicting: 263it [03:29,  1.19it/s]Extractor Predicting: 264it [03:30,  1.20it/s]Extractor Predicting: 265it [03:31,  1.16it/s]Extractor Predicting: 266it [03:32,  1.16it/s]Extractor Predicting: 267it [03:32,  1.17it/s]Extractor Predicting: 268it [03:33,  1.15it/s]Extractor Predicting: 269it [03:34,  1.18it/s]Extractor Predicting: 270it [03:35,  1.20it/s]Extractor Predicting: 271it [03:36,  1.17it/s]Extractor Predicting: 272it [03:37,  1.18it/s]Extractor Predicting: 273it [03:38,  1.19it/s]Extractor Predicting: 274it [03:38,  1.21it/s]Extractor Predicting: 275it [03:39,  1.21it/s]Extractor Predicting: 276it [03:40,  1.25it/s]Extractor Predicting: 277it [03:41,  1.25it/s]Extractor Predicting: 278it [03:42,  1.24it/s]Extractor Predicting: 279it [03:42,  1.23it/s]Extractor Predicting: 280it [03:43,  1.20it/s]Extractor Predicting: 281it [03:44,  1.20it/s]Extractor Predicting: 282it [03:45,  1.27it/s]Extractor Predicting: 282it [03:45,  1.25it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:05,183 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:05,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:05,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:05,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:05,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:54:05,812 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:54:05,813 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:54:06,397 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:54:07,432 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:54:07,432 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:10,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:10,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:10,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:10,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:54:10,486 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:54:11,114 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:54:11,115 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:54:11,691 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:54:11,843 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:54:11,843 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.41391752577319585,
  "recall": 0.1187869822485207,
  "score": 0.18459770114942528,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.17it/s]Extractor Predicting: 2it [00:01,  1.17it/s]Extractor Predicting: 3it [00:02,  1.17it/s]Extractor Predicting: 4it [00:03,  1.17it/s]Extractor Predicting: 5it [00:04,  1.22it/s]Extractor Predicting: 5it [00:04,  1.20it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:54:16,487 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:54:16,488 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:54:16,492 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:54:16,493 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:54:16,494 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:54:19,563 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:54:19,566 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:54:19,578 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:54:19,579 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:54:19,588 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:54:19,591 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:54:19,591 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:54:19,591 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:54:19,591 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:54:19,591 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:54:19,591 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5185185185185185,
  "recall": 0.058333333333333334,
  "score": 0.10486891385767791,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:54:19,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:20,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:21,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:22,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:25,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:25,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:26,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:27,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:28,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:29,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:30,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:31,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:31,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:32,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:33,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:34,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:35,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:36,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:37,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:37,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:38,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|â–‹         | 1/15 [00:19<04:37, 19.82s/it][WARNING|generation_utils.py:914] 2023-08-29 05:54:39,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:40,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:41,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:42,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:43,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:44,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:45,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:46,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:47,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:47,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:48,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:49,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:50,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:51,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:52,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:53,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:54,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:54,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:55,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:56,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|â–ˆâ–Ž        | 2/15 [00:37<04:03, 18.73s/it][WARNING|generation_utils.py:914] 2023-08-29 05:54:57,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:58,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:54:59,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:00,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:01,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:02,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:03,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:04,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:05,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:06,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:07,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:07,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:08,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:09,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:10,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:11,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:12,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:12,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:13,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:14,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|â–ˆâ–ˆ        | 3/15 [00:55<03:40, 18.34s/it][WARNING|generation_utils.py:914] 2023-08-29 05:55:15,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:16,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:17,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:18,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:19,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:19,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:20,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:21,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:22,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:23,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:24,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:25,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:26,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:27,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:28,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:29,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:30,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:31,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:32,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:32,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:33,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|â–ˆâ–ˆâ–‹       | 4/15 [01:14<03:25, 18.64s/it][WARNING|generation_utils.py:914] 2023-08-29 05:55:34,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:35,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:36,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:37,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:38,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:39,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:40,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:41,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:41,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:42,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:43,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:44,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:45,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:46,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:46,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:47,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:48,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:49,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:50,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:51,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:52,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5/15 [01:33<03:05, 18.56s/it][WARNING|generation_utils.py:914] 2023-08-29 05:55:53,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:53,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:54,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:55,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:56,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:57,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:58,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:55:59,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:00,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:01,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:02,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:03,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:04,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:05,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:06,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:06,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:07,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:08,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:09,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:10,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:11,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6/15 [01:52<02:48, 18.77s/it][WARNING|generation_utils.py:914] 2023-08-29 05:56:12,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:12,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:13,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:14,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:15,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:16,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:17,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:18,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:18,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:19,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:20,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:21,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:22,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:23,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:24,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:25,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:25,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:26,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:27,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:28,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7/15 [02:09<02:25, 18.25s/it][WARNING|generation_utils.py:914] 2023-08-29 05:56:29,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:30,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:31,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:32,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:33,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:34,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:35,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:36,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:37,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:38,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:39,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:40,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:41,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:42,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:43,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:44,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:45,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:46,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:47,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:48,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:48,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8/15 [02:30<02:12, 18.97s/it][WARNING|generation_utils.py:914] 2023-08-29 05:56:49,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:50,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:51,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:52,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:53,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:54,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:55,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:56,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:56,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:57,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:58,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:56:59,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:00,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:01,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:02,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:03,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:04,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:05,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:06,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:06,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9/15 [02:47<01:51, 18.63s/it][WARNING|generation_utils.py:914] 2023-08-29 05:57:07,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:08,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:09,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:10,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:11,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:12,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:13,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:14,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:14,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:15,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:16,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:17,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:18,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:19,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:20,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:21,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:22,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:23,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:24,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:25,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:26,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10/15 [03:07<01:34, 18.85s/it][WARNING|generation_utils.py:914] 2023-08-29 05:57:27,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:27,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:29,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:30,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:31,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:31,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:32,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:33,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:34,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:35,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:36,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:37,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:38,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:39,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:40,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:41,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:42,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:43,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:44,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:44,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:45,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11/15 [03:27<01:16, 19.16s/it][WARNING|generation_utils.py:914] 2023-08-29 05:57:46,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:47,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:48,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:49,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:50,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:51,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:52,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:52,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:53,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:54,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:55,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:56,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:57,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:58,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:57:59,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:00,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:00,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:01,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:02,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:03,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:04,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 12/15 [03:45<00:56, 18.87s/it][WARNING|generation_utils.py:914] 2023-08-29 05:58:05,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:05,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:06,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:07,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:08,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:09,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:10,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:11,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:11,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:12,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:13,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:14,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:15,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:15,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:16,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:17,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:19,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:20,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:20,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:21,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:22,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13/15 [04:03<00:37, 18.67s/it][WARNING|generation_utils.py:914] 2023-08-29 05:58:23,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:24,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:25,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:25,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:26,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:27,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:28,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:29,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:30,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:30,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:31,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:32,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:33,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:34,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:35,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:36,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:36,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:37,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:39,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:40,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:41,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14/15 [04:22<00:18, 18.77s/it][WARNING|generation_utils.py:914] 2023-08-29 05:58:42,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:43,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:44,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:45,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:46,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:47,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:48,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:49,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:50,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:51,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:52,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:53,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:54,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:55,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:56,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:57,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:58,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:58:59,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:59:00,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:59:01,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:59:02,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:59:03,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:44<00:00, 19.81s/it]Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [04:44<00:00, 18.98s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:11,230 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:11,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:11,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:11,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:11,235 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:59:11,829 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:59:11,830 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:59:12,399 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:59:13,454 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:59:13,454 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:16,284 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:16,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:16,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:16,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:59:16,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:59:16,908 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:59:16,909 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:59:17,466 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:59:17,623 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:59:17,623 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : country . Context : Later in 2008 , he appeared in a comedy called " The Simpsons " as the character Bart Simpson in the animated television series " Simpsons " . Head Entity : The Simpsons , Tail Entity : American .\n']
['Relation : country . Context : Later in 2008 , he appeared in a comedy called " The Simpsons " as the character Bart Simpson in the animated television series " Simpsons " . Head Entity : The Simpsons , Tail Entity : American .\n', 'Relation : country . Context : Eileen McClelland \'s novel , " The Last Airbender " , won the Academy Award for Best Story . Head Entity : The Last Airbender , Tail Entity : Australia .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : country .', 'success_rate': 0.8607954545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : genre .', 'success_rate': 0.9375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.90625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : distributed by .', 'success_rate': 0.9390625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : instrument .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : occupation .', 'success_rate': 0.9181547619047619, 'errors': {''}}
['Relation : participating team . Context : On 31 March 2014 , the Brazilian national squad played a friendly at Euro 2016 in a friendly against Estonia . Head Entity : Estonians , Tail Entity : Brazilian national squad .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : participating team .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9270833333333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : publisher .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 354, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : spouse .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/4_ext.jsonl'}}
estimate vocab size: 8572
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8672, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.01it/s]Extractor Estimating: 2it [00:01,  1.10it/s]Extractor Estimating: 3it [00:02,  1.18it/s]Extractor Estimating: 4it [00:03,  1.28it/s]Extractor Estimating: 5it [00:04,  1.28it/s]Extractor Estimating: 6it [00:04,  1.27it/s]Extractor Estimating: 7it [00:05,  1.29it/s]Extractor Estimating: 8it [00:06,  1.31it/s]Extractor Estimating: 9it [00:07,  1.32it/s]Extractor Estimating: 10it [00:07,  1.38it/s]Extractor Estimating: 11it [00:08,  1.36it/s]Extractor Estimating: 12it [00:09,  1.32it/s]Extractor Estimating: 13it [00:10,  1.34it/s]Extractor Estimating: 14it [00:10,  1.36it/s]Extractor Estimating: 15it [00:11,  1.38it/s]Extractor Estimating: 16it [00:12,  1.35it/s]Extractor Estimating: 17it [00:13,  1.32it/s]Extractor Estimating: 18it [00:13,  1.34it/s]Extractor Estimating: 19it [00:14,  1.37it/s]Extractor Estimating: 20it [00:15,  1.35it/s]Extractor Estimating: 21it [00:16,  1.32it/s]Extractor Estimating: 22it [00:16,  1.35it/s]Extractor Estimating: 23it [00:17,  1.35it/s]Extractor Estimating: 24it [00:18,  1.32it/s]Extractor Estimating: 25it [00:19,  1.31it/s]Extractor Estimating: 26it [00:19,  1.28it/s]Extractor Estimating: 27it [00:20,  1.27it/s]Extractor Estimating: 28it [00:21,  1.28it/s]Extractor Estimating: 29it [00:22,  1.25it/s]Extractor Estimating: 30it [00:23,  1.23it/s]Extractor Estimating: 31it [00:23,  1.24it/s]Extractor Estimating: 32it [00:24,  1.23it/s]Extractor Estimating: 33it [00:25,  1.21it/s]Extractor Estimating: 34it [00:26,  1.26it/s]Extractor Estimating: 35it [00:27,  1.30it/s]Extractor Estimating: 36it [00:27,  1.31it/s]Extractor Estimating: 37it [00:28,  1.29it/s]Extractor Estimating: 38it [00:29,  1.31it/s]Extractor Estimating: 39it [00:30,  1.29it/s]Extractor Estimating: 40it [00:30,  1.24it/s]Extractor Estimating: 41it [00:31,  1.26it/s]Extractor Estimating: 42it [00:32,  1.28it/s]Extractor Estimating: 43it [00:33,  1.27it/s]Extractor Estimating: 44it [00:33,  1.33it/s]Extractor Estimating: 45it [00:34,  1.36it/s]Extractor Estimating: 46it [00:35,  1.36it/s]Extractor Estimating: 47it [00:36,  1.35it/s]Extractor Estimating: 48it [00:36,  1.32it/s]Extractor Estimating: 49it [00:37,  1.35it/s]Extractor Estimating: 50it [00:38,  1.29it/s]Extractor Estimating: 51it [00:39,  1.26it/s]Extractor Estimating: 52it [00:40,  1.28it/s]Extractor Estimating: 53it [00:41,  1.22it/s]Extractor Estimating: 54it [00:41,  1.25it/s]Extractor Estimating: 55it [00:42,  1.26it/s]Extractor Estimating: 56it [00:43,  1.28it/s]Extractor Estimating: 57it [00:44,  1.27it/s]Extractor Estimating: 58it [00:44,  1.26it/s]Extractor Estimating: 59it [00:45,  1.31it/s]Extractor Estimating: 60it [00:46,  1.31it/s]Extractor Estimating: 61it [00:47,  1.31it/s]Extractor Estimating: 62it [00:47,  1.30it/s]Extractor Estimating: 63it [00:48,  1.35it/s]Extractor Estimating: 64it [00:49,  1.35it/s]Extractor Estimating: 65it [00:50,  1.37it/s]Extractor Estimating: 66it [00:50,  1.36it/s]Extractor Estimating: 67it [00:51,  1.29it/s]Extractor Estimating: 68it [00:52,  1.31it/s]Extractor Estimating: 69it [00:53,  1.35it/s]Extractor Estimating: 70it [00:53,  1.38it/s]Extractor Estimating: 71it [00:54,  1.36it/s]Extractor Estimating: 72it [00:55,  1.37it/s]Extractor Estimating: 73it [00:56,  1.34it/s]Extractor Estimating: 74it [00:56,  1.32it/s]Extractor Estimating: 75it [00:57,  1.29it/s]Extractor Estimating: 76it [00:58,  1.30it/s]Extractor Estimating: 77it [00:59,  1.32it/s]Extractor Estimating: 78it [00:59,  1.36it/s]Extractor Estimating: 79it [01:00,  1.36it/s]Extractor Estimating: 80it [01:01,  1.35it/s]Extractor Estimating: 81it [01:02,  1.35it/s]Extractor Estimating: 82it [01:02,  1.37it/s]Extractor Estimating: 83it [01:03,  1.38it/s]Extractor Estimating: 84it [01:04,  1.34it/s]Extractor Estimating: 85it [01:04,  1.34it/s]Extractor Estimating: 86it [01:05,  1.37it/s]Extractor Estimating: 87it [01:06,  1.20it/s]Extractor Estimating: 88it [01:07,  1.27it/s]Extractor Estimating: 89it [01:08,  1.32it/s]Extractor Estimating: 90it [01:08,  1.30it/s]Extractor Estimating: 91it [01:09,  1.31it/s]Extractor Estimating: 92it [01:10,  1.28it/s]Extractor Estimating: 93it [01:11,  1.28it/s]Extractor Estimating: 94it [01:12,  1.30it/s]Extractor Estimating: 95it [01:12,  1.30it/s]Extractor Estimating: 96it [01:13,  1.29it/s]Extractor Estimating: 97it [01:14,  1.29it/s]Extractor Estimating: 98it [01:15,  1.31it/s]Extractor Estimating: 99it [01:15,  1.30it/s]Extractor Estimating: 100it [01:16,  1.37it/s]Extractor Estimating: 101it [01:17,  1.38it/s]Extractor Estimating: 102it [01:17,  1.36it/s]Extractor Estimating: 103it [01:18,  1.33it/s]Extractor Estimating: 104it [01:19,  1.32it/s]Extractor Estimating: 105it [01:20,  1.34it/s]Extractor Estimating: 106it [01:21,  1.26it/s]Extractor Estimating: 107it [01:21,  1.30it/s]Extractor Estimating: 108it [01:22,  1.29it/s]Extractor Estimating: 109it [01:23,  1.36it/s]Extractor Estimating: 110it [01:23,  1.40it/s]Extractor Estimating: 111it [01:24,  1.37it/s]Extractor Estimating: 112it [01:25,  1.37it/s]Extractor Estimating: 113it [01:26,  1.32it/s]Extractor Estimating: 114it [01:27,  1.33it/s]Extractor Estimating: 115it [01:27,  1.37it/s]Extractor Estimating: 116it [01:28,  1.39it/s]Extractor Estimating: 117it [01:29,  1.36it/s]Extractor Estimating: 118it [01:29,  1.35it/s]Extractor Estimating: 119it [01:30,  1.36it/s]Extractor Estimating: 120it [01:31,  1.35it/s]Extractor Estimating: 121it [01:32,  1.37it/s]Extractor Estimating: 122it [01:32,  1.38it/s]Extractor Estimating: 123it [01:33,  1.34it/s]Extractor Estimating: 124it [01:34,  1.35it/s]Extractor Estimating: 125it [01:35,  1.37it/s]Extractor Estimating: 126it [01:35,  1.39it/s]Extractor Estimating: 127it [01:36,  1.37it/s]Extractor Estimating: 128it [01:37,  1.40it/s]Extractor Estimating: 129it [01:37,  1.42it/s]Extractor Estimating: 130it [01:38,  1.42it/s]Extractor Estimating: 131it [01:39,  1.40it/s]Extractor Estimating: 132it [01:39,  1.42it/s]Extractor Estimating: 133it [01:40,  1.44it/s]Extractor Estimating: 134it [01:41,  1.44it/s]Extractor Estimating: 135it [01:42,  1.37it/s]Extractor Estimating: 136it [01:42,  1.39it/s]Extractor Estimating: 137it [01:43,  1.39it/s]Extractor Estimating: 138it [01:44,  1.34it/s]Extractor Estimating: 139it [01:45,  1.30it/s]Extractor Estimating: 140it [01:45,  1.33it/s]Extractor Estimating: 141it [01:46,  1.38it/s]Extractor Estimating: 142it [01:47,  1.43it/s]Extractor Estimating: 143it [01:47,  1.41it/s]Extractor Estimating: 144it [01:48,  1.42it/s]Extractor Estimating: 145it [01:49,  1.39it/s]Extractor Estimating: 146it [01:50,  1.29it/s]Extractor Estimating: 147it [01:51,  1.29it/s]Extractor Estimating: 148it [01:51,  1.29it/s]Extractor Estimating: 149it [01:52,  1.32it/s]Extractor Estimating: 150it [01:53,  1.31it/s]Extractor Estimating: 151it [01:54,  1.32it/s]Extractor Estimating: 152it [01:54,  1.28it/s]Extractor Estimating: 153it [01:55,  1.31it/s]Extractor Estimating: 154it [01:56,  1.32it/s]Extractor Estimating: 155it [01:57,  1.28it/s]Extractor Estimating: 156it [01:57,  1.29it/s]Extractor Estimating: 157it [01:58,  1.34it/s]Extractor Estimating: 158it [01:59,  1.33it/s]Extractor Estimating: 159it [02:00,  1.36it/s]Extractor Estimating: 160it [02:00,  1.33it/s]Extractor Estimating: 161it [02:01,  1.32it/s]Extractor Estimating: 162it [02:02,  1.31it/s]Extractor Estimating: 163it [02:03,  1.26it/s]Extractor Estimating: 164it [02:03,  1.34it/s]Extractor Estimating: 165it [02:04,  1.34it/s]Extractor Estimating: 166it [02:05,  1.31it/s]Extractor Estimating: 167it [02:06,  1.35it/s]Extractor Estimating: 168it [02:06,  1.32it/s]Extractor Estimating: 169it [02:07,  1.35it/s]Extractor Estimating: 170it [02:08,  1.32it/s]Extractor Estimating: 171it [02:09,  1.34it/s]Extractor Estimating: 172it [02:09,  1.36it/s]Extractor Estimating: 173it [02:10,  1.38it/s]Extractor Estimating: 174it [02:11,  1.33it/s]Extractor Estimating: 175it [02:12,  1.35it/s]Extractor Estimating: 176it [02:12,  1.37it/s]Extractor Estimating: 177it [02:13,  1.29it/s]Extractor Estimating: 178it [02:14,  1.29it/s]Extractor Estimating: 179it [02:15,  1.30it/s]Extractor Estimating: 180it [02:16,  1.30it/s]Extractor Estimating: 181it [02:16,  1.27it/s]Extractor Estimating: 182it [02:17,  1.28it/s]Extractor Estimating: 183it [02:18,  1.33it/s]Extractor Estimating: 184it [02:19,  1.34it/s]Extractor Estimating: 185it [02:19,  1.34it/s]Extractor Estimating: 186it [02:20,  1.35it/s]Extractor Estimating: 187it [02:21,  1.32it/s]Extractor Estimating: 188it [02:22,  1.31it/s]Extractor Estimating: 189it [02:22,  1.31it/s]Extractor Estimating: 190it [02:23,  1.26it/s]Extractor Estimating: 191it [02:24,  1.28it/s]Extractor Estimating: 192it [02:25,  1.32it/s]Extractor Estimating: 193it [02:25,  1.32it/s]Extractor Estimating: 194it [02:26,  1.34it/s]Extractor Estimating: 195it [02:27,  1.32it/s]Extractor Estimating: 196it [02:28,  1.35it/s]Extractor Estimating: 197it [02:28,  1.38it/s]Extractor Estimating: 198it [02:29,  1.40it/s]Extractor Estimating: 199it [02:30,  1.38it/s]Extractor Estimating: 200it [02:31,  1.38it/s]Extractor Estimating: 201it [02:31,  1.34it/s]Extractor Estimating: 202it [02:32,  1.32it/s]Extractor Estimating: 203it [02:33,  1.31it/s]Extractor Estimating: 204it [02:34,  1.28it/s]Extractor Estimating: 205it [02:34,  1.29it/s]Extractor Estimating: 206it [02:35,  1.30it/s]Extractor Estimating: 207it [02:36,  1.27it/s]Extractor Estimating: 208it [02:37,  1.24it/s]Extractor Estimating: 209it [02:38,  1.23it/s]Extractor Estimating: 210it [02:38,  1.26it/s]Extractor Estimating: 211it [02:39,  1.28it/s]Extractor Estimating: 212it [02:40,  1.29it/s]Extractor Estimating: 213it [02:41,  1.31it/s]Extractor Estimating: 214it [02:42,  1.29it/s]Extractor Estimating: 215it [02:42,  1.24it/s]Extractor Estimating: 216it [02:43,  1.21it/s]Extractor Estimating: 217it [02:44,  1.19it/s]Extractor Estimating: 218it [02:45,  1.24it/s]Extractor Estimating: 219it [02:46,  1.28it/s]Extractor Estimating: 220it [02:46,  1.29it/s]Extractor Estimating: 221it [02:47,  1.28it/s]Extractor Estimating: 222it [02:48,  1.30it/s]Extractor Estimating: 223it [02:49,  1.27it/s]Extractor Estimating: 224it [02:49,  1.28it/s]Extractor Estimating: 225it [02:50,  1.32it/s]Extractor Estimating: 226it [02:51,  1.35it/s]Extractor Estimating: 227it [02:52,  1.32it/s]Extractor Estimating: 228it [02:52,  1.35it/s]Extractor Estimating: 229it [02:53,  1.37it/s]Extractor Estimating: 230it [02:54,  1.37it/s]Extractor Estimating: 231it [02:55,  1.34it/s]Extractor Estimating: 232it [02:56,  1.23it/s]Extractor Estimating: 233it [02:56,  1.23it/s]Extractor Estimating: 234it [02:57,  1.25it/s]Extractor Estimating: 235it [02:58,  1.26it/s]Extractor Estimating: 236it [02:59,  1.26it/s]Extractor Estimating: 237it [02:59,  1.29it/s]Extractor Estimating: 238it [03:00,  1.30it/s]Extractor Estimating: 239it [03:01,  1.28it/s]Extractor Estimating: 240it [03:02,  1.28it/s]Extractor Estimating: 241it [03:03,  1.30it/s]Extractor Estimating: 242it [03:03,  1.32it/s]Extractor Estimating: 243it [03:04,  1.36it/s]Extractor Estimating: 244it [03:05,  1.35it/s]Extractor Estimating: 245it [03:06,  1.32it/s]Extractor Estimating: 246it [03:06,  1.34it/s]Extractor Estimating: 247it [03:07,  1.34it/s]Extractor Estimating: 248it [03:08,  1.32it/s]Extractor Estimating: 249it [03:09,  1.29it/s]Extractor Estimating: 250it [03:09,  1.33it/s]Extractor Estimating: 251it [03:10,  1.32it/s]Extractor Estimating: 252it [03:11,  1.33it/s]Extractor Estimating: 253it [03:12,  1.28it/s]Extractor Estimating: 254it [03:12,  1.30it/s]Extractor Estimating: 255it [03:13,  1.29it/s]Extractor Estimating: 256it [03:14,  1.33it/s]Extractor Estimating: 257it [03:15,  1.35it/s]Extractor Estimating: 258it [03:15,  1.30it/s]Extractor Estimating: 259it [03:16,  1.32it/s]Extractor Estimating: 260it [03:17,  1.31it/s]Extractor Estimating: 261it [03:18,  1.29it/s]Extractor Estimating: 262it [03:18,  1.31it/s]Extractor Estimating: 263it [03:19,  1.29it/s]Extractor Estimating: 264it [03:20,  1.32it/s]Extractor Estimating: 265it [03:21,  1.31it/s]Extractor Estimating: 266it [03:22,  1.33it/s]Extractor Estimating: 267it [03:22,  1.32it/s]Extractor Estimating: 268it [03:23,  1.34it/s]Extractor Estimating: 269it [03:24,  1.33it/s]Extractor Estimating: 270it [03:24,  1.35it/s]Extractor Estimating: 271it [03:25,  1.33it/s]Extractor Estimating: 272it [03:26,  1.36it/s]Extractor Estimating: 273it [03:27,  1.33it/s]Extractor Estimating: 274it [03:28,  1.29it/s]Extractor Estimating: 275it [03:28,  1.34it/s]Extractor Estimating: 276it [03:29,  1.37it/s]Extractor Estimating: 277it [03:30,  1.39it/s]Extractor Estimating: 278it [03:30,  1.38it/s]Extractor Estimating: 279it [03:31,  1.38it/s]Extractor Estimating: 280it [03:32,  1.37it/s]Extractor Estimating: 281it [03:33,  1.38it/s]Extractor Estimating: 282it [03:33,  1.36it/s]Extractor Estimating: 283it [03:34,  1.38it/s]Extractor Estimating: 284it [03:35,  1.39it/s]Extractor Estimating: 285it [03:35,  1.36it/s]Extractor Estimating: 286it [03:36,  1.33it/s]Extractor Estimating: 287it [03:37,  1.40it/s]Extractor Estimating: 288it [03:38,  1.37it/s]Extractor Estimating: 289it [03:38,  1.40it/s]Extractor Estimating: 290it [03:39,  1.37it/s]Extractor Estimating: 291it [03:40,  1.36it/s]Extractor Estimating: 292it [03:41,  1.37it/s]Extractor Estimating: 293it [03:41,  1.29it/s]Extractor Estimating: 294it [03:42,  1.32it/s]Extractor Estimating: 295it [03:43,  1.33it/s]Extractor Estimating: 296it [03:44,  1.36it/s]Extractor Estimating: 297it [03:44,  1.37it/s]Extractor Estimating: 298it [03:45,  1.34it/s]Extractor Estimating: 299it [03:46,  1.33it/s]Extractor Estimating: 300it [03:47,  1.38it/s]Extractor Estimating: 301it [03:47,  1.44it/s]Extractor Estimating: 302it [03:48,  1.44it/s]Extractor Estimating: 303it [03:49,  1.43it/s]Extractor Estimating: 304it [03:49,  1.51it/s]Extractor Estimating: 305it [03:50,  1.41it/s]Extractor Estimating: 306it [03:51,  1.42it/s]Extractor Estimating: 307it [03:51,  1.41it/s]Extractor Estimating: 308it [03:52,  1.43it/s]Extractor Estimating: 309it [03:53,  1.48it/s]Extractor Estimating: 310it [03:53,  1.52it/s]Extractor Estimating: 311it [03:54,  1.47it/s]Extractor Estimating: 312it [03:55,  1.51it/s]Extractor Estimating: 313it [03:55,  1.45it/s]Extractor Estimating: 314it [03:56,  1.46it/s]Extractor Estimating: 315it [03:57,  1.51it/s]Extractor Estimating: 316it [03:57,  1.46it/s]Extractor Estimating: 317it [03:58,  1.48it/s]Extractor Estimating: 318it [03:59,  1.53it/s]Extractor Estimating: 319it [03:59,  1.51it/s]Extractor Estimating: 320it [04:00,  1.26it/s]Extractor Estimating: 321it [04:01,  1.30it/s]Extractor Estimating: 322it [04:02,  1.39it/s]Extractor Estimating: 323it [04:02,  1.40it/s]Extractor Estimating: 324it [04:03,  1.43it/s]Extractor Estimating: 325it [04:04,  1.41it/s]Extractor Estimating: 326it [04:05,  1.35it/s]Extractor Estimating: 327it [04:05,  1.35it/s]Extractor Estimating: 328it [04:06,  1.34it/s]Extractor Estimating: 329it [04:07,  1.36it/s]Extractor Estimating: 330it [04:08,  1.38it/s]Extractor Estimating: 331it [04:08,  1.38it/s]Extractor Estimating: 332it [04:09,  1.35it/s]Extractor Estimating: 333it [04:10,  1.37it/s]Extractor Estimating: 334it [04:11,  1.37it/s]Extractor Estimating: 335it [04:11,  1.36it/s]Extractor Estimating: 336it [04:12,  1.34it/s]Extractor Estimating: 337it [04:13,  1.31it/s]Extractor Estimating: 338it [04:14,  1.35it/s]Extractor Estimating: 339it [04:14,  1.36it/s]Extractor Estimating: 340it [04:15,  1.37it/s]Extractor Estimating: 341it [04:16,  1.35it/s]Extractor Estimating: 342it [04:17,  1.33it/s]Extractor Estimating: 343it [04:17,  1.34it/s]Extractor Estimating: 344it [04:18,  1.37it/s]Extractor Estimating: 345it [04:19,  1.37it/s]Extractor Estimating: 346it [04:19,  1.34it/s]Extractor Estimating: 347it [04:20,  1.33it/s]Extractor Estimating: 348it [04:21,  1.24it/s]Extractor Estimating: 349it [04:22,  1.27it/s]Extractor Estimating: 350it [04:23,  1.28it/s]Extractor Estimating: 351it [04:23,  1.29it/s]Extractor Estimating: 352it [04:24,  1.31it/s]Extractor Estimating: 353it [04:25,  1.34it/s]Extractor Estimating: 354it [04:26,  1.35it/s]Extractor Estimating: 355it [04:27,  1.27it/s]Extractor Estimating: 356it [04:27,  1.30it/s]Extractor Estimating: 357it [04:28,  1.36it/s]Extractor Estimating: 358it [04:29,  1.32it/s]Extractor Estimating: 359it [04:30,  1.28it/s]Extractor Estimating: 360it [04:30,  1.32it/s]Extractor Estimating: 361it [04:31,  1.34it/s]Extractor Estimating: 362it [04:32,  1.35it/s]Extractor Estimating: 363it [04:32,  1.34it/s]Extractor Estimating: 364it [04:33,  1.37it/s]Extractor Estimating: 365it [04:34,  1.36it/s]Extractor Estimating: 366it [04:35,  1.33it/s]Extractor Estimating: 367it [04:35,  1.33it/s]Extractor Estimating: 368it [04:36,  1.34it/s]Extractor Estimating: 369it [04:37,  1.36it/s]Extractor Estimating: 370it [04:38,  1.33it/s]Extractor Estimating: 371it [04:38,  1.32it/s]Extractor Estimating: 372it [04:39,  1.36it/s]Extractor Estimating: 373it [04:40,  1.34it/s]Extractor Estimating: 374it [04:41,  1.26it/s]Extractor Estimating: 375it [04:41,  1.51it/s]Extractor Estimating: 375it [04:41,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:11,248 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:11,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:11,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:11,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:11,255 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:04:11,641 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:04:11,642 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:04:12,319 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:04:13,371 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:04:13,371 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:15,094 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:15,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:15,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:15,097 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:04:15,098 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:04:15,840 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:04:15,841 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:04:16,104 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:04:16,263 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:04:16,263 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:58:24,144 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:58:24,167 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_1/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7487 mean pseudo reward: 0.9298432904866796
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl'}
train vocab size: 16818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16918, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.346, loss:719.6534
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.352, loss:683.6912
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.343, loss:689.8747
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 1.344, loss:649.6046
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 1.369, loss:636.0988
>> valid entity prec:0.4549, rec:0.4900, f1:0.4718
>> valid relation prec:0.0950, rec:0.0233, f1:0.0374
>> valid relation with NER prec:0.0950, rec:0.0233, f1:0.0374
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 3.025, loss:669.6508
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 1.348, loss:611.9426
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 1.348, loss:616.0881
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 1.338, loss:649.3161
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 1.341, loss:612.7879
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4710, rec:0.4136, f1:0.4404
>> valid relation prec:0.0363, rec:0.0112, f1:0.0171
>> valid relation with NER prec:0.0363, rec:0.0112, f1:0.0171
g_step 1100, step 164, avg_time 3.038, loss:620.1043
g_step 1200, step 264, avg_time 1.352, loss:613.1956
g_step 1300, step 52, avg_time 1.332, loss:601.6340
g_step 1400, step 152, avg_time 1.349, loss:571.8503
g_step 1500, step 252, avg_time 1.348, loss:590.6243
>> valid entity prec:0.4587, rec:0.4740, f1:0.4662
>> valid relation prec:0.0910, rec:0.0293, f1:0.0443
>> valid relation with NER prec:0.0910, rec:0.0293, f1:0.0443
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 40, avg_time 3.037, loss:567.8999
g_step 1700, step 140, avg_time 1.338, loss:539.4922
g_step 1800, step 240, avg_time 1.347, loss:564.8090
g_step 1900, step 28, avg_time 1.340, loss:565.8324
g_step 2000, step 128, avg_time 1.336, loss:518.0743
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4549, rec:0.4793, f1:0.4668
>> valid relation prec:0.0896, rec:0.0391, f1:0.0544
>> valid relation with NER prec:0.0896, rec:0.0391, f1:0.0544
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 228, avg_time 3.043, loss:548.7221
g_step 2200, step 16, avg_time 1.354, loss:536.0405
g_step 2300, step 116, avg_time 1.341, loss:510.6187
g_step 2400, step 216, avg_time 1.345, loss:520.3944
g_step 2500, step 4, avg_time 1.352, loss:526.5798
>> valid entity prec:0.4660, rec:0.4770, f1:0.4714
>> valid relation prec:0.0939, rec:0.0408, f1:0.0569
>> valid relation with NER prec:0.0939, rec:0.0408, f1:0.0569
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 104, avg_time 3.043, loss:484.8683
g_step 2700, step 204, avg_time 1.349, loss:511.3517
g_step 2800, step 304, avg_time 1.321, loss:483.3915
g_step 2900, step 92, avg_time 1.357, loss:461.7933
g_step 3000, step 192, avg_time 1.351, loss:474.2042
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4459, rec:0.4618, f1:0.4537
>> valid relation prec:0.0591, rec:0.0230, f1:0.0331
>> valid relation with NER prec:0.0591, rec:0.0230, f1:0.0331
g_step 3100, step 292, avg_time 3.034, loss:483.0355
g_step 3200, step 80, avg_time 1.360, loss:457.6833
g_step 3300, step 180, avg_time 1.341, loss:454.3628
g_step 3400, step 280, avg_time 1.352, loss:453.5598
g_step 3500, step 68, avg_time 1.344, loss:441.3564
>> valid entity prec:0.4621, rec:0.4692, f1:0.4657
>> valid relation prec:0.0742, rec:0.0299, f1:0.0426
>> valid relation with NER prec:0.0742, rec:0.0299, f1:0.0426
g_step 3600, step 168, avg_time 3.032, loss:441.5535
g_step 3700, step 268, avg_time 1.340, loss:445.8248
g_step 3800, step 56, avg_time 1.347, loss:419.2970
g_step 3900, step 156, avg_time 1.350, loss:408.5111
g_step 4000, step 256, avg_time 1.343, loss:453.4423
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4736, rec:0.3952, f1:0.4309
>> valid relation prec:0.0570, rec:0.0210, f1:0.0307
>> valid relation with NER prec:0.0570, rec:0.0210, f1:0.0307
g_step 4100, step 44, avg_time 3.034, loss:421.3205
g_step 4200, step 144, avg_time 1.366, loss:402.5996
g_step 4300, step 244, avg_time 1.349, loss:423.8104
g_step 4400, step 32, avg_time 1.367, loss:411.9120
g_step 4500, step 132, avg_time 1.347, loss:382.7863
>> valid entity prec:0.4526, rec:0.3707, f1:0.4076
>> valid relation prec:0.0680, rec:0.0273, f1:0.0390
>> valid relation with NER prec:0.0680, rec:0.0273, f1:0.0390
g_step 4600, step 232, avg_time 3.025, loss:410.9968
g_step 4700, step 20, avg_time 1.353, loss:416.1584
g_step 4800, step 120, avg_time 1.364, loss:392.6095
g_step 4900, step 220, avg_time 1.380, loss:393.6241
g_step 5000, step 8, avg_time 1.337, loss:397.1800
learning rate was adjusted to 0.0008
>> valid entity prec:0.4449, rec:0.4312, f1:0.4380
>> valid relation prec:0.0700, rec:0.0316, f1:0.0436
>> valid relation with NER prec:0.0700, rec:0.0316, f1:0.0436
g_step 5100, step 108, avg_time 3.041, loss:360.4235
g_step 5200, step 208, avg_time 1.365, loss:387.7940
g_step 5300, step 308, avg_time 1.352, loss:393.9135
g_step 5400, step 96, avg_time 1.351, loss:366.4968
g_step 5500, step 196, avg_time 1.360, loss:369.1470
>> valid entity prec:0.4288, rec:0.4321, f1:0.4304
>> valid relation prec:0.0590, rec:0.0253, f1:0.0354
>> valid relation with NER prec:0.0590, rec:0.0253, f1:0.0354
g_step 5600, step 296, avg_time 3.029, loss:354.8898
g_step 5700, step 84, avg_time 1.354, loss:350.2686
g_step 5800, step 184, avg_time 1.343, loss:351.0138
g_step 5900, step 284, avg_time 1.375, loss:370.4305
g_step 6000, step 72, avg_time 1.352, loss:342.0978
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.4556, rec:0.4413, f1:0.4484
>> valid relation prec:0.0710, rec:0.0331, f1:0.0451
>> valid relation with NER prec:0.0710, rec:0.0331, f1:0.0451
g_step 6100, step 172, avg_time 3.037, loss:350.6895
g_step 6200, step 272, avg_time 1.375, loss:348.8590
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:58:24 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:58:24 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-58-24_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:58:25 - WARNING - datasets.builder -   Using custom data configuration default-121c10ace20e9bac
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-121c10ace20e9bac/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:58:25,432 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:58:25,433 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:58:25,433 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:58:25,434 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:58:25,444 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:58:25,447 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:58:25,447 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:58:25,447 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:58:25,447 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:58:25,447 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:58:25,448 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:58:25,582 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:58:28,713 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:58:28,713 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-121c10ace20e9bac/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:02,  2.68ba/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.68ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.19ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:00,  4.48ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  4.67ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  4.79ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  4.86ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.79ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.79ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  4.05ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  3.29ba/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  3.75ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.89ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  4.34ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|â–ˆâ–Ž        | 1/8 [00:00<00:00,  8.25ba/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:00, 10.15ba/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:00<00:00, 10.23ba/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:00<00:00, 10.42ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 10.85ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:00,  8.34ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  9.23ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.65ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 10.96ba/s]
[INFO|trainer.py:414] 2023-08-29 08:58:32,889 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:58:32,905 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:58:32,905 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 08:58:32,905 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:58:32,905 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:58:32,905 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:58:32,905 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:58:32,905 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:57,  3.29it/s]  0%|          | 2/585 [00:00<02:52,  3.37it/s]  1%|          | 3/585 [00:00<02:51,  3.40it/s]  1%|          | 4/585 [00:01<02:50,  3.42it/s]  1%|          | 5/585 [00:01<02:49,  3.42it/s]  1%|          | 6/585 [00:01<02:49,  3.42it/s]  1%|          | 7/585 [00:02<02:48,  3.42it/s]  1%|â–         | 8/585 [00:02<02:48,  3.42it/s]  2%|â–         | 9/585 [00:02<02:48,  3.43it/s]  2%|â–         | 10/585 [00:02<02:47,  3.42it/s]  2%|â–         | 11/585 [00:03<02:47,  3.43it/s]  2%|â–         | 12/585 [00:03<02:47,  3.42it/s]  2%|â–         | 13/585 [00:03<02:47,  3.42it/s]  2%|â–         | 14/585 [00:04<02:46,  3.43it/s]  3%|â–Ž         | 15/585 [00:04<02:46,  3.43it/s]  3%|â–Ž         | 16/585 [00:04<02:45,  3.43it/s]  3%|â–Ž         | 17/585 [00:04<02:45,  3.44it/s]  3%|â–Ž         | 18/585 [00:05<02:44,  3.44it/s]  3%|â–Ž         | 19/585 [00:05<02:44,  3.44it/s]  3%|â–Ž         | 20/585 [00:05<02:44,  3.44it/s]  4%|â–Ž         | 21/585 [00:06<02:44,  3.44it/s]  4%|â–         | 22/585 [00:06<02:43,  3.44it/s]  4%|â–         | 23/585 [00:06<02:43,  3.44it/s]  4%|â–         | 24/585 [00:07<02:43,  3.43it/s]  4%|â–         | 25/585 [00:07<02:43,  3.43it/s]  4%|â–         | 26/585 [00:07<02:42,  3.43it/s]  5%|â–         | 27/585 [00:07<02:42,  3.44it/s]  5%|â–         | 28/585 [00:08<02:42,  3.43it/s]  5%|â–         | 29/585 [00:08<02:41,  3.44it/s]  5%|â–Œ         | 30/585 [00:08<02:41,  3.44it/s]  5%|â–Œ         | 31/585 [00:09<02:41,  3.44it/s]  5%|â–Œ         | 32/585 [00:09<02:40,  3.44it/s]  6%|â–Œ         | 33/585 [00:09<02:40,  3.44it/s]  6%|â–Œ         | 34/585 [00:09<02:40,  3.44it/s]  6%|â–Œ         | 35/585 [00:10<02:39,  3.44it/s]  6%|â–Œ         | 36/585 [00:10<02:39,  3.44it/s]  6%|â–‹         | 37/585 [00:10<02:39,  3.44it/s]  6%|â–‹         | 38/585 [00:11<02:38,  3.44it/s]  7%|â–‹         | 39/585 [00:11<02:38,  3.44it/s]  7%|â–‹         | 40/585 [00:11<02:38,  3.44it/s]  7%|â–‹         | 41/585 [00:11<02:38,  3.43it/s]  7%|â–‹         | 42/585 [00:12<02:38,  3.43it/s]  7%|â–‹         | 43/585 [00:12<02:37,  3.43it/s]  8%|â–Š         | 44/585 [00:12<02:37,  3.44it/s]  8%|â–Š         | 45/585 [00:13<02:37,  3.44it/s]  8%|â–Š         | 46/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 47/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 48/585 [00:13<02:36,  3.44it/s]  8%|â–Š         | 49/585 [00:14<02:35,  3.44it/s]  9%|â–Š         | 50/585 [00:14<02:35,  3.44it/s]  9%|â–Š         | 51/585 [00:14<02:35,  3.44it/s]  9%|â–‰         | 52/585 [00:15<02:34,  3.44it/s]  9%|â–‰         | 53/585 [00:15<02:34,  3.44it/s]  9%|â–‰         | 54/585 [00:15<02:34,  3.44it/s]  9%|â–‰         | 55/585 [00:16<02:34,  3.44it/s] 10%|â–‰         | 56/585 [00:16<02:33,  3.44it/s] 10%|â–‰         | 57/585 [00:16<02:33,  3.44it/s] 10%|â–‰         | 58/585 [00:16<02:33,  3.44it/s] 10%|â–ˆ         | 59/585 [00:17<02:33,  3.43it/s] 10%|â–ˆ         | 60/585 [00:17<02:32,  3.43it/s] 10%|â–ˆ         | 61/585 [00:17<02:32,  3.44it/s] 11%|â–ˆ         | 62/585 [00:18<02:32,  3.44it/s] 11%|â–ˆ         | 63/585 [00:18<02:31,  3.44it/s] 11%|â–ˆ         | 64/585 [00:18<02:31,  3.44it/s] 11%|â–ˆ         | 65/585 [00:18<02:31,  3.44it/s] 11%|â–ˆâ–        | 66/585 [00:19<02:30,  3.44it/s] 11%|â–ˆâ–        | 67/585 [00:19<02:30,  3.44it/s] 12%|â–ˆâ–        | 68/585 [00:19<02:30,  3.44it/s] 12%|â–ˆâ–        | 69/585 [00:20<02:30,  3.44it/s] 12%|â–ˆâ–        | 70/585 [00:20<02:29,  3.44it/s] 12%|â–ˆâ–        | 71/585 [00:20<02:29,  3.44it/s] 12%|â–ˆâ–        | 72/585 [00:20<02:29,  3.44it/s] 12%|â–ˆâ–        | 73/585 [00:21<02:28,  3.44it/s] 13%|â–ˆâ–Ž        | 74/585 [00:21<02:28,  3.44it/s] 13%|â–ˆâ–Ž        | 75/585 [00:21<02:28,  3.44it/s] 13%|â–ˆâ–Ž        | 76/585 [00:22<02:28,  3.42it/s] 13%|â–ˆâ–Ž        | 77/585 [00:22<02:28,  3.42it/s] 13%|â–ˆâ–Ž        | 78/585 [00:22<02:27,  3.43it/s] 14%|â–ˆâ–Ž        | 79/585 [00:23<02:27,  3.43it/s] 14%|â–ˆâ–Ž        | 80/585 [00:23<02:27,  3.43it/s] 14%|â–ˆâ–        | 81/585 [00:23<02:26,  3.43it/s] 14%|â–ˆâ–        | 82/585 [00:23<02:26,  3.43it/s] 14%|â–ˆâ–        | 83/585 [00:24<02:26,  3.43it/s] 14%|â–ˆâ–        | 84/585 [00:24<02:25,  3.43it/s] 15%|â–ˆâ–        | 85/585 [00:24<02:25,  3.43it/s] 15%|â–ˆâ–        | 86/585 [00:25<02:25,  3.43it/s] 15%|â–ˆâ–        | 87/585 [00:25<02:24,  3.43it/s] 15%|â–ˆâ–Œ        | 88/585 [00:25<02:24,  3.44it/s] 15%|â–ˆâ–Œ        | 89/585 [00:25<02:24,  3.44it/s] 15%|â–ˆâ–Œ        | 90/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 91/585 [00:26<02:23,  3.44it/s] 16%|â–ˆâ–Œ        | 92/585 [00:26<02:23,  3.43it/s] 16%|â–ˆâ–Œ        | 93/585 [00:27<02:23,  3.43it/s] 16%|â–ˆâ–Œ        | 94/585 [00:27<02:23,  3.43it/s] 16%|â–ˆâ–Œ        | 95/585 [00:27<02:22,  3.43it/s] 16%|â–ˆâ–‹        | 96/585 [00:27<02:22,  3.43it/s] 17%|â–ˆâ–‹        | 97/585 [00:28<02:22,  3.43it/s] 17%|â–ˆâ–‹        | 98/585 [00:28<02:21,  3.43it/s] 17%|â–ˆâ–‹        | 99/585 [00:28<02:21,  3.44it/s] 17%|â–ˆâ–‹        | 100/585 [00:29<02:21,  3.43it/s] 17%|â–ˆâ–‹        | 101/585 [00:29<02:20,  3.43it/s] 17%|â–ˆâ–‹        | 102/585 [00:29<02:20,  3.43it/s] 18%|â–ˆâ–Š        | 103/585 [00:29<02:20,  3.43it/s] 18%|â–ˆâ–Š        | 104/585 [00:30<02:20,  3.43it/s] 18%|â–ˆâ–Š        | 105/585 [00:30<02:19,  3.43it/s] 18%|â–ˆâ–Š        | 106/585 [00:30<02:19,  3.43it/s] 18%|â–ˆâ–Š        | 107/585 [00:31<02:19,  3.43it/s] 18%|â–ˆâ–Š        | 108/585 [00:31<02:19,  3.43it/s] 19%|â–ˆâ–Š        | 109/585 [00:31<02:18,  3.43it/s] 19%|â–ˆâ–‰        | 110/585 [00:32<02:18,  3.43it/s] 19%|â–ˆâ–‰        | 111/585 [00:32<02:18,  3.42it/s] 19%|â–ˆâ–‰        | 112/585 [00:32<02:18,  3.42it/s] 19%|â–ˆâ–‰        | 113/585 [00:32<02:17,  3.42it/s] 19%|â–ˆâ–‰        | 114/585 [00:33<02:17,  3.43it/s] 20%|â–ˆâ–‰        | 115/585 [00:33<02:17,  3.43it/s] 20%|â–ˆâ–‰        | 116/585 [00:33<02:16,  3.43it/s] 20%|â–ˆâ–ˆ        | 117/585 [00:34<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 08:59:07,046 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:59:07,047 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 08:59:07,047 >>   Batch size = 8

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.95it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.30it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.59it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.76it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.30it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.06it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.96it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.81it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.79it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.58it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.63it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.62it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.64it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.59it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.60it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.53it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.53it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.53it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.56it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.59it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.58it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.64it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.51it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.43it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.50it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.47it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.56it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.57it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.44it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.47it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.58it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.60it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.58it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.51it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.54it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.48it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.53it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.55it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.58it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.57it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.48it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.50it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.54it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.51it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.48it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.50it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.48it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.53it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.57it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.46it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.53it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.47it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.47it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.53it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.52it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.47it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.55it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.48it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.55it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.57it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.54it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.56it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.59it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.54it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.51it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.50it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.54it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.51it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.49it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.52it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.50it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.53it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.54it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.49it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.48it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.53it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.53it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.50it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.50it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.42it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.43it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.46it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.45it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.46it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.37it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.37it/s][A 20%|â–ˆâ–ˆ        | 117/585 [00:43<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:59:16,422 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 08:59:16,436 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:59:18,569 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:59:18,587 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:59:18,597 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|â–ˆâ–ˆ        | 118/585 [00:50<40:03,  5.15s/it] 20%|â–ˆâ–ˆ        | 119/585 [00:50<28:39,  3.69s/it] 21%|â–ˆâ–ˆ        | 120/585 [00:51<20:41,  2.67s/it] 21%|â–ˆâ–ˆ        | 121/585 [00:51<15:11,  1.96s/it] 21%|â–ˆâ–ˆ        | 122/585 [00:51<11:16,  1.46s/it] 21%|â–ˆâ–ˆ        | 123/585 [00:52<08:32,  1.11s/it] 21%|â–ˆâ–ˆ        | 124/585 [00:52<06:38,  1.16it/s] 21%|â–ˆâ–ˆâ–       | 125/585 [00:52<05:18,  1.44it/s] 22%|â–ˆâ–ˆâ–       | 126/585 [00:52<04:22,  1.75it/s] 22%|â–ˆâ–ˆâ–       | 127/585 [00:53<03:43,  2.05it/s] 22%|â–ˆâ–ˆâ–       | 128/585 [00:53<03:15,  2.34it/s] 22%|â–ˆâ–ˆâ–       | 129/585 [00:53<02:56,  2.58it/s] 22%|â–ˆâ–ˆâ–       | 130/585 [00:54<02:43,  2.79it/s] 22%|â–ˆâ–ˆâ–       | 131/585 [00:54<02:33,  2.95it/s] 23%|â–ˆâ–ˆâ–Ž       | 132/585 [00:54<02:26,  3.08it/s] 23%|â–ˆâ–ˆâ–Ž       | 133/585 [00:54<02:21,  3.18it/s] 23%|â–ˆâ–ˆâ–Ž       | 134/585 [00:55<02:18,  3.26it/s] 23%|â–ˆâ–ˆâ–Ž       | 135/585 [00:55<02:15,  3.31it/s] 23%|â–ˆâ–ˆâ–Ž       | 136/585 [00:55<02:14,  3.35it/s] 23%|â–ˆâ–ˆâ–Ž       | 137/585 [00:56<02:12,  3.37it/s] 24%|â–ˆâ–ˆâ–Ž       | 138/585 [00:56<02:11,  3.39it/s] 24%|â–ˆâ–ˆâ–       | 139/585 [00:56<02:10,  3.41it/s] 24%|â–ˆâ–ˆâ–       | 140/585 [00:56<02:10,  3.42it/s] 24%|â–ˆâ–ˆâ–       | 141/585 [00:57<02:10,  3.41it/s] 24%|â–ˆâ–ˆâ–       | 142/585 [00:57<02:09,  3.42it/s] 24%|â–ˆâ–ˆâ–       | 143/585 [00:57<02:08,  3.43it/s] 25%|â–ˆâ–ˆâ–       | 144/585 [00:58<02:08,  3.43it/s] 25%|â–ˆâ–ˆâ–       | 145/585 [00:58<02:08,  3.43it/s] 25%|â–ˆâ–ˆâ–       | 146/585 [00:58<02:07,  3.43it/s] 25%|â–ˆâ–ˆâ–Œ       | 147/585 [00:59<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 148/585 [00:59<02:07,  3.44it/s] 25%|â–ˆâ–ˆâ–Œ       | 149/585 [00:59<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 150/585 [00:59<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 151/585 [01:00<02:06,  3.44it/s] 26%|â–ˆâ–ˆâ–Œ       | 152/585 [01:00<02:06,  3.43it/s] 26%|â–ˆâ–ˆâ–Œ       | 153/585 [01:00<02:05,  3.43it/s] 26%|â–ˆâ–ˆâ–‹       | 154/585 [01:01<02:05,  3.44it/s] 26%|â–ˆâ–ˆâ–‹       | 155/585 [01:01<02:05,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 156/585 [01:01<02:04,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 157/585 [01:01<02:04,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 158/585 [01:02<02:04,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 159/585 [01:02<02:03,  3.44it/s] 27%|â–ˆâ–ˆâ–‹       | 160/585 [01:02<02:03,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 161/585 [01:03<02:03,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 162/585 [01:03<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 163/585 [01:03<02:02,  3.43it/s] 28%|â–ˆâ–ˆâ–Š       | 164/585 [01:03<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 165/585 [01:04<02:02,  3.44it/s] 28%|â–ˆâ–ˆâ–Š       | 166/585 [01:04<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–Š       | 167/585 [01:04<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–Š       | 168/585 [01:05<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 169/585 [01:05<02:01,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 170/585 [01:05<02:00,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 171/585 [01:05<02:00,  3.44it/s] 29%|â–ˆâ–ˆâ–‰       | 172/585 [01:06<02:00,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 173/585 [01:06<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–‰       | 174/585 [01:06<01:59,  3.43it/s] 30%|â–ˆâ–ˆâ–‰       | 175/585 [01:07<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 176/585 [01:07<01:59,  3.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 177/585 [01:07<01:58,  3.44it/s] 30%|â–ˆâ–ˆâ–ˆ       | 178/585 [01:08<01:58,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 179/585 [01:08<01:58,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 180/585 [01:08<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 181/585 [01:08<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆ       | 182/585 [01:09<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 183/585 [01:09<01:57,  3.44it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 184/585 [01:09<01:56,  3.44it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 185/585 [01:10<01:56,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 186/585 [01:10<01:56,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 187/585 [01:10<01:55,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 188/585 [01:10<01:55,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 189/585 [01:11<01:55,  3.43it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 190/585 [01:11<01:55,  3.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 191/585 [01:11<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 192/585 [01:12<01:54,  3.43it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 193/585 [01:12<01:54,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 194/585 [01:12<01:53,  3.44it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 195/585 [01:12<01:53,  3.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 196/585 [01:13<01:53,  3.42it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 197/585 [01:13<01:53,  3.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 198/585 [01:13<01:52,  3.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 199/585 [01:14<01:52,  3.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 200/585 [01:14<01:52,  3.43it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 201/585 [01:14<01:51,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 202/585 [01:15<01:51,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 203/585 [01:15<01:51,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 204/585 [01:15<01:50,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 205/585 [01:15<01:50,  3.44it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 206/585 [01:16<01:50,  3.43it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 207/585 [01:16<01:49,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 208/585 [01:16<01:49,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 209/585 [01:17<01:49,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 210/585 [01:17<01:49,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 211/585 [01:17<01:48,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 212/585 [01:17<01:48,  3.44it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 213/585 [01:18<01:48,  3.44it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 214/585 [01:18<01:48,  3.42it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 215/585 [01:18<01:47,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 216/585 [01:19<01:47,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 217/585 [01:19<01:47,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 218/585 [01:19<01:46,  3.43it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 219/585 [01:19<01:46,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 220/585 [01:20<01:46,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 221/585 [01:20<01:45,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 222/585 [01:20<01:45,  3.43it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 223/585 [01:21<01:45,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 224/585 [01:21<01:45,  3.44it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 225/585 [01:21<01:44,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 226/585 [01:22<01:44,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 227/585 [01:22<01:44,  3.43it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 228/585 [01:22<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 229/585 [01:22<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 230/585 [01:23<01:43,  3.44it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 231/585 [01:23<01:43,  3.44it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 232/585 [01:23<01:42,  3.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 233/585 [01:24<01:42,  3.43it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:24<01:42,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 08:59:57,307 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:59:57,307 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 08:59:57,307 >>   Batch size = 8
{'eval_loss': 1.0509828329086304, 'eval_runtime': 9.3554, 'eval_samples_per_second': 371.871, 'eval_steps_per_second': 46.497, 'epoch': 1.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.92it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.31it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.64it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.96it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.59it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.28it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.03it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.68it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.68it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.61it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.54it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.62it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.67it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.70it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.73it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.69it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.53it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.51it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.49it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.54it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.58it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.56it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.63it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.65it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.71it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.63it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.59it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.54it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.55it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.55it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.60it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.54it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.60it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.65it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.69it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.67it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.65it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.51it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.54it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.58it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.58it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.66it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.55it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.63it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.58it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.63it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.61it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.54it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.56it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.58it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.59it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.66it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.61it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.59it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.60it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.57it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.53it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.59it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.53it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.53it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.63it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.60it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.56it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.67it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.56it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.58it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.56it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.54it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.60it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.48it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.60it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.69it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.62it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.60it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.58it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.50it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.53it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.54it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.50it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.58it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.46it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.55it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.55it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.55it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.58it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.43it/s][A                                                 
                                                 [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 234/585 [01:33<01:42,  3.43it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:00:06,667 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 09:00:06,682 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:00:08,941 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:00:08,957 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:00:08,965 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 235/585 [01:40<30:16,  5.19s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 236/585 [01:41<21:38,  3.72s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 237/585 [01:41<15:36,  2.69s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 238/585 [01:41<11:24,  1.97s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 239/585 [01:42<08:27,  1.47s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 240/585 [01:42<06:24,  1.11s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 241/585 [01:42<04:58,  1.15it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242/585 [01:43<03:58,  1.44it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243/585 [01:43<03:16,  1.74it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244/585 [01:43<02:46,  2.05it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245/585 [01:43<02:25,  2.33it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246/585 [01:44<02:11,  2.58it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247/585 [01:44<02:01,  2.78it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248/585 [01:44<01:54,  2.95it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 249/585 [01:45<01:49,  3.08it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 250/585 [01:45<01:45,  3.18it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 251/585 [01:45<01:42,  3.26it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 252/585 [01:45<01:40,  3.31it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 253/585 [01:46<01:39,  3.35it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 254/585 [01:46<01:38,  3.37it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 255/585 [01:46<01:37,  3.40it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 256/585 [01:47<01:36,  3.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/585 [01:47<01:35,  3.42it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 258/585 [01:47<01:35,  3.41it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/585 [01:47<01:35,  3.42it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 260/585 [01:48<01:34,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 261/585 [01:48<01:34,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 262/585 [01:48<01:34,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 263/585 [01:49<01:33,  3.43it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 264/585 [01:49<01:33,  3.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 265/585 [01:49<01:33,  3.44it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 266/585 [01:49<01:32,  3.44it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 267/585 [01:50<01:32,  3.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 268/585 [01:50<01:32,  3.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 269/585 [01:50<01:32,  3.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 270/585 [01:51<01:31,  3.43it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 271/585 [01:51<01:33,  3.35it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 272/585 [01:51<01:32,  3.37it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 273/585 [01:52<01:31,  3.40it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 274/585 [01:52<01:31,  3.41it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 275/585 [01:52<01:30,  3.42it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 276/585 [01:52<01:30,  3.42it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 277/585 [01:53<01:29,  3.42it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 278/585 [01:53<01:29,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 279/585 [01:53<01:29,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 280/585 [01:54<01:29,  3.42it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 281/585 [01:54<01:28,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 282/585 [01:54<01:28,  3.43it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 283/585 [01:54<01:27,  3.43it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 284/585 [01:55<01:27,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 285/585 [01:55<01:27,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 286/585 [01:55<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 287/585 [01:56<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 288/585 [01:56<01:26,  3.44it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 289/585 [01:56<01:26,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 290/585 [01:56<01:25,  3.44it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 291/585 [01:57<01:26,  3.41it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 292/585 [01:57<01:25,  3.42it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 293/585 [01:57<01:25,  3.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 294/585 [01:58<01:24,  3.43it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 295/585 [01:58<01:24,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 296/585 [01:58<01:24,  3.43it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 297/585 [01:59<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 298/585 [01:59<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 299/585 [01:59<01:23,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300/585 [01:59<01:22,  3.44it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301/585 [02:00<01:22,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302/585 [02:00<01:22,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303/585 [02:00<01:22,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304/585 [02:01<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305/585 [02:01<01:21,  3.43it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306/585 [02:01<01:21,  3.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 307/585 [02:01<01:20,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 308/585 [02:02<01:20,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 309/585 [02:02<01:20,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 310/585 [02:02<01:19,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 311/585 [02:03<01:19,  3.44it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 312/585 [02:03<01:19,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 313/585 [02:03<01:19,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 314/585 [02:03<01:18,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 315/585 [02:04<01:18,  3.44it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/585 [02:04<01:18,  3.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 317/585 [02:04<01:18,  3.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/585 [02:05<01:17,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 319/585 [02:05<01:17,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/585 [02:05<01:17,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 321/585 [02:06<01:16,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 322/585 [02:06<01:16,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 323/585 [02:06<01:16,  3.43it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 324/585 [02:06<01:15,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 325/585 [02:07<01:15,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 326/585 [02:07<01:15,  3.44it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 327/585 [02:07<01:15,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 328/585 [02:08<01:14,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 329/585 [02:08<01:14,  3.43it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 330/585 [02:08<01:14,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 331/585 [02:08<01:13,  3.43it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 332/585 [02:09<01:13,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 333/585 [02:09<01:13,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 334/585 [02:09<01:13,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 335/585 [02:10<01:12,  3.44it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 336/585 [02:10<01:12,  3.44it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 337/585 [02:10<01:12,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 338/585 [02:10<01:12,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 339/585 [02:11<01:11,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 340/585 [02:11<01:11,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 341/585 [02:11<01:11,  3.43it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 342/585 [02:12<01:10,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 343/585 [02:12<01:10,  3.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 344/585 [02:12<01:10,  3.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 345/585 [02:13<01:09,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 346/585 [02:13<01:09,  3.43it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 347/585 [02:13<01:09,  3.44it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 348/585 [02:13<01:09,  3.43it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 349/585 [02:14<01:17,  3.05it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 350/585 [02:14<01:14,  3.15it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:14<01:12,  3.23it/s][INFO|trainer.py:2140] 2023-08-29 09:00:47,852 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:00:47,852 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 09:00:47,852 >>   Batch size = 8
{'eval_loss': 1.0656177997589111, 'eval_runtime': 9.3424, 'eval_samples_per_second': 372.389, 'eval_steps_per_second': 46.562, 'epoch': 2.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.15it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.56it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.74it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.99it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.52it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.23it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.09it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.58it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.66it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.66it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.63it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.72it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.75it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.74it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.78it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.62it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.44it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.48it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.49it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.63it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.70it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.66it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.66it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.69it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.71it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.58it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.46it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.53it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.49it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.63it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.66it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.68it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.69it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.60it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.61it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.60it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.50it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.54it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.61it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.56it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.66it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.56it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.65it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.65it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.62it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.48it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.51it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.50it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.61it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.62it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.63it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.61it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.62it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.62it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.58it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.46it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.54it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.45it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.55it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.65it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.66it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.63it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.63it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.55it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.59it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.51it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.57it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.62it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.48it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.58it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.66it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.60it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.59it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.61it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.45it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.56it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.48it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.55it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.60it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.57it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.49it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.55it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.52it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.56it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.53it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.53it/s][A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 351/585 [02:24<01:12,  3.23it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:00:57,221 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 09:00:57,244 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:00:59,443 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:00:59,457 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:00:59,466 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 352/585 [02:31<19:48,  5.10s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 353/585 [02:31<14:09,  3.66s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 354/585 [02:31<10:11,  2.65s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 355/585 [02:32<07:26,  1.94s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 356/585 [02:32<05:31,  1.45s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 357/585 [02:32<04:10,  1.10s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 358/585 [02:32<03:14,  1.17it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359/585 [02:33<02:35,  1.46it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360/585 [02:33<02:07,  1.76it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361/585 [02:33<01:48,  2.06it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362/585 [02:34<01:35,  2.34it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 363/585 [02:34<01:25,  2.59it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 364/585 [02:34<01:18,  2.80it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 365/585 [02:34<01:14,  2.96it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 366/585 [02:35<01:10,  3.08it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 367/585 [02:35<01:08,  3.18it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 368/585 [02:35<01:06,  3.26it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 369/585 [02:36<01:05,  3.31it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 370/585 [02:36<01:04,  3.35it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 371/585 [02:36<01:03,  3.38it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 372/585 [02:36<01:02,  3.40it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 373/585 [02:37<01:02,  3.41it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/585 [02:37<01:01,  3.42it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 375/585 [02:37<01:01,  3.42it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 376/585 [02:38<01:00,  3.43it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/585 [02:38<01:00,  3.42it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 378/585 [02:38<01:00,  3.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/585 [02:39<01:00,  3.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/585 [02:39<00:59,  3.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 381/585 [02:39<00:59,  3.43it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 382/585 [02:39<00:59,  3.44it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 383/585 [02:40<00:58,  3.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 384/585 [02:40<00:58,  3.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 385/585 [02:40<00:58,  3.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 386/585 [02:41<00:57,  3.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 387/585 [02:41<00:57,  3.44it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 388/585 [02:41<00:57,  3.42it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 389/585 [02:41<00:57,  3.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 390/585 [02:42<00:56,  3.43it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 391/585 [02:42<00:56,  3.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 392/585 [02:42<00:56,  3.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 393/585 [02:43<00:55,  3.44it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 394/585 [02:43<00:55,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 395/585 [02:43<00:55,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 396/585 [02:43<00:55,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 397/585 [02:44<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 398/585 [02:44<00:54,  3.44it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 399/585 [02:44<00:56,  3.32it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 400/585 [02:45<00:55,  3.35it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 401/585 [02:45<00:54,  3.38it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 402/585 [02:45<00:53,  3.40it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 403/585 [02:46<00:53,  3.41it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 404/585 [02:46<00:53,  3.41it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 405/585 [02:46<00:52,  3.42it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 406/585 [02:46<00:52,  3.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 407/585 [02:47<00:51,  3.42it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 408/585 [02:47<00:51,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 409/585 [02:47<00:51,  3.43it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 410/585 [02:48<00:53,  3.27it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 411/585 [02:48<00:52,  3.32it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 412/585 [02:48<00:51,  3.35it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 413/585 [02:49<00:50,  3.38it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 414/585 [02:49<00:50,  3.40it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 415/585 [02:49<00:49,  3.41it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 416/585 [02:49<00:49,  3.42it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417/585 [02:50<00:49,  3.42it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418/585 [02:50<00:48,  3.42it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 419/585 [02:50<00:48,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 420/585 [02:51<00:48,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 421/585 [02:51<00:47,  3.42it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 422/585 [02:51<00:47,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 423/585 [02:51<00:47,  3.43it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 424/585 [02:52<00:46,  3.43it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 425/585 [02:52<00:46,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 426/585 [02:52<00:46,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 427/585 [02:53<00:45,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 428/585 [02:53<00:45,  3.44it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 429/585 [02:53<00:45,  3.44it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 430/585 [02:53<00:45,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 431/585 [02:54<00:44,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 432/585 [02:54<00:44,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 433/585 [02:54<00:44,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 434/585 [02:55<00:43,  3.43it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 435/585 [02:55<00:43,  3.43it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 436/585 [02:55<00:43,  3.43it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 437/585 [02:55<00:43,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 438/585 [02:56<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 439/585 [02:56<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 440/585 [02:56<00:42,  3.44it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 441/585 [02:57<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 442/585 [02:57<00:41,  3.44it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 443/585 [02:57<00:41,  3.43it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 444/585 [02:58<00:41,  3.43it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 445/585 [02:58<00:40,  3.43it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 446/585 [02:58<00:40,  3.43it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 447/585 [02:58<00:40,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 448/585 [02:59<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 449/585 [02:59<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 450/585 [02:59<00:39,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 451/585 [03:00<00:38,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 452/585 [03:00<00:38,  3.44it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 453/585 [03:00<00:38,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 454/585 [03:00<00:38,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 455/585 [03:01<00:37,  3.43it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 456/585 [03:01<00:37,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 457/585 [03:01<00:37,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 458/585 [03:02<00:36,  3.44it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 459/585 [03:02<00:36,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 460/585 [03:02<00:36,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 461/585 [03:02<00:36,  3.43it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 462/585 [03:03<00:35,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 463/585 [03:03<00:35,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 464/585 [03:03<00:35,  3.44it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 465/585 [03:04<00:34,  3.44it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 466/585 [03:04<00:34,  3.44it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 467/585 [03:04<00:34,  3.44it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:05<00:34,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 09:01:37,981 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:01:37,981 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 09:01:37,981 >>   Batch size = 8
{'eval_loss': 1.0782859325408936, 'eval_runtime': 9.3414, 'eval_samples_per_second': 372.429, 'eval_steps_per_second': 46.567, 'epoch': 3.0}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 56.67it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.15it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.63it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 48.02it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.55it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.26it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 47.13it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.68it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.64it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.55it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.67it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.72it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.72it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.63it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.66it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.63it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.60it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.51it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.57it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.55it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.60it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.66it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.62it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.61it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.65it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.55it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.56it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.61it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.55it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.58it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.68it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.54it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.66it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.63it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.62it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.60it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.56it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.58it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.59it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.55it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.61it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.68it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.46it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.55it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.50it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.51it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.57it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.55it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.52it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.61it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.54it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.60it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.66it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.67it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.55it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.54it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.60it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.55it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.58it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.49it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.54it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.53it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.59it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.56it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.65it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.58it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.54it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.57it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.52it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.50it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.46it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.55it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.60it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.62it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.60it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.54it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.49it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.50it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.53it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.53it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.48it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.46it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.51it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.58it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.62it/s][A                                                 
                                                 [A 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 468/585 [03:14<00:34,  3.44it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.62it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:01:47,344 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 09:01:47,357 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:01:49,837 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:01:49,857 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:01:49,865 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 469/585 [03:23<11:03,  5.72s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 470/585 [03:23<07:50,  4.09s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 471/585 [03:24<05:36,  2.95s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 472/585 [03:24<04:03,  2.15s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 473/585 [03:24<02:58,  1.60s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 474/585 [03:24<02:13,  1.20s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 475/585 [03:25<01:42,  1.08it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 476/585 [03:25<01:20,  1.35it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 477/585 [03:25<01:05,  1.66it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 478/585 [03:26<00:54,  1.96it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 479/585 [03:26<00:47,  2.25it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 480/585 [03:26<00:41,  2.51it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 481/585 [03:26<00:38,  2.67it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 482/585 [03:27<00:36,  2.86it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 483/585 [03:27<00:33,  3.01it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 484/585 [03:27<00:32,  3.13it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 485/585 [03:28<00:31,  3.22it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 486/585 [03:28<00:30,  3.28it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 487/585 [03:28<00:29,  3.33it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 488/585 [03:28<00:28,  3.36it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 489/585 [03:29<00:28,  3.38it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 490/585 [03:29<00:27,  3.40it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 491/585 [03:29<00:27,  3.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 492/585 [03:30<00:27,  3.41it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 493/585 [03:30<00:26,  3.42it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 494/585 [03:30<00:26,  3.42it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 495/585 [03:31<00:26,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 496/585 [03:31<00:25,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 497/585 [03:31<00:25,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 498/585 [03:31<00:25,  3.43it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 499/585 [03:32<00:25,  3.44it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [03:32<00:24,  3.44it/s]                                                  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 500/585 [03:32<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 501/585 [03:32<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 502/585 [03:33<00:24,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 503/585 [03:33<00:23,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 504/585 [03:33<00:23,  3.43it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 505/585 [03:33<00:23,  3.44it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 506/585 [03:34<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 507/585 [03:34<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 508/585 [03:34<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 509/585 [03:35<00:22,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 510/585 [03:35<00:21,  3.44it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 511/585 [03:35<00:21,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 512/585 [03:35<00:21,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 513/585 [03:36<00:20,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 514/585 [03:36<00:20,  3.44it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 515/585 [03:36<00:20,  3.42it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 516/585 [03:37<00:20,  3.43it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 517/585 [03:37<00:19,  3.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 518/585 [03:37<00:19,  3.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 519/585 [03:37<00:19,  3.44it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 520/585 [03:38<00:18,  3.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 521/585 [03:38<00:18,  3.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 522/585 [03:38<00:18,  3.43it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 523/585 [03:39<00:18,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 524/585 [03:39<00:17,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 525/585 [03:39<00:17,  3.44it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 526/585 [03:40<00:17,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 527/585 [03:40<00:16,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 528/585 [03:40<00:16,  3.43it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 529/585 [03:40<00:16,  3.43it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 530/585 [03:41<00:16,  3.43it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 531/585 [03:41<00:15,  3.43it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 532/585 [03:41<00:15,  3.43it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 533/585 [03:42<00:15,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 534/585 [03:42<00:14,  3.44it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 535/585 [03:42<00:14,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 536/585 [03:42<00:14,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 537/585 [03:43<00:14,  3.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 538/585 [03:43<00:13,  3.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 539/585 [03:43<00:13,  3.43it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 540/585 [03:44<00:13,  3.44it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 541/585 [03:44<00:12,  3.43it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 542/585 [03:44<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 543/585 [03:44<00:12,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 544/585 [03:45<00:11,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 545/585 [03:45<00:11,  3.44it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 546/585 [03:45<00:11,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 547/585 [03:46<00:11,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 548/585 [03:46<00:10,  3.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 549/585 [03:46<00:10,  3.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 550/585 [03:47<00:10,  3.43it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 551/585 [03:47<00:09,  3.44it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 552/585 [03:47<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 553/585 [03:47<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 554/585 [03:48<00:09,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 555/585 [03:48<00:08,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 556/585 [03:48<00:08,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 557/585 [03:49<00:08,  3.44it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 558/585 [03:49<00:07,  3.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 559/585 [03:49<00:07,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 560/585 [03:49<00:07,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 561/585 [03:50<00:06,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 562/585 [03:50<00:06,  3.43it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 563/585 [03:50<00:06,  3.44it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 564/585 [03:51<00:06,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 565/585 [03:51<00:05,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 566/585 [03:51<00:05,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 567/585 [03:51<00:05,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 568/585 [03:52<00:04,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 569/585 [03:52<00:04,  3.44it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 570/585 [03:52<00:04,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 571/585 [03:53<00:04,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 572/585 [03:53<00:03,  3.44it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 573/585 [03:53<00:03,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 574/585 [03:54<00:03,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 575/585 [03:54<00:02,  3.43it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 576/585 [03:54<00:02,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 577/585 [03:54<00:02,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 578/585 [03:55<00:02,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 579/585 [03:55<00:01,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 580/585 [03:55<00:01,  3.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 581/585 [03:56<00:01,  3.33it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 582/585 [03:56<00:00,  3.36it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 583/585 [03:56<00:00,  3.38it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 584/585 [03:56<00:00,  3.40it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [03:57<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 09:02:30,153 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:02:30,153 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 09:02:30,153 >>   Batch size = 8
{'eval_loss': 1.091413974761963, 'eval_runtime': 9.3439, 'eval_samples_per_second': 372.329, 'eval_steps_per_second': 46.554, 'epoch': 4.0}
{'loss': 0.4111, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/435 [00:00<?, ?it/s][A
  1%|â–         | 6/435 [00:00<00:07, 57.17it/s][A
  3%|â–Ž         | 12/435 [00:00<00:08, 50.42it/s][A
  4%|â–         | 18/435 [00:00<00:08, 48.54it/s][A
  5%|â–Œ         | 23/435 [00:00<00:08, 47.84it/s][A
  6%|â–‹         | 28/435 [00:00<00:08, 47.38it/s][A
  8%|â–Š         | 33/435 [00:00<00:08, 47.04it/s][A
  9%|â–Š         | 38/435 [00:00<00:08, 46.87it/s][A
 10%|â–‰         | 43/435 [00:00<00:08, 46.66it/s][A
 11%|â–ˆ         | 48/435 [00:01<00:08, 46.59it/s][A
 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.51it/s][A
 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.61it/s][A
 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.57it/s][A
 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.61it/s][A
 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.72it/s][A
 18%|â–ˆâ–Š        | 78/435 [00:01<00:08, 42.81it/s][A
 19%|â–ˆâ–‰        | 83/435 [00:01<00:08, 43.91it/s][A
 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 44.70it/s][A
 21%|â–ˆâ–ˆâ–       | 93/435 [00:02<00:07, 45.32it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 45.74it/s][A
 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 45.97it/s][A
 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:07, 46.21it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.37it/s][A
 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.30it/s][A
 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.41it/s][A
 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.48it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.44it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.45it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.53it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.58it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.67it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.68it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.56it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.57it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.59it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.61it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.62it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:04<00:05, 46.50it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.53it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.61it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.59it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.63it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.66it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.47it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.61it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.62it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:05<00:04, 46.59it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.56it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.57it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.55it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.62it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.67it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.65it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.65it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.52it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.55it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.51it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.56it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.59it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.58it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.62it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.60it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.60it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.59it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.56it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:07<00:02, 46.44it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.53it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.55it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.64it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.57it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.52it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.59it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.58it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.58it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:08<00:01, 46.60it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.43it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.31it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.47it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.49it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.60it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.55it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.56it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.57it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.55it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.53it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.48it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.49it/s][A
                                                 [A                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.49it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:06<00:00,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:02:39,547 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 09:02:39,564 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:02:41,849 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:02:41,866 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:02:41,875 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 09:02:46,411 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 09:02:46,414 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117 (score: 1.0509828329086304).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:15<00:00,  3.41it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 585/585 [04:15<00:00,  2.29it/s]
[INFO|trainer.py:1894] 2023-08-29 09:02:48,127 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 09:02:48,146 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:02:50,370 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:02:50,387 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:02:50,395 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:02:50,590 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:50,590 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:50,590 >>   train_loss               =      0.408
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:50,590 >>   train_runtime            = 0:04:15.21
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:50,590 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:50,590 >>   train_samples_per_second =    146.937
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:50,590 >>   train_steps_per_second   =      2.292
{'eval_loss': 1.0968626737594604, 'eval_runtime': 9.3733, 'eval_samples_per_second': 371.16, 'eval_steps_per_second': 46.408, 'epoch': 5.0}
{'train_runtime': 255.2116, 'train_samples_per_second': 146.937, 'train_steps_per_second': 2.292, 'train_loss': 0.4080338111290565, 'epoch': 5.0}
08/29/2023 09:02:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 09:02:50,623 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:02:50,624 >>   Num examples = 3479
[INFO|trainer.py:2145] 2023-08-29 09:02:50,624 >>   Batch size = 8
  0%|          | 0/435 [00:00<?, ?it/s]  1%|â–         | 6/435 [00:00<00:07, 57.69it/s]  3%|â–Ž         | 12/435 [00:00<00:08, 50.76it/s]  4%|â–         | 18/435 [00:00<00:08, 48.91it/s]  5%|â–Œ         | 23/435 [00:00<00:08, 48.28it/s]  6%|â–‹         | 28/435 [00:00<00:08, 47.89it/s]  8%|â–Š         | 33/435 [00:00<00:08, 47.61it/s]  9%|â–Š         | 38/435 [00:00<00:08, 47.34it/s] 10%|â–‰         | 43/435 [00:00<00:08, 47.15it/s] 11%|â–ˆ         | 48/435 [00:00<00:08, 47.01it/s] 12%|â–ˆâ–        | 53/435 [00:01<00:08, 46.94it/s] 13%|â–ˆâ–Ž        | 58/435 [00:01<00:08, 46.92it/s] 14%|â–ˆâ–        | 63/435 [00:01<00:07, 46.95it/s] 16%|â–ˆâ–Œ        | 68/435 [00:01<00:07, 46.98it/s] 17%|â–ˆâ–‹        | 73/435 [00:01<00:07, 46.79it/s] 18%|â–ˆâ–Š        | 78/435 [00:01<00:07, 46.82it/s] 19%|â–ˆâ–‰        | 83/435 [00:01<00:07, 46.87it/s] 20%|â–ˆâ–ˆ        | 88/435 [00:01<00:07, 46.83it/s] 21%|â–ˆâ–ˆâ–       | 93/435 [00:01<00:07, 46.75it/s] 23%|â–ˆâ–ˆâ–Ž       | 98/435 [00:02<00:07, 46.71it/s] 24%|â–ˆâ–ˆâ–Ž       | 103/435 [00:02<00:07, 46.76it/s] 25%|â–ˆâ–ˆâ–       | 108/435 [00:02<00:06, 46.84it/s] 26%|â–ˆâ–ˆâ–Œ       | 113/435 [00:02<00:06, 46.85it/s] 27%|â–ˆâ–ˆâ–‹       | 118/435 [00:02<00:06, 46.79it/s] 28%|â–ˆâ–ˆâ–Š       | 123/435 [00:02<00:06, 46.85it/s] 29%|â–ˆâ–ˆâ–‰       | 128/435 [00:02<00:06, 46.83it/s] 31%|â–ˆâ–ˆâ–ˆ       | 133/435 [00:02<00:06, 46.86it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 138/435 [00:02<00:06, 46.87it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 143/435 [00:03<00:06, 46.79it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 148/435 [00:03<00:06, 46.76it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 153/435 [00:03<00:06, 46.70it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 158/435 [00:03<00:05, 46.79it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 163/435 [00:03<00:05, 46.77it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 168/435 [00:03<00:05, 46.77it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 173/435 [00:03<00:05, 46.67it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 178/435 [00:03<00:05, 46.84it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 183/435 [00:03<00:05, 46.77it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 188/435 [00:03<00:05, 46.81it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 193/435 [00:04<00:05, 46.77it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 198/435 [00:04<00:05, 46.75it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 203/435 [00:04<00:04, 46.71it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 208/435 [00:04<00:04, 46.70it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 213/435 [00:04<00:04, 46.52it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 218/435 [00:04<00:04, 46.49it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 223/435 [00:04<00:04, 46.47it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 228/435 [00:04<00:04, 46.46it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 233/435 [00:04<00:04, 46.45it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 238/435 [00:05<00:04, 46.11it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 243/435 [00:05<00:04, 46.39it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 248/435 [00:05<00:04, 46.49it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 253/435 [00:05<00:03, 46.62it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 258/435 [00:05<00:03, 46.64it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 263/435 [00:05<00:03, 46.55it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 268/435 [00:05<00:03, 46.66it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 273/435 [00:05<00:03, 46.69it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 278/435 [00:05<00:03, 46.66it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 283/435 [00:06<00:03, 46.72it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 288/435 [00:06<00:03, 46.82it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 293/435 [00:06<00:03, 46.64it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 298/435 [00:06<00:02, 46.72it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 303/435 [00:06<00:02, 46.74it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 308/435 [00:06<00:02, 46.71it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313/435 [00:06<00:02, 46.78it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 318/435 [00:06<00:02, 46.81it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 323/435 [00:06<00:02, 46.64it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 328/435 [00:06<00:02, 46.77it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 333/435 [00:07<00:02, 46.77it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 338/435 [00:07<00:02, 46.80it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 343/435 [00:07<00:01, 46.73it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 348/435 [00:07<00:01, 46.78it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 353/435 [00:07<00:01, 46.58it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 358/435 [00:07<00:01, 46.72it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 363/435 [00:07<00:01, 46.74it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 368/435 [00:07<00:01, 46.78it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 373/435 [00:07<00:01, 46.82it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 378/435 [00:08<00:01, 46.68it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 383/435 [00:08<00:01, 46.67it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 388/435 [00:08<00:01, 46.72it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 393/435 [00:08<00:00, 46.42it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398/435 [00:08<00:00, 46.66it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 403/435 [00:08<00:00, 46.70it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 408/435 [00:08<00:00, 46.63it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 413/435 [00:08<00:00, 46.73it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 418/435 [00:08<00:00, 46.76it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 423/435 [00:09<00:00, 46.70it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 428/435 [00:09<00:00, 46.70it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 433/435 [00:09<00:00, 46.74it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 435/435 [00:09<00:00, 46.82it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:02:59,938 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:59,938 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:59,938 >>   eval_loss               =      1.051
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:59,938 >>   eval_runtime            = 0:00:09.31
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:59,938 >>   eval_samples            =       3479
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:59,938 >>   eval_samples_per_second =    373.519
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:59,938 >>   eval_steps_per_second   =     46.703
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:02:59,938 >>   perplexity              =     2.8605
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:06,634 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:06,661 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:06,661 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:06,661 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:06,661 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:03:07,306 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:03:07,307 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:03:07,915 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:03:08,956 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:03:08,956 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:11,810 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:11,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:11,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:11,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:03:11,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:03:12,437 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:03:12,438 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:03:13,014 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:03:13,165 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:03:13,165 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/generator/iter5/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/dev.jsonl', 'labels': ['country', 'followed by', 'genre', 'member of', 'subsidiary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 13489
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13589, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.16it/s]Extractor Predicting: 2it [00:01,  1.16it/s]Extractor Predicting: 3it [00:02,  1.17it/s]Extractor Predicting: 4it [00:03,  1.17it/s]Extractor Predicting: 5it [00:04,  1.16it/s]Extractor Predicting: 6it [00:05,  1.16it/s]Extractor Predicting: 7it [00:06,  1.16it/s]Extractor Predicting: 8it [00:06,  1.17it/s]Extractor Predicting: 9it [00:07,  1.17it/s]Extractor Predicting: 10it [00:08,  1.20it/s]Extractor Predicting: 11it [00:09,  1.18it/s]Extractor Predicting: 12it [00:10,  1.16it/s]Extractor Predicting: 13it [00:11,  1.15it/s]Extractor Predicting: 14it [00:12,  1.15it/s]Extractor Predicting: 15it [00:12,  1.19it/s]Extractor Predicting: 16it [00:13,  1.17it/s]Extractor Predicting: 17it [00:14,  1.19it/s]Extractor Predicting: 18it [00:15,  1.22it/s]Extractor Predicting: 19it [00:16,  1.20it/s]Extractor Predicting: 20it [00:16,  1.20it/s]Extractor Predicting: 21it [00:17,  1.20it/s]Extractor Predicting: 22it [00:18,  1.22it/s]Extractor Predicting: 23it [00:19,  1.21it/s]Extractor Predicting: 24it [00:20,  1.13it/s]Extractor Predicting: 25it [00:21,  1.13it/s]Extractor Predicting: 26it [00:22,  1.12it/s]Extractor Predicting: 27it [00:23,  1.13it/s]Extractor Predicting: 28it [00:23,  1.17it/s]Extractor Predicting: 29it [00:24,  1.16it/s]Extractor Predicting: 30it [00:25,  1.14it/s]Extractor Predicting: 31it [00:26,  1.15it/s]Extractor Predicting: 32it [00:27,  1.16it/s]Extractor Predicting: 33it [00:28,  1.14it/s]Extractor Predicting: 34it [00:29,  1.18it/s]Extractor Predicting: 35it [00:29,  1.18it/s]Extractor Predicting: 36it [00:30,  1.19it/s]Extractor Predicting: 37it [00:31,  1.18it/s]Extractor Predicting: 38it [00:32,  1.20it/s]Extractor Predicting: 39it [00:33,  1.21it/s]Extractor Predicting: 40it [00:34,  1.19it/s]Extractor Predicting: 41it [00:34,  1.20it/s]Extractor Predicting: 42it [00:35,  1.20it/s]Extractor Predicting: 43it [00:36,  1.22it/s]Extractor Predicting: 44it [00:37,  1.23it/s]Extractor Predicting: 45it [00:38,  1.21it/s]Extractor Predicting: 46it [00:39,  1.20it/s]Extractor Predicting: 47it [00:39,  1.18it/s]Extractor Predicting: 48it [00:40,  1.20it/s]Extractor Predicting: 49it [00:41,  1.19it/s]Extractor Predicting: 50it [00:42,  1.20it/s]Extractor Predicting: 51it [00:43,  1.21it/s]Extractor Predicting: 52it [00:44,  1.21it/s]Extractor Predicting: 53it [00:44,  1.21it/s]Extractor Predicting: 54it [00:45,  1.20it/s]Extractor Predicting: 55it [00:46,  1.18it/s]Extractor Predicting: 56it [00:47,  1.20it/s]Extractor Predicting: 57it [00:48,  1.19it/s]Extractor Predicting: 58it [00:49,  1.18it/s]Extractor Predicting: 59it [00:49,  1.21it/s]Extractor Predicting: 60it [00:50,  1.25it/s]Extractor Predicting: 61it [00:51,  1.26it/s]Extractor Predicting: 62it [00:52,  1.24it/s]Extractor Predicting: 63it [00:53,  1.21it/s]Extractor Predicting: 64it [00:53,  1.23it/s]Extractor Predicting: 65it [00:54,  1.24it/s]Extractor Predicting: 66it [00:55,  1.23it/s]Extractor Predicting: 67it [00:56,  1.23it/s]Extractor Predicting: 68it [00:57,  1.23it/s]Extractor Predicting: 69it [00:57,  1.27it/s]Extractor Predicting: 70it [00:58,  1.26it/s]Extractor Predicting: 71it [00:59,  1.27it/s]Extractor Predicting: 72it [01:00,  1.24it/s]Extractor Predicting: 73it [01:01,  1.25it/s]Extractor Predicting: 74it [01:01,  1.24it/s]Extractor Predicting: 75it [01:02,  1.21it/s]Extractor Predicting: 76it [01:03,  1.21it/s]Extractor Predicting: 77it [01:04,  1.23it/s]Extractor Predicting: 78it [01:05,  1.24it/s]Extractor Predicting: 79it [01:05,  1.26it/s]Extractor Predicting: 80it [01:06,  1.26it/s]Extractor Predicting: 81it [01:07,  1.25it/s]Extractor Predicting: 82it [01:08,  1.24it/s]Extractor Predicting: 83it [01:09,  1.25it/s]Extractor Predicting: 84it [01:09,  1.26it/s]Extractor Predicting: 85it [01:10,  1.26it/s]Extractor Predicting: 86it [01:11,  1.27it/s]Extractor Predicting: 87it [01:12,  1.29it/s]Extractor Predicting: 88it [01:13,  1.28it/s]Extractor Predicting: 89it [01:13,  1.28it/s]Extractor Predicting: 90it [01:14,  1.28it/s]Extractor Predicting: 91it [01:15,  1.30it/s]Extractor Predicting: 92it [01:16,  1.33it/s]Extractor Predicting: 93it [01:16,  1.31it/s]Extractor Predicting: 94it [01:17,  1.21it/s]Extractor Predicting: 95it [01:18,  1.24it/s]Extractor Predicting: 96it [01:19,  1.24it/s]Extractor Predicting: 97it [01:20,  1.25it/s]Extractor Predicting: 98it [01:20,  1.25it/s]Extractor Predicting: 99it [01:21,  1.24it/s]Extractor Predicting: 100it [01:22,  1.21it/s]Extractor Predicting: 101it [01:23,  1.22it/s]Extractor Predicting: 102it [01:24,  1.28it/s]Extractor Predicting: 103it [01:24,  1.29it/s]Extractor Predicting: 104it [01:25,  1.28it/s]Extractor Predicting: 105it [01:26,  1.28it/s]Extractor Predicting: 106it [01:27,  1.28it/s]Extractor Predicting: 107it [01:28,  1.27it/s]Extractor Predicting: 108it [01:28,  1.27it/s]Extractor Predicting: 109it [01:29,  1.27it/s]Extractor Predicting: 110it [01:30,  1.28it/s]Extractor Predicting: 111it [01:31,  1.28it/s]Extractor Predicting: 112it [01:31,  1.29it/s]Extractor Predicting: 113it [01:32,  1.32it/s]Extractor Predicting: 114it [01:33,  1.32it/s]Extractor Predicting: 115it [01:34,  1.33it/s]Extractor Predicting: 116it [01:35,  1.30it/s]Extractor Predicting: 117it [01:35,  1.26it/s]Extractor Predicting: 118it [01:36,  1.27it/s]Extractor Predicting: 119it [01:37,  1.23it/s]Extractor Predicting: 120it [01:38,  1.25it/s]Extractor Predicting: 121it [01:39,  1.24it/s]Extractor Predicting: 122it [01:39,  1.22it/s]Extractor Predicting: 123it [01:40,  1.23it/s]Extractor Predicting: 124it [01:41,  1.23it/s]Extractor Predicting: 125it [01:42,  1.25it/s]Extractor Predicting: 126it [01:43,  1.22it/s]Extractor Predicting: 127it [01:43,  1.25it/s]Extractor Predicting: 128it [01:44,  1.24it/s]Extractor Predicting: 129it [01:45,  1.23it/s]Extractor Predicting: 130it [01:46,  1.21it/s]Extractor Predicting: 131it [01:47,  1.24it/s]Extractor Predicting: 132it [01:48,  1.22it/s]Extractor Predicting: 133it [01:48,  1.21it/s]Extractor Predicting: 134it [01:49,  1.19it/s]Extractor Predicting: 135it [01:50,  1.22it/s]Extractor Predicting: 136it [01:51,  1.20it/s]Extractor Predicting: 137it [01:52,  1.21it/s]Extractor Predicting: 138it [01:53,  1.19it/s]Extractor Predicting: 139it [01:53,  1.20it/s]Extractor Predicting: 140it [01:54,  1.19it/s]Extractor Predicting: 141it [01:55,  1.21it/s]Extractor Predicting: 142it [01:56,  1.20it/s]Extractor Predicting: 143it [01:57,  1.21it/s]Extractor Predicting: 144it [01:57,  1.49it/s]Extractor Predicting: 144it [01:57,  1.23it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:17,865 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:17,871 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:17,871 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:17,871 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:17,871 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:05:18,211 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:05:18,212 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:05:18,478 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:05:19,496 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:05:19,496 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:22,022 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:22,026 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:22,026 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:22,026 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:05:22,026 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:05:22,665 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:05:22,667 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:05:23,275 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:05:23,484 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:05:23,484 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.208955223880597,
  "recall": 0.0482897384305835,
  "score": 0.07844968480037358,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19485
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19585, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.23it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.25it/s]Extractor Predicting: 5it [00:03,  1.26it/s]Extractor Predicting: 6it [00:04,  1.27it/s]Extractor Predicting: 7it [00:05,  1.30it/s]Extractor Predicting: 8it [00:06,  1.33it/s]Extractor Predicting: 9it [00:07,  1.30it/s]Extractor Predicting: 10it [00:07,  1.30it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:09,  1.28it/s]Extractor Predicting: 13it [00:10,  1.27it/s]Extractor Predicting: 14it [00:10,  1.28it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:13,  1.27it/s]Extractor Predicting: 18it [00:14,  1.29it/s]Extractor Predicting: 19it [00:14,  1.32it/s]Extractor Predicting: 20it [00:15,  1.28it/s]Extractor Predicting: 21it [00:16,  1.29it/s]Extractor Predicting: 22it [00:17,  1.30it/s]Extractor Predicting: 23it [00:17,  1.30it/s]Extractor Predicting: 24it [00:18,  1.29it/s]Extractor Predicting: 25it [00:19,  1.28it/s]Extractor Predicting: 26it [00:20,  1.28it/s]Extractor Predicting: 27it [00:21,  1.26it/s]Extractor Predicting: 28it [00:21,  1.26it/s]Extractor Predicting: 29it [00:22,  1.27it/s]Extractor Predicting: 30it [00:23,  1.29it/s]Extractor Predicting: 31it [00:24,  1.27it/s]Extractor Predicting: 32it [00:24,  1.30it/s]Extractor Predicting: 33it [00:25,  1.29it/s]Extractor Predicting: 34it [00:26,  1.26it/s]Extractor Predicting: 35it [00:27,  1.27it/s]Extractor Predicting: 36it [00:28,  1.28it/s]Extractor Predicting: 37it [00:28,  1.30it/s]Extractor Predicting: 38it [00:29,  1.31it/s]Extractor Predicting: 39it [00:30,  1.29it/s]Extractor Predicting: 40it [00:31,  1.27it/s]Extractor Predicting: 41it [00:31,  1.27it/s]Extractor Predicting: 42it [00:32,  1.30it/s]Extractor Predicting: 43it [00:33,  1.27it/s]Extractor Predicting: 44it [00:34,  1.26it/s]Extractor Predicting: 45it [00:35,  1.26it/s]Extractor Predicting: 46it [00:35,  1.24it/s]Extractor Predicting: 47it [00:36,  1.27it/s]Extractor Predicting: 48it [00:37,  1.25it/s]Extractor Predicting: 49it [00:38,  1.29it/s]Extractor Predicting: 50it [00:39,  1.27it/s]Extractor Predicting: 51it [00:39,  1.26it/s]Extractor Predicting: 52it [00:40,  1.25it/s]Extractor Predicting: 53it [00:41,  1.25it/s]Extractor Predicting: 54it [00:42,  1.25it/s]Extractor Predicting: 55it [00:43,  1.28it/s]Extractor Predicting: 56it [00:43,  1.27it/s]Extractor Predicting: 57it [00:44,  1.25it/s]Extractor Predicting: 58it [00:45,  1.25it/s]Extractor Predicting: 59it [00:46,  1.25it/s]Extractor Predicting: 60it [00:47,  1.24it/s]Extractor Predicting: 61it [00:47,  1.23it/s]Extractor Predicting: 62it [00:48,  1.26it/s]Extractor Predicting: 63it [00:49,  1.26it/s]Extractor Predicting: 64it [00:50,  1.29it/s]Extractor Predicting: 65it [00:50,  1.26it/s]Extractor Predicting: 66it [00:51,  1.23it/s]Extractor Predicting: 67it [00:52,  1.23it/s]Extractor Predicting: 68it [00:53,  1.23it/s]Extractor Predicting: 69it [00:54,  1.22it/s]Extractor Predicting: 70it [00:55,  1.23it/s]Extractor Predicting: 71it [00:55,  1.23it/s]Extractor Predicting: 72it [00:56,  1.14it/s]Extractor Predicting: 73it [00:57,  1.18it/s]Extractor Predicting: 74it [00:58,  1.19it/s]Extractor Predicting: 75it [00:59,  1.22it/s]Extractor Predicting: 76it [01:00,  1.21it/s]Extractor Predicting: 77it [01:00,  1.25it/s]Extractor Predicting: 78it [01:01,  1.24it/s]Extractor Predicting: 79it [01:02,  1.27it/s]Extractor Predicting: 80it [01:03,  1.29it/s]Extractor Predicting: 81it [01:03,  1.29it/s]Extractor Predicting: 82it [01:04,  1.29it/s]Extractor Predicting: 83it [01:05,  1.27it/s]Extractor Predicting: 84it [01:06,  1.25it/s]Extractor Predicting: 85it [01:07,  1.22it/s]Extractor Predicting: 86it [01:08,  1.20it/s]Extractor Predicting: 87it [01:08,  1.21it/s]Extractor Predicting: 88it [01:09,  1.21it/s]Extractor Predicting: 89it [01:10,  1.19it/s]Extractor Predicting: 90it [01:11,  1.18it/s]Extractor Predicting: 91it [01:12,  1.17it/s]Extractor Predicting: 92it [01:13,  1.20it/s]Extractor Predicting: 93it [01:13,  1.22it/s]Extractor Predicting: 94it [01:14,  1.22it/s]Extractor Predicting: 95it [01:15,  1.23it/s]Extractor Predicting: 96it [01:16,  1.20it/s]Extractor Predicting: 97it [01:17,  1.18it/s]Extractor Predicting: 98it [01:18,  1.18it/s]Extractor Predicting: 99it [01:18,  1.20it/s]Extractor Predicting: 100it [01:19,  1.20it/s]Extractor Predicting: 101it [01:20,  1.23it/s]Extractor Predicting: 102it [01:21,  1.21it/s]Extractor Predicting: 103it [01:22,  1.19it/s]Extractor Predicting: 104it [01:23,  1.22it/s]Extractor Predicting: 105it [01:23,  1.21it/s]Extractor Predicting: 106it [01:24,  1.21it/s]Extractor Predicting: 107it [01:25,  1.23it/s]Extractor Predicting: 108it [01:26,  1.21it/s]Extractor Predicting: 109it [01:27,  1.21it/s]Extractor Predicting: 110it [01:28,  1.20it/s]Extractor Predicting: 111it [01:28,  1.22it/s]Extractor Predicting: 112it [01:29,  1.21it/s]Extractor Predicting: 113it [01:30,  1.22it/s]Extractor Predicting: 114it [01:31,  1.23it/s]Extractor Predicting: 115it [01:32,  1.19it/s]Extractor Predicting: 116it [01:32,  1.22it/s]Extractor Predicting: 117it [01:33,  1.22it/s]Extractor Predicting: 118it [01:34,  1.21it/s]Extractor Predicting: 119it [01:35,  1.21it/s]Extractor Predicting: 120it [01:36,  1.24it/s]Extractor Predicting: 121it [01:37,  1.25it/s]Extractor Predicting: 122it [01:37,  1.26it/s]Extractor Predicting: 123it [01:38,  1.25it/s]Extractor Predicting: 124it [01:39,  1.26it/s]Extractor Predicting: 125it [01:40,  1.29it/s]Extractor Predicting: 126it [01:40,  1.28it/s]Extractor Predicting: 127it [01:41,  1.27it/s]Extractor Predicting: 128it [01:42,  1.29it/s]Extractor Predicting: 129it [01:43,  1.29it/s]Extractor Predicting: 130it [01:44,  1.27it/s]Extractor Predicting: 131it [01:44,  1.30it/s]Extractor Predicting: 132it [01:45,  1.30it/s]Extractor Predicting: 133it [01:46,  1.32it/s]Extractor Predicting: 134it [01:47,  1.28it/s]Extractor Predicting: 135it [01:47,  1.31it/s]Extractor Predicting: 136it [01:48,  1.31it/s]Extractor Predicting: 137it [01:49,  1.32it/s]Extractor Predicting: 138it [01:50,  1.31it/s]Extractor Predicting: 139it [01:50,  1.30it/s]Extractor Predicting: 140it [01:51,  1.30it/s]Extractor Predicting: 141it [01:52,  1.28it/s]Extractor Predicting: 142it [01:53,  1.27it/s]Extractor Predicting: 143it [01:54,  1.28it/s]Extractor Predicting: 144it [01:54,  1.27it/s]Extractor Predicting: 145it [01:55,  1.31it/s]Extractor Predicting: 146it [01:56,  1.33it/s]Extractor Predicting: 147it [01:57,  1.33it/s]Extractor Predicting: 148it [01:57,  1.35it/s]Extractor Predicting: 149it [01:58,  1.34it/s]Extractor Predicting: 150it [01:59,  1.35it/s]Extractor Predicting: 151it [01:59,  1.36it/s]Extractor Predicting: 152it [02:00,  1.37it/s]Extractor Predicting: 153it [02:01,  1.35it/s]Extractor Predicting: 154it [02:02,  1.35it/s]Extractor Predicting: 155it [02:02,  1.39it/s]Extractor Predicting: 156it [02:03,  1.38it/s]Extractor Predicting: 157it [02:04,  1.44it/s]Extractor Predicting: 158it [02:04,  1.45it/s]Extractor Predicting: 159it [02:05,  1.42it/s]Extractor Predicting: 160it [02:06,  1.37it/s]Extractor Predicting: 161it [02:07,  1.36it/s]Extractor Predicting: 162it [02:07,  1.37it/s]Extractor Predicting: 163it [02:08,  1.40it/s]Extractor Predicting: 164it [02:09,  1.39it/s]Extractor Predicting: 165it [02:10,  1.40it/s]Extractor Predicting: 166it [02:10,  1.36it/s]Extractor Predicting: 167it [02:11,  1.25it/s]Extractor Predicting: 168it [02:12,  1.28it/s]Extractor Predicting: 169it [02:13,  1.34it/s]Extractor Predicting: 170it [02:13,  1.35it/s]Extractor Predicting: 171it [02:14,  1.36it/s]Extractor Predicting: 172it [02:15,  1.30it/s]Extractor Predicting: 173it [02:16,  1.29it/s]Extractor Predicting: 174it [02:17,  1.26it/s]Extractor Predicting: 175it [02:17,  1.22it/s]Extractor Predicting: 176it [02:18,  1.23it/s]Extractor Predicting: 177it [02:19,  1.24it/s]Extractor Predicting: 178it [02:20,  1.22it/s]Extractor Predicting: 179it [02:21,  1.22it/s]Extractor Predicting: 180it [02:22,  1.23it/s]Extractor Predicting: 181it [02:22,  1.23it/s]Extractor Predicting: 182it [02:23,  1.22it/s]Extractor Predicting: 183it [02:24,  1.22it/s]Extractor Predicting: 184it [02:25,  1.21it/s]Extractor Predicting: 185it [02:26,  1.19it/s]Extractor Predicting: 186it [02:27,  1.19it/s]Extractor Predicting: 187it [02:27,  1.20it/s]Extractor Predicting: 188it [02:28,  1.20it/s]Extractor Predicting: 189it [02:29,  1.21it/s]Extractor Predicting: 190it [02:30,  1.22it/s]Extractor Predicting: 191it [02:31,  1.19it/s]Extractor Predicting: 192it [02:32,  1.17it/s]Extractor Predicting: 193it [02:32,  1.18it/s]Extractor Predicting: 194it [02:33,  1.18it/s]Extractor Predicting: 195it [02:34,  1.19it/s]Extractor Predicting: 196it [02:35,  1.20it/s]Extractor Predicting: 197it [02:36,  1.19it/s]Extractor Predicting: 198it [02:37,  1.21it/s]Extractor Predicting: 199it [02:37,  1.22it/s]Extractor Predicting: 200it [02:38,  1.20it/s]Extractor Predicting: 201it [02:39,  1.19it/s]Extractor Predicting: 202it [02:40,  1.25it/s]Extractor Predicting: 203it [02:41,  1.23it/s]Extractor Predicting: 204it [02:41,  1.25it/s]Extractor Predicting: 205it [02:42,  1.24it/s]Extractor Predicting: 206it [02:43,  1.23it/s]Extractor Predicting: 207it [02:44,  1.21it/s]Extractor Predicting: 208it [02:45,  1.22it/s]Extractor Predicting: 209it [02:46,  1.17it/s]Extractor Predicting: 210it [02:46,  1.19it/s]Extractor Predicting: 211it [02:47,  1.18it/s]Extractor Predicting: 212it [02:48,  1.19it/s]Extractor Predicting: 213it [02:49,  1.19it/s]Extractor Predicting: 214it [02:50,  1.18it/s]Extractor Predicting: 215it [02:51,  1.21it/s]Extractor Predicting: 216it [02:51,  1.19it/s]Extractor Predicting: 217it [02:52,  1.20it/s]Extractor Predicting: 218it [02:53,  1.21it/s]Extractor Predicting: 219it [02:54,  1.19it/s]Extractor Predicting: 220it [02:55,  1.20it/s]Extractor Predicting: 221it [02:56,  1.18it/s]Extractor Predicting: 222it [02:57,  1.18it/s]Extractor Predicting: 223it [02:57,  1.20it/s]Extractor Predicting: 224it [02:58,  1.20it/s]Extractor Predicting: 225it [02:59,  1.21it/s]Extractor Predicting: 226it [03:00,  1.20it/s]Extractor Predicting: 227it [03:01,  1.24it/s]Extractor Predicting: 228it [03:01,  1.23it/s]Extractor Predicting: 229it [03:02,  1.22it/s]Extractor Predicting: 230it [03:03,  1.26it/s]Extractor Predicting: 231it [03:04,  1.27it/s]Extractor Predicting: 232it [03:04,  1.29it/s]Extractor Predicting: 233it [03:05,  1.27it/s]Extractor Predicting: 234it [03:06,  1.28it/s]Extractor Predicting: 235it [03:07,  1.26it/s]Extractor Predicting: 236it [03:08,  1.25it/s]Extractor Predicting: 237it [03:09,  1.22it/s]Extractor Predicting: 238it [03:09,  1.24it/s]Extractor Predicting: 239it [03:10,  1.22it/s]Extractor Predicting: 240it [03:11,  1.23it/s]Extractor Predicting: 241it [03:12,  1.18it/s]Extractor Predicting: 242it [03:13,  1.21it/s]Extractor Predicting: 243it [03:14,  1.21it/s]Extractor Predicting: 244it [03:14,  1.22it/s]Extractor Predicting: 245it [03:15,  1.25it/s]Extractor Predicting: 246it [03:16,  1.24it/s]Extractor Predicting: 247it [03:17,  1.24it/s]Extractor Predicting: 248it [03:18,  1.24it/s]Extractor Predicting: 249it [03:18,  1.22it/s]Extractor Predicting: 250it [03:19,  1.24it/s]Extractor Predicting: 251it [03:20,  1.25it/s]Extractor Predicting: 252it [03:21,  1.25it/s]Extractor Predicting: 253it [03:22,  1.23it/s]Extractor Predicting: 254it [03:22,  1.24it/s]Extractor Predicting: 255it [03:23,  1.21it/s]Extractor Predicting: 256it [03:24,  1.08it/s]Extractor Predicting: 257it [03:25,  1.11it/s]Extractor Predicting: 258it [03:26,  1.12it/s]Extractor Predicting: 259it [03:27,  1.15it/s]Extractor Predicting: 260it [03:28,  1.15it/s]Extractor Predicting: 261it [03:29,  1.18it/s]Extractor Predicting: 262it [03:29,  1.17it/s]Extractor Predicting: 263it [03:30,  1.16it/s]Extractor Predicting: 264it [03:31,  1.16it/s]Extractor Predicting: 265it [03:32,  1.12it/s]Extractor Predicting: 266it [03:33,  1.13it/s]Extractor Predicting: 267it [03:34,  1.14it/s]Extractor Predicting: 268it [03:35,  1.13it/s]Extractor Predicting: 269it [03:36,  1.16it/s]Extractor Predicting: 270it [03:36,  1.17it/s]Extractor Predicting: 271it [03:37,  1.15it/s]Extractor Predicting: 272it [03:38,  1.17it/s]Extractor Predicting: 273it [03:39,  1.17it/s]Extractor Predicting: 274it [03:40,  1.19it/s]Extractor Predicting: 275it [03:41,  1.21it/s]Extractor Predicting: 276it [03:41,  1.24it/s]Extractor Predicting: 277it [03:42,  1.24it/s]Extractor Predicting: 278it [03:43,  1.23it/s]Extractor Predicting: 279it [03:44,  1.22it/s]Extractor Predicting: 280it [03:45,  1.18it/s]Extractor Predicting: 281it [03:46,  1.18it/s]Extractor Predicting: 282it [03:46,  1.26it/s]Extractor Predicting: 282it [03:46,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:17,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:17,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:17,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:17,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:17,316 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:09:18,024 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:09:18,025 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:09:18,293 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:09:19,288 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:09:19,288 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:21,233 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:21,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:21,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:21,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:09:21,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:09:21,573 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:09:21,575 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:09:21,843 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:09:21,994 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:09:21,994 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.29280332553910104,
  "recall": 0.16671597633136095,
  "score": 0.21246111791874825,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1186
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1286, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.16it/s]Extractor Predicting: 2it [00:01,  1.16it/s]Extractor Predicting: 3it [00:02,  1.17it/s]Extractor Predicting: 4it [00:03,  1.17it/s]Extractor Predicting: 5it [00:04,  1.20it/s]Extractor Predicting: 5it [00:04,  1.19it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4666666666666667,
  "recall": 0.11666666666666667,
  "score": 0.18666666666666668,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_1/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/', 'labels': ['contains administrative territorial entity', 'distributed by', 'field of work', 'instrument', 'located on terrain feature', 'occupation', 'participating team', 'platform', 'publisher', 'spouse'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_1/extractor/iter1/results_single_is_eval_True_limit5000.json'
