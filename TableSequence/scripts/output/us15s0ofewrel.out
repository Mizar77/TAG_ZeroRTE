Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:24<07:40, 24.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:48<07:11, 23.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [01:10<06:39, 23.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:33<06:09, 23.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [02:02<06:18, 25.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [02:26<05:48, 24.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:52<05:28, 25.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [03:20<05:12, 26.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [03:45<04:42, 25.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [04:08<04:08, 24.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [04:32<03:42, 24.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [04:59<03:21, 25.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [05:20<02:49, 24.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [05:43<02:22, 23.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [06:06<01:57, 23.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [06:33<01:38, 24.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [07:03<01:18, 26.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [07:26<00:50, 25.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [07:55<00:26, 26.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [08:20<00:00, 25.97s/it]Generating: 100%|██████████| 20/20 [08:20<00:00, 25.03s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : military branch . Context : Later in the year , the command under the command of Gen. Thomas L. B. Nesbitt was established to continue the campaign against the Japanese , led by Chief of Staff Sir Richard R. Nesbitt , on Okinawa . Head Entity : Thomas L. B. Nesbitt , Tail Entity : commanding general .\n']
['Relation : military branch . Context : Later in the year , the command under the command of Gen. Thomas L. B. Nesbitt was established to continue the campaign against the Japanese , led by Chief of Staff Sir Richard R. Nesbitt , on Okinawa . Head Entity : Thomas L. B. Nesbitt , Tail Entity : commanding general .\n', 'Relation : military branch . Context : General Sir Herbert Wilfrid Wilford , ( 9 January 1789 - 7 February 1897 ) was a Royal Canadian Air Force officer who commanded the First Air Force . Head Entity : Herbert Wilfrid Wilfrid , Tail Entity : Royal Canadian Air Force .\n']
['Relation : military branch . Context : Later in the year , the command under the command of Gen. Thomas L. B. Nesbitt was established to continue the campaign against the Japanese , led by Chief of Staff Sir Richard R. Nesbitt , on Okinawa . Head Entity : Thomas L. B. Nesbitt , Tail Entity : commanding general .\n', 'Relation : military branch . Context : General Sir Herbert Wilfrid Wilford , ( 9 January 1789 - 7 February 1897 ) was a Royal Canadian Air Force officer who commanded the First Air Force . Head Entity : Herbert Wilfrid Wilfrid , Tail Entity : Royal Canadian Air Force .\n', 'Relation : military branch . Context : This was the first Russian warplane to be operated in combat with the Warsaw Pact . Head Entity : Soviet Warplane , Tail Entity : Warsaw Pact .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 480, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : military branch .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 33, 'raw': 64}
{'target': 600, 'success': 54, 'raw': 96}
{'target': 600, 'success': 73, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 136, 'raw': 224}
{'target': 600, 'success': 157, 'raw': 256}
{'target': 600, 'success': 176, 'raw': 288}
{'target': 600, 'success': 194, 'raw': 320}
{'target': 600, 'success': 216, 'raw': 352}
{'target': 600, 'success': 237, 'raw': 384}
{'target': 600, 'success': 257, 'raw': 416}
{'target': 600, 'success': 276, 'raw': 448}
{'target': 600, 'success': 295, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 333, 'raw': 544}
{'target': 600, 'success': 353, 'raw': 576}
{'target': 600, 'success': 372, 'raw': 608}
{'target': 600, 'success': 392, 'raw': 640}
{'target': 600, 'success': 417, 'raw': 672}
{'target': 600, 'success': 437, 'raw': 704}
{'target': 600, 'success': 459, 'raw': 736}
{'target': 600, 'success': 478, 'raw': 768}
{'target': 600, 'success': 495, 'raw': 800}
{'target': 600, 'success': 511, 'raw': 832}
{'target': 600, 'success': 532, 'raw': 864}
{'target': 600, 'success': 549, 'raw': 896}
{'target': 600, 'success': 564, 'raw': 928}
{'target': 600, 'success': 582, 'raw': 960}
{'target': 600, 'success': 602, 'raw': 992}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6068548387096774, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'The Simpsons Comic Comedy\', \'voice type\', \'\', \'After this show , " The Simpsons : The Last Simpsons " was remade as " The Simpsons Comic Comedy " and as " Simpsons - themed video games on Xbox 360 , PlayStation 3 , and Xbox 360 .\')'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 572, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7463942307692307, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : Following his leadership in the 1972 elections under the leadership of Alastair Campbell , he represented the electorate of New South Wales from 1993 until the 2002 election under the leadership of Greg Clark . Head Entity : Greg Clark , Tail Entity : Australian electoral district .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 622, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7475961538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : language of work or name . Context : Later in the year ( 1143–1230 ) he married daughter of Louis of Saxony and had two sons married to this writer , the King of England , Thomas II , and the Marquess of Suffolk . Head Entity : Louis of Saxony , Tail Entity : language of work or name .\n']
['Relation : language of work or name . Context : Later in the year ( 1143–1230 ) he married daughter of Louis of Saxony and had two sons married to this writer , the King of England , Thomas II , and the Marquess of Suffolk . Head Entity : Louis of Saxony , Tail Entity : language of work or name .\n', 'Relation : language of work or name . Context : Eileen Waugh ( born 14 March 1950 ) is an American writer - songwriter , songwriter , and producer . Head Entity : Eileen Waugh , Tail Entity : English .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 167, 'raw': 256}
{'target': 600, 'success': 192, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 286, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 422, 'raw': 608}
{'target': 600, 'success': 442, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 504, 'raw': 736}
{'target': 600, 'success': 531, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.6944444444444444, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('Gérard Côteau', 'language of work or name', '', 'Gérard Côteau ( 8 February 1913 – 11 August 1986 ) was an Italian composer and arranger .')"}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amain - sur - Mardin , and found the church deserted and no sign of the invading French forces . Head Entity : Amain - sur - Mardin , Tail Entity : France .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.77375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', '(\'MiniB1\', \'manufacturer\', \'\', \'A new model , a " MiniB1 " , was introduced in 2008 .\')'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7725, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 389, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 429, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 501, 'raw': 704}
{'target': 600, 'success': 527, 'raw': 736}
{'target': 600, 'success': 552, 'raw': 768}
{'target': 600, 'success': 572, 'raw': 800}
{'target': 600, 'success': 594, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.7175925925925926, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'" The Oprah Winfrey Show\', \'nominated for\', \'\', \'When in 2009 , she appeared as the host of " The Oprah Winfrey Show " on " Variety " , and she did host a number of other shows , including a number of awards shows for children \\\'s programming , including " The Oprah Winfrey Show " , and a number of movies .\')'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('American Airlines', 'operating system', '', 'The company was founded in 1969 by American Airlines engineer Neil Armstrong , who had retired from service in 1967 after retiring from the USAF as early as 1958 .')"}}
['Relation : original broadcaster . Context : Later in the year , the band formed New River Band with producer Brian Lauter . Head Entity : New River Band , Tail Entity : New River Times .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : Later in the year , the court of the king , the sovereign of Morocco , abolished the " sovereign " designation in favour of a civil magistrate , King Mohamed , to determine how much royal patronage they might allow in each department . Head Entity : Mohamed , Tail Entity : ruler .\n']
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 191, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 229, 'raw': 352}
{'target': 600, 'success': 253, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 295, 'raw': 448}
{'target': 600, 'success': 319, 'raw': 480}
{'target': 600, 'success': 343, 'raw': 512}
{'target': 600, 'success': 365, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 410, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 449, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 484, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 529, 'raw': 800}
{'target': 600, 'success': 550, 'raw': 832}
{'target': 600, 'success': 574, 'raw': 864}
{'target': 600, 'success': 591, 'raw': 896}
{'target': 600, 'success': 609, 'raw': 928}
{'prompt': 'Relation : position held .', 'success_rate': 0.65625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 55, 'raw': 96}
{'target': 600, 'success': 70, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 113, 'raw': 192}
{'target': 600, 'success': 133, 'raw': 224}
{'target': 600, 'success': 150, 'raw': 256}
{'target': 600, 'success': 171, 'raw': 288}
{'target': 600, 'success': 193, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 241, 'raw': 416}
{'target': 600, 'success': 256, 'raw': 448}
{'target': 600, 'success': 272, 'raw': 480}
{'target': 600, 'success': 292, 'raw': 512}
{'target': 600, 'success': 309, 'raw': 544}
{'target': 600, 'success': 326, 'raw': 576}
{'target': 600, 'success': 345, 'raw': 608}
{'target': 600, 'success': 368, 'raw': 640}
{'target': 600, 'success': 383, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 426, 'raw': 736}
{'target': 600, 'success': 441, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 491, 'raw': 864}
{'target': 600, 'success': 510, 'raw': 896}
{'target': 600, 'success': 531, 'raw': 928}
{'target': 600, 'success': 551, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 587, 'raw': 1024}
{'target': 600, 'success': 603, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5710227272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 314, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 453, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 524, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 571, 'raw': 800}
{'target': 600, 'success': 591, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.796875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 17789
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17889, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.80s/it]Extractor Estimating: 2it [00:18,  7.73s/it]Extractor Estimating: 3it [00:18,  4.57s/it]Extractor Estimating: 4it [00:19,  3.07s/it]Extractor Estimating: 5it [00:21,  2.50s/it]Extractor Estimating: 6it [00:22,  1.92s/it]Extractor Estimating: 7it [00:22,  1.54s/it]Extractor Estimating: 8it [00:27,  2.59s/it]Extractor Estimating: 9it [00:28,  2.02s/it]Extractor Estimating: 10it [00:29,  1.68s/it]Extractor Estimating: 11it [00:30,  1.42s/it]Extractor Estimating: 12it [00:30,  1.23s/it]Extractor Estimating: 13it [00:31,  1.13s/it]Extractor Estimating: 14it [00:32,  1.07s/it]Extractor Estimating: 15it [00:33,  1.03it/s]Extractor Estimating: 16it [00:34,  1.09it/s]Extractor Estimating: 17it [00:35,  1.15it/s]Extractor Estimating: 18it [00:35,  1.14it/s]Extractor Estimating: 19it [00:36,  1.15it/s]Extractor Estimating: 20it [00:37,  1.18it/s]Extractor Estimating: 21it [00:38,  1.19it/s]Extractor Estimating: 22it [00:39,  1.17it/s]Extractor Estimating: 23it [00:40,  1.21it/s]Extractor Estimating: 24it [00:41,  1.17it/s]Extractor Estimating: 25it [00:41,  1.19it/s]Extractor Estimating: 26it [00:42,  1.21it/s]Extractor Estimating: 27it [00:43,  1.17it/s]Extractor Estimating: 28it [00:44,  1.16it/s]Extractor Estimating: 29it [00:45,  1.19it/s]Extractor Estimating: 30it [00:46,  1.19it/s]Extractor Estimating: 31it [00:46,  1.18it/s]Extractor Estimating: 32it [00:47,  1.24it/s]Extractor Estimating: 33it [00:48,  1.25it/s]Extractor Estimating: 34it [00:49,  1.20it/s]Extractor Estimating: 35it [00:50,  1.22it/s]Extractor Estimating: 36it [00:50,  1.21it/s]Extractor Estimating: 37it [00:51,  1.22it/s]Extractor Estimating: 38it [00:52,  1.19it/s]Extractor Estimating: 39it [00:53,  1.24it/s]Extractor Estimating: 40it [00:54,  1.24it/s]Extractor Estimating: 41it [00:54,  1.25it/s]Extractor Estimating: 42it [00:55,  1.19it/s]Extractor Estimating: 43it [00:56,  1.21it/s]Extractor Estimating: 44it [00:57,  1.22it/s]Extractor Estimating: 45it [00:58,  1.19it/s]Extractor Estimating: 46it [00:59,  1.21it/s]Extractor Estimating: 47it [00:59,  1.21it/s]Extractor Estimating: 48it [01:00,  1.23it/s]Extractor Estimating: 49it [01:01,  1.23it/s]Extractor Estimating: 50it [01:02,  1.23it/s]Extractor Estimating: 51it [01:03,  1.27it/s]Extractor Estimating: 52it [01:03,  1.27it/s]Extractor Estimating: 53it [01:04,  1.28it/s]Extractor Estimating: 54it [01:05,  1.30it/s]Extractor Estimating: 55it [01:06,  1.29it/s]Extractor Estimating: 56it [01:06,  1.29it/s]Extractor Estimating: 57it [01:07,  1.33it/s]Extractor Estimating: 58it [01:08,  1.30it/s]Extractor Estimating: 59it [01:09,  1.31it/s]Extractor Estimating: 60it [01:10,  1.26it/s]Extractor Estimating: 61it [01:10,  1.27it/s]Extractor Estimating: 62it [01:11,  1.27it/s]Extractor Estimating: 63it [01:12,  1.29it/s]Extractor Estimating: 64it [01:13,  1.27it/s]Extractor Estimating: 65it [01:14,  1.25it/s]Extractor Estimating: 66it [01:14,  1.27it/s]Extractor Estimating: 67it [01:15,  1.29it/s]Extractor Estimating: 68it [01:16,  1.21it/s]Extractor Estimating: 69it [01:17,  1.23it/s]Extractor Estimating: 70it [01:17,  1.27it/s]Extractor Estimating: 71it [01:18,  1.29it/s]Extractor Estimating: 72it [01:19,  1.24it/s]Extractor Estimating: 73it [01:20,  1.29it/s]Extractor Estimating: 74it [01:21,  1.18it/s]Extractor Estimating: 75it [01:22,  1.19it/s]Extractor Estimating: 76it [01:23,  1.16it/s]Extractor Estimating: 77it [01:23,  1.18it/s]Extractor Estimating: 78it [01:24,  1.23it/s]Extractor Estimating: 79it [01:25,  1.23it/s]Extractor Estimating: 80it [01:26,  1.21it/s]Extractor Estimating: 81it [01:27,  1.19it/s]Extractor Estimating: 82it [01:27,  1.23it/s]Extractor Estimating: 83it [01:28,  1.23it/s]Extractor Estimating: 84it [01:29,  1.24it/s]Extractor Estimating: 85it [01:30,  1.20it/s]Extractor Estimating: 86it [01:31,  1.14it/s]Extractor Estimating: 87it [01:32,  1.16it/s]Extractor Estimating: 88it [01:33,  1.17it/s]Extractor Estimating: 89it [01:33,  1.16it/s]Extractor Estimating: 90it [01:34,  1.20it/s]Extractor Estimating: 91it [01:35,  1.22it/s]Extractor Estimating: 92it [01:36,  1.20it/s]Extractor Estimating: 93it [01:37,  1.22it/s]Extractor Estimating: 94it [01:38,  1.05s/it]Extractor Estimating: 95it [01:39,  1.00it/s]Extractor Estimating: 96it [01:40,  1.09it/s]Extractor Estimating: 97it [01:41,  1.09it/s]Extractor Estimating: 98it [01:41,  1.17it/s]Extractor Estimating: 99it [01:42,  1.19it/s]Extractor Estimating: 100it [01:43,  1.21it/s]Extractor Estimating: 101it [01:44,  1.24it/s]Extractor Estimating: 102it [01:45,  1.24it/s]Extractor Estimating: 103it [01:45,  1.25it/s]Extractor Estimating: 104it [01:46,  1.24it/s]Extractor Estimating: 105it [01:47,  1.19it/s]Extractor Estimating: 106it [01:48,  1.17it/s]Extractor Estimating: 107it [01:49,  1.17it/s]Extractor Estimating: 108it [01:50,  1.21it/s]Extractor Estimating: 109it [01:50,  1.21it/s]Extractor Estimating: 110it [01:51,  1.26it/s]Extractor Estimating: 111it [01:52,  1.27it/s]Extractor Estimating: 112it [01:53,  1.27it/s]Extractor Estimating: 113it [01:54,  1.29it/s]Extractor Estimating: 114it [01:54,  1.31it/s]Extractor Estimating: 115it [01:55,  1.26it/s]Extractor Estimating: 116it [01:56,  1.20it/s]Extractor Estimating: 117it [01:57,  1.22it/s]Extractor Estimating: 118it [01:58,  1.24it/s]Extractor Estimating: 119it [01:58,  1.28it/s]Extractor Estimating: 120it [01:59,  1.27it/s]Extractor Estimating: 121it [02:00,  1.24it/s]Extractor Estimating: 122it [02:01,  1.18it/s]Extractor Estimating: 123it [02:02,  1.17it/s]Extractor Estimating: 124it [02:03,  1.21it/s]Extractor Estimating: 125it [02:03,  1.24it/s]Extractor Estimating: 126it [02:04,  1.28it/s]Extractor Estimating: 127it [02:05,  1.29it/s]Extractor Estimating: 128it [02:06,  1.31it/s]Extractor Estimating: 129it [02:06,  1.32it/s]Extractor Estimating: 130it [02:07,  1.33it/s]Extractor Estimating: 131it [02:08,  1.32it/s]Extractor Estimating: 132it [02:09,  1.31it/s]Extractor Estimating: 133it [02:09,  1.32it/s]Extractor Estimating: 134it [02:10,  1.32it/s]Extractor Estimating: 135it [02:11,  1.34it/s]Extractor Estimating: 136it [02:12,  1.31it/s]Extractor Estimating: 137it [02:12,  1.33it/s]Extractor Estimating: 138it [02:13,  1.25it/s]Extractor Estimating: 139it [02:14,  1.30it/s]Extractor Estimating: 140it [02:15,  1.35it/s]Extractor Estimating: 141it [02:15,  1.34it/s]Extractor Estimating: 142it [02:16,  1.34it/s]Extractor Estimating: 143it [02:17,  1.33it/s]Extractor Estimating: 144it [02:18,  1.30it/s]Extractor Estimating: 145it [02:18,  1.34it/s]Extractor Estimating: 146it [02:19,  1.34it/s]Extractor Estimating: 147it [02:20,  1.34it/s]Extractor Estimating: 148it [02:21,  1.31it/s]Extractor Estimating: 149it [02:21,  1.30it/s]Extractor Estimating: 150it [02:22,  1.33it/s]Extractor Estimating: 151it [02:23,  1.28it/s]Extractor Estimating: 152it [02:24,  1.27it/s]Extractor Estimating: 153it [02:25,  1.28it/s]Extractor Estimating: 154it [02:25,  1.23it/s]Extractor Estimating: 155it [02:26,  1.21it/s]Extractor Estimating: 156it [02:27,  1.24it/s]Extractor Estimating: 157it [02:28,  1.24it/s]Extractor Estimating: 158it [02:29,  1.25it/s]Extractor Estimating: 159it [02:29,  1.25it/s]Extractor Estimating: 160it [02:30,  1.24it/s]Extractor Estimating: 161it [02:31,  1.22it/s]Extractor Estimating: 162it [02:32,  1.24it/s]Extractor Estimating: 163it [02:33,  1.25it/s]Extractor Estimating: 164it [02:34,  1.22it/s]Extractor Estimating: 165it [02:35,  1.16it/s]Extractor Estimating: 166it [02:35,  1.21it/s]Extractor Estimating: 167it [02:36,  1.24it/s]Extractor Estimating: 168it [02:37,  1.25it/s]Extractor Estimating: 169it [02:38,  1.25it/s]Extractor Estimating: 170it [02:38,  1.27it/s]Extractor Estimating: 171it [02:39,  1.25it/s]Extractor Estimating: 172it [02:40,  1.25it/s]Extractor Estimating: 173it [02:41,  1.23it/s]Extractor Estimating: 174it [02:42,  1.19it/s]Extractor Estimating: 175it [02:43,  1.22it/s]Extractor Estimating: 176it [02:43,  1.27it/s]Extractor Estimating: 177it [02:44,  1.27it/s]Extractor Estimating: 178it [02:45,  1.25it/s]Extractor Estimating: 179it [02:46,  1.23it/s]Extractor Estimating: 180it [02:47,  1.21it/s]Extractor Estimating: 181it [02:47,  1.23it/s]Extractor Estimating: 182it [02:48,  1.24it/s]Extractor Estimating: 183it [02:49,  1.22it/s]Extractor Estimating: 184it [02:50,  1.26it/s]Extractor Estimating: 185it [02:51,  1.26it/s]Extractor Estimating: 186it [02:51,  1.30it/s]Extractor Estimating: 187it [02:52,  1.25it/s]Extractor Estimating: 188it [02:53,  1.21it/s]Extractor Estimating: 189it [02:54,  1.23it/s]Extractor Estimating: 190it [02:55,  1.21it/s]Extractor Estimating: 191it [02:55,  1.24it/s]Extractor Estimating: 192it [02:56,  1.25it/s]Extractor Estimating: 193it [02:57,  1.22it/s]Extractor Estimating: 194it [02:58,  1.21it/s]Extractor Estimating: 195it [02:59,  1.24it/s]Extractor Estimating: 196it [02:59,  1.25it/s]Extractor Estimating: 197it [03:00,  1.19it/s]Extractor Estimating: 198it [03:01,  1.18it/s]Extractor Estimating: 199it [03:02,  1.21it/s]Extractor Estimating: 200it [03:03,  1.22it/s]Extractor Estimating: 201it [03:04,  1.19it/s]Extractor Estimating: 202it [03:05,  1.19it/s]Extractor Estimating: 203it [03:05,  1.24it/s]Extractor Estimating: 204it [03:06,  1.26it/s]Extractor Estimating: 205it [03:07,  1.23it/s]Extractor Estimating: 206it [03:08,  1.18it/s]Extractor Estimating: 207it [03:09,  1.18it/s]Extractor Estimating: 208it [03:10,  1.20it/s]Extractor Estimating: 209it [03:10,  1.21it/s]Extractor Estimating: 210it [03:11,  1.19it/s]Extractor Estimating: 211it [03:12,  1.23it/s]Extractor Estimating: 212it [03:13,  1.21it/s]Extractor Estimating: 213it [03:14,  1.22it/s]Extractor Estimating: 214it [03:14,  1.26it/s]Extractor Estimating: 215it [03:15,  1.24it/s]Extractor Estimating: 216it [03:16,  1.23it/s]Extractor Estimating: 217it [03:17,  1.26it/s]Extractor Estimating: 218it [03:18,  1.24it/s]Extractor Estimating: 219it [03:18,  1.27it/s]Extractor Estimating: 220it [03:19,  1.31it/s]Extractor Estimating: 221it [03:20,  1.30it/s]Extractor Estimating: 222it [03:21,  1.24it/s]Extractor Estimating: 223it [03:21,  1.26it/s]Extractor Estimating: 224it [03:22,  1.19it/s]Extractor Estimating: 225it [03:23,  1.17it/s]Extractor Estimating: 226it [03:24,  1.21it/s]Extractor Estimating: 227it [03:25,  1.26it/s]Extractor Estimating: 228it [03:26,  1.23it/s]Extractor Estimating: 229it [03:27,  1.20it/s]Extractor Estimating: 230it [03:27,  1.17it/s]Extractor Estimating: 231it [03:28,  1.19it/s]Extractor Estimating: 232it [03:29,  1.22it/s]Extractor Estimating: 233it [03:30,  1.21it/s]Extractor Estimating: 234it [03:31,  1.18it/s]Extractor Estimating: 235it [03:32,  1.21it/s]Extractor Estimating: 236it [03:32,  1.19it/s]Extractor Estimating: 237it [03:33,  1.20it/s]Extractor Estimating: 238it [03:34,  1.19it/s]Extractor Estimating: 239it [03:35,  1.21it/s]Extractor Estimating: 240it [03:36,  1.25it/s]Extractor Estimating: 241it [03:36,  1.25it/s]Extractor Estimating: 242it [03:37,  1.23it/s]Extractor Estimating: 243it [03:38,  1.21it/s]Extractor Estimating: 244it [03:39,  1.18it/s]Extractor Estimating: 245it [03:40,  1.23it/s]Extractor Estimating: 246it [03:41,  1.22it/s]Extractor Estimating: 247it [03:41,  1.18it/s]Extractor Estimating: 248it [03:42,  1.16it/s]Extractor Estimating: 249it [03:43,  1.09it/s]Extractor Estimating: 250it [03:44,  1.12it/s]Extractor Estimating: 251it [03:45,  1.17it/s]Extractor Estimating: 252it [03:46,  1.17it/s]Extractor Estimating: 253it [03:47,  1.16it/s]Extractor Estimating: 254it [03:48,  1.18it/s]Extractor Estimating: 255it [03:48,  1.17it/s]Extractor Estimating: 256it [03:49,  1.18it/s]Extractor Estimating: 257it [03:50,  1.22it/s]Extractor Estimating: 258it [03:51,  1.26it/s]Extractor Estimating: 259it [03:52,  1.27it/s]Extractor Estimating: 260it [03:52,  1.28it/s]Extractor Estimating: 261it [03:53,  1.25it/s]Extractor Estimating: 262it [03:54,  1.27it/s]Extractor Estimating: 263it [03:55,  1.23it/s]Extractor Estimating: 264it [03:56,  1.25it/s]Extractor Estimating: 265it [03:56,  1.23it/s]Extractor Estimating: 266it [03:57,  1.27it/s]Extractor Estimating: 267it [03:58,  1.27it/s]Extractor Estimating: 268it [03:59,  1.28it/s]Extractor Estimating: 269it [03:59,  1.28it/s]Extractor Estimating: 270it [04:00,  1.29it/s]Extractor Estimating: 271it [04:01,  1.27it/s]Extractor Estimating: 272it [04:02,  1.26it/s]Extractor Estimating: 273it [04:03,  1.28it/s]Extractor Estimating: 274it [04:03,  1.27it/s]Extractor Estimating: 275it [04:04,  1.24it/s]Extractor Estimating: 276it [04:05,  1.22it/s]Extractor Estimating: 277it [04:06,  1.18it/s]Extractor Estimating: 278it [04:07,  1.20it/s]Extractor Estimating: 279it [04:08,  1.19it/s]Extractor Estimating: 280it [04:09,  1.17it/s]Extractor Estimating: 281it [04:09,  1.19it/s]Extractor Estimating: 282it [04:10,  1.16it/s]Extractor Estimating: 283it [04:11,  1.16it/s]Extractor Estimating: 284it [04:12,  1.07it/s]Extractor Estimating: 285it [04:13,  1.10it/s]Extractor Estimating: 286it [04:14,  1.14it/s]Extractor Estimating: 287it [04:15,  1.17it/s]Extractor Estimating: 288it [04:16,  1.14it/s]Extractor Estimating: 289it [04:17,  1.15it/s]Extractor Estimating: 290it [04:17,  1.12it/s]Extractor Estimating: 291it [04:18,  1.12it/s]Extractor Estimating: 292it [04:19,  1.15it/s]Extractor Estimating: 293it [04:20,  1.16it/s]Extractor Estimating: 294it [04:21,  1.19it/s]Extractor Estimating: 295it [04:22,  1.19it/s]Extractor Estimating: 296it [04:22,  1.20it/s]Extractor Estimating: 297it [04:23,  1.19it/s]Extractor Estimating: 298it [04:24,  1.16it/s]Extractor Estimating: 299it [04:25,  1.17it/s]Extractor Estimating: 300it [04:26,  1.20it/s]Extractor Estimating: 301it [04:27,  1.23it/s]Extractor Estimating: 302it [04:27,  1.23it/s]Extractor Estimating: 303it [04:28,  1.27it/s]Extractor Estimating: 304it [04:29,  1.23it/s]Extractor Estimating: 305it [04:30,  1.29it/s]Extractor Estimating: 306it [04:30,  1.29it/s]Extractor Estimating: 307it [04:31,  1.29it/s]Extractor Estimating: 308it [04:32,  1.24it/s]Extractor Estimating: 309it [04:33,  1.26it/s]Extractor Estimating: 310it [04:34,  1.28it/s]Extractor Estimating: 311it [04:34,  1.29it/s]Extractor Estimating: 312it [04:35,  1.26it/s]Extractor Estimating: 313it [04:36,  1.30it/s]Extractor Estimating: 314it [04:37,  1.24it/s]Extractor Estimating: 315it [04:38,  1.24it/s]Extractor Estimating: 316it [04:38,  1.29it/s]Extractor Estimating: 317it [04:39,  1.21it/s]Extractor Estimating: 318it [04:40,  1.22it/s]Extractor Estimating: 319it [04:41,  1.24it/s]Extractor Estimating: 320it [04:42,  1.22it/s]Extractor Estimating: 321it [04:43,  1.24it/s]Extractor Estimating: 322it [04:43,  1.31it/s]Extractor Estimating: 323it [04:44,  1.25it/s]Extractor Estimating: 324it [04:45,  1.23it/s]Extractor Estimating: 325it [04:46,  1.25it/s]Extractor Estimating: 326it [04:46,  1.26it/s]Extractor Estimating: 327it [04:47,  1.25it/s]Extractor Estimating: 328it [04:48,  1.23it/s]Extractor Estimating: 329it [04:49,  1.27it/s]Extractor Estimating: 330it [04:50,  1.24it/s]Extractor Estimating: 331it [04:50,  1.25it/s]Extractor Estimating: 332it [04:51,  1.17it/s]Extractor Estimating: 333it [04:52,  1.22it/s]Extractor Estimating: 334it [04:53,  1.20it/s]Extractor Estimating: 335it [04:54,  1.23it/s]Extractor Estimating: 336it [04:55,  1.21it/s]Extractor Estimating: 337it [04:55,  1.24it/s]Extractor Estimating: 338it [04:56,  1.23it/s]Extractor Estimating: 339it [04:57,  1.23it/s]Extractor Estimating: 340it [04:58,  1.23it/s]Extractor Estimating: 341it [04:59,  1.27it/s]Extractor Estimating: 342it [04:59,  1.28it/s]Extractor Estimating: 343it [05:00,  1.25it/s]Extractor Estimating: 344it [05:01,  1.22it/s]Extractor Estimating: 345it [05:02,  1.19it/s]Extractor Estimating: 346it [05:03,  1.19it/s]Extractor Estimating: 347it [05:04,  1.20it/s]Extractor Estimating: 348it [05:05,  1.17it/s]Extractor Estimating: 349it [05:05,  1.17it/s]Extractor Estimating: 350it [05:06,  1.18it/s]Extractor Estimating: 351it [05:07,  1.21it/s]Extractor Estimating: 352it [05:08,  1.21it/s]Extractor Estimating: 353it [05:09,  1.22it/s]Extractor Estimating: 354it [05:09,  1.23it/s]Extractor Estimating: 355it [05:10,  1.26it/s]Extractor Estimating: 356it [05:11,  1.27it/s]Extractor Estimating: 357it [05:12,  1.29it/s]Extractor Estimating: 358it [05:13,  1.26it/s]Extractor Estimating: 359it [05:13,  1.28it/s]Extractor Estimating: 360it [05:14,  1.24it/s]Extractor Estimating: 361it [05:15,  1.24it/s]Extractor Estimating: 362it [05:16,  1.26it/s]Extractor Estimating: 363it [05:17,  1.25it/s]Extractor Estimating: 364it [05:17,  1.24it/s]Extractor Estimating: 365it [05:18,  1.21it/s]Extractor Estimating: 366it [05:19,  1.24it/s]Extractor Estimating: 367it [05:20,  1.27it/s]Extractor Estimating: 368it [05:21,  1.29it/s]Extractor Estimating: 369it [05:21,  1.30it/s]Extractor Estimating: 370it [05:22,  1.30it/s]Extractor Estimating: 371it [05:23,  1.30it/s]Extractor Estimating: 372it [05:24,  1.25it/s]Extractor Estimating: 373it [05:25,  1.22it/s]Extractor Estimating: 374it [05:25,  1.24it/s]Extractor Estimating: 375it [05:26,  1.22it/s]Extractor Estimating: 376it [05:27,  1.22it/s]Extractor Estimating: 377it [05:28,  1.19it/s]Extractor Estimating: 378it [05:29,  1.23it/s]Extractor Estimating: 379it [05:29,  1.25it/s]Extractor Estimating: 380it [05:30,  1.26it/s]Extractor Estimating: 381it [05:31,  1.27it/s]Extractor Estimating: 382it [05:32,  1.28it/s]Extractor Estimating: 383it [05:33,  1.26it/s]Extractor Estimating: 384it [05:33,  1.25it/s]Extractor Estimating: 385it [05:34,  1.26it/s]Extractor Estimating: 386it [05:35,  1.25it/s]Extractor Estimating: 387it [05:36,  1.26it/s]Extractor Estimating: 388it [05:37,  1.25it/s]Extractor Estimating: 389it [05:37,  1.27it/s]Extractor Estimating: 390it [05:38,  1.28it/s]Extractor Estimating: 391it [05:39,  1.29it/s]Extractor Estimating: 392it [05:40,  1.24it/s]Extractor Estimating: 393it [05:41,  1.22it/s]Extractor Estimating: 394it [05:41,  1.21it/s]Extractor Estimating: 395it [05:42,  1.23it/s]Extractor Estimating: 396it [05:43,  1.24it/s]Extractor Estimating: 397it [05:44,  1.30it/s]Extractor Estimating: 398it [05:44,  1.30it/s]Extractor Estimating: 399it [05:45,  1.26it/s]Extractor Estimating: 400it [05:46,  1.26it/s]Extractor Estimating: 401it [05:47,  1.30it/s]Extractor Estimating: 402it [05:48,  1.31it/s]Extractor Estimating: 403it [05:48,  1.31it/s]Extractor Estimating: 404it [05:49,  1.32it/s]Extractor Estimating: 405it [05:50,  1.35it/s]Extractor Estimating: 406it [05:51,  1.33it/s]Extractor Estimating: 407it [05:51,  1.32it/s]Extractor Estimating: 408it [05:52,  1.23it/s]Extractor Estimating: 409it [05:53,  1.26it/s]Extractor Estimating: 410it [05:54,  1.29it/s]Extractor Estimating: 411it [05:54,  1.32it/s]Extractor Estimating: 412it [05:55,  1.33it/s]Extractor Estimating: 413it [05:56,  1.34it/s]Extractor Estimating: 414it [05:57,  1.29it/s]Extractor Estimating: 415it [05:58,  1.29it/s]Extractor Estimating: 416it [05:58,  1.30it/s]Extractor Estimating: 417it [05:59,  1.28it/s]Extractor Estimating: 418it [06:00,  1.31it/s]Extractor Estimating: 419it [06:01,  1.31it/s]Extractor Estimating: 420it [06:01,  1.35it/s]Extractor Estimating: 421it [06:02,  1.32it/s]Extractor Estimating: 422it [06:03,  1.32it/s]Extractor Estimating: 423it [06:03,  1.36it/s]Extractor Estimating: 424it [06:04,  1.30it/s]Extractor Estimating: 425it [06:05,  1.29it/s]Extractor Estimating: 426it [06:06,  1.23it/s]Extractor Estimating: 427it [06:07,  1.24it/s]Extractor Estimating: 428it [06:08,  1.25it/s]Extractor Estimating: 429it [06:08,  1.24it/s]Extractor Estimating: 430it [06:09,  1.23it/s]Extractor Estimating: 431it [06:10,  1.22it/s]Extractor Estimating: 432it [06:11,  1.21it/s]Extractor Estimating: 433it [06:12,  1.24it/s]Extractor Estimating: 434it [06:13,  1.22it/s]Extractor Estimating: 435it [06:13,  1.23it/s]Extractor Estimating: 436it [06:14,  1.23it/s]Extractor Estimating: 437it [06:15,  1.24it/s]Extractor Estimating: 438it [06:16,  1.21it/s]Extractor Estimating: 439it [06:17,  1.23it/s]Extractor Estimating: 440it [06:17,  1.23it/s]Extractor Estimating: 441it [06:18,  1.22it/s]Extractor Estimating: 442it [06:19,  1.22it/s]Extractor Estimating: 443it [06:20,  1.21it/s]Extractor Estimating: 444it [06:21,  1.23it/s]Extractor Estimating: 445it [06:22,  1.18it/s]Extractor Estimating: 446it [06:23,  1.15it/s]Extractor Estimating: 447it [06:23,  1.18it/s]Extractor Estimating: 448it [06:24,  1.20it/s]Extractor Estimating: 449it [06:25,  1.20it/s]Extractor Estimating: 450it [06:26,  1.17it/s]Extractor Estimating: 451it [06:27,  1.14it/s]Extractor Estimating: 452it [06:28,  1.15it/s]Extractor Estimating: 453it [06:28,  1.16it/s]Extractor Estimating: 454it [06:29,  1.19it/s]Extractor Estimating: 455it [06:30,  1.17it/s]Extractor Estimating: 456it [06:31,  1.17it/s]Extractor Estimating: 457it [06:32,  1.19it/s]Extractor Estimating: 458it [06:33,  1.21it/s]Extractor Estimating: 459it [06:33,  1.20it/s]Extractor Estimating: 460it [06:34,  1.18it/s]Extractor Estimating: 461it [06:35,  1.15it/s]Extractor Estimating: 462it [06:36,  1.14it/s]Extractor Estimating: 463it [06:37,  1.14it/s]Extractor Estimating: 464it [06:38,  1.15it/s]Extractor Estimating: 465it [06:39,  1.17it/s]Extractor Estimating: 466it [06:40,  1.19it/s]Extractor Estimating: 467it [06:40,  1.23it/s]Extractor Estimating: 468it [06:41,  1.20it/s]Extractor Estimating: 469it [06:42,  1.19it/s]Extractor Estimating: 470it [06:43,  1.17it/s]Extractor Estimating: 471it [06:44,  1.23it/s]Extractor Estimating: 472it [06:44,  1.24it/s]Extractor Estimating: 473it [06:45,  1.23it/s]Extractor Estimating: 474it [06:46,  1.21it/s]Extractor Estimating: 475it [06:47,  1.20it/s]Extractor Estimating: 476it [06:48,  1.21it/s]Extractor Estimating: 477it [06:49,  1.23it/s]Extractor Estimating: 478it [06:49,  1.19it/s]Extractor Estimating: 479it [06:50,  1.17it/s]Extractor Estimating: 480it [06:51,  1.16it/s]Extractor Estimating: 481it [06:52,  1.21it/s]Extractor Estimating: 482it [06:53,  1.14it/s]Extractor Estimating: 483it [06:54,  1.17it/s]Extractor Estimating: 484it [06:55,  1.19it/s]Extractor Estimating: 485it [06:55,  1.17it/s]Extractor Estimating: 486it [06:56,  1.19it/s]Extractor Estimating: 487it [06:57,  1.23it/s]Extractor Estimating: 488it [06:58,  1.27it/s]Extractor Estimating: 489it [06:59,  1.26it/s]Extractor Estimating: 490it [06:59,  1.23it/s]Extractor Estimating: 491it [07:00,  1.26it/s]Extractor Estimating: 492it [07:01,  1.24it/s]Extractor Estimating: 493it [07:02,  1.23it/s]Extractor Estimating: 494it [07:03,  1.22it/s]Extractor Estimating: 495it [07:04,  1.19it/s]Extractor Estimating: 496it [07:05,  1.13it/s]Extractor Estimating: 497it [07:05,  1.19it/s]Extractor Estimating: 498it [07:06,  1.19it/s]Extractor Estimating: 499it [07:07,  1.21it/s]Extractor Estimating: 500it [07:08,  1.20it/s]Extractor Estimating: 500it [07:08,  1.17it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 2029 mean pseudo reward: 0.967948963862939
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 15355
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15455, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15455, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 15, avg_time 1.671, loss:312.5670
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 30, avg_time 1.355, loss:219.8158
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 45, avg_time 1.355, loss:194.8412
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 60, avg_time 1.345, loss:170.2948
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 75, avg_time 1.357, loss:154.6133
>> valid entity prec:0.6278, rec:0.6717, f1:0.6490
>> valid relation prec:0.4272, rec:0.3088, f1:0.3585
>> valid relation with NER prec:0.4272, rec:0.3088, f1:0.3585
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 5, avg_time 3.037, loss:132.4624
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 20, avg_time 1.340, loss:128.5982
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 35, avg_time 1.343, loss:136.8929
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 50, avg_time 1.368, loss:123.9210
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 65, avg_time 1.360, loss:128.3715
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6313, rec:0.6452, f1:0.6382
>> valid relation prec:0.3642, rec:0.3083, f1:0.3339
>> valid relation with NER prec:0.3642, rec:0.3083, f1:0.3339
g_step 1100, step 80, avg_time 3.036, loss:134.2363
g_step 1200, step 10, avg_time 1.361, loss:121.2596
g_step 1300, step 25, avg_time 1.347, loss:125.0292
g_step 1400, step 40, avg_time 1.350, loss:100.6030
g_step 1500, step 55, avg_time 1.357, loss:97.8513
>> valid entity prec:0.6114, rec:0.6083, f1:0.6098
>> valid relation prec:0.3640, rec:0.2785, f1:0.3156
>> valid relation with NER prec:0.3640, rec:0.2785, f1:0.3156
g_step 1600, step 70, avg_time 3.027, loss:101.7876
g_step 1700, step 85, avg_time 1.361, loss:92.2385
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:22:48 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:22:48 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-22-48_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:22:49 - WARNING - datasets.builder -   Using custom data configuration default-d71e8aedda72ef8b
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d71e8aedda72ef8b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:22:49,751 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:22:49,752 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:22:49,752 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:22:49,753 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:22:49,766 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:22:49,769 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:22:49,769 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:22:49,769 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:22:49,769 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:22:49,769 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:22:49,769 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:22:49,912 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:22:53,016 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:22:53,019 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d71e8aedda72ef8b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 00:22:53 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1551006ad170> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  2.94ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  3.82ba/s]100%|██████████| 3/3 [00:00<00:00,  5.37ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.19ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.38ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.44ba/s]100%|██████████| 4/4 [00:00<00:00,  5.53ba/s]100%|██████████| 4/4 [00:00<00:00,  5.03ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  8.99ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  9.35ba/s]100%|██████████| 3/3 [00:00<00:00, 13.61ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.02ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.27ba/s]100%|██████████| 4/4 [00:00<00:00, 11.57ba/s]
[INFO|trainer.py:414] 2023-08-29 00:22:55,267 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:22:55,278 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:22:55,278 >>   Num examples = 2029
[INFO|trainer.py:1149] 2023-08-29 00:22:55,278 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:22:55,278 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:22:55,278 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:22:55,278 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:22:55,278 >>   Total optimization steps = 160
  0%|          | 0/160 [00:00<?, ?it/s]  1%|          | 1/160 [00:00<00:48,  3.25it/s]  1%|▏         | 2/160 [00:00<00:46,  3.36it/s]  2%|▏         | 3/160 [00:00<00:46,  3.41it/s]  2%|▎         | 4/160 [00:01<00:45,  3.41it/s]  3%|▎         | 5/160 [00:01<00:45,  3.42it/s]  4%|▍         | 6/160 [00:01<00:44,  3.43it/s]  4%|▍         | 7/160 [00:02<00:44,  3.44it/s]  5%|▌         | 8/160 [00:02<00:44,  3.44it/s]  6%|▌         | 9/160 [00:02<00:43,  3.44it/s]  6%|▋         | 10/160 [00:02<00:43,  3.44it/s]  7%|▋         | 11/160 [00:03<00:43,  3.44it/s]  8%|▊         | 12/160 [00:03<00:43,  3.44it/s]  8%|▊         | 13/160 [00:03<00:42,  3.44it/s]  9%|▉         | 14/160 [00:04<00:42,  3.44it/s]  9%|▉         | 15/160 [00:04<00:42,  3.43it/s] 10%|█         | 16/160 [00:04<00:41,  3.43it/s] 11%|█         | 17/160 [00:04<00:41,  3.43it/s] 11%|█▏        | 18/160 [00:05<00:41,  3.44it/s] 12%|█▏        | 19/160 [00:05<00:41,  3.44it/s] 12%|█▎        | 20/160 [00:05<00:40,  3.44it/s] 13%|█▎        | 21/160 [00:06<00:40,  3.44it/s] 14%|█▍        | 22/160 [00:06<00:40,  3.44it/s] 14%|█▍        | 23/160 [00:06<00:39,  3.44it/s] 15%|█▌        | 24/160 [00:06<00:39,  3.44it/s] 16%|█▌        | 25/160 [00:07<00:39,  3.44it/s] 16%|█▋        | 26/160 [00:07<00:39,  3.43it/s] 17%|█▋        | 27/160 [00:07<00:38,  3.43it/s] 18%|█▊        | 28/160 [00:08<00:38,  3.43it/s] 18%|█▊        | 29/160 [00:08<00:38,  3.44it/s] 19%|█▉        | 30/160 [00:08<00:37,  3.44it/s] 19%|█▉        | 31/160 [00:09<00:37,  3.44it/s] 20%|██        | 32/160 [00:09<00:34,  3.71it/s][INFO|trainer.py:2140] 2023-08-29 00:23:04,532 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:23:04,532 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:23:04,532 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.92it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.74it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.98it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.26it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.83it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.54it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.31it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.86it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.71it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.74it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.77it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.83it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.90it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.88it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.88it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.90it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.67it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.56it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.51it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.58it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.44it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.58it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.60it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.77it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.79it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.74it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.58it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.53it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.59it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.57it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.61it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.72it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.72it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.74it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.69it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.58it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.54it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.60it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.58it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.60it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.69it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.77it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.70it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.67it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.45it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.52it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.56it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.54it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.58it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.48it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.55it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.66it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.66it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.56it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.49it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.46it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.56it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.58it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.59it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.65it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.67it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.64it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.56it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.45it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.54it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.44it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.54it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.54it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.59it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.62it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.64it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.47it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.58it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.57it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.50it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.51it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.57it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.58it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.60it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.59it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.59it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.59it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.51it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.55it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.53it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.54it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.61it/s][A                                                
                                                 [A 20%|██        | 32/160 [00:18<00:34,  3.71it/s]
100%|██████████| 438/438 [00:09<00:00, 46.61it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:23:13,935 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-32
[INFO|configuration_utils.py:351] 2023-08-29 00:23:13,950 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-32/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:23:16,052 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-32/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:23:16,067 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-32/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:23:16,076 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-32/special_tokens_map.json
 21%|██        | 33/160 [00:25<10:40,  5.05s/it] 21%|██▏       | 34/160 [00:25<07:36,  3.62s/it] 22%|██▏       | 35/160 [00:26<05:27,  2.62s/it] 22%|██▎       | 36/160 [00:26<03:58,  1.92s/it] 23%|██▎       | 37/160 [00:26<02:56,  1.43s/it] 24%|██▍       | 38/160 [00:26<02:12,  1.09s/it] 24%|██▍       | 39/160 [00:27<01:42,  1.18it/s] 25%|██▌       | 40/160 [00:27<01:21,  1.47it/s] 26%|██▌       | 41/160 [00:27<01:07,  1.77it/s] 26%|██▋       | 42/160 [00:28<00:56,  2.07it/s] 27%|██▋       | 43/160 [00:28<00:49,  2.35it/s] 28%|██▊       | 44/160 [00:28<00:44,  2.60it/s] 28%|██▊       | 45/160 [00:28<00:40,  2.81it/s] 29%|██▉       | 46/160 [00:29<00:38,  2.97it/s] 29%|██▉       | 47/160 [00:29<00:36,  3.10it/s] 30%|███       | 48/160 [00:29<00:35,  3.19it/s] 31%|███       | 49/160 [00:30<00:34,  3.26it/s] 31%|███▏      | 50/160 [00:30<00:33,  3.31it/s] 32%|███▏      | 51/160 [00:30<00:32,  3.34it/s] 32%|███▎      | 52/160 [00:30<00:32,  3.36it/s] 33%|███▎      | 53/160 [00:31<00:31,  3.38it/s] 34%|███▍      | 54/160 [00:31<00:31,  3.39it/s] 34%|███▍      | 55/160 [00:31<00:30,  3.40it/s] 35%|███▌      | 56/160 [00:32<00:30,  3.41it/s] 36%|███▌      | 57/160 [00:32<00:30,  3.42it/s] 36%|███▋      | 58/160 [00:32<00:29,  3.42it/s] 37%|███▋      | 59/160 [00:33<00:29,  3.42it/s] 38%|███▊      | 60/160 [00:33<00:29,  3.42it/s] 38%|███▊      | 61/160 [00:33<00:28,  3.43it/s] 39%|███▉      | 62/160 [00:33<00:28,  3.43it/s] 39%|███▉      | 63/160 [00:34<00:28,  3.42it/s] 40%|████      | 64/160 [00:34<00:25,  3.70it/s][INFO|trainer.py:2140] 2023-08-29 00:23:29,689 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:23:29,689 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:23:29,689 >>   Batch size = 8
{'eval_loss': 1.0343413352966309, 'eval_runtime': 9.389, 'eval_samples_per_second': 372.459, 'eval_steps_per_second': 46.651, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.25it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.63it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.78it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.13it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.72it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.34it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.96it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.50it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.50it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.51it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.57it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.66it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.73it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.76it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.67it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.60it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.34it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.31it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.41it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.48it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.60it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.64it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.69it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.69it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.58it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.40it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.25it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.32it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.40it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.55it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.59it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.66it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.69it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.53it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.46it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.33it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.28it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.41it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.48it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.48it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.57it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.65it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.59it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.53it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.36it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.37it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.35it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.45it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.47it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.56it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.54it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.54it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.45it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.46it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.37it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.34it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.40it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.44it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.53it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.55it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.46it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.45it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.41it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.39it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.31it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.39it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.49it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.52it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.57it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.49it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.42it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.45it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.36it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.37it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.45it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.47it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.54it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.51it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.44it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.37it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.38it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.36it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.35it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.37it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.34it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.44it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.54it/s][A                                                
                                                 [A 40%|████      | 64/160 [00:43<00:25,  3.70it/s]
100%|██████████| 438/438 [00:09<00:00, 46.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:23:39,116 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-64
[INFO|configuration_utils.py:351] 2023-08-29 00:23:39,132 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-64/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:23:41,301 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-64/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:23:41,321 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-64/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:23:41,331 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-64/special_tokens_map.json
 41%|████      | 65/160 [00:50<08:00,  5.06s/it] 41%|████▏     | 66/160 [00:50<05:41,  3.63s/it] 42%|████▏     | 67/160 [00:51<04:04,  2.63s/it] 42%|████▎     | 68/160 [00:51<02:57,  1.93s/it] 43%|████▎     | 69/160 [00:51<02:10,  1.44s/it] 44%|████▍     | 70/160 [00:52<01:38,  1.09s/it] 44%|████▍     | 71/160 [00:52<01:15,  1.17it/s] 45%|████▌     | 72/160 [00:52<01:00,  1.46it/s] 46%|████▌     | 73/160 [00:52<00:49,  1.76it/s] 46%|████▋     | 74/160 [00:53<00:42,  2.01it/s] 47%|████▋     | 75/160 [00:53<00:37,  2.29it/s] 48%|████▊     | 76/160 [00:53<00:33,  2.54it/s] 48%|████▊     | 77/160 [00:54<00:30,  2.75it/s] 49%|████▉     | 78/160 [00:54<00:28,  2.92it/s] 49%|████▉     | 79/160 [00:54<00:26,  3.06it/s] 50%|█████     | 80/160 [00:55<00:25,  3.16it/s] 51%|█████     | 81/160 [00:55<00:24,  3.24it/s] 51%|█████▏    | 82/160 [00:55<00:23,  3.29it/s] 52%|█████▏    | 83/160 [00:55<00:23,  3.33it/s] 52%|█████▎    | 84/160 [00:56<00:22,  3.36it/s] 53%|█████▎    | 85/160 [00:56<00:22,  3.38it/s] 54%|█████▍    | 86/160 [00:56<00:21,  3.39it/s] 54%|█████▍    | 87/160 [00:57<00:21,  3.40it/s] 55%|█████▌    | 88/160 [00:57<00:21,  3.41it/s] 56%|█████▌    | 89/160 [00:57<00:20,  3.42it/s] 56%|█████▋    | 90/160 [00:57<00:20,  3.42it/s] 57%|█████▋    | 91/160 [00:58<00:20,  3.39it/s] 57%|█████▊    | 92/160 [00:58<00:19,  3.40it/s] 58%|█████▊    | 93/160 [00:58<00:19,  3.41it/s] 59%|█████▉    | 94/160 [00:59<00:19,  3.41it/s] 59%|█████▉    | 95/160 [00:59<00:19,  3.42it/s] 60%|██████    | 96/160 [00:59<00:17,  3.69it/s][INFO|trainer.py:2140] 2023-08-29 00:23:54,959 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:23:54,959 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:23:54,959 >>   Batch size = 8
{'eval_loss': 1.0054068565368652, 'eval_runtime': 9.4166, 'eval_samples_per_second': 371.366, 'eval_steps_per_second': 46.514, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.24it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.67it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.60it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.01it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.64it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.21it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.89it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.43it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.43it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.49it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.51it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.62it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.66it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.68it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.56it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.39it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.27it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.23it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.34it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.44it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.46it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.59it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.51it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.54it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.42it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.16it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.26it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.28it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.33it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.44it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.52it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.52it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.44it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.27it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.26it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.12it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.24it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.33it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.40it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.42it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.49it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.41it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.32it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.34it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.29it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.24it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.34it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.41it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.49it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.53it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.40it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.33it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.35it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.28it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.30it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.26it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.33it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.46it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.52it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.48it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.30it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.37it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.37it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.37it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.38it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.30it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.40it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.38it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.39it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.46it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.35it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.35it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.39it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.35it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.35it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.42it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.37it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.32it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.35it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.36it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.30it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.32it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.34it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.35it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.40it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.37it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.43it/s][A                                                
                                                 [A 60%|██████    | 96/160 [01:09<00:17,  3.69it/s]
100%|██████████| 438/438 [00:09<00:00, 46.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:24:04,417 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-96
[INFO|configuration_utils.py:351] 2023-08-29 00:24:04,452 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-96/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:24:06,797 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-96/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:24:06,811 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-96/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:24:06,819 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-96/special_tokens_map.json
 61%|██████    | 97/160 [01:16<05:26,  5.18s/it] 61%|██████▏   | 98/160 [01:16<03:50,  3.71s/it] 62%|██████▏   | 99/160 [01:16<02:43,  2.69s/it] 62%|██████▎   | 100/160 [01:17<01:58,  1.97s/it] 63%|██████▎   | 101/160 [01:17<01:26,  1.47s/it] 64%|██████▍   | 102/160 [01:17<01:04,  1.11s/it] 64%|██████▍   | 103/160 [01:18<00:49,  1.15it/s] 65%|██████▌   | 104/160 [01:18<00:38,  1.44it/s] 66%|██████▌   | 105/160 [01:18<00:31,  1.74it/s] 66%|██████▋   | 106/160 [01:18<00:26,  2.04it/s] 67%|██████▋   | 107/160 [01:19<00:22,  2.33it/s] 68%|██████▊   | 108/160 [01:19<00:20,  2.57it/s] 68%|██████▊   | 109/160 [01:19<00:18,  2.78it/s] 69%|██████▉   | 110/160 [01:20<00:16,  2.94it/s] 69%|██████▉   | 111/160 [01:20<00:15,  3.07it/s] 70%|███████   | 112/160 [01:20<00:15,  3.17it/s] 71%|███████   | 113/160 [01:20<00:14,  3.24it/s] 71%|███████▏  | 114/160 [01:21<00:13,  3.30it/s] 72%|███████▏  | 115/160 [01:21<00:13,  3.33it/s] 72%|███████▎  | 116/160 [01:21<00:13,  3.36it/s] 73%|███████▎  | 117/160 [01:22<00:12,  3.38it/s] 74%|███████▍  | 118/160 [01:22<00:12,  3.39it/s] 74%|███████▍  | 119/160 [01:22<00:12,  3.40it/s] 75%|███████▌  | 120/160 [01:23<00:11,  3.40it/s] 76%|███████▌  | 121/160 [01:23<00:11,  3.40it/s] 76%|███████▋  | 122/160 [01:23<00:11,  3.41it/s] 77%|███████▋  | 123/160 [01:23<00:10,  3.41it/s] 78%|███████▊  | 124/160 [01:24<00:10,  3.42it/s] 78%|███████▊  | 125/160 [01:24<00:10,  3.42it/s] 79%|███████▉  | 126/160 [01:24<00:09,  3.42it/s] 79%|███████▉  | 127/160 [01:25<00:09,  3.42it/s] 80%|████████  | 128/160 [01:25<00:08,  3.69it/s][INFO|trainer.py:2140] 2023-08-29 00:24:20,578 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:24:20,579 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:24:20,579 >>   Batch size = 8
{'eval_loss': 1.012895107269287, 'eval_runtime': 9.4349, 'eval_samples_per_second': 370.646, 'eval_steps_per_second': 46.424, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.94it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.38it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.59it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.86it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.38it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.10it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.82it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.49it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.37it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.42it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.48it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.48it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.54it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.52it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.46it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.37it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.27it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.25it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.29it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.28it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.43it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.51it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.42it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.47it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.36it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.30it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.30it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.32it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.25it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.33it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.36it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.33it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.25it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.22it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.25it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.33it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.35it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.27it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.44it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.49it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.38it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.28it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.25it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.26it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.30it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.30it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 45.92it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.37it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.47it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.38it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.39it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.39it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.29it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.36it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.37it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.32it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.28it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.27it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.42it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.41it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.24it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.33it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.25it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.33it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.34it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.35it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.27it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.35it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.40it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.22it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.06it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 45.95it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.04it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.17it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.17it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.25it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.26it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.18it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.26it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.27it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.36it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.33it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.33it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.30it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.35it/s][A                                                 
                                                 [A 80%|████████  | 128/160 [01:34<00:08,  3.69it/s]
100%|██████████| 438/438 [00:09<00:00, 46.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:24:30,038 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-128
[INFO|configuration_utils.py:351] 2023-08-29 00:24:30,066 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-128/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:24:32,532 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-128/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:24:32,551 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-128/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:24:32,566 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-128/special_tokens_map.json
 81%|████████  | 129/160 [01:42<02:45,  5.35s/it] 81%|████████▏ | 130/160 [01:42<01:55,  3.83s/it] 82%|████████▏ | 131/160 [01:43<01:20,  2.77s/it] 82%|████████▎ | 132/160 [01:43<00:56,  2.03s/it] 83%|████████▎ | 133/160 [01:43<00:40,  1.51s/it] 84%|████████▍ | 134/160 [01:43<00:29,  1.14s/it] 84%|████████▍ | 135/160 [01:44<00:22,  1.13it/s] 85%|████████▌ | 136/160 [01:44<00:17,  1.41it/s] 86%|████████▌ | 137/160 [01:44<00:13,  1.71it/s] 86%|████████▋ | 138/160 [01:45<00:10,  2.02it/s] 87%|████████▋ | 139/160 [01:45<00:09,  2.30it/s] 88%|████████▊ | 140/160 [01:45<00:07,  2.55it/s] 88%|████████▊ | 141/160 [01:46<00:06,  2.76it/s] 89%|████████▉ | 142/160 [01:46<00:06,  2.93it/s] 89%|████████▉ | 143/160 [01:46<00:05,  3.06it/s] 90%|█████████ | 144/160 [01:46<00:05,  3.16it/s] 91%|█████████ | 145/160 [01:47<00:04,  3.24it/s] 91%|█████████▏| 146/160 [01:47<00:04,  3.29it/s] 92%|█████████▏| 147/160 [01:47<00:03,  3.33it/s] 92%|█████████▎| 148/160 [01:48<00:03,  3.36it/s] 93%|█████████▎| 149/160 [01:48<00:03,  3.38it/s] 94%|█████████▍| 150/160 [01:48<00:02,  3.39it/s] 94%|█████████▍| 151/160 [01:48<00:02,  3.40it/s] 95%|█████████▌| 152/160 [01:49<00:02,  3.39it/s] 96%|█████████▌| 153/160 [01:49<00:02,  3.40it/s] 96%|█████████▋| 154/160 [01:49<00:01,  3.41it/s] 97%|█████████▋| 155/160 [01:50<00:01,  3.41it/s] 98%|█████████▊| 156/160 [01:50<00:01,  3.42it/s] 98%|█████████▊| 157/160 [01:50<00:00,  3.42it/s] 99%|█████████▉| 158/160 [01:50<00:00,  3.42it/s] 99%|█████████▉| 159/160 [01:51<00:00,  3.42it/s]100%|██████████| 160/160 [01:51<00:00,  3.69it/s][INFO|trainer.py:2140] 2023-08-29 00:24:46,777 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:24:46,777 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:24:46,777 >>   Batch size = 8
{'eval_loss': 1.0177112817764282, 'eval_runtime': 9.4499, 'eval_samples_per_second': 370.056, 'eval_steps_per_second': 46.35, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.00it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.46it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.60it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.72it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.47it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.16it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.88it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.46it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.28it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.33it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.40it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.46it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.51it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.50it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.51it/s][A
 19%|█▉        | 83/438 [00:01<00:08, 43.74it/s][A
 20%|██        | 88/438 [00:01<00:07, 44.96it/s][A
 21%|██        | 93/438 [00:01<00:07, 45.45it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.78it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.89it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.12it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.28it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.31it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.38it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.14it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.13it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.21it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.27it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.28it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.40it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.40it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.41it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.31it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.26it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.18it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.25it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.35it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.26it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.36it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.44it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.47it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.45it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.38it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.17it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.23it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.32it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.22it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.43it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.36it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.40it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.37it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.25it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.16it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.17it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.12it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.22it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.21it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.33it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.38it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.46it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.36it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.29it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.28it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.26it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.31it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.29it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.33it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.44it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.48it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.33it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.21it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.30it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.25it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.28it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.30it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.29it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.19it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.38it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.26it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.30it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.32it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.17it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.25it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.15it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.32it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.30it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.25it/s][A                                                 
                                                 [A100%|██████████| 160/160 [02:00<00:00,  3.69it/s]
100%|██████████| 438/438 [00:09<00:00, 46.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:24:56,248 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-29 00:24:56,264 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:24:58,680 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:24:58,704 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:24:58,715 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-160/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:25:03,779 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:25:03,782 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-64 (score: 1.0054068565368652).
                                                 100%|██████████| 160/160 [02:10<00:00,  3.69it/s]100%|██████████| 160/160 [02:10<00:00,  1.23it/s]
[INFO|trainer.py:1894] 2023-08-29 00:25:05,562 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 00:25:05,582 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:25:08,192 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:25:08,207 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:25:08,217 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:25:08,389 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:08,389 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:08,389 >>   train_loss               =      0.588
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:08,389 >>   train_runtime            = 0:02:10.27
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:08,389 >>   train_samples            =       2029
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:08,389 >>   train_samples_per_second =     77.876
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:08,389 >>   train_steps_per_second   =      1.228
{'eval_loss': 1.019715666770935, 'eval_runtime': 9.4663, 'eval_samples_per_second': 369.417, 'eval_steps_per_second': 46.27, 'epoch': 5.0}
{'train_runtime': 130.2719, 'train_samples_per_second': 77.876, 'train_steps_per_second': 1.228, 'train_loss': 0.5880428314208984, 'epoch': 5.0}
08/29/2023 00:25:08 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:25:08,426 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:25:08,426 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:25:08,427 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 57.58it/s]  3%|▎         | 12/438 [00:00<00:08, 50.85it/s]  4%|▍         | 18/438 [00:00<00:08, 49.05it/s]  5%|▌         | 23/438 [00:00<00:08, 48.31it/s]  6%|▋         | 28/438 [00:00<00:08, 47.79it/s]  8%|▊         | 33/438 [00:00<00:08, 47.57it/s]  9%|▊         | 38/438 [00:00<00:08, 47.33it/s] 10%|▉         | 43/438 [00:00<00:08, 47.09it/s] 11%|█         | 48/438 [00:01<00:08, 46.63it/s] 12%|█▏        | 53/438 [00:01<00:08, 46.58it/s] 13%|█▎        | 58/438 [00:01<00:08, 46.60it/s] 14%|█▍        | 63/438 [00:01<00:08, 46.65it/s] 16%|█▌        | 68/438 [00:01<00:07, 46.76it/s] 17%|█▋        | 73/438 [00:01<00:07, 46.78it/s] 18%|█▊        | 78/438 [00:01<00:07, 46.75it/s] 19%|█▉        | 83/438 [00:01<00:07, 46.82it/s] 20%|██        | 88/438 [00:01<00:07, 46.79it/s] 21%|██        | 93/438 [00:01<00:07, 46.63it/s] 22%|██▏       | 98/438 [00:02<00:07, 46.53it/s] 24%|██▎       | 103/438 [00:02<00:07, 46.58it/s] 25%|██▍       | 108/438 [00:02<00:07, 46.60it/s] 26%|██▌       | 113/438 [00:02<00:06, 46.60it/s] 27%|██▋       | 118/438 [00:02<00:06, 46.72it/s] 28%|██▊       | 123/438 [00:02<00:06, 46.72it/s] 29%|██▉       | 128/438 [00:02<00:06, 46.65it/s] 30%|███       | 133/438 [00:02<00:06, 46.62it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.04it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.11it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.22it/s] 35%|███▍      | 153/438 [00:03<00:06, 46.29it/s] 36%|███▌      | 158/438 [00:03<00:06, 46.42it/s] 37%|███▋      | 163/438 [00:03<00:05, 46.52it/s] 38%|███▊      | 168/438 [00:03<00:05, 46.53it/s] 39%|███▉      | 173/438 [00:03<00:05, 46.49it/s] 41%|████      | 178/438 [00:03<00:05, 46.49it/s] 42%|████▏     | 183/438 [00:03<00:05, 46.49it/s] 43%|████▎     | 188/438 [00:04<00:05, 46.55it/s] 44%|████▍     | 193/438 [00:04<00:05, 46.59it/s] 45%|████▌     | 198/438 [00:04<00:05, 46.45it/s] 46%|████▋     | 203/438 [00:04<00:05, 46.52it/s] 47%|████▋     | 208/438 [00:04<00:04, 46.56it/s] 49%|████▊     | 213/438 [00:04<00:04, 46.55it/s] 50%|████▉     | 218/438 [00:04<00:04, 46.58it/s] 51%|█████     | 223/438 [00:04<00:04, 46.53it/s] 52%|█████▏    | 228/438 [00:04<00:04, 46.50it/s] 53%|█████▎    | 233/438 [00:04<00:04, 46.48it/s] 54%|█████▍    | 238/438 [00:05<00:04, 46.49it/s] 55%|█████▌    | 243/438 [00:05<00:04, 46.52it/s] 57%|█████▋    | 248/438 [00:05<00:04, 46.51it/s] 58%|█████▊    | 253/438 [00:05<00:03, 46.42it/s] 59%|█████▉    | 258/438 [00:05<00:03, 46.47it/s] 60%|██████    | 263/438 [00:05<00:03, 46.36it/s] 61%|██████    | 268/438 [00:05<00:03, 45.00it/s] 62%|██████▏   | 273/438 [00:05<00:03, 45.53it/s] 63%|██████▎   | 278/438 [00:05<00:03, 45.88it/s] 65%|██████▍   | 283/438 [00:06<00:03, 46.10it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.25it/s] 67%|██████▋   | 293/438 [00:06<00:03, 46.31it/s] 68%|██████▊   | 298/438 [00:06<00:03, 46.26it/s] 69%|██████▉   | 303/438 [00:06<00:02, 46.43it/s] 70%|███████   | 308/438 [00:06<00:02, 46.35it/s] 71%|███████▏  | 313/438 [00:06<00:02, 46.36it/s] 73%|███████▎  | 318/438 [00:06<00:02, 46.30it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.42it/s] 75%|███████▍  | 328/438 [00:07<00:02, 46.45it/s] 76%|███████▌  | 333/438 [00:07<00:02, 46.55it/s] 77%|███████▋  | 338/438 [00:07<00:02, 46.55it/s] 78%|███████▊  | 343/438 [00:07<00:02, 46.55it/s] 79%|███████▉  | 348/438 [00:07<00:01, 46.41it/s] 81%|████████  | 353/438 [00:07<00:01, 46.43it/s] 82%|████████▏ | 358/438 [00:07<00:01, 46.39it/s] 83%|████████▎ | 363/438 [00:07<00:01, 46.40it/s] 84%|████████▍ | 368/438 [00:07<00:01, 46.52it/s] 85%|████████▌ | 373/438 [00:08<00:01, 46.48it/s] 86%|████████▋ | 378/438 [00:08<00:01, 46.54it/s] 87%|████████▋ | 383/438 [00:08<00:01, 46.60it/s] 89%|████████▊ | 388/438 [00:08<00:01, 46.52it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.44it/s] 91%|█████████ | 398/438 [00:08<00:00, 46.47it/s] 92%|█████████▏| 403/438 [00:08<00:00, 46.38it/s] 93%|█████████▎| 408/438 [00:08<00:00, 46.36it/s] 94%|█████████▍| 413/438 [00:08<00:00, 46.33it/s] 95%|█████████▌| 418/438 [00:08<00:00, 46.46it/s] 97%|█████████▋| 423/438 [00:09<00:00, 46.49it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.51it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.45it/s]100%|██████████| 438/438 [00:09<00:00, 46.51it/s]100%|██████████| 438/438 [00:09<00:00, 46.59it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:25:17,853 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:17,853 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:17,853 >>   eval_loss               =     1.0054
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:17,853 >>   eval_runtime            = 0:00:09.42
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:17,853 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:17,853 >>   eval_samples_per_second =    370.983
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:17,853 >>   eval_steps_per_second   =     46.466
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:25:17,853 >>   perplexity              =      2.733
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:24,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:24,720 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:24,720 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:24,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:24,721 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:25:25,318 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:25:25,319 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:25:25,931 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:25:27,006 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:25:27,006 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:29,949 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:29,954 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:29,955 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:29,955 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:25:29,955 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:25:30,589 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:25:30,590 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:25:31,163 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:25:31,318 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:25:31,318 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-160
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-64
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-96
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-32
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-128
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.28it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:02,  1.33it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.28it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:06,  1.26it/s]Extractor Predicting: 10it [00:07,  1.26it/s]Extractor Predicting: 11it [00:08,  1.25it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.28it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:13,  1.30it/s]Extractor Predicting: 19it [00:14,  1.28it/s]Extractor Predicting: 20it [00:15,  1.26it/s]Extractor Predicting: 21it [00:16,  1.26it/s]Extractor Predicting: 22it [00:17,  1.24it/s]Extractor Predicting: 23it [00:17,  1.26it/s]Extractor Predicting: 24it [00:18,  1.30it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.33it/s]Extractor Predicting: 27it [00:20,  1.34it/s]Extractor Predicting: 28it [00:21,  1.34it/s]Extractor Predicting: 29it [00:22,  1.33it/s]Extractor Predicting: 30it [00:23,  1.34it/s]Extractor Predicting: 31it [00:23,  1.32it/s]Extractor Predicting: 32it [00:24,  1.30it/s]Extractor Predicting: 33it [00:25,  1.32it/s]Extractor Predicting: 34it [00:26,  1.29it/s]Extractor Predicting: 35it [00:27,  1.30it/s]Extractor Predicting: 36it [00:27,  1.27it/s]Extractor Predicting: 37it [00:28,  1.27it/s]Extractor Predicting: 38it [00:29,  1.24it/s]Extractor Predicting: 39it [00:30,  1.26it/s]Extractor Predicting: 40it [00:31,  1.25it/s]Extractor Predicting: 41it [00:31,  1.23it/s]Extractor Predicting: 42it [00:32,  1.23it/s]Extractor Predicting: 43it [00:33,  1.27it/s]Extractor Predicting: 44it [00:34,  1.26it/s]Extractor Predicting: 45it [00:35,  1.26it/s]Extractor Predicting: 46it [00:35,  1.29it/s]Extractor Predicting: 47it [00:36,  1.28it/s]Extractor Predicting: 48it [00:37,  1.25it/s]Extractor Predicting: 49it [00:38,  1.23it/s]Extractor Predicting: 50it [00:39,  1.21it/s]Extractor Predicting: 51it [00:39,  1.21it/s]Extractor Predicting: 52it [00:40,  1.19it/s]Extractor Predicting: 53it [00:41,  1.21it/s]Extractor Predicting: 54it [00:42,  1.22it/s]Extractor Predicting: 55it [00:43,  1.22it/s]Extractor Predicting: 56it [00:44,  1.23it/s]Extractor Predicting: 57it [00:44,  1.23it/s]Extractor Predicting: 58it [00:45,  1.23it/s]Extractor Predicting: 59it [00:46,  1.24it/s]Extractor Predicting: 60it [00:47,  1.21it/s]Extractor Predicting: 61it [00:48,  1.19it/s]Extractor Predicting: 62it [00:49,  1.18it/s]Extractor Predicting: 63it [00:49,  1.18it/s]Extractor Predicting: 64it [00:50,  1.18it/s]Extractor Predicting: 65it [00:51,  1.17it/s]Extractor Predicting: 66it [00:52,  1.16it/s]Extractor Predicting: 67it [00:53,  1.15it/s]Extractor Predicting: 68it [00:54,  1.15it/s]Extractor Predicting: 69it [00:55,  1.14it/s]Extractor Predicting: 70it [00:56,  1.15it/s]Extractor Predicting: 71it [00:56,  1.14it/s]Extractor Predicting: 72it [00:57,  1.14it/s]Extractor Predicting: 73it [00:58,  1.13it/s]Extractor Predicting: 74it [00:59,  1.13it/s]Extractor Predicting: 75it [01:00,  1.06it/s]Extractor Predicting: 76it [01:01,  1.10it/s]Extractor Predicting: 77it [01:02,  1.11it/s]Extractor Predicting: 78it [01:03,  1.11it/s]Extractor Predicting: 79it [01:04,  1.14it/s]Extractor Predicting: 80it [01:05,  1.13it/s]Extractor Predicting: 81it [01:05,  1.11it/s]Extractor Predicting: 82it [01:06,  1.12it/s]Extractor Predicting: 83it [01:07,  1.15it/s]Extractor Predicting: 84it [01:08,  1.16it/s]Extractor Predicting: 85it [01:09,  1.16it/s]Extractor Predicting: 86it [01:10,  1.17it/s]Extractor Predicting: 87it [01:11,  1.13it/s]Extractor Predicting: 88it [01:11,  1.15it/s]Extractor Predicting: 89it [01:12,  1.17it/s]Extractor Predicting: 90it [01:13,  1.19it/s]Extractor Predicting: 91it [01:14,  1.20it/s]Extractor Predicting: 92it [01:15,  1.18it/s]Extractor Predicting: 93it [01:16,  1.21it/s]Extractor Predicting: 94it [01:16,  1.21it/s]Extractor Predicting: 95it [01:17,  1.21it/s]Extractor Predicting: 96it [01:18,  1.20it/s]Extractor Predicting: 97it [01:19,  1.21it/s]Extractor Predicting: 98it [01:20,  1.19it/s]Extractor Predicting: 99it [01:21,  1.20it/s]Extractor Predicting: 100it [01:21,  1.22it/s]Extractor Predicting: 101it [01:22,  1.22it/s]Extractor Predicting: 102it [01:23,  1.19it/s]Extractor Predicting: 103it [01:24,  1.16it/s]Extractor Predicting: 104it [01:25,  1.17it/s]Extractor Predicting: 105it [01:26,  1.18it/s]Extractor Predicting: 106it [01:27,  1.18it/s]Extractor Predicting: 107it [01:27,  1.17it/s]Extractor Predicting: 108it [01:28,  1.18it/s]Extractor Predicting: 109it [01:29,  1.18it/s]Extractor Predicting: 110it [01:30,  1.16it/s]Extractor Predicting: 111it [01:31,  1.16it/s]Extractor Predicting: 112it [01:32,  1.17it/s]Extractor Predicting: 113it [01:33,  1.17it/s]Extractor Predicting: 114it [01:33,  1.17it/s]Extractor Predicting: 115it [01:34,  1.18it/s]Extractor Predicting: 116it [01:35,  1.17it/s]Extractor Predicting: 117it [01:36,  1.19it/s]Extractor Predicting: 118it [01:37,  1.21it/s]Extractor Predicting: 119it [01:37,  1.25it/s]Extractor Predicting: 120it [01:38,  1.27it/s]Extractor Predicting: 121it [01:39,  1.24it/s]Extractor Predicting: 122it [01:40,  1.29it/s]Extractor Predicting: 123it [01:40,  1.30it/s]Extractor Predicting: 124it [01:41,  1.29it/s]Extractor Predicting: 125it [01:42,  1.30it/s]Extractor Predicting: 126it [01:43,  1.32it/s]Extractor Predicting: 127it [01:43,  1.35it/s]Extractor Predicting: 128it [01:44,  1.34it/s]Extractor Predicting: 129it [01:45,  1.36it/s]Extractor Predicting: 130it [01:46,  1.31it/s]Extractor Predicting: 131it [01:47,  1.31it/s]Extractor Predicting: 132it [01:47,  1.36it/s]Extractor Predicting: 133it [01:48,  1.37it/s]Extractor Predicting: 134it [01:49,  1.31it/s]Extractor Predicting: 135it [01:49,  1.32it/s]Extractor Predicting: 136it [01:50,  1.35it/s]Extractor Predicting: 137it [01:51,  1.36it/s]Extractor Predicting: 138it [01:52,  1.35it/s]Extractor Predicting: 139it [01:52,  1.35it/s]Extractor Predicting: 140it [01:53,  1.38it/s]Extractor Predicting: 141it [01:54,  1.39it/s]Extractor Predicting: 142it [01:54,  1.41it/s]Extractor Predicting: 143it [01:55,  1.39it/s]Extractor Predicting: 144it [01:56,  1.36it/s]Extractor Predicting: 145it [01:56,  1.52it/s]Extractor Predicting: 145it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:37,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:37,724 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:37,724 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:37,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:37,725 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:27:38,344 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:27:38,345 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:27:38,925 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:27:39,960 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:27:39,960 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:42,842 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:42,847 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:42,847 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:42,847 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:27:42,847 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:27:43,477 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:27:43,478 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:27:44,027 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:27:44,182 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:27:44,182 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5135013501350135,
  "recall": 0.32627966828710325,
  "score": 0.39902080783353727,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.25it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 6it [00:04,  1.20it/s]Extractor Predicting: 7it [00:05,  1.22it/s]Extractor Predicting: 8it [00:06,  1.22it/s]Extractor Predicting: 9it [00:07,  1.19it/s]Extractor Predicting: 10it [00:08,  1.20it/s]Extractor Predicting: 11it [00:09,  1.22it/s]Extractor Predicting: 12it [00:09,  1.24it/s]Extractor Predicting: 13it [00:10,  1.22it/s]Extractor Predicting: 14it [00:11,  1.23it/s]Extractor Predicting: 15it [00:12,  1.25it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.23it/s]Extractor Predicting: 18it [00:14,  1.23it/s]Extractor Predicting: 19it [00:15,  1.21it/s]Extractor Predicting: 20it [00:16,  1.21it/s]Extractor Predicting: 21it [00:17,  1.22it/s]Extractor Predicting: 22it [00:18,  1.18it/s]Extractor Predicting: 23it [00:18,  1.20it/s]Extractor Predicting: 24it [00:19,  1.23it/s]Extractor Predicting: 25it [00:20,  1.24it/s]Extractor Predicting: 26it [00:21,  1.22it/s]Extractor Predicting: 27it [00:22,  1.21it/s]Extractor Predicting: 28it [00:22,  1.20it/s]Extractor Predicting: 29it [00:23,  1.19it/s]Extractor Predicting: 30it [00:24,  1.17it/s]Extractor Predicting: 31it [00:25,  1.17it/s]Extractor Predicting: 32it [00:26,  1.17it/s]Extractor Predicting: 33it [00:27,  1.19it/s]Extractor Predicting: 34it [00:28,  1.21it/s]Extractor Predicting: 35it [00:28,  1.21it/s]Extractor Predicting: 36it [00:29,  1.23it/s]Extractor Predicting: 37it [00:30,  1.27it/s]Extractor Predicting: 38it [00:31,  1.26it/s]Extractor Predicting: 39it [00:31,  1.25it/s]Extractor Predicting: 40it [00:32,  1.26it/s]Extractor Predicting: 41it [00:33,  1.25it/s]Extractor Predicting: 42it [00:34,  1.25it/s]Extractor Predicting: 43it [00:35,  1.24it/s]Extractor Predicting: 44it [00:35,  1.24it/s]Extractor Predicting: 45it [00:36,  1.22it/s]Extractor Predicting: 46it [00:37,  1.25it/s]Extractor Predicting: 47it [00:38,  1.25it/s]Extractor Predicting: 48it [00:39,  1.25it/s]Extractor Predicting: 49it [00:40,  1.24it/s]Extractor Predicting: 50it [00:40,  1.23it/s]Extractor Predicting: 51it [00:41,  1.23it/s]Extractor Predicting: 52it [00:42,  1.24it/s]Extractor Predicting: 53it [00:43,  1.24it/s]Extractor Predicting: 54it [00:44,  1.25it/s]Extractor Predicting: 55it [00:44,  1.22it/s]Extractor Predicting: 56it [00:45,  1.23it/s]Extractor Predicting: 57it [00:46,  1.25it/s]Extractor Predicting: 58it [00:47,  1.27it/s]Extractor Predicting: 59it [00:48,  1.28it/s]Extractor Predicting: 60it [00:48,  1.28it/s]Extractor Predicting: 61it [00:49,  1.26it/s]Extractor Predicting: 62it [00:50,  1.27it/s]Extractor Predicting: 63it [00:51,  1.25it/s]Extractor Predicting: 64it [00:52,  1.25it/s]Extractor Predicting: 65it [00:53,  1.15it/s]Extractor Predicting: 66it [00:53,  1.16it/s]Extractor Predicting: 67it [00:54,  1.19it/s]Extractor Predicting: 68it [00:55,  1.24it/s]Extractor Predicting: 69it [00:56,  1.26it/s]Extractor Predicting: 70it [00:56,  1.25it/s]Extractor Predicting: 71it [00:57,  1.24it/s]Extractor Predicting: 72it [00:58,  1.26it/s]Extractor Predicting: 73it [00:59,  1.22it/s]Extractor Predicting: 74it [01:00,  1.23it/s]Extractor Predicting: 75it [01:01,  1.24it/s]Extractor Predicting: 76it [01:01,  1.25it/s]Extractor Predicting: 77it [01:02,  1.22it/s]Extractor Predicting: 78it [01:03,  1.21it/s]Extractor Predicting: 79it [01:04,  1.22it/s]Extractor Predicting: 80it [01:05,  1.21it/s]Extractor Predicting: 81it [01:05,  1.23it/s]Extractor Predicting: 82it [01:06,  1.21it/s]Extractor Predicting: 83it [01:07,  1.21it/s]Extractor Predicting: 84it [01:08,  1.22it/s]Extractor Predicting: 85it [01:09,  1.23it/s]Extractor Predicting: 86it [01:10,  1.21it/s]Extractor Predicting: 87it [01:10,  1.21it/s]Extractor Predicting: 88it [01:11,  1.21it/s]Extractor Predicting: 89it [01:12,  1.22it/s]Extractor Predicting: 90it [01:13,  1.24it/s]Extractor Predicting: 91it [01:14,  1.23it/s]Extractor Predicting: 92it [01:15,  1.22it/s]Extractor Predicting: 93it [01:15,  1.20it/s]Extractor Predicting: 94it [01:16,  1.22it/s]Extractor Predicting: 95it [01:17,  1.22it/s]Extractor Predicting: 96it [01:18,  1.21it/s]Extractor Predicting: 97it [01:19,  1.23it/s]Extractor Predicting: 98it [01:19,  1.23it/s]Extractor Predicting: 99it [01:20,  1.24it/s]Extractor Predicting: 100it [01:21,  1.24it/s]Extractor Predicting: 101it [01:22,  1.26it/s]Extractor Predicting: 102it [01:23,  1.26it/s]Extractor Predicting: 103it [01:23,  1.25it/s]Extractor Predicting: 104it [01:24,  1.22it/s]Extractor Predicting: 105it [01:25,  1.20it/s]Extractor Predicting: 106it [01:26,  1.21it/s]Extractor Predicting: 107it [01:27,  1.21it/s]Extractor Predicting: 108it [01:28,  1.22it/s]Extractor Predicting: 109it [01:28,  1.21it/s]Extractor Predicting: 110it [01:29,  1.22it/s]Extractor Predicting: 111it [01:30,  1.23it/s]Extractor Predicting: 112it [01:31,  1.20it/s]Extractor Predicting: 113it [01:32,  1.20it/s]Extractor Predicting: 114it [01:33,  1.19it/s]Extractor Predicting: 115it [01:33,  1.17it/s]Extractor Predicting: 116it [01:34,  1.16it/s]Extractor Predicting: 117it [01:35,  1.17it/s]Extractor Predicting: 118it [01:36,  1.18it/s]Extractor Predicting: 119it [01:37,  1.19it/s]Extractor Predicting: 120it [01:38,  1.20it/s]Extractor Predicting: 121it [01:38,  1.22it/s]Extractor Predicting: 122it [01:39,  1.23it/s]Extractor Predicting: 123it [01:40,  1.23it/s]Extractor Predicting: 124it [01:41,  1.23it/s]Extractor Predicting: 125it [01:42,  1.22it/s]Extractor Predicting: 126it [01:43,  1.20it/s]Extractor Predicting: 127it [01:43,  1.20it/s]Extractor Predicting: 128it [01:44,  1.20it/s]Extractor Predicting: 129it [01:45,  1.22it/s]Extractor Predicting: 130it [01:46,  1.22it/s]Extractor Predicting: 131it [01:47,  1.23it/s]Extractor Predicting: 132it [01:47,  1.25it/s]Extractor Predicting: 133it [01:48,  1.22it/s]Extractor Predicting: 134it [01:49,  1.22it/s]Extractor Predicting: 135it [01:50,  1.21it/s]Extractor Predicting: 136it [01:51,  1.21it/s]Extractor Predicting: 137it [01:52,  1.20it/s]Extractor Predicting: 138it [01:52,  1.24it/s]Extractor Predicting: 139it [01:53,  1.23it/s]Extractor Predicting: 140it [01:54,  1.20it/s]Extractor Predicting: 141it [01:55,  1.21it/s]Extractor Predicting: 142it [01:56,  1.22it/s]Extractor Predicting: 143it [01:56,  1.22it/s]Extractor Predicting: 144it [01:57,  1.20it/s]Extractor Predicting: 145it [01:58,  1.22it/s]Extractor Predicting: 146it [01:59,  1.22it/s]Extractor Predicting: 147it [02:00,  1.25it/s]Extractor Predicting: 148it [02:01,  1.22it/s]Extractor Predicting: 149it [02:01,  1.23it/s]Extractor Predicting: 150it [02:02,  1.24it/s]Extractor Predicting: 151it [02:03,  1.26it/s]Extractor Predicting: 152it [02:04,  1.27it/s]Extractor Predicting: 153it [02:04,  1.27it/s]Extractor Predicting: 154it [02:05,  1.28it/s]Extractor Predicting: 155it [02:06,  1.27it/s]Extractor Predicting: 156it [02:07,  1.27it/s]Extractor Predicting: 157it [02:08,  1.14it/s]Extractor Predicting: 158it [02:09,  1.15it/s]Extractor Predicting: 159it [02:10,  1.18it/s]Extractor Predicting: 160it [02:10,  1.22it/s]Extractor Predicting: 161it [02:11,  1.23it/s]Extractor Predicting: 162it [02:12,  1.25it/s]Extractor Predicting: 163it [02:13,  1.27it/s]Extractor Predicting: 164it [02:13,  1.27it/s]Extractor Predicting: 165it [02:14,  1.25it/s]Extractor Predicting: 166it [02:15,  1.25it/s]Extractor Predicting: 167it [02:16,  1.25it/s]Extractor Predicting: 168it [02:17,  1.25it/s]Extractor Predicting: 169it [02:17,  1.28it/s]Extractor Predicting: 170it [02:18,  1.29it/s]Extractor Predicting: 171it [02:19,  1.26it/s]Extractor Predicting: 172it [02:20,  1.24it/s]Extractor Predicting: 173it [02:21,  1.25it/s]Extractor Predicting: 174it [02:21,  1.27it/s]Extractor Predicting: 175it [02:22,  1.28it/s]Extractor Predicting: 176it [02:23,  1.29it/s]Extractor Predicting: 177it [02:24,  1.24it/s]Extractor Predicting: 178it [02:25,  1.26it/s]Extractor Predicting: 179it [02:25,  1.25it/s]Extractor Predicting: 180it [02:26,  1.25it/s]Extractor Predicting: 181it [02:27,  1.25it/s]Extractor Predicting: 182it [02:28,  1.24it/s]Extractor Predicting: 183it [02:29,  1.25it/s]Extractor Predicting: 184it [02:29,  1.26it/s]Extractor Predicting: 185it [02:30,  1.27it/s]Extractor Predicting: 186it [02:31,  1.29it/s]Extractor Predicting: 187it [02:32,  1.32it/s]Extractor Predicting: 188it [02:32,  1.30it/s]Extractor Predicting: 189it [02:33,  1.26it/s]Extractor Predicting: 190it [02:34,  1.25it/s]Extractor Predicting: 191it [02:35,  1.24it/s]Extractor Predicting: 192it [02:36,  1.25it/s]Extractor Predicting: 193it [02:36,  1.26it/s]Extractor Predicting: 194it [02:37,  1.27it/s]Extractor Predicting: 195it [02:38,  1.28it/s]Extractor Predicting: 196it [02:39,  1.28it/s]Extractor Predicting: 197it [02:40,  1.25it/s]Extractor Predicting: 198it [02:40,  1.25it/s]Extractor Predicting: 199it [02:41,  1.24it/s]Extractor Predicting: 200it [02:42,  1.24it/s]Extractor Predicting: 201it [02:43,  1.25it/s]Extractor Predicting: 202it [02:44,  1.24it/s]Extractor Predicting: 203it [02:44,  1.24it/s]Extractor Predicting: 204it [02:45,  1.22it/s]Extractor Predicting: 205it [02:46,  1.20it/s]Extractor Predicting: 206it [02:47,  1.23it/s]Extractor Predicting: 207it [02:48,  1.22it/s]Extractor Predicting: 208it [02:49,  1.25it/s]Extractor Predicting: 209it [02:49,  1.26it/s]Extractor Predicting: 210it [02:50,  1.25it/s]Extractor Predicting: 211it [02:51,  1.24it/s]Extractor Predicting: 212it [02:52,  1.23it/s]Extractor Predicting: 213it [02:53,  1.22it/s]Extractor Predicting: 214it [02:53,  1.22it/s]Extractor Predicting: 215it [02:54,  1.21it/s]Extractor Predicting: 216it [02:55,  1.22it/s]Extractor Predicting: 217it [02:56,  1.20it/s]Extractor Predicting: 218it [02:57,  1.21it/s]Extractor Predicting: 219it [02:58,  1.21it/s]Extractor Predicting: 220it [02:58,  1.23it/s]Extractor Predicting: 221it [02:59,  1.25it/s]Extractor Predicting: 222it [03:00,  1.21it/s]Extractor Predicting: 223it [03:01,  1.19it/s]Extractor Predicting: 224it [03:02,  1.22it/s]Extractor Predicting: 225it [03:03,  1.21it/s]Extractor Predicting: 226it [03:03,  1.20it/s]Extractor Predicting: 227it [03:04,  1.21it/s]Extractor Predicting: 228it [03:05,  1.22it/s]Extractor Predicting: 229it [03:06,  1.24it/s]Extractor Predicting: 230it [03:07,  1.21it/s]Extractor Predicting: 231it [03:07,  1.22it/s]Extractor Predicting: 232it [03:08,  1.24it/s]Extractor Predicting: 233it [03:09,  1.22it/s]Extractor Predicting: 234it [03:10,  1.21it/s]Extractor Predicting: 235it [03:11,  1.22it/s]Extractor Predicting: 236it [03:12,  1.22it/s]Extractor Predicting: 237it [03:12,  1.20it/s]Extractor Predicting: 238it [03:13,  1.21it/s]Extractor Predicting: 239it [03:14,  1.21it/s]Extractor Predicting: 240it [03:15,  1.20it/s]Extractor Predicting: 241it [03:16,  1.22it/s]Extractor Predicting: 242it [03:16,  1.23it/s]Extractor Predicting: 243it [03:17,  1.21it/s]Extractor Predicting: 244it [03:18,  1.23it/s]Extractor Predicting: 245it [03:19,  1.21it/s]Extractor Predicting: 246it [03:20,  1.23it/s]Extractor Predicting: 247it [03:21,  1.22it/s]Extractor Predicting: 248it [03:21,  1.22it/s]Extractor Predicting: 249it [03:22,  1.23it/s]Extractor Predicting: 250it [03:23,  1.22it/s]Extractor Predicting: 251it [03:24,  1.23it/s]Extractor Predicting: 252it [03:25,  1.22it/s]Extractor Predicting: 253it [03:25,  1.23it/s]Extractor Predicting: 254it [03:26,  1.23it/s]Extractor Predicting: 255it [03:27,  1.23it/s]Extractor Predicting: 256it [03:28,  1.23it/s]Extractor Predicting: 257it [03:29,  1.25it/s]Extractor Predicting: 258it [03:29,  1.25it/s]Extractor Predicting: 259it [03:30,  1.27it/s]Extractor Predicting: 260it [03:31,  1.25it/s]Extractor Predicting: 261it [03:32,  1.26it/s]Extractor Predicting: 262it [03:33,  1.25it/s]Extractor Predicting: 263it [03:34,  1.15it/s]Extractor Predicting: 264it [03:35,  1.15it/s]Extractor Predicting: 265it [03:35,  1.17it/s]Extractor Predicting: 266it [03:36,  1.19it/s]Extractor Predicting: 267it [03:37,  1.21it/s]Extractor Predicting: 268it [03:38,  1.23it/s]Extractor Predicting: 269it [03:39,  1.24it/s]Extractor Predicting: 270it [03:39,  1.23it/s]Extractor Predicting: 271it [03:40,  1.24it/s]Extractor Predicting: 272it [03:41,  1.27it/s]Extractor Predicting: 273it [03:42,  1.28it/s]Extractor Predicting: 274it [03:42,  1.28it/s]Extractor Predicting: 275it [03:43,  1.26it/s]Extractor Predicting: 276it [03:44,  1.24it/s]Extractor Predicting: 277it [03:45,  1.26it/s]Extractor Predicting: 278it [03:46,  1.24it/s]Extractor Predicting: 279it [03:47,  1.25it/s]Extractor Predicting: 280it [03:47,  1.23it/s]Extractor Predicting: 281it [03:48,  1.24it/s]Extractor Predicting: 282it [03:49,  1.23it/s]Extractor Predicting: 283it [03:50,  1.23it/s]Extractor Predicting: 284it [03:51,  1.23it/s]Extractor Predicting: 285it [03:51,  1.23it/s]Extractor Predicting: 286it [03:52,  1.21it/s]Extractor Predicting: 287it [03:53,  1.23it/s]Extractor Predicting: 288it [03:54,  1.23it/s]Extractor Predicting: 289it [03:55,  1.24it/s]Extractor Predicting: 290it [03:55,  1.23it/s]Extractor Predicting: 291it [03:56,  1.23it/s]Extractor Predicting: 292it [03:57,  1.24it/s]Extractor Predicting: 293it [03:58,  1.24it/s]Extractor Predicting: 294it [03:59,  1.22it/s]Extractor Predicting: 295it [03:59,  1.25it/s]Extractor Predicting: 296it [04:00,  1.25it/s]Extractor Predicting: 297it [04:01,  1.23it/s]Extractor Predicting: 298it [04:02,  1.25it/s]Extractor Predicting: 299it [04:03,  1.25it/s]Extractor Predicting: 300it [04:04,  1.24it/s]Extractor Predicting: 301it [04:04,  1.28it/s]Extractor Predicting: 302it [04:05,  1.27it/s]Extractor Predicting: 303it [04:06,  1.30it/s]Extractor Predicting: 304it [04:07,  1.28it/s]Extractor Predicting: 305it [04:07,  1.26it/s]Extractor Predicting: 306it [04:08,  1.26it/s]Extractor Predicting: 307it [04:09,  1.24it/s]Extractor Predicting: 308it [04:10,  1.23it/s]Extractor Predicting: 309it [04:11,  1.24it/s]Extractor Predicting: 310it [04:12,  1.22it/s]Extractor Predicting: 311it [04:12,  1.21it/s]Extractor Predicting: 312it [04:13,  1.18it/s]Extractor Predicting: 313it [04:14,  1.20it/s]Extractor Predicting: 314it [04:15,  1.20it/s]Extractor Predicting: 315it [04:16,  1.23it/s]Extractor Predicting: 316it [04:16,  1.24it/s]Extractor Predicting: 317it [04:17,  1.23it/s]Extractor Predicting: 318it [04:18,  1.23it/s]Extractor Predicting: 319it [04:19,  1.23it/s]Extractor Predicting: 320it [04:20,  1.22it/s]Extractor Predicting: 321it [04:21,  1.23it/s]Extractor Predicting: 322it [04:21,  1.22it/s]Extractor Predicting: 323it [04:22,  1.26it/s]Extractor Predicting: 324it [04:23,  1.23it/s]Extractor Predicting: 325it [04:24,  1.24it/s]Extractor Predicting: 326it [04:25,  1.23it/s]Extractor Predicting: 327it [04:25,  1.25it/s]Extractor Predicting: 328it [04:26,  1.26it/s]Extractor Predicting: 329it [04:27,  1.24it/s]Extractor Predicting: 330it [04:28,  1.22it/s]Extractor Predicting: 331it [04:29,  1.22it/s]Extractor Predicting: 332it [04:29,  1.21it/s]Extractor Predicting: 333it [04:30,  1.22it/s]Extractor Predicting: 334it [04:31,  1.21it/s]Extractor Predicting: 335it [04:32,  1.22it/s]Extractor Predicting: 336it [04:33,  1.25it/s]Extractor Predicting: 337it [04:33,  1.26it/s]Extractor Predicting: 338it [04:34,  1.25it/s]Extractor Predicting: 339it [04:35,  1.23it/s]Extractor Predicting: 340it [04:36,  1.23it/s]Extractor Predicting: 341it [04:37,  1.25it/s]Extractor Predicting: 342it [04:38,  1.22it/s]Extractor Predicting: 343it [04:38,  1.23it/s]Extractor Predicting: 344it [04:39,  1.24it/s]Extractor Predicting: 345it [04:40,  1.25it/s]Extractor Predicting: 346it [04:41,  1.25it/s]Extractor Predicting: 347it [04:42,  1.24it/s]Extractor Predicting: 348it [04:42,  1.24it/s]Extractor Predicting: 349it [04:43,  1.22it/s]Extractor Predicting: 350it [04:44,  1.22it/s]Extractor Predicting: 351it [04:45,  1.10it/s]Extractor Predicting: 352it [04:46,  1.12it/s]Extractor Predicting: 353it [04:47,  1.14it/s]Extractor Predicting: 354it [04:48,  1.16it/s]Extractor Predicting: 355it [04:48,  1.19it/s]Extractor Predicting: 356it [04:49,  1.20it/s]Extractor Predicting: 357it [04:50,  1.22it/s]Extractor Predicting: 358it [04:51,  1.24it/s]Extractor Predicting: 359it [04:52,  1.23it/s]Extractor Predicting: 360it [04:52,  1.23it/s]Extractor Predicting: 361it [04:53,  1.24it/s]Extractor Predicting: 362it [04:54,  1.24it/s]Extractor Predicting: 363it [04:55,  1.26it/s]Extractor Predicting: 364it [04:56,  1.26it/s]Extractor Predicting: 365it [04:56,  1.25it/s]Extractor Predicting: 366it [04:57,  1.25it/s]Extractor Predicting: 367it [04:58,  1.23it/s]Extractor Predicting: 368it [04:59,  1.26it/s]Extractor Predicting: 369it [05:00,  1.26it/s]Extractor Predicting: 370it [05:00,  1.28it/s]Extractor Predicting: 371it [05:01,  1.27it/s]Extractor Predicting: 372it [05:02,  1.27it/s]Extractor Predicting: 373it [05:03,  1.26it/s]Extractor Predicting: 374it [05:04,  1.25it/s]Extractor Predicting: 375it [05:04,  1.26it/s]Extractor Predicting: 376it [05:05,  1.26it/s]Extractor Predicting: 377it [05:06,  1.26it/s]Extractor Predicting: 378it [05:07,  1.27it/s]Extractor Predicting: 379it [05:08,  1.23it/s]Extractor Predicting: 380it [05:08,  1.24it/s]Extractor Predicting: 381it [05:09,  1.27it/s]Extractor Predicting: 382it [05:10,  1.27it/s]Extractor Predicting: 383it [05:11,  1.23it/s]Extractor Predicting: 384it [05:12,  1.24it/s]Extractor Predicting: 385it [05:12,  1.25it/s]Extractor Predicting: 386it [05:13,  1.28it/s]Extractor Predicting: 387it [05:14,  1.29it/s]Extractor Predicting: 388it [05:15,  1.29it/s]Extractor Predicting: 389it [05:16,  1.26it/s]Extractor Predicting: 390it [05:16,  1.27it/s]Extractor Predicting: 391it [05:17,  1.27it/s]Extractor Predicting: 392it [05:18,  1.28it/s]Extractor Predicting: 393it [05:19,  1.29it/s]Extractor Predicting: 394it [05:19,  1.31it/s]Extractor Predicting: 395it [05:20,  1.28it/s]Extractor Predicting: 396it [05:21,  1.27it/s]Extractor Predicting: 397it [05:22,  1.28it/s]Extractor Predicting: 398it [05:22,  1.29it/s]Extractor Predicting: 399it [05:23,  1.30it/s]Extractor Predicting: 400it [05:24,  1.33it/s]Extractor Predicting: 401it [05:25,  1.30it/s]Extractor Predicting: 402it [05:26,  1.30it/s]Extractor Predicting: 403it [05:26,  1.29it/s]Extractor Predicting: 404it [05:27,  1.28it/s]Extractor Predicting: 405it [05:28,  1.28it/s]Extractor Predicting: 406it [05:29,  1.27it/s]Extractor Predicting: 407it [05:30,  1.26it/s]Extractor Predicting: 408it [05:30,  1.27it/s]Extractor Predicting: 409it [05:31,  1.28it/s]Extractor Predicting: 409it [05:31,  1.23it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:25,356 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:25,361 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:25,362 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:25,362 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:25,362 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:33:25,982 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:33:25,983 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:33:26,552 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:33:27,620 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:33:27,620 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:28,979 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:28,981 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:28,981 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:28,981 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:33:28,981 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:33:29,330 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:33:29,331 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:33:29,594 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:33:29,748 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:33:29,748 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.33576017130620983,
  "recall": 0.23968205441760929,
  "score": 0.2797003210845523,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.11it/s]Extractor Predicting: 2it [00:01,  1.12it/s]Extractor Predicting: 3it [00:02,  1.13it/s]Extractor Predicting: 4it [00:03,  1.17it/s]Extractor Predicting: 5it [00:04,  1.17it/s]Extractor Predicting: 6it [00:05,  1.18it/s]Extractor Predicting: 7it [00:06,  1.17it/s]Extractor Predicting: 8it [00:06,  1.19it/s]Extractor Predicting: 9it [00:07,  1.23it/s]Extractor Predicting: 10it [00:08,  1.21it/s]Extractor Predicting: 11it [00:09,  1.23it/s]Extractor Predicting: 12it [00:10,  1.20it/s]Extractor Predicting: 13it [00:10,  1.23it/s]Extractor Predicting: 13it [00:10,  1.19it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:33:41,187 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:33:41,188 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:33:41,192 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:33:41,192 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:33:41,195 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:33:44,464 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:33:44,464 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:33:44,482 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:33:44,483 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:33:44,489 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:44,497 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:44,498 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:44,498 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:44,498 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:44,498 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:33:44,498 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2073170731707317,
  "recall": 0.049490538573508006,
  "score": 0.0799059929494712,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:33:44,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:45,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:46,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:48,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:49,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:50,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:51,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:52,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:53,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:54,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:55,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:56,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:57,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:58,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:33:59,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:00,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:01,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:02,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:03,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:04,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:05,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:21<06:49, 21.54s/it][WARNING|generation_utils.py:914] 2023-08-29 00:34:06,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:07,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:08,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:09,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:09,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:10,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:11,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:12,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:13,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:14,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:15,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:16,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:17,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:17,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:18,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:19,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:20,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:21,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:22,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:23,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:24,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:40<06:04, 20.25s/it][WARNING|generation_utils.py:914] 2023-08-29 00:34:25,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:26,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:27,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:28,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:29,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:31,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:32,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:33,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:34,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:35,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:36,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:37,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:37,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:38,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:39,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:40,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:41,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:42,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:43,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:45,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:46,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [01:02<05:53, 20.78s/it][WARNING|generation_utils.py:914] 2023-08-29 00:34:47,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:48,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:49,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:49,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:50,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:51,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:52,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:53,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:54,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:55,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:56,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:57,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:58,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:58,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:34:59,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:00,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:01,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:02,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:03,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:04,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:05,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:06,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:22<05:30, 20.63s/it][WARNING|generation_utils.py:914] 2023-08-29 00:35:07,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:08,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:09,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:09,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:10,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:11,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:13,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:14,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:15,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:16,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:17,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:18,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:19,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:20,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:20,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:21,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:22,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:23,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:24,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:25,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:26,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:27,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:28,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:28,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:29,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:30,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:31,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:32,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:33,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:34,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:50<05:49, 23.30s/it][WARNING|generation_utils.py:914] 2023-08-29 00:35:35,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:36,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:37,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:38,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:39,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:39,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:40,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:41,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:42,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:43,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:44,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:45,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:46,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:47,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:48,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:48,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:49,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:50,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:51,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:52,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:53,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:54,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:55,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:56,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [02:12<05:19, 22.80s/it][WARNING|generation_utils.py:914] 2023-08-29 00:35:57,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:58,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:35:59,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:00,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:01,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:02,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:03,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:04,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:05,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:06,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:07,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:08,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:09,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:10,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:11,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:12,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:13,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:13,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:15,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:16,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:17,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:18,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:34<04:51, 22.46s/it][WARNING|generation_utils.py:914] 2023-08-29 00:36:19,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:20,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:20,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:21,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:22,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:23,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:24,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:25,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:26,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:28,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:29,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:30,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:31,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:32,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:33,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:34,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:35,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:36,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:37,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:38,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:39,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:40,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:41,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:42,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:43,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:59<04:41, 23.45s/it][WARNING|generation_utils.py:914] 2023-08-29 00:36:44,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:45,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:46,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:47,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:48,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:49,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:50,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:51,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:52,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:53,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:53,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:54,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:55,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:56,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:57,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:58,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:36:59,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:00,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:01,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:02,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:03,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:03,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:04,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [03:20<04:09, 22.70s/it][WARNING|generation_utils.py:914] 2023-08-29 00:37:05,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:06,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:07,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:08,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:09,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:10,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:11,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:12,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:13,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:14,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:15,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:16,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:17,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:18,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:19,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:20,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:21,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:22,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:23,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:24,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:25,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [03:41<03:39, 21.99s/it][WARNING|generation_utils.py:914] 2023-08-29 00:37:26,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:26,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:27,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:28,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:29,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:30,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:31,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:32,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:33,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:34,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:35,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:36,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:36,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:37,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:38,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:39,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:40,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:41,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:42,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:43,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:44,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:45,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [04:01<03:13, 21.46s/it][WARNING|generation_utils.py:914] 2023-08-29 00:37:46,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:47,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:48,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:49,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:49,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:50,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:51,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:52,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:53,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:54,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:55,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:56,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:57,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:57,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:58,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:37:59,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:00,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:01,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:02,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:03,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:04,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:05,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:06,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:07,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [04:23<02:53, 21.67s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:08,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:09,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:10,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:11,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:12,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:13,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:14,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:15,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:15,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:16,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:17,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:18,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:19,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:20,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:20,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:21,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:22,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:23,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:23,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:24,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:25,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [04:41<02:24, 20.59s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:26,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:27,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:28,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:29,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:30,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:31,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:32,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:33,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:33,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:35,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:36,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:36,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:37,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:38,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:39,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:40,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:41,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:42,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:43,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:44,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:45,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [05:01<02:01, 20.21s/it][WARNING|generation_utils.py:914] 2023-08-29 00:38:45,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:46,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:47,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:48,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:49,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:50,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:51,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:52,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:53,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:54,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:54,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:56,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:56,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:57,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:58,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:38:59,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:00,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:01,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:02,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:03,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:04,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:05,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [05:21<01:41, 20.24s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:06,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:07,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:08,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:08,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:09,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:10,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:11,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:12,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:13,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:14,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:15,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:16,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:16,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:17,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:18,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:19,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:20,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:21,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:22,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:23,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:24,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:24,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:25,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [05:42<01:21, 20.33s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:26,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:27,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:28,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:29,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:30,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:31,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:32,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:32,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:33,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:34,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:35,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:36,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:37,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:37,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:38,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:39,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:40,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:41,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:42,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:43,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:43,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:44,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:45,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:46,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:47,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:48,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [06:04<01:02, 20.91s/it][WARNING|generation_utils.py:914] 2023-08-29 00:39:49,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:49,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:50,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:51,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:52,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:53,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:54,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:55,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:56,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:57,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:58,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:59,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:39:59,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:00,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:01,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:02,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:03,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:04,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:05,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:06,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:07,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [06:23<00:40, 20.32s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:07,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:08,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:09,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:10,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:11,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:12,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:13,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:14,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:15,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:16,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:17,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:18,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:19,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:20,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:21,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:22,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:23,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:24,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:25,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:26,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:27,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:28,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:29,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [06:45<00:20, 20.86s/it][WARNING|generation_utils.py:914] 2023-08-29 00:40:30,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:31,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:32,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:33,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:34,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:36,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:36,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:37,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:38,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:39,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:40,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:41,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:42,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:43,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:44,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:45,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:46,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:47,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:48,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:49,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:40:50,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [07:06<00:00, 20.91s/it]Generating: 100%|██████████| 20/20 [07:06<00:00, 21.32s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:56,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:56,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:56,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:56,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:56,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:40:57,264 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:40:57,265 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:40:57,932 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:40:59,013 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:40:59,013 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:00,322 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:00,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:00,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:00,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:41:00,326 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:41:01,062 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:41:01,063 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:41:01,327 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:41:01,491 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:41:01,491 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9032738095238095, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9017857142857143, 'errors': {''}}
['Relation : screenwriter . Context : Later in the year , the film screenwriter , William Gibson , wrote a screenplay based on the novel , starring Robert Downey Jr . , based on a novel by Ernest Cline . Head Entity : The novel , Tail Entity : William Gibson .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8778409090909091, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 81, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 136, 'raw': 224}
{'target': 600, 'success': 158, 'raw': 256}
{'target': 600, 'success': 180, 'raw': 288}
{'target': 600, 'success': 197, 'raw': 320}
{'target': 600, 'success': 219, 'raw': 352}
{'target': 600, 'success': 243, 'raw': 384}
{'target': 600, 'success': 265, 'raw': 416}
{'target': 600, 'success': 283, 'raw': 448}
{'target': 600, 'success': 301, 'raw': 480}
{'target': 600, 'success': 321, 'raw': 512}
{'target': 600, 'success': 339, 'raw': 544}
{'target': 600, 'success': 364, 'raw': 576}
{'target': 600, 'success': 384, 'raw': 608}
{'target': 600, 'success': 404, 'raw': 640}
{'target': 600, 'success': 429, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 467, 'raw': 736}
{'target': 600, 'success': 487, 'raw': 768}
{'target': 600, 'success': 508, 'raw': 800}
{'target': 600, 'success': 529, 'raw': 832}
{'target': 600, 'success': 550, 'raw': 864}
{'target': 600, 'success': 571, 'raw': 896}
{'target': 600, 'success': 595, 'raw': 928}
{'target': 600, 'success': 613, 'raw': 960}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6385416666666667, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 403, 'raw': 512}
{'target': 600, 'success': 430, 'raw': 544}
{'target': 600, 'success': 456, 'raw': 576}
{'target': 600, 'success': 481, 'raw': 608}
{'target': 600, 'success': 508, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 590, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7981770833333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 385, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 431, 'raw': 576}
{'target': 600, 'success': 457, 'raw': 608}
{'target': 600, 'success': 481, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 604, 'raw': 800}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.755, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 484, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8464673913043478, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.859375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8098958333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'X - Force\', \'operating system\', \'\', \'" X - Force " is a console version of the " Fallout 4 " game from Square Enix .\')'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9181547619047619, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : position held .', 'success_rate': 0.8355978260869565, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 266, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 336, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 412, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 478, 'raw': 640}
{'target': 600, 'success': 504, 'raw': 672}
{'target': 600, 'success': 526, 'raw': 704}
{'target': 600, 'success': 551, 'raw': 736}
{'target': 600, 'success': 575, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 622, 'raw': 832}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.7475961538461539, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 546, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9017857142857143, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : religion .', 'success_rate': 0.842391304347826, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9211309523809523, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 13257
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13357, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.24it/s]Extractor Estimating: 2it [00:01,  1.23it/s]Extractor Estimating: 3it [00:02,  1.17it/s]Extractor Estimating: 4it [00:03,  1.23it/s]Extractor Estimating: 5it [00:04,  1.22it/s]Extractor Estimating: 6it [00:04,  1.23it/s]Extractor Estimating: 7it [00:05,  1.30it/s]Extractor Estimating: 8it [00:06,  1.29it/s]Extractor Estimating: 9it [00:07,  1.28it/s]Extractor Estimating: 10it [00:08,  1.24it/s]Extractor Estimating: 11it [00:08,  1.24it/s]Extractor Estimating: 12it [00:09,  1.26it/s]Extractor Estimating: 13it [00:10,  1.26it/s]Extractor Estimating: 14it [00:11,  1.24it/s]Extractor Estimating: 15it [00:12,  1.21it/s]Extractor Estimating: 16it [00:12,  1.21it/s]Extractor Estimating: 17it [00:13,  1.22it/s]Extractor Estimating: 18it [00:14,  1.23it/s]Extractor Estimating: 19it [00:15,  1.26it/s]Extractor Estimating: 20it [00:16,  1.28it/s]Extractor Estimating: 21it [00:16,  1.27it/s]Extractor Estimating: 22it [00:17,  1.25it/s]Extractor Estimating: 23it [00:18,  1.27it/s]Extractor Estimating: 24it [00:19,  1.26it/s]Extractor Estimating: 25it [00:20,  1.24it/s]Extractor Estimating: 26it [00:20,  1.24it/s]Extractor Estimating: 27it [00:21,  1.23it/s]Extractor Estimating: 28it [00:22,  1.26it/s]Extractor Estimating: 29it [00:23,  1.30it/s]Extractor Estimating: 30it [00:23,  1.30it/s]Extractor Estimating: 31it [00:24,  1.31it/s]Extractor Estimating: 32it [00:25,  1.27it/s]Extractor Estimating: 33it [00:26,  1.29it/s]Extractor Estimating: 34it [00:27,  1.29it/s]Extractor Estimating: 35it [00:27,  1.31it/s]Extractor Estimating: 36it [00:28,  1.18it/s]Extractor Estimating: 37it [00:29,  1.22it/s]Extractor Estimating: 38it [00:30,  1.24it/s]Extractor Estimating: 39it [00:31,  1.25it/s]Extractor Estimating: 40it [00:31,  1.25it/s]Extractor Estimating: 41it [00:32,  1.23it/s]Extractor Estimating: 42it [00:33,  1.27it/s]Extractor Estimating: 43it [00:34,  1.26it/s]Extractor Estimating: 44it [00:35,  1.28it/s]Extractor Estimating: 45it [00:35,  1.27it/s]Extractor Estimating: 46it [00:36,  1.25it/s]Extractor Estimating: 47it [00:37,  1.27it/s]Extractor Estimating: 48it [00:38,  1.25it/s]Extractor Estimating: 49it [00:39,  1.25it/s]Extractor Estimating: 50it [00:39,  1.27it/s]Extractor Estimating: 51it [00:40,  1.33it/s]Extractor Estimating: 52it [00:41,  1.32it/s]Extractor Estimating: 53it [00:42,  1.31it/s]Extractor Estimating: 54it [00:42,  1.34it/s]Extractor Estimating: 55it [00:43,  1.35it/s]Extractor Estimating: 56it [00:44,  1.39it/s]Extractor Estimating: 57it [00:44,  1.41it/s]Extractor Estimating: 58it [00:45,  1.41it/s]Extractor Estimating: 59it [00:46,  1.42it/s]Extractor Estimating: 60it [00:47,  1.40it/s]Extractor Estimating: 61it [00:47,  1.37it/s]Extractor Estimating: 62it [00:48,  1.38it/s]Extractor Estimating: 63it [00:49,  1.41it/s]Extractor Estimating: 64it [00:49,  1.43it/s]Extractor Estimating: 65it [00:50,  1.43it/s]Extractor Estimating: 66it [00:51,  1.47it/s]Extractor Estimating: 67it [00:51,  1.51it/s]Extractor Estimating: 68it [00:52,  1.46it/s]Extractor Estimating: 69it [00:53,  1.43it/s]Extractor Estimating: 70it [00:53,  1.46it/s]Extractor Estimating: 71it [00:54,  1.44it/s]Extractor Estimating: 72it [00:55,  1.28it/s]Extractor Estimating: 73it [00:56,  1.30it/s]Extractor Estimating: 74it [00:57,  1.32it/s]Extractor Estimating: 75it [00:57,  1.39it/s]Extractor Estimating: 76it [00:58,  1.35it/s]Extractor Estimating: 77it [00:59,  1.29it/s]Extractor Estimating: 78it [01:00,  1.32it/s]Extractor Estimating: 79it [01:00,  1.29it/s]Extractor Estimating: 80it [01:01,  1.29it/s]Extractor Estimating: 81it [01:02,  1.29it/s]Extractor Estimating: 82it [01:03,  1.27it/s]Extractor Estimating: 83it [01:04,  1.24it/s]Extractor Estimating: 84it [01:04,  1.24it/s]Extractor Estimating: 85it [01:05,  1.20it/s]Extractor Estimating: 86it [01:06,  1.19it/s]Extractor Estimating: 87it [01:07,  1.19it/s]Extractor Estimating: 88it [01:08,  1.22it/s]Extractor Estimating: 89it [01:09,  1.23it/s]Extractor Estimating: 90it [01:09,  1.23it/s]Extractor Estimating: 91it [01:10,  1.21it/s]Extractor Estimating: 92it [01:11,  1.25it/s]Extractor Estimating: 93it [01:12,  1.27it/s]Extractor Estimating: 94it [01:13,  1.29it/s]Extractor Estimating: 95it [01:13,  1.24it/s]Extractor Estimating: 96it [01:14,  1.22it/s]Extractor Estimating: 97it [01:15,  1.20it/s]Extractor Estimating: 98it [01:16,  1.21it/s]Extractor Estimating: 99it [01:17,  1.21it/s]Extractor Estimating: 100it [01:18,  1.22it/s]Extractor Estimating: 101it [01:18,  1.23it/s]Extractor Estimating: 102it [01:19,  1.26it/s]Extractor Estimating: 103it [01:20,  1.22it/s]Extractor Estimating: 104it [01:21,  1.25it/s]Extractor Estimating: 105it [01:22,  1.17it/s]Extractor Estimating: 106it [01:22,  1.20it/s]Extractor Estimating: 107it [01:23,  1.21it/s]Extractor Estimating: 108it [01:24,  1.22it/s]Extractor Estimating: 109it [01:25,  1.22it/s]Extractor Estimating: 110it [01:26,  1.26it/s]Extractor Estimating: 111it [01:26,  1.25it/s]Extractor Estimating: 112it [01:27,  1.27it/s]Extractor Estimating: 113it [01:28,  1.29it/s]Extractor Estimating: 114it [01:29,  1.34it/s]Extractor Estimating: 115it [01:29,  1.31it/s]Extractor Estimating: 116it [01:30,  1.29it/s]Extractor Estimating: 117it [01:31,  1.29it/s]Extractor Estimating: 118it [01:32,  1.31it/s]Extractor Estimating: 119it [01:33,  1.29it/s]Extractor Estimating: 120it [01:33,  1.27it/s]Extractor Estimating: 121it [01:34,  1.23it/s]Extractor Estimating: 122it [01:35,  1.22it/s]Extractor Estimating: 123it [01:36,  1.21it/s]Extractor Estimating: 124it [01:37,  1.23it/s]Extractor Estimating: 125it [01:38,  1.13it/s]Extractor Estimating: 126it [01:38,  1.22it/s]Extractor Estimating: 127it [01:39,  1.25it/s]Extractor Estimating: 128it [01:40,  1.28it/s]Extractor Estimating: 129it [01:41,  1.30it/s]Extractor Estimating: 130it [01:41,  1.32it/s]Extractor Estimating: 131it [01:42,  1.36it/s]Extractor Estimating: 132it [01:43,  1.39it/s]Extractor Estimating: 133it [01:43,  1.39it/s]Extractor Estimating: 134it [01:44,  1.42it/s]Extractor Estimating: 135it [01:45,  1.40it/s]Extractor Estimating: 136it [01:46,  1.34it/s]Extractor Estimating: 137it [01:46,  1.36it/s]Extractor Estimating: 138it [01:47,  1.38it/s]Extractor Estimating: 139it [01:48,  1.35it/s]Extractor Estimating: 140it [01:49,  1.33it/s]Extractor Estimating: 141it [01:49,  1.33it/s]Extractor Estimating: 142it [01:50,  1.38it/s]Extractor Estimating: 143it [01:51,  1.36it/s]Extractor Estimating: 144it [01:52,  1.32it/s]Extractor Estimating: 145it [01:52,  1.31it/s]Extractor Estimating: 146it [01:53,  1.30it/s]Extractor Estimating: 147it [01:54,  1.30it/s]Extractor Estimating: 148it [01:55,  1.33it/s]Extractor Estimating: 149it [01:55,  1.36it/s]Extractor Estimating: 150it [01:56,  1.35it/s]Extractor Estimating: 151it [01:57,  1.35it/s]Extractor Estimating: 152it [01:58,  1.33it/s]Extractor Estimating: 153it [01:58,  1.31it/s]Extractor Estimating: 154it [01:59,  1.27it/s]Extractor Estimating: 155it [02:00,  1.25it/s]Extractor Estimating: 156it [02:01,  1.20it/s]Extractor Estimating: 157it [02:02,  1.27it/s]Extractor Estimating: 158it [02:02,  1.31it/s]Extractor Estimating: 159it [02:03,  1.31it/s]Extractor Estimating: 160it [02:04,  1.27it/s]Extractor Estimating: 161it [02:05,  1.29it/s]Extractor Estimating: 162it [02:06,  1.32it/s]Extractor Estimating: 163it [02:06,  1.31it/s]Extractor Estimating: 164it [02:07,  1.32it/s]Extractor Estimating: 165it [02:08,  1.33it/s]Extractor Estimating: 166it [02:09,  1.31it/s]Extractor Estimating: 167it [02:09,  1.33it/s]Extractor Estimating: 168it [02:10,  1.32it/s]Extractor Estimating: 169it [02:11,  1.29it/s]Extractor Estimating: 170it [02:12,  1.31it/s]Extractor Estimating: 171it [02:12,  1.29it/s]Extractor Estimating: 172it [02:13,  1.29it/s]Extractor Estimating: 173it [02:14,  1.32it/s]Extractor Estimating: 174it [02:15,  1.28it/s]Extractor Estimating: 175it [02:16,  1.27it/s]Extractor Estimating: 176it [02:16,  1.23it/s]Extractor Estimating: 177it [02:17,  1.27it/s]Extractor Estimating: 178it [02:18,  1.28it/s]Extractor Estimating: 179it [02:19,  1.30it/s]Extractor Estimating: 180it [02:19,  1.28it/s]Extractor Estimating: 181it [02:20,  1.25it/s]Extractor Estimating: 182it [02:21,  1.23it/s]Extractor Estimating: 183it [02:22,  1.23it/s]Extractor Estimating: 184it [02:23,  1.25it/s]Extractor Estimating: 185it [02:24,  1.25it/s]Extractor Estimating: 186it [02:24,  1.24it/s]Extractor Estimating: 187it [02:25,  1.21it/s]Extractor Estimating: 188it [02:26,  1.25it/s]Extractor Estimating: 189it [02:27,  1.26it/s]Extractor Estimating: 190it [02:28,  1.23it/s]Extractor Estimating: 191it [02:28,  1.23it/s]Extractor Estimating: 192it [02:29,  1.20it/s]Extractor Estimating: 193it [02:30,  1.22it/s]Extractor Estimating: 194it [02:31,  1.22it/s]Extractor Estimating: 195it [02:32,  1.25it/s]Extractor Estimating: 196it [02:32,  1.30it/s]Extractor Estimating: 197it [02:33,  1.26it/s]Extractor Estimating: 198it [02:34,  1.27it/s]Extractor Estimating: 199it [02:35,  1.23it/s]Extractor Estimating: 200it [02:36,  1.27it/s]Extractor Estimating: 201it [02:36,  1.29it/s]Extractor Estimating: 202it [02:37,  1.27it/s]Extractor Estimating: 203it [02:38,  1.23it/s]Extractor Estimating: 204it [02:39,  1.23it/s]Extractor Estimating: 205it [02:40,  1.25it/s]Extractor Estimating: 206it [02:40,  1.25it/s]Extractor Estimating: 207it [02:41,  1.28it/s]Extractor Estimating: 208it [02:42,  1.31it/s]Extractor Estimating: 209it [02:43,  1.32it/s]Extractor Estimating: 210it [02:43,  1.34it/s]Extractor Estimating: 211it [02:44,  1.36it/s]Extractor Estimating: 212it [02:45,  1.34it/s]Extractor Estimating: 213it [02:46,  1.34it/s]Extractor Estimating: 214it [02:46,  1.35it/s]Extractor Estimating: 215it [02:47,  1.34it/s]Extractor Estimating: 216it [02:48,  1.37it/s]Extractor Estimating: 217it [02:49,  1.35it/s]Extractor Estimating: 218it [02:49,  1.32it/s]Extractor Estimating: 219it [02:50,  1.19it/s]Extractor Estimating: 220it [02:51,  1.23it/s]Extractor Estimating: 221it [02:52,  1.26it/s]Extractor Estimating: 222it [02:53,  1.29it/s]Extractor Estimating: 223it [02:53,  1.31it/s]Extractor Estimating: 224it [02:54,  1.30it/s]Extractor Estimating: 225it [02:55,  1.28it/s]Extractor Estimating: 226it [02:56,  1.31it/s]Extractor Estimating: 227it [02:56,  1.29it/s]Extractor Estimating: 228it [02:57,  1.35it/s]Extractor Estimating: 229it [02:58,  1.36it/s]Extractor Estimating: 230it [02:59,  1.37it/s]Extractor Estimating: 231it [02:59,  1.38it/s]Extractor Estimating: 232it [03:00,  1.32it/s]Extractor Estimating: 233it [03:01,  1.33it/s]Extractor Estimating: 234it [03:02,  1.33it/s]Extractor Estimating: 235it [03:02,  1.35it/s]Extractor Estimating: 236it [03:03,  1.38it/s]Extractor Estimating: 237it [03:04,  1.36it/s]Extractor Estimating: 238it [03:05,  1.34it/s]Extractor Estimating: 239it [03:05,  1.37it/s]Extractor Estimating: 240it [03:06,  1.36it/s]Extractor Estimating: 241it [03:07,  1.38it/s]Extractor Estimating: 242it [03:07,  1.36it/s]Extractor Estimating: 243it [03:08,  1.37it/s]Extractor Estimating: 244it [03:09,  1.38it/s]Extractor Estimating: 245it [03:10,  1.33it/s]Extractor Estimating: 246it [03:10,  1.33it/s]Extractor Estimating: 247it [03:11,  1.35it/s]Extractor Estimating: 248it [03:12,  1.36it/s]Extractor Estimating: 249it [03:13,  1.35it/s]Extractor Estimating: 250it [03:13,  1.32it/s]Extractor Estimating: 251it [03:14,  1.32it/s]Extractor Estimating: 252it [03:15,  1.31it/s]Extractor Estimating: 253it [03:16,  1.33it/s]Extractor Estimating: 254it [03:16,  1.31it/s]Extractor Estimating: 255it [03:17,  1.35it/s]Extractor Estimating: 256it [03:18,  1.36it/s]Extractor Estimating: 257it [03:19,  1.36it/s]Extractor Estimating: 258it [03:19,  1.37it/s]Extractor Estimating: 259it [03:20,  1.32it/s]Extractor Estimating: 260it [03:21,  1.32it/s]Extractor Estimating: 261it [03:22,  1.34it/s]Extractor Estimating: 262it [03:22,  1.32it/s]Extractor Estimating: 263it [03:23,  1.34it/s]Extractor Estimating: 264it [03:24,  1.36it/s]Extractor Estimating: 265it [03:25,  1.34it/s]Extractor Estimating: 266it [03:25,  1.34it/s]Extractor Estimating: 267it [03:26,  1.35it/s]Extractor Estimating: 268it [03:27,  1.33it/s]Extractor Estimating: 269it [03:28,  1.31it/s]Extractor Estimating: 270it [03:28,  1.31it/s]Extractor Estimating: 271it [03:29,  1.25it/s]Extractor Estimating: 272it [03:30,  1.27it/s]Extractor Estimating: 273it [03:31,  1.26it/s]Extractor Estimating: 274it [03:32,  1.27it/s]Extractor Estimating: 275it [03:32,  1.26it/s]Extractor Estimating: 276it [03:33,  1.19it/s]Extractor Estimating: 277it [03:34,  1.23it/s]Extractor Estimating: 278it [03:35,  1.26it/s]Extractor Estimating: 279it [03:36,  1.25it/s]Extractor Estimating: 280it [03:37,  1.20it/s]Extractor Estimating: 281it [03:37,  1.19it/s]Extractor Estimating: 282it [03:38,  1.19it/s]Extractor Estimating: 283it [03:39,  1.21it/s]Extractor Estimating: 284it [03:40,  1.23it/s]Extractor Estimating: 285it [03:41,  1.23it/s]Extractor Estimating: 286it [03:42,  1.22it/s]Extractor Estimating: 287it [03:42,  1.21it/s]Extractor Estimating: 288it [03:43,  1.23it/s]Extractor Estimating: 289it [03:44,  1.24it/s]Extractor Estimating: 290it [03:45,  1.23it/s]Extractor Estimating: 291it [03:46,  1.23it/s]Extractor Estimating: 292it [03:46,  1.23it/s]Extractor Estimating: 293it [03:47,  1.15it/s]Extractor Estimating: 294it [03:48,  1.15it/s]Extractor Estimating: 295it [03:49,  1.14it/s]Extractor Estimating: 296it [03:50,  1.17it/s]Extractor Estimating: 297it [03:51,  1.18it/s]Extractor Estimating: 298it [03:52,  1.15it/s]Extractor Estimating: 299it [03:52,  1.19it/s]Extractor Estimating: 300it [03:53,  1.17it/s]Extractor Estimating: 301it [03:54,  1.20it/s]Extractor Estimating: 302it [03:55,  1.21it/s]Extractor Estimating: 303it [03:56,  1.24it/s]Extractor Estimating: 304it [03:56,  1.26it/s]Extractor Estimating: 305it [03:57,  1.27it/s]Extractor Estimating: 306it [03:58,  1.35it/s]Extractor Estimating: 307it [03:59,  1.37it/s]Extractor Estimating: 308it [03:59,  1.40it/s]Extractor Estimating: 309it [04:00,  1.36it/s]Extractor Estimating: 310it [04:01,  1.38it/s]Extractor Estimating: 311it [04:01,  1.42it/s]Extractor Estimating: 312it [04:02,  1.35it/s]Extractor Estimating: 313it [04:03,  1.34it/s]Extractor Estimating: 314it [04:04,  1.43it/s]Extractor Estimating: 315it [04:04,  1.40it/s]Extractor Estimating: 316it [04:05,  1.43it/s]Extractor Estimating: 317it [04:06,  1.40it/s]Extractor Estimating: 318it [04:06,  1.40it/s]Extractor Estimating: 319it [04:07,  1.43it/s]Extractor Estimating: 320it [04:08,  1.48it/s]Extractor Estimating: 321it [04:08,  1.49it/s]Extractor Estimating: 322it [04:09,  1.43it/s]Extractor Estimating: 323it [04:10,  1.41it/s]Extractor Estimating: 324it [04:11,  1.37it/s]Extractor Estimating: 325it [04:12,  1.31it/s]Extractor Estimating: 326it [04:12,  1.31it/s]Extractor Estimating: 327it [04:13,  1.29it/s]Extractor Estimating: 328it [04:14,  1.29it/s]Extractor Estimating: 329it [04:15,  1.28it/s]Extractor Estimating: 330it [04:15,  1.33it/s]Extractor Estimating: 331it [04:16,  1.33it/s]Extractor Estimating: 332it [04:17,  1.31it/s]Extractor Estimating: 333it [04:18,  1.28it/s]Extractor Estimating: 334it [04:18,  1.28it/s]Extractor Estimating: 335it [04:19,  1.32it/s]Extractor Estimating: 336it [04:20,  1.30it/s]Extractor Estimating: 337it [04:21,  1.27it/s]Extractor Estimating: 338it [04:22,  1.25it/s]Extractor Estimating: 339it [04:22,  1.26it/s]Extractor Estimating: 340it [04:23,  1.25it/s]Extractor Estimating: 341it [04:24,  1.26it/s]Extractor Estimating: 342it [04:25,  1.25it/s]Extractor Estimating: 343it [04:26,  1.27it/s]Extractor Estimating: 344it [04:26,  1.24it/s]Extractor Estimating: 345it [04:27,  1.26it/s]Extractor Estimating: 346it [04:28,  1.25it/s]Extractor Estimating: 347it [04:29,  1.24it/s]Extractor Estimating: 348it [04:30,  1.28it/s]Extractor Estimating: 349it [04:30,  1.31it/s]Extractor Estimating: 350it [04:31,  1.24it/s]Extractor Estimating: 351it [04:32,  1.31it/s]Extractor Estimating: 352it [04:33,  1.32it/s]Extractor Estimating: 353it [04:33,  1.32it/s]Extractor Estimating: 354it [04:34,  1.35it/s]Extractor Estimating: 355it [04:35,  1.37it/s]Extractor Estimating: 356it [04:35,  1.38it/s]Extractor Estimating: 357it [04:36,  1.36it/s]Extractor Estimating: 358it [04:37,  1.35it/s]Extractor Estimating: 359it [04:38,  1.37it/s]Extractor Estimating: 360it [04:38,  1.36it/s]Extractor Estimating: 361it [04:39,  1.33it/s]Extractor Estimating: 362it [04:40,  1.36it/s]Extractor Estimating: 363it [04:41,  1.37it/s]Extractor Estimating: 364it [04:41,  1.38it/s]Extractor Estimating: 365it [04:42,  1.38it/s]Extractor Estimating: 366it [04:43,  1.40it/s]Extractor Estimating: 367it [04:44,  1.39it/s]Extractor Estimating: 368it [04:44,  1.42it/s]Extractor Estimating: 369it [04:45,  1.46it/s]Extractor Estimating: 370it [04:46,  1.42it/s]Extractor Estimating: 371it [04:46,  1.40it/s]Extractor Estimating: 372it [04:47,  1.35it/s]Extractor Estimating: 373it [04:48,  1.38it/s]Extractor Estimating: 374it [04:49,  1.38it/s]Extractor Estimating: 375it [04:49,  1.41it/s]Extractor Estimating: 376it [04:50,  1.39it/s]Extractor Estimating: 377it [04:51,  1.38it/s]Extractor Estimating: 378it [04:51,  1.34it/s]Extractor Estimating: 379it [04:52,  1.32it/s]Extractor Estimating: 380it [04:53,  1.33it/s]Extractor Estimating: 381it [04:54,  1.32it/s]Extractor Estimating: 382it [04:55,  1.33it/s]Extractor Estimating: 383it [04:55,  1.30it/s]Extractor Estimating: 384it [04:56,  1.29it/s]Extractor Estimating: 385it [04:57,  1.30it/s]Extractor Estimating: 386it [04:58,  1.27it/s]Extractor Estimating: 387it [04:58,  1.30it/s]Extractor Estimating: 388it [04:59,  1.35it/s]Extractor Estimating: 389it [05:00,  1.35it/s]Extractor Estimating: 390it [05:01,  1.35it/s]Extractor Estimating: 391it [05:01,  1.36it/s]Extractor Estimating: 392it [05:02,  1.33it/s]Extractor Estimating: 393it [05:03,  1.36it/s]Extractor Estimating: 394it [05:04,  1.37it/s]Extractor Estimating: 395it [05:04,  1.39it/s]Extractor Estimating: 396it [05:05,  1.36it/s]Extractor Estimating: 397it [05:06,  1.33it/s]Extractor Estimating: 398it [05:06,  1.36it/s]Extractor Estimating: 399it [05:07,  1.35it/s]Extractor Estimating: 400it [05:08,  1.33it/s]Extractor Estimating: 401it [05:09,  1.36it/s]Extractor Estimating: 402it [05:09,  1.35it/s]Extractor Estimating: 403it [05:10,  1.28it/s]Extractor Estimating: 404it [05:11,  1.30it/s]Extractor Estimating: 405it [05:12,  1.38it/s]Extractor Estimating: 406it [05:12,  1.42it/s]Extractor Estimating: 407it [05:13,  1.40it/s]Extractor Estimating: 408it [05:14,  1.40it/s]Extractor Estimating: 409it [05:14,  1.45it/s]Extractor Estimating: 410it [05:15,  1.48it/s]Extractor Estimating: 411it [05:16,  1.46it/s]Extractor Estimating: 412it [05:17,  1.42it/s]Extractor Estimating: 413it [05:17,  1.43it/s]Extractor Estimating: 414it [05:18,  1.41it/s]Extractor Estimating: 415it [05:19,  1.38it/s]Extractor Estimating: 416it [05:19,  1.40it/s]Extractor Estimating: 417it [05:20,  1.39it/s]Extractor Estimating: 418it [05:21,  1.42it/s]Extractor Estimating: 419it [05:22,  1.41it/s]Extractor Estimating: 420it [05:22,  1.41it/s]Extractor Estimating: 421it [05:23,  1.39it/s]Extractor Estimating: 422it [05:24,  1.41it/s]Extractor Estimating: 423it [05:24,  1.42it/s]Extractor Estimating: 424it [05:25,  1.42it/s]Extractor Estimating: 425it [05:26,  1.40it/s]Extractor Estimating: 426it [05:27,  1.33it/s]Extractor Estimating: 427it [05:27,  1.32it/s]Extractor Estimating: 428it [05:28,  1.27it/s]Extractor Estimating: 429it [05:29,  1.27it/s]Extractor Estimating: 430it [05:30,  1.25it/s]Extractor Estimating: 431it [05:31,  1.20it/s]Extractor Estimating: 432it [05:32,  1.18it/s]Extractor Estimating: 433it [05:32,  1.20it/s]Extractor Estimating: 434it [05:33,  1.19it/s]Extractor Estimating: 435it [05:34,  1.23it/s]Extractor Estimating: 436it [05:35,  1.26it/s]Extractor Estimating: 437it [05:36,  1.23it/s]Extractor Estimating: 438it [05:36,  1.26it/s]Extractor Estimating: 439it [05:37,  1.27it/s]Extractor Estimating: 440it [05:38,  1.27it/s]Extractor Estimating: 441it [05:39,  1.27it/s]Extractor Estimating: 442it [05:39,  1.30it/s]Extractor Estimating: 443it [05:40,  1.27it/s]Extractor Estimating: 444it [05:41,  1.23it/s]Extractor Estimating: 445it [05:42,  1.25it/s]Extractor Estimating: 446it [05:43,  1.24it/s]Extractor Estimating: 447it [05:44,  1.24it/s]Extractor Estimating: 448it [05:44,  1.23it/s]Extractor Estimating: 449it [05:45,  1.26it/s]Extractor Estimating: 450it [05:46,  1.23it/s]Extractor Estimating: 451it [05:47,  1.24it/s]Extractor Estimating: 452it [05:48,  1.25it/s]Extractor Estimating: 453it [05:48,  1.25it/s]Extractor Estimating: 454it [05:49,  1.24it/s]Extractor Estimating: 455it [05:50,  1.25it/s]Extractor Estimating: 456it [05:51,  1.28it/s]Extractor Estimating: 457it [05:52,  1.24it/s]Extractor Estimating: 458it [05:52,  1.21it/s]Extractor Estimating: 459it [05:53,  1.23it/s]Extractor Estimating: 460it [05:54,  1.23it/s]Extractor Estimating: 461it [05:55,  1.23it/s]Extractor Estimating: 462it [05:56,  1.25it/s]Extractor Estimating: 463it [05:56,  1.25it/s]Extractor Estimating: 464it [05:57,  1.23it/s]Extractor Estimating: 465it [05:58,  1.26it/s]Extractor Estimating: 466it [05:59,  1.26it/s]Extractor Estimating: 467it [06:00,  1.19it/s]Extractor Estimating: 468it [06:01,  1.22it/s]Extractor Estimating: 469it [06:01,  1.24it/s]Extractor Estimating: 470it [06:02,  1.24it/s]Extractor Estimating: 471it [06:03,  1.28it/s]Extractor Estimating: 472it [06:04,  1.28it/s]Extractor Estimating: 473it [06:04,  1.31it/s]Extractor Estimating: 474it [06:05,  1.28it/s]Extractor Estimating: 475it [06:06,  1.27it/s]Extractor Estimating: 476it [06:07,  1.27it/s]Extractor Estimating: 477it [06:08,  1.28it/s]Extractor Estimating: 478it [06:08,  1.30it/s]Extractor Estimating: 479it [06:09,  1.30it/s]Extractor Estimating: 480it [06:10,  1.29it/s]Extractor Estimating: 481it [06:11,  1.31it/s]Extractor Estimating: 482it [06:11,  1.31it/s]Extractor Estimating: 483it [06:12,  1.29it/s]Extractor Estimating: 484it [06:13,  1.27it/s]Extractor Estimating: 485it [06:14,  1.28it/s]Extractor Estimating: 486it [06:15,  1.29it/s]Extractor Estimating: 487it [06:15,  1.30it/s]Extractor Estimating: 488it [06:16,  1.30it/s]Extractor Estimating: 489it [06:17,  1.32it/s]Extractor Estimating: 490it [06:18,  1.30it/s]Extractor Estimating: 491it [06:18,  1.33it/s]Extractor Estimating: 492it [06:19,  1.30it/s]Extractor Estimating: 493it [06:20,  1.30it/s]Extractor Estimating: 494it [06:21,  1.24it/s]Extractor Estimating: 495it [06:22,  1.23it/s]Extractor Estimating: 496it [06:22,  1.20it/s]Extractor Estimating: 497it [06:23,  1.24it/s]Extractor Estimating: 498it [06:24,  1.26it/s]Extractor Estimating: 499it [06:25,  1.27it/s]Extractor Estimating: 500it [06:25,  1.30it/s]Extractor Estimating: 500it [06:25,  1.30it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:41,341 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:41,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:41,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:41,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:41,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:47:41,960 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:47:41,961 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:47:42,533 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:47:43,581 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:47:43,581 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:46,598 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:46,604 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:46,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:46,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:47:46,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:47:47,220 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:47:47,221 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:47:47,785 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:47:47,938 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:47:47,938 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 02:18:32,295 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 02:18:32,338 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 4008 mean pseudo reward: 0.9887356788006687
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 16155
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16255, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16255, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.340, loss:340.9323
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 33, avg_time 1.337, loss:320.0254
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 133, avg_time 1.321, loss:288.9831
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 66, avg_time 1.332, loss:257.1649
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 166, avg_time 1.331, loss:260.9912
>> valid entity prec:0.6158, rec:0.6454, f1:0.6303
>> valid relation prec:0.4307, rec:0.3077, f1:0.3590
>> valid relation with NER prec:0.4307, rec:0.3077, f1:0.3590
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 99, avg_time 3.021, loss:244.3374
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 32, avg_time 1.326, loss:238.3150
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 132, avg_time 1.317, loss:234.7474
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 65, avg_time 1.334, loss:223.7419
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 165, avg_time 1.334, loss:246.3384
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6102, rec:0.6427, f1:0.6260
>> valid relation prec:0.4024, rec:0.3188, f1:0.3558
>> valid relation with NER prec:0.4024, rec:0.3188, f1:0.3558
g_step 1100, step 98, avg_time 3.018, loss:214.2379
g_step 1200, step 31, avg_time 1.327, loss:188.6807
g_step 1300, step 131, avg_time 1.324, loss:199.5226
g_step 1400, step 64, avg_time 1.324, loss:201.1857
g_step 1500, step 164, avg_time 1.331, loss:180.2292
>> valid entity prec:0.6078, rec:0.6443, f1:0.6255
>> valid relation prec:0.3738, rec:0.3054, f1:0.3362
>> valid relation with NER prec:0.3738, rec:0.3054, f1:0.3362
g_step 1600, step 97, avg_time 3.007, loss:178.0918
g_step 1700, step 30, avg_time 1.328, loss:170.9369
g_step 1800, step 130, avg_time 1.331, loss:158.2567
g_step 1900, step 63, avg_time 1.338, loss:144.7254
g_step 2000, step 163, avg_time 1.314, loss:152.7300
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6540, rec:0.6397, f1:0.6468
>> valid relation prec:0.4354, rec:0.3366, f1:0.3797
>> valid relation with NER prec:0.4354, rec:0.3366, f1:0.3797
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 96, avg_time 3.002, loss:144.1299
g_step 2200, step 29, avg_time 1.320, loss:142.0952
g_step 2300, step 129, avg_time 1.320, loss:137.0214
g_step 2400, step 62, avg_time 1.321, loss:107.4937
g_step 2500, step 162, avg_time 1.338, loss:113.8277
>> valid entity prec:0.6396, rec:0.6515, f1:0.6455
>> valid relation prec:0.3834, rec:0.3443, f1:0.3628
>> valid relation with NER prec:0.3834, rec:0.3443, f1:0.3628
g_step 2600, step 95, avg_time 2.988, loss:115.5284
g_step 2700, step 28, avg_time 1.330, loss:118.1239
g_step 2800, step 128, avg_time 1.329, loss:117.1899
g_step 2900, step 61, avg_time 1.331, loss:101.4969
g_step 3000, step 161, avg_time 1.330, loss:112.0783
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6574, rec:0.6641, f1:0.6607
>> valid relation prec:0.4663, rec:0.3540, f1:0.4025
>> valid relation with NER prec:0.4663, rec:0.3540, f1:0.4025
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 94, avg_time 2.973, loss:105.6055
g_step 3200, step 27, avg_time 1.337, loss:103.4208
g_step 3300, step 127, avg_time 1.331, loss:98.5221
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 02:18:32 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 02:18:32 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_02-18-32_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 02:18:33 - WARNING - datasets.builder -   Using custom data configuration default-7dcf1c5711f61f5d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-7dcf1c5711f61f5d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 02:18:33,553 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:18:33,554 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:18:33,554 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:18:33,555 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:18:33,564 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:33,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:33,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:33,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:33,567 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:33,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:33,567 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 02:18:33,690 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:18:36,793 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 02:18:36,796 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-7dcf1c5711f61f5d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.16ba/s] 40%|████      | 2/5 [00:00<00:00,  3.99ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.39ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.61ba/s]100%|██████████| 5/5 [00:00<00:00,  5.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.17ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.35ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.43ba/s]100%|██████████| 4/4 [00:00<00:00,  5.50ba/s]100%|██████████| 4/4 [00:00<00:00,  5.01ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.79ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.98ba/s]100%|██████████| 5/5 [00:00<00:00,  9.76ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.45ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.02ba/s]100%|██████████| 4/4 [00:00<00:00, 11.18ba/s]
[INFO|trainer.py:414] 2023-08-29 02:18:39,720 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 02:18:39,731 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 02:18:39,731 >>   Num examples = 4008
[INFO|trainer.py:1149] 2023-08-29 02:18:39,731 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 02:18:39,731 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 02:18:39,731 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 02:18:39,731 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 02:18:39,731 >>   Total optimization steps = 315
  0%|          | 0/315 [00:00<?, ?it/s]  0%|          | 1/315 [00:00<01:35,  3.27it/s]  1%|          | 2/315 [00:00<01:33,  3.36it/s]  1%|          | 3/315 [00:00<01:31,  3.40it/s]  1%|▏         | 4/315 [00:01<01:31,  3.42it/s]  2%|▏         | 5/315 [00:01<01:30,  3.42it/s]  2%|▏         | 6/315 [00:01<01:29,  3.43it/s]  2%|▏         | 7/315 [00:02<01:29,  3.44it/s]  3%|▎         | 8/315 [00:02<01:29,  3.44it/s]  3%|▎         | 9/315 [00:02<01:28,  3.44it/s]  3%|▎         | 10/315 [00:02<01:28,  3.44it/s]  3%|▎         | 11/315 [00:03<01:28,  3.44it/s]  4%|▍         | 12/315 [00:03<01:27,  3.45it/s]  4%|▍         | 13/315 [00:03<01:27,  3.44it/s]  4%|▍         | 14/315 [00:04<01:27,  3.44it/s]  5%|▍         | 15/315 [00:04<01:27,  3.44it/s]  5%|▌         | 16/315 [00:04<01:26,  3.44it/s]  5%|▌         | 17/315 [00:04<01:26,  3.44it/s]  6%|▌         | 18/315 [00:05<01:26,  3.44it/s]  6%|▌         | 19/315 [00:05<01:26,  3.44it/s]  6%|▋         | 20/315 [00:05<01:25,  3.44it/s]  7%|▋         | 21/315 [00:06<01:25,  3.44it/s]  7%|▋         | 22/315 [00:06<01:25,  3.44it/s]  7%|▋         | 23/315 [00:06<01:24,  3.44it/s]  8%|▊         | 24/315 [00:06<01:24,  3.43it/s]  8%|▊         | 25/315 [00:07<01:24,  3.43it/s]  8%|▊         | 26/315 [00:07<01:24,  3.44it/s]  9%|▊         | 27/315 [00:07<01:23,  3.43it/s]  9%|▉         | 28/315 [00:08<01:23,  3.44it/s]  9%|▉         | 29/315 [00:08<01:23,  3.44it/s] 10%|▉         | 30/315 [00:08<01:22,  3.44it/s] 10%|▉         | 31/315 [00:09<01:22,  3.44it/s] 10%|█         | 32/315 [00:09<01:22,  3.44it/s] 10%|█         | 33/315 [00:09<01:22,  3.44it/s] 11%|█         | 34/315 [00:09<01:21,  3.44it/s] 11%|█         | 35/315 [00:10<01:21,  3.43it/s] 11%|█▏        | 36/315 [00:10<01:21,  3.43it/s] 12%|█▏        | 37/315 [00:10<01:20,  3.43it/s] 12%|█▏        | 38/315 [00:11<01:20,  3.43it/s] 12%|█▏        | 39/315 [00:11<01:20,  3.44it/s] 13%|█▎        | 40/315 [00:11<01:19,  3.44it/s] 13%|█▎        | 41/315 [00:11<01:19,  3.44it/s] 13%|█▎        | 42/315 [00:12<01:19,  3.44it/s] 14%|█▎        | 43/315 [00:12<01:19,  3.44it/s] 14%|█▍        | 44/315 [00:12<01:18,  3.44it/s] 14%|█▍        | 45/315 [00:13<01:18,  3.43it/s] 15%|█▍        | 46/315 [00:13<01:18,  3.43it/s] 15%|█▍        | 47/315 [00:13<01:18,  3.43it/s] 15%|█▌        | 48/315 [00:13<01:17,  3.43it/s] 16%|█▌        | 49/315 [00:14<01:17,  3.43it/s] 16%|█▌        | 50/315 [00:14<01:17,  3.43it/s] 16%|█▌        | 51/315 [00:14<01:16,  3.44it/s] 17%|█▋        | 52/315 [00:15<01:16,  3.44it/s] 17%|█▋        | 53/315 [00:15<01:16,  3.44it/s] 17%|█▋        | 54/315 [00:15<01:15,  3.44it/s] 17%|█▋        | 55/315 [00:16<01:15,  3.44it/s] 18%|█▊        | 56/315 [00:16<01:15,  3.44it/s] 18%|█▊        | 57/315 [00:16<01:15,  3.40it/s] 18%|█▊        | 58/315 [00:16<01:15,  3.41it/s] 19%|█▊        | 59/315 [00:17<01:14,  3.42it/s] 19%|█▉        | 60/315 [00:17<01:14,  3.42it/s] 19%|█▉        | 61/315 [00:17<01:14,  3.43it/s] 20%|█▉        | 62/315 [00:18<01:13,  3.43it/s] 20%|██        | 63/315 [00:18<01:07,  3.75it/s][INFO|trainer.py:2140] 2023-08-29 02:18:58,007 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:18:58,007 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 02:18:58,007 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.26it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.55it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.82it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.13it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.72it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.45it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.19it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.68it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.52it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.57it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.27it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.42it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.54it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.69it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.72it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.74it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.48it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.47it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.46it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.50it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.57it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.69it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.77it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.71it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.72it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.59it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.49it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.49it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.49it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.46it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.55it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.61it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.69it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.73it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.49it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.62it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.46it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.46it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.40it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.42it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.36it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.40it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.47it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.41it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.44it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.36it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.25it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.33it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.35it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.37it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.48it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.51it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.52it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.61it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.54it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.49it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.46it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.36it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.38it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.40it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.48it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.53it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.56it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.52it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.46it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.47it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.35it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.37it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.41it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.40it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.52it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.59it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.45it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.48it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.47it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.33it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.34it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.39it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.38it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.50it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.51it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.46it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.45it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.46it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.38it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.35it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.49it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:09<00:00, 46.49it/s][A 20%|██        | 63/315 [00:27<01:07,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:19:07,433 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-63
[INFO|configuration_utils.py:351] 2023-08-29 02:19:07,450 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-63/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:19:10,333 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-63/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:19:10,364 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:19:10,370 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-63/special_tokens_map.json
 20%|██        | 64/315 [00:35<22:24,  5.36s/it] 21%|██        | 65/315 [00:35<15:59,  3.84s/it] 21%|██        | 66/315 [00:36<11:30,  2.77s/it] 21%|██▏       | 67/315 [00:36<08:23,  2.03s/it] 22%|██▏       | 68/315 [00:36<06:12,  1.51s/it] 22%|██▏       | 69/315 [00:36<04:41,  1.14s/it] 22%|██▏       | 70/315 [00:37<03:37,  1.13it/s] 23%|██▎       | 71/315 [00:37<02:52,  1.41it/s] 23%|██▎       | 72/315 [00:37<02:21,  1.71it/s] 23%|██▎       | 73/315 [00:38<01:59,  2.02it/s] 23%|██▎       | 74/315 [00:38<01:44,  2.30it/s] 24%|██▍       | 75/315 [00:38<01:33,  2.55it/s] 24%|██▍       | 76/315 [00:39<01:26,  2.76it/s] 24%|██▍       | 77/315 [00:39<01:21,  2.93it/s] 25%|██▍       | 78/315 [00:39<01:17,  3.07it/s] 25%|██▌       | 79/315 [00:39<01:14,  3.17it/s] 25%|██▌       | 80/315 [00:40<01:12,  3.24it/s] 26%|██▌       | 81/315 [00:40<01:10,  3.30it/s] 26%|██▌       | 82/315 [00:40<01:09,  3.34it/s] 26%|██▋       | 83/315 [00:41<01:08,  3.37it/s] 27%|██▋       | 84/315 [00:41<01:08,  3.39it/s] 27%|██▋       | 85/315 [00:41<01:07,  3.40it/s] 27%|██▋       | 86/315 [00:41<01:07,  3.41it/s] 28%|██▊       | 87/315 [00:42<01:06,  3.41it/s] 28%|██▊       | 88/315 [00:42<01:06,  3.42it/s] 28%|██▊       | 89/315 [00:42<01:06,  3.42it/s] 29%|██▊       | 90/315 [00:43<01:05,  3.43it/s] 29%|██▉       | 91/315 [00:43<01:05,  3.43it/s] 29%|██▉       | 92/315 [00:43<01:05,  3.43it/s] 30%|██▉       | 93/315 [00:43<01:04,  3.43it/s] 30%|██▉       | 94/315 [00:44<01:04,  3.43it/s] 30%|███       | 95/315 [00:44<01:04,  3.43it/s] 30%|███       | 96/315 [00:44<01:03,  3.43it/s] 31%|███       | 97/315 [00:45<01:03,  3.43it/s] 31%|███       | 98/315 [00:45<01:03,  3.42it/s] 31%|███▏      | 99/315 [00:45<01:03,  3.42it/s] 32%|███▏      | 100/315 [00:46<01:02,  3.43it/s] 32%|███▏      | 101/315 [00:46<01:02,  3.43it/s] 32%|███▏      | 102/315 [00:46<01:02,  3.43it/s] 33%|███▎      | 103/315 [00:46<01:01,  3.42it/s] 33%|███▎      | 104/315 [00:47<01:01,  3.43it/s] 33%|███▎      | 105/315 [00:47<01:01,  3.43it/s] 34%|███▎      | 106/315 [00:47<01:00,  3.43it/s] 34%|███▍      | 107/315 [00:48<01:00,  3.43it/s] 34%|███▍      | 108/315 [00:48<01:00,  3.43it/s] 35%|███▍      | 109/315 [00:48<01:00,  3.42it/s] 35%|███▍      | 110/315 [00:48<00:59,  3.42it/s] 35%|███▌      | 111/315 [00:49<00:59,  3.42it/s] 36%|███▌      | 112/315 [00:49<00:59,  3.43it/s] 36%|███▌      | 113/315 [00:49<00:58,  3.43it/s] 36%|███▌      | 114/315 [00:50<00:58,  3.43it/s] 37%|███▋      | 115/315 [00:50<00:58,  3.43it/s] 37%|███▋      | 116/315 [00:50<00:58,  3.43it/s] 37%|███▋      | 117/315 [00:50<00:57,  3.43it/s] 37%|███▋      | 118/315 [00:51<00:57,  3.42it/s] 38%|███▊      | 119/315 [00:51<00:57,  3.42it/s] 38%|███▊      | 120/315 [00:51<00:57,  3.40it/s] 38%|███▊      | 121/315 [00:52<00:56,  3.41it/s] 39%|███▊      | 122/315 [00:52<00:56,  3.41it/s] 39%|███▉      | 123/315 [00:52<00:57,  3.35it/s] 39%|███▉      | 124/315 [00:53<00:56,  3.37it/s] 40%|███▉      | 125/315 [00:53<00:56,  3.39it/s] 40%|████      | 126/315 [00:53<00:50,  3.72it/s][INFO|trainer.py:2140] 2023-08-29 02:19:33,270 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:19:33,270 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 02:19:33,270 >>   Batch size = 8
{'eval_loss': 1.0422061681747437, 'eval_runtime': 9.4138, 'eval_samples_per_second': 371.478, 'eval_steps_per_second': 46.528, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.10it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.31it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.58it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.91it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.22it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.03it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.64it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.42it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.42it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.41it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.39it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.49it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.56it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.51it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.54it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.51it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.29it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.34it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.32it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.26it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.37it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.52it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.44it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.50it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.41it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.28it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.18it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.28it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.26it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.32it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.43it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.41it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.43it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.40it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.20it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.15it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.22it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.24it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.39it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.41it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.44it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.41it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.32it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.16it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.12it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.21it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.22it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.35it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.45it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.41it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.53it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.35it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.24it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.24it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.22it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.31it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.34it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.25it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.36it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.41it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.38it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.25it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.23it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.27it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.21it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.35it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.33it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.29it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.37it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.33it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.34it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.27it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.22it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.35it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.35it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.33it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.36it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.29it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.37it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.35it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.27it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.22it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.27it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.33it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.35it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.37it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.45it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.45it/s][A 40%|████      | 126/315 [01:02<00:50,  3.72it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:19:42,742 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-126
[INFO|configuration_utils.py:351] 2023-08-29 02:19:42,768 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-126/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:19:45,140 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-126/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:19:45,170 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:19:45,178 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-126/special_tokens_map.json
 40%|████      | 127/315 [01:10<16:26,  5.25s/it] 41%|████      | 128/315 [01:10<11:43,  3.76s/it] 41%|████      | 129/315 [01:10<08:26,  2.72s/it] 41%|████▏     | 130/315 [01:11<06:08,  1.99s/it] 42%|████▏     | 131/315 [01:11<04:32,  1.48s/it] 42%|████▏     | 132/315 [01:11<03:25,  1.12s/it] 42%|████▏     | 133/315 [01:12<02:39,  1.14it/s] 43%|████▎     | 134/315 [01:12<02:06,  1.43it/s] 43%|████▎     | 135/315 [01:12<01:43,  1.73it/s] 43%|████▎     | 136/315 [01:13<01:27,  2.04it/s] 43%|████▎     | 137/315 [01:13<01:16,  2.32it/s] 44%|████▍     | 138/315 [01:13<01:08,  2.57it/s] 44%|████▍     | 139/315 [01:13<01:03,  2.77it/s] 44%|████▍     | 140/315 [01:14<00:59,  2.94it/s] 45%|████▍     | 141/315 [01:14<00:56,  3.07it/s] 45%|████▌     | 142/315 [01:14<00:54,  3.17it/s] 45%|████▌     | 143/315 [01:15<00:52,  3.25it/s] 46%|████▌     | 144/315 [01:15<00:51,  3.30it/s] 46%|████▌     | 145/315 [01:15<00:50,  3.34it/s] 46%|████▋     | 146/315 [01:15<00:50,  3.36it/s] 47%|████▋     | 147/315 [01:16<00:49,  3.39it/s] 47%|████▋     | 148/315 [01:16<00:49,  3.40it/s] 47%|████▋     | 149/315 [01:16<00:48,  3.41it/s] 48%|████▊     | 150/315 [01:17<00:48,  3.40it/s] 48%|████▊     | 151/315 [01:17<00:48,  3.41it/s] 48%|████▊     | 152/315 [01:17<00:47,  3.41it/s] 49%|████▊     | 153/315 [01:17<00:47,  3.42it/s] 49%|████▉     | 154/315 [01:18<00:47,  3.42it/s] 49%|████▉     | 155/315 [01:18<00:46,  3.42it/s] 50%|████▉     | 156/315 [01:18<00:46,  3.42it/s] 50%|████▉     | 157/315 [01:19<00:46,  3.42it/s] 50%|█████     | 158/315 [01:19<00:45,  3.42it/s] 50%|█████     | 159/315 [01:19<00:45,  3.42it/s] 51%|█████     | 160/315 [01:20<00:45,  3.42it/s] 51%|█████     | 161/315 [01:20<00:45,  3.41it/s] 51%|█████▏    | 162/315 [01:20<00:44,  3.41it/s] 52%|█████▏    | 163/315 [01:20<00:44,  3.42it/s] 52%|█████▏    | 164/315 [01:21<00:44,  3.42it/s] 52%|█████▏    | 165/315 [01:21<00:43,  3.42it/s] 53%|█████▎    | 166/315 [01:21<00:43,  3.42it/s] 53%|█████▎    | 167/315 [01:22<00:43,  3.42it/s] 53%|█████▎    | 168/315 [01:22<00:42,  3.42it/s] 54%|█████▎    | 169/315 [01:22<00:42,  3.42it/s] 54%|█████▍    | 170/315 [01:22<00:42,  3.42it/s] 54%|█████▍    | 171/315 [01:23<00:42,  3.42it/s] 55%|█████▍    | 172/315 [01:23<00:41,  3.41it/s] 55%|█████▍    | 173/315 [01:23<00:41,  3.41it/s] 55%|█████▌    | 174/315 [01:24<00:41,  3.41it/s] 56%|█████▌    | 175/315 [01:24<00:40,  3.42it/s] 56%|█████▌    | 176/315 [01:24<00:40,  3.42it/s] 56%|█████▌    | 177/315 [01:25<00:40,  3.42it/s] 57%|█████▋    | 178/315 [01:25<00:40,  3.42it/s] 57%|█████▋    | 179/315 [01:25<00:39,  3.42it/s] 57%|█████▋    | 180/315 [01:25<00:39,  3.42it/s] 57%|█████▋    | 181/315 [01:26<00:39,  3.42it/s] 58%|█████▊    | 182/315 [01:26<00:38,  3.42it/s] 58%|█████▊    | 183/315 [01:26<00:38,  3.42it/s] 58%|█████▊    | 184/315 [01:27<00:38,  3.42it/s] 59%|█████▊    | 185/315 [01:27<00:38,  3.42it/s] 59%|█████▉    | 186/315 [01:27<00:37,  3.42it/s] 59%|█████▉    | 187/315 [01:27<00:37,  3.42it/s] 60%|█████▉    | 188/315 [01:28<00:37,  3.41it/s] 60%|██████    | 189/315 [01:28<00:33,  3.73it/s][INFO|trainer.py:2140] 2023-08-29 02:20:08,174 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:20:08,174 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 02:20:08,174 >>   Batch size = 8
{'eval_loss': 1.0565581321716309, 'eval_runtime': 9.446, 'eval_samples_per_second': 370.209, 'eval_steps_per_second': 46.369, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.37it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.26it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.48it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.80it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.43it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.16it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.98it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.40it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.27it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.29it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.40it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.38it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.35it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.46it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.52it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.47it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.24it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.12it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.19it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.22it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.27it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.33it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.40it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.41it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.39it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.24it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.15it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.16it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.21it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.23it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.29it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.35it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.33it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.15it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.09it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.20it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.18it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.24it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.30it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.36it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.43it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.41it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.31it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.16it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.19it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.22it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.22it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.29it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.36it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.37it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.35it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.31it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.18it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.13it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.24it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.23it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.28it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.40it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.38it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.39it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.30it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.26it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.22it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.23it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.23it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.28it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.36it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.37it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.33it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.33it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.22it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.21it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.15it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.20it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.26it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.28it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.40it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.35it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.14it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.22it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.19it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.26it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.28it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.29it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.29it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.44it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.44it/s][A 60%|██████    | 189/315 [01:37<00:33,  3.73it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:20:17,647 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-189
[INFO|configuration_utils.py:351] 2023-08-29 02:20:17,662 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-189/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:20:19,980 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-189/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:20:19,995 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-189/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:20:20,005 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-189/special_tokens_map.json
 60%|██████    | 190/315 [01:45<10:54,  5.24s/it] 61%|██████    | 191/315 [01:45<07:45,  3.75s/it] 61%|██████    | 192/315 [01:45<05:33,  2.72s/it] 61%|██████▏   | 193/315 [01:46<04:02,  1.99s/it] 62%|██████▏   | 194/315 [01:46<02:58,  1.48s/it] 62%|██████▏   | 195/315 [01:46<02:14,  1.12s/it] 62%|██████▏   | 196/315 [01:47<01:43,  1.15it/s] 63%|██████▎   | 197/315 [01:47<01:22,  1.43it/s] 63%|██████▎   | 198/315 [01:47<01:07,  1.74it/s] 63%|██████▎   | 199/315 [01:47<00:56,  2.04it/s] 63%|██████▎   | 200/315 [01:48<00:49,  2.32it/s] 64%|██████▍   | 201/315 [01:48<00:44,  2.57it/s] 64%|██████▍   | 202/315 [01:48<00:40,  2.78it/s] 64%|██████▍   | 203/315 [01:49<00:38,  2.95it/s] 65%|██████▍   | 204/315 [01:49<00:36,  3.08it/s] 65%|██████▌   | 205/315 [01:49<00:34,  3.17it/s] 65%|██████▌   | 206/315 [01:49<00:33,  3.25it/s] 66%|██████▌   | 207/315 [01:50<00:32,  3.30it/s] 66%|██████▌   | 208/315 [01:50<00:32,  3.34it/s] 66%|██████▋   | 209/315 [01:50<00:31,  3.37it/s] 67%|██████▋   | 210/315 [01:51<00:31,  3.39it/s] 67%|██████▋   | 211/315 [01:51<00:30,  3.40it/s] 67%|██████▋   | 212/315 [01:51<00:30,  3.41it/s] 68%|██████▊   | 213/315 [01:51<00:30,  3.40it/s] 68%|██████▊   | 214/315 [01:52<00:29,  3.41it/s] 68%|██████▊   | 215/315 [01:52<00:29,  3.42it/s] 69%|██████▊   | 216/315 [01:52<00:29,  3.33it/s] 69%|██████▉   | 217/315 [01:53<00:29,  3.36it/s] 69%|██████▉   | 218/315 [01:53<00:28,  3.38it/s] 70%|██████▉   | 219/315 [01:53<00:28,  3.39it/s] 70%|██████▉   | 220/315 [01:54<00:27,  3.40it/s] 70%|███████   | 221/315 [01:54<00:27,  3.41it/s] 70%|███████   | 222/315 [01:54<00:27,  3.41it/s] 71%|███████   | 223/315 [01:54<00:26,  3.42it/s] 71%|███████   | 224/315 [01:55<00:26,  3.41it/s] 71%|███████▏  | 225/315 [01:55<00:26,  3.42it/s] 72%|███████▏  | 226/315 [01:55<00:26,  3.42it/s] 72%|███████▏  | 227/315 [01:56<00:25,  3.42it/s] 72%|███████▏  | 228/315 [01:56<00:25,  3.43it/s] 73%|███████▎  | 229/315 [01:56<00:25,  3.42it/s] 73%|███████▎  | 230/315 [01:56<00:24,  3.43it/s] 73%|███████▎  | 231/315 [01:57<00:24,  3.42it/s] 74%|███████▎  | 232/315 [01:57<00:24,  3.42it/s] 74%|███████▍  | 233/315 [01:57<00:23,  3.42it/s] 74%|███████▍  | 234/315 [01:58<00:23,  3.43it/s] 75%|███████▍  | 235/315 [01:58<00:23,  3.43it/s] 75%|███████▍  | 236/315 [01:58<00:23,  3.42it/s] 75%|███████▌  | 237/315 [01:59<00:22,  3.41it/s] 76%|███████▌  | 238/315 [01:59<00:22,  3.42it/s] 76%|███████▌  | 239/315 [01:59<00:22,  3.42it/s] 76%|███████▌  | 240/315 [01:59<00:21,  3.43it/s] 77%|███████▋  | 241/315 [02:00<00:21,  3.42it/s] 77%|███████▋  | 242/315 [02:00<00:21,  3.42it/s] 77%|███████▋  | 243/315 [02:00<00:21,  3.42it/s] 77%|███████▋  | 244/315 [02:01<00:20,  3.42it/s] 78%|███████▊  | 245/315 [02:01<00:20,  3.42it/s] 78%|███████▊  | 246/315 [02:01<00:20,  3.42it/s] 78%|███████▊  | 247/315 [02:01<00:19,  3.42it/s] 79%|███████▊  | 248/315 [02:02<00:19,  3.42it/s] 79%|███████▉  | 249/315 [02:02<00:19,  3.42it/s] 79%|███████▉  | 250/315 [02:02<00:19,  3.42it/s] 80%|███████▉  | 251/315 [02:03<00:18,  3.42it/s] 80%|████████  | 252/315 [02:03<00:16,  3.75it/s][INFO|trainer.py:2140] 2023-08-29 02:20:43,045 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:20:43,045 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 02:20:43,045 >>   Batch size = 8
{'eval_loss': 1.0719112157821655, 'eval_runtime': 9.4554, 'eval_samples_per_second': 369.843, 'eval_steps_per_second': 46.323, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.36it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.25it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.49it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.71it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.39it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.13it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.90it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.33it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.24it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.30it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.32it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.46it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.41it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.45it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.54it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.48it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.21it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.09it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.19it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.13it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.30it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.43it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.37it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.49it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.45it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.24it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.19it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.17it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.16it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.23it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.32it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.37it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.45it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.43it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.31it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.28it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.23it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.17it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.22it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.38it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.38it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.47it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.37it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.31it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.25it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.25it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.10it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.27it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.34it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.38it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.46it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.28it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.18it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.25it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.20it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.24it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.22it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.31it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.37it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.39it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.32it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.19it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.22it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.22it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.17it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.33it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.35it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.38it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.35it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.28it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.27it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.21it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.23it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.14it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.20it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.31it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.34it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.36it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.32it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.22it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.26it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.20it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.24it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.24it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.28it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.36it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.47it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.47it/s][A 80%|████████  | 252/315 [02:12<00:16,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:20:52,535 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-252
[INFO|configuration_utils.py:351] 2023-08-29 02:20:52,552 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-252/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:20:54,884 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-252/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:20:54,903 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-252/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:20:54,919 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-252/special_tokens_map.json
 80%|████████  | 253/315 [02:19<05:19,  5.15s/it] 81%|████████  | 254/315 [02:20<03:45,  3.69s/it] 81%|████████  | 255/315 [02:20<02:40,  2.67s/it] 81%|████████▏ | 256/315 [02:20<01:55,  1.96s/it] 82%|████████▏ | 257/315 [02:21<01:24,  1.46s/it] 82%|████████▏ | 258/315 [02:21<01:03,  1.11s/it] 82%|████████▏ | 259/315 [02:21<00:48,  1.16it/s] 83%|████████▎ | 260/315 [02:21<00:38,  1.45it/s] 83%|████████▎ | 261/315 [02:22<00:30,  1.75it/s] 83%|████████▎ | 262/315 [02:22<00:25,  2.05it/s] 83%|████████▎ | 263/315 [02:22<00:22,  2.33it/s] 84%|████████▍ | 264/315 [02:23<00:19,  2.58it/s] 84%|████████▍ | 265/315 [02:23<00:17,  2.78it/s] 84%|████████▍ | 266/315 [02:23<00:16,  2.95it/s] 85%|████████▍ | 267/315 [02:23<00:15,  3.08it/s] 85%|████████▌ | 268/315 [02:24<00:14,  3.18it/s] 85%|████████▌ | 269/315 [02:24<00:14,  3.25it/s] 86%|████████▌ | 270/315 [02:24<00:13,  3.30it/s] 86%|████████▌ | 271/315 [02:25<00:13,  3.34it/s] 86%|████████▋ | 272/315 [02:25<00:12,  3.37it/s] 87%|████████▋ | 273/315 [02:25<00:12,  3.38it/s] 87%|████████▋ | 274/315 [02:25<00:12,  3.40it/s] 87%|████████▋ | 275/315 [02:26<00:11,  3.41it/s] 88%|████████▊ | 276/315 [02:26<00:11,  3.41it/s] 88%|████████▊ | 277/315 [02:26<00:11,  3.42it/s] 88%|████████▊ | 278/315 [02:27<00:10,  3.42it/s] 89%|████████▊ | 279/315 [02:27<00:10,  3.42it/s] 89%|████████▉ | 280/315 [02:27<00:10,  3.42it/s] 89%|████████▉ | 281/315 [02:28<00:09,  3.42it/s] 90%|████████▉ | 282/315 [02:28<00:09,  3.42it/s] 90%|████████▉ | 283/315 [02:28<00:09,  3.42it/s] 90%|█████████ | 284/315 [02:28<00:09,  3.43it/s] 90%|█████████ | 285/315 [02:29<00:08,  3.43it/s] 91%|█████████ | 286/315 [02:29<00:08,  3.43it/s] 91%|█████████ | 287/315 [02:29<00:08,  3.42it/s] 91%|█████████▏| 288/315 [02:30<00:07,  3.42it/s] 92%|█████████▏| 289/315 [02:30<00:07,  3.42it/s] 92%|█████████▏| 290/315 [02:30<00:07,  3.42it/s] 92%|█████████▏| 291/315 [02:30<00:07,  3.42it/s] 93%|█████████▎| 292/315 [02:31<00:06,  3.42it/s] 93%|█████████▎| 293/315 [02:31<00:06,  3.43it/s] 93%|█████████▎| 294/315 [02:31<00:06,  3.42it/s] 94%|█████████▎| 295/315 [02:32<00:05,  3.42it/s] 94%|█████████▍| 296/315 [02:32<00:05,  3.42it/s] 94%|█████████▍| 297/315 [02:32<00:05,  3.43it/s] 95%|█████████▍| 298/315 [02:32<00:04,  3.42it/s] 95%|█████████▍| 299/315 [02:33<00:04,  3.42it/s] 95%|█████████▌| 300/315 [02:33<00:04,  3.42it/s] 96%|█████████▌| 301/315 [02:33<00:04,  3.42it/s] 96%|█████████▌| 302/315 [02:34<00:03,  3.42it/s] 96%|█████████▌| 303/315 [02:34<00:03,  3.43it/s] 97%|█████████▋| 304/315 [02:34<00:03,  3.42it/s] 97%|█████████▋| 305/315 [02:35<00:02,  3.42it/s] 97%|█████████▋| 306/315 [02:35<00:02,  3.42it/s] 97%|█████████▋| 307/315 [02:35<00:02,  3.42it/s] 98%|█████████▊| 308/315 [02:35<00:02,  3.42it/s] 98%|█████████▊| 309/315 [02:36<00:01,  3.41it/s] 98%|█████████▊| 310/315 [02:36<00:01,  3.42it/s] 99%|█████████▊| 311/315 [02:36<00:01,  3.42it/s] 99%|█████████▉| 312/315 [02:37<00:00,  3.42it/s] 99%|█████████▉| 313/315 [02:37<00:00,  3.42it/s]100%|█████████▉| 314/315 [02:37<00:00,  3.42it/s]100%|██████████| 315/315 [02:37<00:00,  3.75it/s][INFO|trainer.py:2140] 2023-08-29 02:21:17,600 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:21:17,600 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 02:21:17,600 >>   Batch size = 8
{'eval_loss': 1.0848435163497925, 'eval_runtime': 9.4534, 'eval_samples_per_second': 369.921, 'eval_steps_per_second': 46.333, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.93it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.16it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.51it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.81it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.09it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.82it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.29it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.28it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.34it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.46it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.46it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.49it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.54it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.51it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.33it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.14it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.17it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.25it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.29it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.37it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.46it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.45it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.47it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.36it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.21it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.13it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.24it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.30it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.30it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.39it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.46it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.45it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.36it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.26it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.14it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.23it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.32it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.35it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.28it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.37it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.42it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.36it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.29it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.15it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.25it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.30it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.33it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.35it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.27it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.35it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.31it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.32it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.25it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.13it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.25it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.31it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.36it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.38it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.27it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.32it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.26it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.26it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.17it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.21it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.32it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.35it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.38it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.35it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.27it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.26it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.21it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.29it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.17it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.27it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.35it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.37it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.40it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.24it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.24it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.26it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.21it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.27it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.19it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.27it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.22it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.38it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.38it/s][A100%|██████████| 315/315 [02:47<00:00,  3.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:21:27,065 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-315
[INFO|configuration_utils.py:351] 2023-08-29 02:21:27,078 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-315/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:21:29,335 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-315/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:21:29,358 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-315/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:21:29,372 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-315/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:21:33,982 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:21:33,985 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-63 (score: 1.0422061681747437).
                                                 100%|██████████| 315/315 [02:55<00:00,  3.75it/s]100%|██████████| 315/315 [02:55<00:00,  1.79it/s]
[INFO|trainer.py:1894] 2023-08-29 02:21:35,639 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 02:21:35,653 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:21:37,992 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:21:38,007 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:21:38,018 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:21:38,201 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:38,201 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:38,201 >>   train_loss               =     0.4719
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:38,201 >>   train_runtime            = 0:02:55.90
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:38,201 >>   train_samples            =       4008
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:38,201 >>   train_samples_per_second =    113.926
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:38,201 >>   train_steps_per_second   =      1.791
{'eval_loss': 1.0913804769515991, 'eval_runtime': 9.4522, 'eval_samples_per_second': 369.965, 'eval_steps_per_second': 46.338, 'epoch': 5.0}
{'train_runtime': 175.9042, 'train_samples_per_second': 113.926, 'train_steps_per_second': 1.791, 'train_loss': 0.47191583542596727, 'epoch': 5.0}
08/29/2023 02:21:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:21:38,244 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:21:38,244 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 02:21:38,244 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 58.10it/s]  3%|▎         | 12/438 [00:00<00:08, 51.12it/s]  4%|▍         | 18/438 [00:00<00:08, 49.17it/s]  5%|▌         | 23/438 [00:00<00:08, 48.37it/s]  6%|▋         | 28/438 [00:00<00:08, 47.81it/s]  8%|▊         | 33/438 [00:00<00:08, 47.54it/s]  9%|▊         | 38/438 [00:00<00:08, 47.36it/s] 10%|▉         | 43/438 [00:00<00:08, 47.02it/s] 11%|█         | 48/438 [00:01<00:08, 46.58it/s] 12%|█▏        | 53/438 [00:01<00:08, 46.62it/s] 13%|█▎        | 58/438 [00:01<00:08, 46.64it/s] 14%|█▍        | 63/438 [00:01<00:08, 46.72it/s] 16%|█▌        | 68/438 [00:01<00:07, 46.75it/s] 17%|█▋        | 73/438 [00:01<00:07, 46.75it/s] 18%|█▊        | 78/438 [00:01<00:07, 46.82it/s] 19%|█▉        | 83/438 [00:01<00:07, 46.87it/s] 20%|██        | 88/438 [00:01<00:07, 46.78it/s] 21%|██        | 93/438 [00:01<00:07, 46.57it/s] 22%|██▏       | 98/438 [00:02<00:07, 46.48it/s] 24%|██▎       | 103/438 [00:02<00:07, 46.55it/s] 25%|██▍       | 108/438 [00:02<00:07, 46.67it/s] 26%|██▌       | 113/438 [00:02<00:06, 46.71it/s] 27%|██▋       | 118/438 [00:02<00:06, 46.70it/s] 28%|██▊       | 123/438 [00:02<00:06, 46.75it/s] 29%|██▉       | 128/438 [00:02<00:06, 46.78it/s] 30%|███       | 133/438 [00:02<00:06, 46.72it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.61it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.47it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.56it/s] 35%|███▍      | 153/438 [00:03<00:06, 46.58it/s] 36%|███▌      | 158/438 [00:03<00:06, 46.63it/s] 37%|███▋      | 163/438 [00:03<00:05, 46.68it/s] 38%|███▊      | 168/438 [00:03<00:05, 46.75it/s] 39%|███▉      | 173/438 [00:03<00:05, 46.69it/s] 41%|████      | 178/438 [00:03<00:05, 46.74it/s] 42%|████▏     | 183/438 [00:03<00:05, 46.65it/s] 43%|████▎     | 188/438 [00:04<00:05, 46.47it/s] 44%|████▍     | 193/438 [00:04<00:05, 46.52it/s] 45%|████▌     | 198/438 [00:04<00:05, 46.56it/s] 46%|████▋     | 203/438 [00:04<00:05, 46.62it/s] 47%|████▋     | 208/438 [00:04<00:04, 46.64it/s] 49%|████▊     | 213/438 [00:04<00:04, 46.60it/s] 50%|████▉     | 218/438 [00:04<00:04, 46.63it/s] 51%|█████     | 223/438 [00:04<00:04, 46.70it/s] 52%|█████▏    | 228/438 [00:04<00:04, 46.61it/s] 53%|█████▎    | 233/438 [00:04<00:04, 46.56it/s] 54%|█████▍    | 238/438 [00:05<00:04, 46.48it/s] 55%|█████▌    | 243/438 [00:05<00:04, 46.55it/s] 57%|█████▋    | 248/438 [00:05<00:04, 46.57it/s] 58%|█████▊    | 253/438 [00:05<00:03, 46.60it/s] 59%|█████▉    | 258/438 [00:05<00:03, 46.66it/s] 60%|██████    | 263/438 [00:05<00:03, 46.62it/s] 61%|██████    | 268/438 [00:05<00:03, 46.57it/s] 62%|██████▏   | 273/438 [00:05<00:03, 46.66it/s] 63%|██████▎   | 278/438 [00:05<00:03, 46.59it/s] 65%|██████▍   | 283/438 [00:06<00:03, 46.58it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.58it/s] 67%|██████▋   | 293/438 [00:06<00:03, 45.13it/s] 68%|██████▊   | 298/438 [00:06<00:03, 45.69it/s] 69%|██████▉   | 303/438 [00:06<00:02, 46.01it/s] 70%|███████   | 308/438 [00:06<00:02, 46.28it/s] 71%|███████▏  | 313/438 [00:06<00:02, 46.39it/s] 73%|███████▎  | 318/438 [00:06<00:02, 46.50it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.47it/s] 75%|███████▍  | 328/438 [00:07<00:02, 46.51it/s] 76%|███████▌  | 333/438 [00:07<00:02, 46.48it/s] 77%|███████▋  | 338/438 [00:07<00:02, 46.40it/s] 78%|███████▊  | 343/438 [00:07<00:02, 46.38it/s] 79%|███████▉  | 348/438 [00:07<00:01, 46.51it/s] 81%|████████  | 353/438 [00:07<00:01, 46.58it/s] 82%|████████▏ | 358/438 [00:07<00:01, 46.66it/s] 83%|████████▎ | 363/438 [00:07<00:01, 46.60it/s] 84%|████████▍ | 368/438 [00:07<00:01, 46.60it/s] 85%|████████▌ | 373/438 [00:07<00:01, 46.57it/s] 86%|████████▋ | 378/438 [00:08<00:01, 46.60it/s] 87%|████████▋ | 383/438 [00:08<00:01, 46.51it/s] 89%|████████▊ | 388/438 [00:08<00:01, 46.52it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.56it/s] 91%|█████████ | 398/438 [00:08<00:00, 46.62it/s] 92%|█████████▏| 403/438 [00:08<00:00, 46.55it/s] 93%|█████████▎| 408/438 [00:08<00:00, 46.57it/s] 94%|█████████▍| 413/438 [00:08<00:00, 46.57it/s] 95%|█████████▌| 418/438 [00:08<00:00, 46.54it/s] 97%|█████████▋| 423/438 [00:09<00:00, 46.51it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.50it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.48it/s]100%|██████████| 438/438 [00:09<00:00, 46.58it/s]100%|██████████| 438/438 [00:09<00:00, 46.70it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:21:47,649 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:47,650 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:47,650 >>   eval_loss               =     1.0422
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:47,650 >>   eval_runtime            = 0:00:09.40
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:47,650 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:47,650 >>   eval_samples_per_second =    371.802
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:47,650 >>   eval_steps_per_second   =     46.568
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:21:47,650 >>   perplexity              =     2.8355
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:54,280 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:54,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:54,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:54,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:54,285 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:21:54,911 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:21:54,913 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:21:55,514 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:21:56,552 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:21:56,552 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:59,384 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:59,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:59,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:59,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:21:59,388 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:22:00,024 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:22:00,025 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:22:00,593 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:22:00,743 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:22:00,743 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-126
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-252
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-189
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-63
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-315
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.34it/s]Extractor Predicting: 4it [00:02,  1.33it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.29it/s]Extractor Predicting: 8it [00:06,  1.29it/s]Extractor Predicting: 9it [00:06,  1.27it/s]Extractor Predicting: 10it [00:07,  1.27it/s]Extractor Predicting: 11it [00:08,  1.26it/s]Extractor Predicting: 12it [00:09,  1.28it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:13,  1.30it/s]Extractor Predicting: 18it [00:13,  1.31it/s]Extractor Predicting: 19it [00:14,  1.29it/s]Extractor Predicting: 20it [00:15,  1.27it/s]Extractor Predicting: 21it [00:16,  1.27it/s]Extractor Predicting: 22it [00:17,  1.25it/s]Extractor Predicting: 23it [00:17,  1.27it/s]Extractor Predicting: 24it [00:18,  1.31it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.33it/s]Extractor Predicting: 27it [00:20,  1.35it/s]Extractor Predicting: 28it [00:21,  1.35it/s]Extractor Predicting: 29it [00:22,  1.34it/s]Extractor Predicting: 30it [00:23,  1.35it/s]Extractor Predicting: 31it [00:23,  1.33it/s]Extractor Predicting: 32it [00:24,  1.31it/s]Extractor Predicting: 33it [00:25,  1.32it/s]Extractor Predicting: 34it [00:26,  1.29it/s]Extractor Predicting: 35it [00:26,  1.31it/s]Extractor Predicting: 36it [00:27,  1.29it/s]Extractor Predicting: 37it [00:28,  1.28it/s]Extractor Predicting: 38it [00:29,  1.26it/s]Extractor Predicting: 39it [00:30,  1.28it/s]Extractor Predicting: 40it [00:30,  1.27it/s]Extractor Predicting: 41it [00:31,  1.24it/s]Extractor Predicting: 42it [00:32,  1.24it/s]Extractor Predicting: 43it [00:33,  1.27it/s]Extractor Predicting: 44it [00:34,  1.26it/s]Extractor Predicting: 45it [00:34,  1.27it/s]Extractor Predicting: 46it [00:35,  1.23it/s]Extractor Predicting: 47it [00:36,  1.23it/s]Extractor Predicting: 48it [00:37,  1.23it/s]Extractor Predicting: 49it [00:38,  1.22it/s]Extractor Predicting: 50it [00:39,  1.21it/s]Extractor Predicting: 51it [00:39,  1.21it/s]Extractor Predicting: 52it [00:40,  1.20it/s]Extractor Predicting: 53it [00:41,  1.22it/s]Extractor Predicting: 54it [00:42,  1.22it/s]Extractor Predicting: 55it [00:43,  1.23it/s]Extractor Predicting: 56it [00:43,  1.24it/s]Extractor Predicting: 57it [00:44,  1.24it/s]Extractor Predicting: 58it [00:45,  1.24it/s]Extractor Predicting: 59it [00:46,  1.25it/s]Extractor Predicting: 60it [00:47,  1.22it/s]Extractor Predicting: 61it [00:48,  1.20it/s]Extractor Predicting: 62it [00:48,  1.19it/s]Extractor Predicting: 63it [00:49,  1.18it/s]Extractor Predicting: 64it [00:50,  1.19it/s]Extractor Predicting: 65it [00:51,  1.18it/s]Extractor Predicting: 66it [00:52,  1.17it/s]Extractor Predicting: 67it [00:53,  1.16it/s]Extractor Predicting: 68it [00:54,  1.15it/s]Extractor Predicting: 69it [00:54,  1.15it/s]Extractor Predicting: 70it [00:55,  1.15it/s]Extractor Predicting: 71it [00:56,  1.14it/s]Extractor Predicting: 72it [00:57,  1.14it/s]Extractor Predicting: 73it [00:58,  1.13it/s]Extractor Predicting: 74it [00:59,  1.14it/s]Extractor Predicting: 75it [01:00,  1.13it/s]Extractor Predicting: 76it [01:01,  1.15it/s]Extractor Predicting: 77it [01:01,  1.15it/s]Extractor Predicting: 78it [01:02,  1.14it/s]Extractor Predicting: 79it [01:03,  1.16it/s]Extractor Predicting: 80it [01:04,  1.14it/s]Extractor Predicting: 81it [01:05,  1.12it/s]Extractor Predicting: 82it [01:06,  1.13it/s]Extractor Predicting: 83it [01:07,  1.16it/s]Extractor Predicting: 84it [01:08,  1.16it/s]Extractor Predicting: 85it [01:08,  1.17it/s]Extractor Predicting: 86it [01:09,  1.17it/s]Extractor Predicting: 87it [01:10,  1.14it/s]Extractor Predicting: 88it [01:11,  1.15it/s]Extractor Predicting: 89it [01:12,  1.18it/s]Extractor Predicting: 90it [01:13,  1.19it/s]Extractor Predicting: 91it [01:13,  1.21it/s]Extractor Predicting: 92it [01:14,  1.18it/s]Extractor Predicting: 93it [01:15,  1.21it/s]Extractor Predicting: 94it [01:16,  1.22it/s]Extractor Predicting: 95it [01:17,  1.21it/s]Extractor Predicting: 96it [01:18,  1.20it/s]Extractor Predicting: 97it [01:18,  1.21it/s]Extractor Predicting: 98it [01:19,  1.20it/s]Extractor Predicting: 99it [01:20,  1.21it/s]Extractor Predicting: 100it [01:21,  1.22it/s]Extractor Predicting: 101it [01:22,  1.23it/s]Extractor Predicting: 102it [01:23,  1.20it/s]Extractor Predicting: 103it [01:24,  1.17it/s]Extractor Predicting: 104it [01:24,  1.18it/s]Extractor Predicting: 105it [01:25,  1.19it/s]Extractor Predicting: 106it [01:26,  1.20it/s]Extractor Predicting: 107it [01:27,  1.18it/s]Extractor Predicting: 108it [01:28,  1.19it/s]Extractor Predicting: 109it [01:29,  1.19it/s]Extractor Predicting: 110it [01:29,  1.17it/s]Extractor Predicting: 111it [01:30,  1.17it/s]Extractor Predicting: 112it [01:31,  1.17it/s]Extractor Predicting: 113it [01:32,  1.18it/s]Extractor Predicting: 114it [01:33,  1.18it/s]Extractor Predicting: 115it [01:34,  1.19it/s]Extractor Predicting: 116it [01:34,  1.18it/s]Extractor Predicting: 117it [01:35,  1.21it/s]Extractor Predicting: 118it [01:36,  1.23it/s]Extractor Predicting: 119it [01:37,  1.26it/s]Extractor Predicting: 120it [01:38,  1.27it/s]Extractor Predicting: 121it [01:38,  1.25it/s]Extractor Predicting: 122it [01:39,  1.29it/s]Extractor Predicting: 123it [01:40,  1.31it/s]Extractor Predicting: 124it [01:41,  1.29it/s]Extractor Predicting: 125it [01:41,  1.30it/s]Extractor Predicting: 126it [01:42,  1.32it/s]Extractor Predicting: 127it [01:43,  1.25it/s]Extractor Predicting: 128it [01:44,  1.26it/s]Extractor Predicting: 129it [01:45,  1.30it/s]Extractor Predicting: 130it [01:45,  1.27it/s]Extractor Predicting: 131it [01:46,  1.28it/s]Extractor Predicting: 132it [01:47,  1.33it/s]Extractor Predicting: 133it [01:48,  1.34it/s]Extractor Predicting: 134it [01:48,  1.28it/s]Extractor Predicting: 135it [01:49,  1.30it/s]Extractor Predicting: 136it [01:50,  1.33it/s]Extractor Predicting: 137it [01:51,  1.34it/s]Extractor Predicting: 138it [01:51,  1.33it/s]Extractor Predicting: 139it [01:52,  1.34it/s]Extractor Predicting: 140it [01:53,  1.37it/s]Extractor Predicting: 141it [01:53,  1.38it/s]Extractor Predicting: 142it [01:54,  1.40it/s]Extractor Predicting: 143it [01:55,  1.37it/s]Extractor Predicting: 144it [01:56,  1.35it/s]Extractor Predicting: 145it [01:56,  1.50it/s]Extractor Predicting: 145it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:05,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:05,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:05,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:05,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:05,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:24:06,077 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:24:06,078 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:24:06,767 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:24:07,809 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:24:07,809 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:10,355 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:10,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:10,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:10,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:24:10,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:24:10,736 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:24:10,738 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:24:11,417 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:24:11,578 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:24:11,578 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5696757230499562,
  "recall": 0.37174721189591076,
  "score": 0.44990482782488317,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.24it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 6it [00:04,  1.21it/s]Extractor Predicting: 7it [00:05,  1.22it/s]Extractor Predicting: 8it [00:06,  1.22it/s]Extractor Predicting: 9it [00:07,  1.21it/s]Extractor Predicting: 10it [00:08,  1.21it/s]Extractor Predicting: 11it [00:08,  1.22it/s]Extractor Predicting: 12it [00:09,  1.25it/s]Extractor Predicting: 13it [00:10,  1.23it/s]Extractor Predicting: 14it [00:11,  1.24it/s]Extractor Predicting: 15it [00:12,  1.26it/s]Extractor Predicting: 16it [00:12,  1.27it/s]Extractor Predicting: 17it [00:13,  1.24it/s]Extractor Predicting: 18it [00:14,  1.24it/s]Extractor Predicting: 19it [00:15,  1.22it/s]Extractor Predicting: 20it [00:16,  1.22it/s]Extractor Predicting: 21it [00:17,  1.23it/s]Extractor Predicting: 22it [00:17,  1.19it/s]Extractor Predicting: 23it [00:18,  1.21it/s]Extractor Predicting: 24it [00:19,  1.23it/s]Extractor Predicting: 25it [00:20,  1.25it/s]Extractor Predicting: 26it [00:21,  1.22it/s]Extractor Predicting: 27it [00:22,  1.21it/s]Extractor Predicting: 28it [00:22,  1.21it/s]Extractor Predicting: 29it [00:23,  1.20it/s]Extractor Predicting: 30it [00:24,  1.17it/s]Extractor Predicting: 31it [00:25,  1.18it/s]Extractor Predicting: 32it [00:26,  1.18it/s]Extractor Predicting: 33it [00:27,  1.20it/s]Extractor Predicting: 34it [00:27,  1.21it/s]Extractor Predicting: 35it [00:28,  1.22it/s]Extractor Predicting: 36it [00:29,  1.23it/s]Extractor Predicting: 37it [00:30,  1.28it/s]Extractor Predicting: 38it [00:31,  1.27it/s]Extractor Predicting: 39it [00:31,  1.26it/s]Extractor Predicting: 40it [00:32,  1.27it/s]Extractor Predicting: 41it [00:33,  1.26it/s]Extractor Predicting: 42it [00:34,  1.18it/s]Extractor Predicting: 43it [00:35,  1.19it/s]Extractor Predicting: 44it [00:35,  1.21it/s]Extractor Predicting: 45it [00:36,  1.19it/s]Extractor Predicting: 46it [00:37,  1.23it/s]Extractor Predicting: 47it [00:38,  1.23it/s]Extractor Predicting: 48it [00:39,  1.23it/s]Extractor Predicting: 49it [00:40,  1.23it/s]Extractor Predicting: 50it [00:40,  1.22it/s]Extractor Predicting: 51it [00:41,  1.22it/s]Extractor Predicting: 52it [00:42,  1.23it/s]Extractor Predicting: 53it [00:43,  1.23it/s]Extractor Predicting: 54it [00:44,  1.24it/s]Extractor Predicting: 55it [00:44,  1.22it/s]Extractor Predicting: 56it [00:45,  1.23it/s]Extractor Predicting: 57it [00:46,  1.25it/s]Extractor Predicting: 58it [00:47,  1.27it/s]Extractor Predicting: 59it [00:48,  1.28it/s]Extractor Predicting: 60it [00:48,  1.27it/s]Extractor Predicting: 61it [00:49,  1.26it/s]Extractor Predicting: 62it [00:50,  1.26it/s]Extractor Predicting: 63it [00:51,  1.25it/s]Extractor Predicting: 64it [00:52,  1.25it/s]Extractor Predicting: 65it [00:52,  1.24it/s]Extractor Predicting: 66it [00:53,  1.22it/s]Extractor Predicting: 67it [00:54,  1.25it/s]Extractor Predicting: 68it [00:55,  1.27it/s]Extractor Predicting: 69it [00:55,  1.28it/s]Extractor Predicting: 70it [00:56,  1.27it/s]Extractor Predicting: 71it [00:57,  1.26it/s]Extractor Predicting: 72it [00:58,  1.27it/s]Extractor Predicting: 73it [00:59,  1.23it/s]Extractor Predicting: 74it [01:00,  1.24it/s]Extractor Predicting: 75it [01:00,  1.24it/s]Extractor Predicting: 76it [01:01,  1.26it/s]Extractor Predicting: 77it [01:02,  1.22it/s]Extractor Predicting: 78it [01:03,  1.22it/s]Extractor Predicting: 79it [01:04,  1.23it/s]Extractor Predicting: 80it [01:04,  1.22it/s]Extractor Predicting: 81it [01:05,  1.24it/s]Extractor Predicting: 82it [01:06,  1.22it/s]Extractor Predicting: 83it [01:07,  1.22it/s]Extractor Predicting: 84it [01:08,  1.22it/s]Extractor Predicting: 85it [01:08,  1.24it/s]Extractor Predicting: 86it [01:09,  1.23it/s]Extractor Predicting: 87it [01:10,  1.23it/s]Extractor Predicting: 88it [01:11,  1.23it/s]Extractor Predicting: 89it [01:12,  1.23it/s]Extractor Predicting: 90it [01:13,  1.25it/s]Extractor Predicting: 91it [01:13,  1.24it/s]Extractor Predicting: 92it [01:14,  1.23it/s]Extractor Predicting: 93it [01:15,  1.21it/s]Extractor Predicting: 94it [01:16,  1.23it/s]Extractor Predicting: 95it [01:17,  1.22it/s]Extractor Predicting: 96it [01:17,  1.21it/s]Extractor Predicting: 97it [01:18,  1.23it/s]Extractor Predicting: 98it [01:19,  1.23it/s]Extractor Predicting: 99it [01:20,  1.24it/s]Extractor Predicting: 100it [01:21,  1.25it/s]Extractor Predicting: 101it [01:21,  1.26it/s]Extractor Predicting: 102it [01:22,  1.26it/s]Extractor Predicting: 103it [01:23,  1.25it/s]Extractor Predicting: 104it [01:24,  1.22it/s]Extractor Predicting: 105it [01:25,  1.21it/s]Extractor Predicting: 106it [01:26,  1.21it/s]Extractor Predicting: 107it [01:26,  1.21it/s]Extractor Predicting: 108it [01:27,  1.22it/s]Extractor Predicting: 109it [01:28,  1.21it/s]Extractor Predicting: 110it [01:29,  1.22it/s]Extractor Predicting: 111it [01:30,  1.23it/s]Extractor Predicting: 112it [01:31,  1.20it/s]Extractor Predicting: 113it [01:31,  1.20it/s]Extractor Predicting: 114it [01:32,  1.19it/s]Extractor Predicting: 115it [01:33,  1.17it/s]Extractor Predicting: 116it [01:34,  1.16it/s]Extractor Predicting: 117it [01:35,  1.18it/s]Extractor Predicting: 118it [01:36,  1.19it/s]Extractor Predicting: 119it [01:36,  1.19it/s]Extractor Predicting: 120it [01:37,  1.21it/s]Extractor Predicting: 121it [01:38,  1.23it/s]Extractor Predicting: 122it [01:39,  1.24it/s]Extractor Predicting: 123it [01:40,  1.25it/s]Extractor Predicting: 124it [01:40,  1.24it/s]Extractor Predicting: 125it [01:41,  1.23it/s]Extractor Predicting: 126it [01:42,  1.21it/s]Extractor Predicting: 127it [01:43,  1.21it/s]Extractor Predicting: 128it [01:44,  1.21it/s]Extractor Predicting: 129it [01:45,  1.23it/s]Extractor Predicting: 130it [01:45,  1.23it/s]Extractor Predicting: 131it [01:46,  1.24it/s]Extractor Predicting: 132it [01:47,  1.26it/s]Extractor Predicting: 133it [01:48,  1.23it/s]Extractor Predicting: 134it [01:49,  1.22it/s]Extractor Predicting: 135it [01:49,  1.21it/s]Extractor Predicting: 136it [01:50,  1.21it/s]Extractor Predicting: 137it [01:51,  1.20it/s]Extractor Predicting: 138it [01:52,  1.24it/s]Extractor Predicting: 139it [01:53,  1.23it/s]Extractor Predicting: 140it [01:54,  1.10it/s]Extractor Predicting: 141it [01:55,  1.14it/s]Extractor Predicting: 142it [01:55,  1.17it/s]Extractor Predicting: 143it [01:56,  1.18it/s]Extractor Predicting: 144it [01:57,  1.18it/s]Extractor Predicting: 145it [01:58,  1.20it/s]Extractor Predicting: 146it [01:59,  1.21it/s]Extractor Predicting: 147it [01:59,  1.24it/s]Extractor Predicting: 148it [02:00,  1.22it/s]Extractor Predicting: 149it [02:01,  1.24it/s]Extractor Predicting: 150it [02:02,  1.24it/s]Extractor Predicting: 151it [02:03,  1.27it/s]Extractor Predicting: 152it [02:03,  1.28it/s]Extractor Predicting: 153it [02:04,  1.28it/s]Extractor Predicting: 154it [02:05,  1.29it/s]Extractor Predicting: 155it [02:06,  1.28it/s]Extractor Predicting: 156it [02:07,  1.28it/s]Extractor Predicting: 157it [02:07,  1.26it/s]Extractor Predicting: 158it [02:08,  1.22it/s]Extractor Predicting: 159it [02:09,  1.23it/s]Extractor Predicting: 160it [02:10,  1.26it/s]Extractor Predicting: 161it [02:11,  1.26it/s]Extractor Predicting: 162it [02:11,  1.27it/s]Extractor Predicting: 163it [02:12,  1.28it/s]Extractor Predicting: 164it [02:13,  1.28it/s]Extractor Predicting: 165it [02:14,  1.26it/s]Extractor Predicting: 166it [02:15,  1.25it/s]Extractor Predicting: 167it [02:15,  1.26it/s]Extractor Predicting: 168it [02:16,  1.26it/s]Extractor Predicting: 169it [02:17,  1.28it/s]Extractor Predicting: 170it [02:18,  1.29it/s]Extractor Predicting: 171it [02:18,  1.26it/s]Extractor Predicting: 172it [02:19,  1.25it/s]Extractor Predicting: 173it [02:20,  1.25it/s]Extractor Predicting: 174it [02:21,  1.27it/s]Extractor Predicting: 175it [02:22,  1.29it/s]Extractor Predicting: 176it [02:22,  1.29it/s]Extractor Predicting: 177it [02:23,  1.25it/s]Extractor Predicting: 178it [02:24,  1.27it/s]Extractor Predicting: 179it [02:25,  1.26it/s]Extractor Predicting: 180it [02:26,  1.26it/s]Extractor Predicting: 181it [02:26,  1.26it/s]Extractor Predicting: 182it [02:27,  1.25it/s]Extractor Predicting: 183it [02:28,  1.26it/s]Extractor Predicting: 184it [02:29,  1.26it/s]Extractor Predicting: 185it [02:30,  1.28it/s]Extractor Predicting: 186it [02:30,  1.30it/s]Extractor Predicting: 187it [02:31,  1.33it/s]Extractor Predicting: 188it [02:32,  1.31it/s]Extractor Predicting: 189it [02:33,  1.27it/s]Extractor Predicting: 190it [02:33,  1.26it/s]Extractor Predicting: 191it [02:34,  1.24it/s]Extractor Predicting: 192it [02:35,  1.26it/s]Extractor Predicting: 193it [02:36,  1.27it/s]Extractor Predicting: 194it [02:37,  1.28it/s]Extractor Predicting: 195it [02:37,  1.29it/s]Extractor Predicting: 196it [02:38,  1.29it/s]Extractor Predicting: 197it [02:39,  1.25it/s]Extractor Predicting: 198it [02:40,  1.26it/s]Extractor Predicting: 199it [02:41,  1.25it/s]Extractor Predicting: 200it [02:41,  1.25it/s]Extractor Predicting: 201it [02:42,  1.25it/s]Extractor Predicting: 202it [02:43,  1.25it/s]Extractor Predicting: 203it [02:44,  1.25it/s]Extractor Predicting: 204it [02:45,  1.23it/s]Extractor Predicting: 205it [02:45,  1.20it/s]Extractor Predicting: 206it [02:46,  1.24it/s]Extractor Predicting: 207it [02:47,  1.23it/s]Extractor Predicting: 208it [02:48,  1.26it/s]Extractor Predicting: 209it [02:49,  1.27it/s]Extractor Predicting: 210it [02:49,  1.25it/s]Extractor Predicting: 211it [02:50,  1.24it/s]Extractor Predicting: 212it [02:51,  1.24it/s]Extractor Predicting: 213it [02:52,  1.23it/s]Extractor Predicting: 214it [02:53,  1.22it/s]Extractor Predicting: 215it [02:54,  1.22it/s]Extractor Predicting: 216it [02:54,  1.22it/s]Extractor Predicting: 217it [02:55,  1.21it/s]Extractor Predicting: 218it [02:56,  1.21it/s]Extractor Predicting: 219it [02:57,  1.22it/s]Extractor Predicting: 220it [02:58,  1.23it/s]Extractor Predicting: 221it [02:58,  1.27it/s]Extractor Predicting: 222it [02:59,  1.22it/s]Extractor Predicting: 223it [03:00,  1.20it/s]Extractor Predicting: 224it [03:01,  1.24it/s]Extractor Predicting: 225it [03:02,  1.24it/s]Extractor Predicting: 226it [03:02,  1.22it/s]Extractor Predicting: 227it [03:03,  1.23it/s]Extractor Predicting: 228it [03:04,  1.24it/s]Extractor Predicting: 229it [03:05,  1.26it/s]Extractor Predicting: 230it [03:06,  1.22it/s]Extractor Predicting: 231it [03:07,  1.23it/s]Extractor Predicting: 232it [03:07,  1.25it/s]Extractor Predicting: 233it [03:08,  1.23it/s]Extractor Predicting: 234it [03:09,  1.22it/s]Extractor Predicting: 235it [03:10,  1.22it/s]Extractor Predicting: 236it [03:11,  1.23it/s]Extractor Predicting: 237it [03:11,  1.21it/s]Extractor Predicting: 238it [03:12,  1.21it/s]Extractor Predicting: 239it [03:13,  1.22it/s]Extractor Predicting: 240it [03:14,  1.21it/s]Extractor Predicting: 241it [03:15,  1.22it/s]Extractor Predicting: 242it [03:16,  1.23it/s]Extractor Predicting: 243it [03:16,  1.22it/s]Extractor Predicting: 244it [03:17,  1.23it/s]Extractor Predicting: 245it [03:18,  1.22it/s]Extractor Predicting: 246it [03:19,  1.23it/s]Extractor Predicting: 247it [03:20,  1.13it/s]Extractor Predicting: 248it [03:21,  1.15it/s]Extractor Predicting: 249it [03:21,  1.18it/s]Extractor Predicting: 250it [03:22,  1.18it/s]Extractor Predicting: 251it [03:23,  1.21it/s]Extractor Predicting: 252it [03:24,  1.20it/s]Extractor Predicting: 253it [03:25,  1.21it/s]Extractor Predicting: 254it [03:26,  1.22it/s]Extractor Predicting: 255it [03:26,  1.23it/s]Extractor Predicting: 256it [03:27,  1.23it/s]Extractor Predicting: 257it [03:28,  1.25it/s]Extractor Predicting: 258it [03:29,  1.25it/s]Extractor Predicting: 259it [03:29,  1.27it/s]Extractor Predicting: 260it [03:30,  1.25it/s]Extractor Predicting: 261it [03:31,  1.26it/s]Extractor Predicting: 262it [03:32,  1.25it/s]Extractor Predicting: 263it [03:33,  1.26it/s]Extractor Predicting: 264it [03:34,  1.22it/s]Extractor Predicting: 265it [03:34,  1.23it/s]Extractor Predicting: 266it [03:35,  1.23it/s]Extractor Predicting: 267it [03:36,  1.24it/s]Extractor Predicting: 268it [03:37,  1.25it/s]Extractor Predicting: 269it [03:38,  1.26it/s]Extractor Predicting: 270it [03:38,  1.25it/s]Extractor Predicting: 271it [03:39,  1.26it/s]Extractor Predicting: 272it [03:40,  1.29it/s]Extractor Predicting: 273it [03:41,  1.29it/s]Extractor Predicting: 274it [03:41,  1.30it/s]Extractor Predicting: 275it [03:42,  1.27it/s]Extractor Predicting: 276it [03:43,  1.25it/s]Extractor Predicting: 277it [03:44,  1.27it/s]Extractor Predicting: 278it [03:45,  1.25it/s]Extractor Predicting: 279it [03:45,  1.26it/s]Extractor Predicting: 280it [03:46,  1.24it/s]Extractor Predicting: 281it [03:47,  1.25it/s]Extractor Predicting: 282it [03:48,  1.24it/s]Extractor Predicting: 283it [03:49,  1.24it/s]Extractor Predicting: 284it [03:49,  1.24it/s]Extractor Predicting: 285it [03:50,  1.24it/s]Extractor Predicting: 286it [03:51,  1.22it/s]Extractor Predicting: 287it [03:52,  1.23it/s]Extractor Predicting: 288it [03:53,  1.24it/s]Extractor Predicting: 289it [03:53,  1.24it/s]Extractor Predicting: 290it [03:54,  1.24it/s]Extractor Predicting: 291it [03:55,  1.24it/s]Extractor Predicting: 292it [03:56,  1.25it/s]Extractor Predicting: 293it [03:57,  1.25it/s]Extractor Predicting: 294it [03:58,  1.23it/s]Extractor Predicting: 295it [03:58,  1.26it/s]Extractor Predicting: 296it [03:59,  1.26it/s]Extractor Predicting: 297it [04:00,  1.23it/s]Extractor Predicting: 298it [04:01,  1.26it/s]Extractor Predicting: 299it [04:02,  1.25it/s]Extractor Predicting: 300it [04:02,  1.24it/s]Extractor Predicting: 301it [04:03,  1.28it/s]Extractor Predicting: 302it [04:04,  1.27it/s]Extractor Predicting: 303it [04:05,  1.30it/s]Extractor Predicting: 304it [04:05,  1.28it/s]Extractor Predicting: 305it [04:06,  1.26it/s]Extractor Predicting: 306it [04:07,  1.26it/s]Extractor Predicting: 307it [04:08,  1.24it/s]Extractor Predicting: 308it [04:09,  1.23it/s]Extractor Predicting: 309it [04:09,  1.24it/s]Extractor Predicting: 310it [04:10,  1.23it/s]Extractor Predicting: 311it [04:11,  1.22it/s]Extractor Predicting: 312it [04:12,  1.19it/s]Extractor Predicting: 313it [04:13,  1.20it/s]Extractor Predicting: 314it [04:14,  1.21it/s]Extractor Predicting: 315it [04:14,  1.24it/s]Extractor Predicting: 316it [04:15,  1.24it/s]Extractor Predicting: 317it [04:16,  1.23it/s]Extractor Predicting: 318it [04:17,  1.24it/s]Extractor Predicting: 319it [04:18,  1.23it/s]Extractor Predicting: 320it [04:18,  1.23it/s]Extractor Predicting: 321it [04:19,  1.23it/s]Extractor Predicting: 322it [04:20,  1.22it/s]Extractor Predicting: 323it [04:21,  1.26it/s]Extractor Predicting: 324it [04:22,  1.24it/s]Extractor Predicting: 325it [04:22,  1.25it/s]Extractor Predicting: 326it [04:23,  1.24it/s]Extractor Predicting: 327it [04:24,  1.26it/s]Extractor Predicting: 328it [04:25,  1.27it/s]Extractor Predicting: 329it [04:26,  1.25it/s]Extractor Predicting: 330it [04:27,  1.23it/s]Extractor Predicting: 331it [04:27,  1.23it/s]Extractor Predicting: 332it [04:28,  1.22it/s]Extractor Predicting: 333it [04:29,  1.23it/s]Extractor Predicting: 334it [04:30,  1.23it/s]Extractor Predicting: 335it [04:31,  1.23it/s]Extractor Predicting: 336it [04:31,  1.26it/s]Extractor Predicting: 337it [04:32,  1.27it/s]Extractor Predicting: 338it [04:33,  1.26it/s]Extractor Predicting: 339it [04:34,  1.24it/s]Extractor Predicting: 340it [04:35,  1.25it/s]Extractor Predicting: 341it [04:35,  1.27it/s]Extractor Predicting: 342it [04:36,  1.23it/s]Extractor Predicting: 343it [04:37,  1.24it/s]Extractor Predicting: 344it [04:38,  1.25it/s]Extractor Predicting: 345it [04:39,  1.26it/s]Extractor Predicting: 346it [04:39,  1.26it/s]Extractor Predicting: 347it [04:40,  1.25it/s]Extractor Predicting: 348it [04:41,  1.25it/s]Extractor Predicting: 349it [04:42,  1.23it/s]Extractor Predicting: 350it [04:43,  1.23it/s]Extractor Predicting: 351it [04:43,  1.22it/s]Extractor Predicting: 352it [04:44,  1.20it/s]Extractor Predicting: 353it [04:45,  1.20it/s]Extractor Predicting: 354it [04:46,  1.20it/s]Extractor Predicting: 355it [04:47,  1.22it/s]Extractor Predicting: 356it [04:48,  1.22it/s]Extractor Predicting: 357it [04:48,  1.25it/s]Extractor Predicting: 358it [04:49,  1.26it/s]Extractor Predicting: 359it [04:50,  1.25it/s]Extractor Predicting: 360it [04:51,  1.24it/s]Extractor Predicting: 361it [04:52,  1.14it/s]Extractor Predicting: 362it [04:53,  1.17it/s]Extractor Predicting: 363it [04:53,  1.20it/s]Extractor Predicting: 364it [04:54,  1.22it/s]Extractor Predicting: 365it [04:55,  1.22it/s]Extractor Predicting: 366it [04:56,  1.22it/s]Extractor Predicting: 367it [04:57,  1.22it/s]Extractor Predicting: 368it [04:57,  1.24it/s]Extractor Predicting: 369it [04:58,  1.25it/s]Extractor Predicting: 370it [04:59,  1.27it/s]Extractor Predicting: 371it [05:00,  1.27it/s]Extractor Predicting: 372it [05:01,  1.27it/s]Extractor Predicting: 373it [05:01,  1.26it/s]Extractor Predicting: 374it [05:02,  1.25it/s]Extractor Predicting: 375it [05:03,  1.26it/s]Extractor Predicting: 376it [05:04,  1.26it/s]Extractor Predicting: 377it [05:04,  1.26it/s]Extractor Predicting: 378it [05:05,  1.28it/s]Extractor Predicting: 379it [05:06,  1.24it/s]Extractor Predicting: 380it [05:07,  1.24it/s]Extractor Predicting: 381it [05:08,  1.27it/s]Extractor Predicting: 382it [05:08,  1.27it/s]Extractor Predicting: 383it [05:09,  1.23it/s]Extractor Predicting: 384it [05:10,  1.24it/s]Extractor Predicting: 385it [05:11,  1.25it/s]Extractor Predicting: 386it [05:12,  1.28it/s]Extractor Predicting: 387it [05:12,  1.29it/s]Extractor Predicting: 388it [05:13,  1.28it/s]Extractor Predicting: 389it [05:14,  1.26it/s]Extractor Predicting: 390it [05:15,  1.27it/s]Extractor Predicting: 391it [05:16,  1.27it/s]Extractor Predicting: 392it [05:16,  1.28it/s]Extractor Predicting: 393it [05:17,  1.29it/s]Extractor Predicting: 394it [05:18,  1.31it/s]Extractor Predicting: 395it [05:19,  1.28it/s]Extractor Predicting: 396it [05:19,  1.27it/s]Extractor Predicting: 397it [05:20,  1.28it/s]Extractor Predicting: 398it [05:21,  1.28it/s]Extractor Predicting: 399it [05:22,  1.30it/s]Extractor Predicting: 400it [05:22,  1.33it/s]Extractor Predicting: 401it [05:23,  1.30it/s]Extractor Predicting: 402it [05:24,  1.31it/s]Extractor Predicting: 403it [05:25,  1.29it/s]Extractor Predicting: 404it [05:26,  1.28it/s]Extractor Predicting: 405it [05:26,  1.28it/s]Extractor Predicting: 406it [05:27,  1.27it/s]Extractor Predicting: 407it [05:28,  1.27it/s]Extractor Predicting: 408it [05:29,  1.27it/s]Extractor Predicting: 409it [05:30,  1.29it/s]Extractor Predicting: 409it [05:30,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:51,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:51,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:51,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:51,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:51,050 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:29:51,669 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:29:51,670 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:29:52,233 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:29:53,306 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:29:53,306 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:56,148 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:56,152 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:56,152 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:56,152 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:29:56,152 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:29:56,797 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:29:56,799 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:29:57,368 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:29:57,529 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:29:57,529 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35551811288963775,
  "recall": 0.2580250687863039,
  "score": 0.2990256864481843,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.11it/s]Extractor Predicting: 2it [00:01,  1.11it/s]Extractor Predicting: 3it [00:02,  1.12it/s]Extractor Predicting: 4it [00:03,  1.17it/s]Extractor Predicting: 5it [00:04,  1.16it/s]Extractor Predicting: 6it [00:05,  1.17it/s]Extractor Predicting: 7it [00:06,  1.17it/s]Extractor Predicting: 8it [00:06,  1.18it/s]Extractor Predicting: 9it [00:07,  1.22it/s]Extractor Predicting: 10it [00:08,  1.21it/s]Extractor Predicting: 11it [00:09,  1.23it/s]Extractor Predicting: 12it [00:10,  1.20it/s]Extractor Predicting: 13it [00:10,  1.22it/s]Extractor Predicting: 13it [00:10,  1.19it/s]
[INFO|configuration_utils.py:515] 2023-08-29 02:30:09,043 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:30:09,044 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:30:09,050 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:30:09,050 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 02:30:09,055 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:30:12,153 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 02:30:12,156 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 02:30:12,167 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:30:12,168 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:30:12,176 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:30:12,179 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:30:12,179 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:30:12,179 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:30:12,179 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:30:12,179 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:30:12,179 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.1797752808988764,
  "recall": 0.046579330422125184,
  "score": 0.07398843930635839,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 02:30:12,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:13,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:14,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:15,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:16,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:17,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:18,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:19,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:20,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:21,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:22,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:22,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:23,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:24,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:25,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:27,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:27,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:29,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:30,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:30,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:19<06:10, 19.51s/it][WARNING|generation_utils.py:914] 2023-08-29 02:30:31,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:32,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:33,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:34,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:35,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:36,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:37,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:38,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:39,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:40,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:41,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:42,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:43,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:44,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:44,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:45,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:46,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:47,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:48,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:49,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:37<05:40, 18.90s/it][WARNING|generation_utils.py:914] 2023-08-29 02:30:50,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:51,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:52,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:53,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:54,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:55,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:55,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:56,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:57,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:58,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:30:59,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:00,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:01,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:02,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:03,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:04,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:04,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:05,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:06,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:07,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:56<05:17, 18.66s/it][WARNING|generation_utils.py:914] 2023-08-29 02:31:08,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:09,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:10,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:11,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:12,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:13,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:14,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:15,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:16,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:16,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:17,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:18,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:19,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:20,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:21,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:22,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:23,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:24,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:25,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:25,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:26,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:15<05:00, 18.76s/it][WARNING|generation_utils.py:914] 2023-08-29 02:31:27,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:28,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:29,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:30,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:31,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:32,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:33,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:34,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:34,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:35,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:36,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:37,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:38,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:39,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:40,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:41,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:41,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:43,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:43,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:44,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:45,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:46,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:47,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:48,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:49,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:50,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:51,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:39<05:12, 20.82s/it][WARNING|generation_utils.py:914] 2023-08-29 02:31:52,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:52,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:53,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:54,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:55,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:56,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:57,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:58,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:59,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:31:59,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:00,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:01,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:02,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:03,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:04,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:05,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:06,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:07,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:08,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:09,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:10,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:10,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:11,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:12,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [02:01<04:54, 21.01s/it][WARNING|generation_utils.py:914] 2023-08-29 02:32:13,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:14,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:15,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:16,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:17,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:18,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:19,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:20,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:21,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:22,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:23,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:23,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:25,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:26,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:27,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:28,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:29,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:30,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:31,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:31,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:32,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:33,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:22<04:33, 21.04s/it][WARNING|generation_utils.py:914] 2023-08-29 02:32:34,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:35,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:36,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:37,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:38,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:39,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:40,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:41,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:42,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:43,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:44,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:45,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:46,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:47,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:48,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:49,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:50,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:51,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:52,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:53,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:54,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:55,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:56,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:57,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:46<04:23, 21.96s/it][WARNING|generation_utils.py:914] 2023-08-29 02:32:58,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:32:59,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:00,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:01,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:02,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:02,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:03,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:04,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:05,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:06,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:07,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:08,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:09,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:10,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:11,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:12,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:13,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:14,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:15,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:16,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:16,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [03:05<03:52, 21.13s/it][WARNING|generation_utils.py:914] 2023-08-29 02:33:17,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:18,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:19,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:20,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:21,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:22,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:23,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:24,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:25,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:26,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:27,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:28,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:29,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:29,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:30,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:31,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:32,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:33,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:34,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:35,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [03:23<03:22, 20.22s/it][WARNING|generation_utils.py:914] 2023-08-29 02:33:36,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:36,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:37,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:38,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:39,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:40,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:41,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:42,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:43,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:44,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:45,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:46,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:47,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:48,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:49,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:50,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:51,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:52,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:53,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:53,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:54,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:55,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:44<03:03, 20.39s/it][WARNING|generation_utils.py:914] 2023-08-29 02:33:56,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:57,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:58,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:33:59,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:00,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:01,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:02,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:03,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:04,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:05,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:06,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:07,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:08,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:09,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:09,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:10,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:11,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:12,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:13,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:14,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:15,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:16,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [04:05<02:43, 20.45s/it][WARNING|generation_utils.py:914] 2023-08-29 02:34:17,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:18,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:19,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:19,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:20,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:21,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:22,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:23,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:23,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:24,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:25,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:26,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:26,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:27,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:28,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:29,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:30,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:30,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:31,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:32,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [04:20<02:13, 19.04s/it][WARNING|generation_utils.py:914] 2023-08-29 02:34:33,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:34,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:35,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:36,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:36,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:37,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:38,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:39,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:40,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:41,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:42,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:43,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:43,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:44,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:45,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:46,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:47,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:48,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:49,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:50,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:51,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:39<01:54, 19.01s/it][WARNING|generation_utils.py:914] 2023-08-29 02:34:52,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:53,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:53,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:54,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:55,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:56,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:57,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:58,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:59,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:34:59,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:00,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:01,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:02,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:03,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:04,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:05,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:06,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:07,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:08,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:09,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:10,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:58<01:35, 19.04s/it][WARNING|generation_utils.py:914] 2023-08-29 02:35:11,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:12,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:13,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:13,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:14,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:15,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:16,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:17,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:18,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:19,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:20,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:21,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:21,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:22,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:23,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:24,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:25,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:26,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:27,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:28,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:28,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:29,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:30,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [05:19<01:17, 19.50s/it][WARNING|generation_utils.py:914] 2023-08-29 02:35:31,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:32,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:33,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:34,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:35,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:35,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:36,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:37,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:38,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:39,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:40,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:41,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:41,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:42,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:43,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:44,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:45,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:46,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:47,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:47,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:48,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:49,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:50,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:51,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [05:39<00:59, 19.73s/it][WARNING|generation_utils.py:914] 2023-08-29 02:35:52,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:52,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:53,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:54,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:55,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:56,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:57,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:58,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:35:59,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:00,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:01,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:02,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:02,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:03,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:04,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:05,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:06,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:07,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:09,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:57<00:38, 19.24s/it][WARNING|generation_utils.py:914] 2023-08-29 02:36:10,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:11,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:12,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:12,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:14,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:15,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:16,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:17,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:18,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:19,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:20,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:21,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:22,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:23,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:24,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:25,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:26,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:27,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:28,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:29,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:30,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:31,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [06:19<00:20, 20.05s/it][WARNING|generation_utils.py:914] 2023-08-29 02:36:32,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:33,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:33,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:34,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:35,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:36,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:37,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:38,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:39,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:40,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:41,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:42,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:43,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:44,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:45,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:46,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:47,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:48,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:49,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:36:50,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [06:39<00:00, 19.95s/it]Generating: 100%|██████████| 20/20 [06:39<00:00, 19.97s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:58,077 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:58,081 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:58,081 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:58,081 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:36:58,081 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:36:58,803 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:36:58,804 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:36:59,472 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:37:00,529 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:37:00,529 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:37:03,656 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:37:03,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:37:03,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:37:03,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:37:03,663 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:37:03,984 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:37:03,985 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:37:04,247 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:37:04,410 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:37:04,410 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.95, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9578125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 157, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 203, 'raw': 288}
{'target': 600, 'success': 227, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 321, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 369, 'raw': 512}
{'target': 600, 'success': 393, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 434, 'raw': 608}
{'target': 600, 'success': 457, 'raw': 640}
{'target': 600, 'success': 484, 'raw': 672}
{'target': 600, 'success': 504, 'raw': 704}
{'target': 600, 'success': 526, 'raw': 736}
{'target': 600, 'success': 543, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 595, 'raw': 832}
{'target': 600, 'success': 621, 'raw': 864}
{'prompt': 'Relation : voice type .', 'success_rate': 0.71875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 569, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.80859375, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 620, 'raw': 768}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.8072916666666666, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : location .', 'success_rate': 0.8943452380952381, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9390625, 'errors': {''}}
['Relation : member of political party . Context : Following his leadership in the 2010 elections , the party won the parliamentary elections , but at the end of 2010 , the party was lost in the May 2010 provincial election . Head Entity : Prime Minister , Tail Entity : NDP .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8821022727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 521, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.859375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 560, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9226190476190477, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : position held .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)', '("William \'s wife Margo", \'position held\', \'\', "Following the death in December 1953 of his friend William \'s wife Margo , the family moved into a newly built home in Cambridge , Massachusetts .")'}}
['Relation : position played on team / speciality . Context : On 31 March 2014 , the Brazilian national squad returned to their squad for the 2018 FIFA World Cup , where they were placed in Group One . Head Entity : Brazil national squad , Tail Entity : national squad .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 425, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 477, 'raw': 608}
{'target': 600, 'success': 501, 'raw': 640}
{'target': 600, 'success': 526, 'raw': 672}
{'target': 600, 'success': 548, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.78515625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.959375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : religion .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 11543
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11643, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.27it/s]Extractor Estimating: 2it [00:01,  1.23it/s]Extractor Estimating: 3it [00:02,  1.27it/s]Extractor Estimating: 4it [00:03,  1.25it/s]Extractor Estimating: 5it [00:04,  1.24it/s]Extractor Estimating: 6it [00:04,  1.28it/s]Extractor Estimating: 7it [00:05,  1.26it/s]Extractor Estimating: 8it [00:06,  1.27it/s]Extractor Estimating: 9it [00:07,  1.28it/s]Extractor Estimating: 10it [00:07,  1.27it/s]Extractor Estimating: 11it [00:08,  1.25it/s]Extractor Estimating: 12it [00:09,  1.24it/s]Extractor Estimating: 13it [00:10,  1.28it/s]Extractor Estimating: 14it [00:11,  1.30it/s]Extractor Estimating: 15it [00:11,  1.28it/s]Extractor Estimating: 16it [00:12,  1.26it/s]Extractor Estimating: 17it [00:13,  1.26it/s]Extractor Estimating: 18it [00:14,  1.30it/s]Extractor Estimating: 19it [00:15,  1.21it/s]Extractor Estimating: 20it [00:15,  1.21it/s]Extractor Estimating: 21it [00:16,  1.22it/s]Extractor Estimating: 22it [00:17,  1.23it/s]Extractor Estimating: 23it [00:18,  1.27it/s]Extractor Estimating: 24it [00:19,  1.26it/s]Extractor Estimating: 25it [00:19,  1.25it/s]Extractor Estimating: 26it [00:20,  1.26it/s]Extractor Estimating: 27it [00:21,  1.28it/s]Extractor Estimating: 28it [00:22,  1.27it/s]Extractor Estimating: 29it [00:23,  1.28it/s]Extractor Estimating: 30it [00:23,  1.27it/s]Extractor Estimating: 31it [00:24,  1.27it/s]Extractor Estimating: 32it [00:25,  1.29it/s]Extractor Estimating: 33it [00:26,  1.28it/s]Extractor Estimating: 34it [00:26,  1.35it/s]Extractor Estimating: 35it [00:27,  1.32it/s]Extractor Estimating: 36it [00:28,  1.31it/s]Extractor Estimating: 37it [00:29,  1.26it/s]Extractor Estimating: 38it [00:29,  1.29it/s]Extractor Estimating: 39it [00:30,  1.27it/s]Extractor Estimating: 40it [00:31,  1.28it/s]Extractor Estimating: 41it [00:32,  1.32it/s]Extractor Estimating: 42it [00:33,  1.31it/s]Extractor Estimating: 43it [00:33,  1.31it/s]Extractor Estimating: 44it [00:34,  1.30it/s]Extractor Estimating: 45it [00:35,  1.27it/s]Extractor Estimating: 46it [00:36,  1.24it/s]Extractor Estimating: 47it [00:37,  1.24it/s]Extractor Estimating: 48it [00:37,  1.25it/s]Extractor Estimating: 49it [00:38,  1.27it/s]Extractor Estimating: 50it [00:39,  1.28it/s]Extractor Estimating: 51it [00:40,  1.32it/s]Extractor Estimating: 52it [00:40,  1.39it/s]Extractor Estimating: 53it [00:41,  1.38it/s]Extractor Estimating: 54it [00:42,  1.43it/s]Extractor Estimating: 55it [00:42,  1.39it/s]Extractor Estimating: 56it [00:43,  1.37it/s]Extractor Estimating: 57it [00:44,  1.42it/s]Extractor Estimating: 58it [00:44,  1.44it/s]Extractor Estimating: 59it [00:45,  1.48it/s]Extractor Estimating: 60it [00:46,  1.44it/s]Extractor Estimating: 61it [00:46,  1.43it/s]Extractor Estimating: 62it [00:47,  1.43it/s]Extractor Estimating: 63it [00:48,  1.47it/s]Extractor Estimating: 64it [00:49,  1.40it/s]Extractor Estimating: 65it [00:49,  1.45it/s]Extractor Estimating: 66it [00:50,  1.51it/s]Extractor Estimating: 67it [00:50,  1.55it/s]Extractor Estimating: 68it [00:51,  1.54it/s]Extractor Estimating: 69it [00:52,  1.54it/s]Extractor Estimating: 70it [00:52,  1.49it/s]Extractor Estimating: 71it [00:53,  1.48it/s]Extractor Estimating: 72it [00:54,  1.49it/s]Extractor Estimating: 73it [00:55,  1.49it/s]Extractor Estimating: 74it [00:55,  1.51it/s]Extractor Estimating: 75it [00:56,  1.53it/s]Extractor Estimating: 76it [00:57,  1.40it/s]Extractor Estimating: 77it [00:57,  1.40it/s]Extractor Estimating: 78it [00:58,  1.34it/s]Extractor Estimating: 79it [00:59,  1.28it/s]Extractor Estimating: 80it [01:00,  1.27it/s]Extractor Estimating: 81it [01:01,  1.25it/s]Extractor Estimating: 82it [01:01,  1.24it/s]Extractor Estimating: 83it [01:02,  1.26it/s]Extractor Estimating: 84it [01:03,  1.24it/s]Extractor Estimating: 85it [01:04,  1.27it/s]Extractor Estimating: 86it [01:05,  1.25it/s]Extractor Estimating: 87it [01:05,  1.27it/s]Extractor Estimating: 88it [01:06,  1.26it/s]Extractor Estimating: 89it [01:07,  1.22it/s]Extractor Estimating: 90it [01:08,  1.26it/s]Extractor Estimating: 91it [01:09,  1.22it/s]Extractor Estimating: 92it [01:09,  1.24it/s]Extractor Estimating: 93it [01:10,  1.26it/s]Extractor Estimating: 94it [01:11,  1.25it/s]Extractor Estimating: 95it [01:12,  1.25it/s]Extractor Estimating: 96it [01:13,  1.24it/s]Extractor Estimating: 97it [01:14,  1.21it/s]Extractor Estimating: 98it [01:14,  1.21it/s]Extractor Estimating: 99it [01:15,  1.22it/s]Extractor Estimating: 100it [01:16,  1.25it/s]Extractor Estimating: 101it [01:17,  1.25it/s]Extractor Estimating: 102it [01:18,  1.27it/s]Extractor Estimating: 103it [01:18,  1.26it/s]Extractor Estimating: 104it [01:19,  1.30it/s]Extractor Estimating: 105it [01:20,  1.23it/s]Extractor Estimating: 106it [01:21,  1.24it/s]Extractor Estimating: 107it [01:22,  1.23it/s]Extractor Estimating: 108it [01:22,  1.27it/s]Extractor Estimating: 109it [01:23,  1.20it/s]Extractor Estimating: 110it [01:24,  1.21it/s]Extractor Estimating: 111it [01:25,  1.22it/s]Extractor Estimating: 112it [01:26,  1.23it/s]Extractor Estimating: 113it [01:26,  1.26it/s]Extractor Estimating: 114it [01:27,  1.22it/s]Extractor Estimating: 115it [01:28,  1.25it/s]Extractor Estimating: 116it [01:29,  1.25it/s]Extractor Estimating: 117it [01:30,  1.27it/s]Extractor Estimating: 118it [01:30,  1.28it/s]Extractor Estimating: 119it [01:31,  1.27it/s]Extractor Estimating: 120it [01:32,  1.29it/s]Extractor Estimating: 121it [01:33,  1.25it/s]Extractor Estimating: 122it [01:34,  1.24it/s]Extractor Estimating: 123it [01:35,  1.14it/s]Extractor Estimating: 124it [01:36,  1.13it/s]Extractor Estimating: 125it [01:36,  1.18it/s]Extractor Estimating: 126it [01:37,  1.26it/s]Extractor Estimating: 127it [01:38,  1.31it/s]Extractor Estimating: 128it [01:38,  1.28it/s]Extractor Estimating: 129it [01:39,  1.31it/s]Extractor Estimating: 130it [01:40,  1.34it/s]Extractor Estimating: 131it [01:41,  1.34it/s]Extractor Estimating: 132it [01:41,  1.36it/s]Extractor Estimating: 133it [01:42,  1.37it/s]Extractor Estimating: 134it [01:43,  1.36it/s]Extractor Estimating: 135it [01:43,  1.40it/s]Extractor Estimating: 136it [01:44,  1.39it/s]Extractor Estimating: 137it [01:45,  1.37it/s]Extractor Estimating: 138it [01:46,  1.35it/s]Extractor Estimating: 139it [01:46,  1.36it/s]Extractor Estimating: 140it [01:47,  1.36it/s]Extractor Estimating: 141it [01:48,  1.38it/s]Extractor Estimating: 142it [01:49,  1.34it/s]Extractor Estimating: 143it [01:49,  1.31it/s]Extractor Estimating: 144it [01:50,  1.32it/s]Extractor Estimating: 145it [01:51,  1.31it/s]Extractor Estimating: 146it [01:52,  1.29it/s]Extractor Estimating: 147it [01:53,  1.33it/s]Extractor Estimating: 148it [01:53,  1.38it/s]Extractor Estimating: 149it [01:54,  1.37it/s]Extractor Estimating: 150it [01:55,  1.36it/s]Extractor Estimating: 151it [01:55,  1.34it/s]Extractor Estimating: 152it [01:56,  1.32it/s]Extractor Estimating: 153it [01:57,  1.28it/s]Extractor Estimating: 154it [01:58,  1.28it/s]Extractor Estimating: 155it [01:59,  1.31it/s]Extractor Estimating: 156it [01:59,  1.29it/s]Extractor Estimating: 157it [02:00,  1.30it/s]Extractor Estimating: 158it [02:01,  1.31it/s]Extractor Estimating: 159it [02:02,  1.26it/s]Extractor Estimating: 160it [02:02,  1.30it/s]Extractor Estimating: 161it [02:03,  1.31it/s]Extractor Estimating: 162it [02:04,  1.31it/s]Extractor Estimating: 163it [02:05,  1.31it/s]Extractor Estimating: 164it [02:06,  1.29it/s]Extractor Estimating: 165it [02:06,  1.26it/s]Extractor Estimating: 166it [02:07,  1.24it/s]Extractor Estimating: 167it [02:08,  1.22it/s]Extractor Estimating: 168it [02:09,  1.25it/s]Extractor Estimating: 169it [02:10,  1.26it/s]Extractor Estimating: 170it [02:10,  1.29it/s]Extractor Estimating: 171it [02:11,  1.32it/s]Extractor Estimating: 172it [02:12,  1.28it/s]Extractor Estimating: 173it [02:13,  1.31it/s]Extractor Estimating: 174it [02:13,  1.30it/s]Extractor Estimating: 175it [02:14,  1.29it/s]Extractor Estimating: 176it [02:15,  1.30it/s]Extractor Estimating: 177it [02:16,  1.29it/s]Extractor Estimating: 178it [02:16,  1.31it/s]Extractor Estimating: 179it [02:17,  1.24it/s]Extractor Estimating: 180it [02:18,  1.26it/s]Extractor Estimating: 181it [02:19,  1.13it/s]Extractor Estimating: 182it [02:20,  1.17it/s]Extractor Estimating: 183it [02:21,  1.19it/s]Extractor Estimating: 184it [02:22,  1.18it/s]Extractor Estimating: 185it [02:22,  1.22it/s]Extractor Estimating: 186it [02:23,  1.19it/s]Extractor Estimating: 187it [02:24,  1.22it/s]Extractor Estimating: 188it [02:25,  1.21it/s]Extractor Estimating: 189it [02:26,  1.20it/s]Extractor Estimating: 190it [02:26,  1.27it/s]Extractor Estimating: 191it [02:27,  1.27it/s]Extractor Estimating: 192it [02:28,  1.25it/s]Extractor Estimating: 193it [02:29,  1.28it/s]Extractor Estimating: 194it [02:30,  1.19it/s]Extractor Estimating: 195it [02:31,  1.22it/s]Extractor Estimating: 196it [02:31,  1.21it/s]Extractor Estimating: 197it [02:32,  1.23it/s]Extractor Estimating: 198it [02:33,  1.22it/s]Extractor Estimating: 199it [02:34,  1.24it/s]Extractor Estimating: 200it [02:35,  1.23it/s]Extractor Estimating: 201it [02:35,  1.25it/s]Extractor Estimating: 202it [02:36,  1.29it/s]Extractor Estimating: 203it [02:37,  1.31it/s]Extractor Estimating: 204it [02:38,  1.33it/s]Extractor Estimating: 205it [02:38,  1.35it/s]Extractor Estimating: 206it [02:39,  1.36it/s]Extractor Estimating: 207it [02:40,  1.36it/s]Extractor Estimating: 208it [02:41,  1.30it/s]Extractor Estimating: 209it [02:41,  1.28it/s]Extractor Estimating: 210it [02:42,  1.28it/s]Extractor Estimating: 211it [02:43,  1.32it/s]Extractor Estimating: 212it [02:44,  1.30it/s]Extractor Estimating: 213it [02:45,  1.27it/s]Extractor Estimating: 214it [02:45,  1.29it/s]Extractor Estimating: 215it [02:46,  1.32it/s]Extractor Estimating: 216it [02:47,  1.32it/s]Extractor Estimating: 217it [02:47,  1.31it/s]Extractor Estimating: 218it [02:48,  1.32it/s]Extractor Estimating: 219it [02:49,  1.34it/s]Extractor Estimating: 220it [02:50,  1.34it/s]Extractor Estimating: 221it [02:50,  1.33it/s]Extractor Estimating: 222it [02:51,  1.33it/s]Extractor Estimating: 223it [02:52,  1.28it/s]Extractor Estimating: 224it [02:53,  1.29it/s]Extractor Estimating: 225it [02:54,  1.29it/s]Extractor Estimating: 226it [02:54,  1.27it/s]Extractor Estimating: 227it [02:55,  1.26it/s]Extractor Estimating: 228it [02:56,  1.29it/s]Extractor Estimating: 229it [02:57,  1.32it/s]Extractor Estimating: 230it [02:57,  1.30it/s]Extractor Estimating: 231it [02:58,  1.33it/s]Extractor Estimating: 232it [02:59,  1.36it/s]Extractor Estimating: 233it [03:00,  1.39it/s]Extractor Estimating: 234it [03:00,  1.38it/s]Extractor Estimating: 235it [03:01,  1.42it/s]Extractor Estimating: 236it [03:02,  1.39it/s]Extractor Estimating: 237it [03:02,  1.38it/s]Extractor Estimating: 238it [03:03,  1.35it/s]Extractor Estimating: 239it [03:04,  1.33it/s]Extractor Estimating: 240it [03:05,  1.37it/s]Extractor Estimating: 241it [03:05,  1.36it/s]Extractor Estimating: 242it [03:06,  1.39it/s]Extractor Estimating: 243it [03:07,  1.34it/s]Extractor Estimating: 244it [03:08,  1.38it/s]Extractor Estimating: 245it [03:08,  1.37it/s]Extractor Estimating: 246it [03:09,  1.35it/s]Extractor Estimating: 247it [03:10,  1.36it/s]Extractor Estimating: 248it [03:11,  1.33it/s]Extractor Estimating: 249it [03:11,  1.33it/s]Extractor Estimating: 250it [03:12,  1.38it/s]Extractor Estimating: 251it [03:13,  1.37it/s]Extractor Estimating: 252it [03:14,  1.33it/s]Extractor Estimating: 253it [03:14,  1.31it/s]Extractor Estimating: 254it [03:15,  1.33it/s]Extractor Estimating: 255it [03:16,  1.32it/s]Extractor Estimating: 256it [03:17,  1.23it/s]Extractor Estimating: 257it [03:18,  1.27it/s]Extractor Estimating: 258it [03:18,  1.31it/s]Extractor Estimating: 259it [03:19,  1.24it/s]Extractor Estimating: 260it [03:20,  1.24it/s]Extractor Estimating: 261it [03:21,  1.14it/s]Extractor Estimating: 262it [03:22,  1.20it/s]Extractor Estimating: 263it [03:23,  1.22it/s]Extractor Estimating: 264it [03:23,  1.25it/s]Extractor Estimating: 265it [03:24,  1.24it/s]Extractor Estimating: 266it [03:25,  1.25it/s]Extractor Estimating: 267it [03:26,  1.30it/s]Extractor Estimating: 268it [03:26,  1.26it/s]Extractor Estimating: 269it [03:27,  1.26it/s]Extractor Estimating: 270it [03:28,  1.27it/s]Extractor Estimating: 271it [03:29,  1.28it/s]Extractor Estimating: 272it [03:30,  1.29it/s]Extractor Estimating: 273it [03:30,  1.28it/s]Extractor Estimating: 274it [03:31,  1.33it/s]Extractor Estimating: 275it [03:32,  1.27it/s]Extractor Estimating: 276it [03:33,  1.19it/s]Extractor Estimating: 277it [03:34,  1.17it/s]Extractor Estimating: 278it [03:35,  1.19it/s]Extractor Estimating: 279it [03:35,  1.18it/s]Extractor Estimating: 280it [03:36,  1.17it/s]Extractor Estimating: 281it [03:37,  1.20it/s]Extractor Estimating: 282it [03:38,  1.24it/s]Extractor Estimating: 283it [03:39,  1.19it/s]Extractor Estimating: 284it [03:40,  1.20it/s]Extractor Estimating: 285it [03:40,  1.18it/s]Extractor Estimating: 286it [03:41,  1.16it/s]Extractor Estimating: 287it [03:42,  1.16it/s]Extractor Estimating: 288it [03:43,  1.17it/s]Extractor Estimating: 289it [03:44,  1.17it/s]Extractor Estimating: 290it [03:45,  1.19it/s]Extractor Estimating: 291it [03:46,  1.19it/s]Extractor Estimating: 292it [03:46,  1.20it/s]Extractor Estimating: 293it [03:47,  1.19it/s]Extractor Estimating: 294it [03:48,  1.20it/s]Extractor Estimating: 295it [03:49,  1.16it/s]Extractor Estimating: 296it [03:50,  1.19it/s]Extractor Estimating: 297it [03:51,  1.16it/s]Extractor Estimating: 298it [03:52,  1.16it/s]Extractor Estimating: 299it [03:52,  1.20it/s]Extractor Estimating: 300it [03:53,  1.18it/s]Extractor Estimating: 301it [03:54,  1.28it/s]Extractor Estimating: 302it [03:55,  1.24it/s]Extractor Estimating: 303it [03:55,  1.29it/s]Extractor Estimating: 304it [03:56,  1.34it/s]Extractor Estimating: 305it [03:57,  1.39it/s]Extractor Estimating: 306it [03:57,  1.46it/s]Extractor Estimating: 307it [03:58,  1.45it/s]Extractor Estimating: 308it [03:59,  1.43it/s]Extractor Estimating: 309it [03:59,  1.45it/s]Extractor Estimating: 310it [04:00,  1.48it/s]Extractor Estimating: 311it [04:01,  1.50it/s]Extractor Estimating: 312it [04:01,  1.49it/s]Extractor Estimating: 313it [04:02,  1.52it/s]Extractor Estimating: 314it [04:03,  1.52it/s]Extractor Estimating: 315it [04:03,  1.55it/s]Extractor Estimating: 316it [04:04,  1.51it/s]Extractor Estimating: 317it [04:05,  1.48it/s]Extractor Estimating: 318it [04:05,  1.45it/s]Extractor Estimating: 319it [04:06,  1.43it/s]Extractor Estimating: 320it [04:07,  1.44it/s]Extractor Estimating: 321it [04:07,  1.45it/s]Extractor Estimating: 322it [04:08,  1.43it/s]Extractor Estimating: 323it [04:09,  1.42it/s]Extractor Estimating: 324it [04:10,  1.42it/s]Extractor Estimating: 325it [04:10,  1.40it/s]Extractor Estimating: 326it [04:11,  1.37it/s]Extractor Estimating: 327it [04:12,  1.32it/s]Extractor Estimating: 328it [04:13,  1.32it/s]Extractor Estimating: 329it [04:13,  1.31it/s]Extractor Estimating: 330it [04:14,  1.27it/s]Extractor Estimating: 331it [04:15,  1.33it/s]Extractor Estimating: 332it [04:16,  1.29it/s]Extractor Estimating: 333it [04:17,  1.32it/s]Extractor Estimating: 334it [04:17,  1.36it/s]Extractor Estimating: 335it [04:18,  1.35it/s]Extractor Estimating: 336it [04:19,  1.28it/s]Extractor Estimating: 337it [04:20,  1.29it/s]Extractor Estimating: 338it [04:20,  1.31it/s]Extractor Estimating: 339it [04:21,  1.35it/s]Extractor Estimating: 340it [04:22,  1.35it/s]Extractor Estimating: 341it [04:23,  1.35it/s]Extractor Estimating: 342it [04:23,  1.35it/s]Extractor Estimating: 343it [04:24,  1.31it/s]Extractor Estimating: 344it [04:25,  1.31it/s]Extractor Estimating: 345it [04:26,  1.31it/s]Extractor Estimating: 346it [04:26,  1.25it/s]Extractor Estimating: 347it [04:27,  1.25it/s]Extractor Estimating: 348it [04:28,  1.26it/s]Extractor Estimating: 349it [04:29,  1.16it/s]Extractor Estimating: 350it [04:30,  1.20it/s]Extractor Estimating: 351it [04:31,  1.23it/s]Extractor Estimating: 352it [04:31,  1.32it/s]Extractor Estimating: 353it [04:32,  1.31it/s]Extractor Estimating: 354it [04:33,  1.37it/s]Extractor Estimating: 355it [04:33,  1.36it/s]Extractor Estimating: 356it [04:34,  1.36it/s]Extractor Estimating: 357it [04:35,  1.36it/s]Extractor Estimating: 358it [04:36,  1.39it/s]Extractor Estimating: 359it [04:36,  1.36it/s]Extractor Estimating: 360it [04:37,  1.39it/s]Extractor Estimating: 361it [04:38,  1.43it/s]Extractor Estimating: 362it [04:38,  1.45it/s]Extractor Estimating: 363it [04:39,  1.42it/s]Extractor Estimating: 364it [04:40,  1.42it/s]Extractor Estimating: 365it [04:40,  1.42it/s]Extractor Estimating: 366it [04:41,  1.38it/s]Extractor Estimating: 367it [04:42,  1.39it/s]Extractor Estimating: 368it [04:43,  1.36it/s]Extractor Estimating: 369it [04:43,  1.37it/s]Extractor Estimating: 370it [04:44,  1.35it/s]Extractor Estimating: 371it [04:45,  1.31it/s]Extractor Estimating: 372it [04:46,  1.33it/s]Extractor Estimating: 373it [04:47,  1.34it/s]Extractor Estimating: 374it [04:47,  1.36it/s]Extractor Estimating: 375it [04:48,  1.39it/s]Extractor Estimating: 376it [04:49,  1.38it/s]Extractor Estimating: 377it [04:49,  1.36it/s]Extractor Estimating: 378it [04:50,  1.34it/s]Extractor Estimating: 379it [04:51,  1.33it/s]Extractor Estimating: 380it [04:52,  1.29it/s]Extractor Estimating: 381it [04:53,  1.29it/s]Extractor Estimating: 382it [04:53,  1.33it/s]Extractor Estimating: 383it [04:54,  1.35it/s]Extractor Estimating: 384it [04:55,  1.34it/s]Extractor Estimating: 385it [04:55,  1.36it/s]Extractor Estimating: 386it [04:56,  1.35it/s]Extractor Estimating: 387it [04:57,  1.36it/s]Extractor Estimating: 388it [04:58,  1.34it/s]Extractor Estimating: 389it [04:58,  1.33it/s]Extractor Estimating: 390it [04:59,  1.32it/s]Extractor Estimating: 391it [05:00,  1.36it/s]Extractor Estimating: 392it [05:01,  1.40it/s]Extractor Estimating: 393it [05:01,  1.42it/s]Extractor Estimating: 394it [05:02,  1.42it/s]Extractor Estimating: 395it [05:03,  1.39it/s]Extractor Estimating: 396it [05:03,  1.41it/s]Extractor Estimating: 397it [05:04,  1.31it/s]Extractor Estimating: 398it [05:05,  1.29it/s]Extractor Estimating: 399it [05:06,  1.31it/s]Extractor Estimating: 400it [05:07,  1.30it/s]Extractor Estimating: 401it [05:07,  1.33it/s]Extractor Estimating: 402it [05:08,  1.35it/s]Extractor Estimating: 403it [05:09,  1.38it/s]Extractor Estimating: 404it [05:09,  1.38it/s]Extractor Estimating: 405it [05:10,  1.42it/s]Extractor Estimating: 406it [05:11,  1.39it/s]Extractor Estimating: 407it [05:12,  1.41it/s]Extractor Estimating: 408it [05:12,  1.37it/s]Extractor Estimating: 409it [05:13,  1.38it/s]Extractor Estimating: 410it [05:14,  1.36it/s]Extractor Estimating: 411it [05:14,  1.40it/s]Extractor Estimating: 412it [05:15,  1.38it/s]Extractor Estimating: 413it [05:16,  1.40it/s]Extractor Estimating: 414it [05:17,  1.40it/s]Extractor Estimating: 415it [05:17,  1.41it/s]Extractor Estimating: 416it [05:18,  1.42it/s]Extractor Estimating: 417it [05:19,  1.40it/s]Extractor Estimating: 418it [05:19,  1.38it/s]Extractor Estimating: 419it [05:20,  1.37it/s]Extractor Estimating: 420it [05:21,  1.36it/s]Extractor Estimating: 421it [05:22,  1.35it/s]Extractor Estimating: 422it [05:22,  1.34it/s]Extractor Estimating: 423it [05:23,  1.35it/s]Extractor Estimating: 424it [05:24,  1.38it/s]Extractor Estimating: 425it [05:25,  1.28it/s]Extractor Estimating: 426it [05:26,  1.29it/s]Extractor Estimating: 427it [05:26,  1.26it/s]Extractor Estimating: 428it [05:27,  1.26it/s]Extractor Estimating: 429it [05:28,  1.29it/s]Extractor Estimating: 430it [05:29,  1.28it/s]Extractor Estimating: 431it [05:30,  1.26it/s]Extractor Estimating: 432it [05:30,  1.26it/s]Extractor Estimating: 433it [05:31,  1.26it/s]Extractor Estimating: 434it [05:32,  1.27it/s]Extractor Estimating: 435it [05:33,  1.28it/s]Extractor Estimating: 436it [05:33,  1.28it/s]Extractor Estimating: 437it [05:34,  1.22it/s]Extractor Estimating: 438it [05:35,  1.22it/s]Extractor Estimating: 439it [05:36,  1.25it/s]Extractor Estimating: 440it [05:37,  1.25it/s]Extractor Estimating: 441it [05:38,  1.27it/s]Extractor Estimating: 442it [05:38,  1.23it/s]Extractor Estimating: 443it [05:39,  1.22it/s]Extractor Estimating: 444it [05:40,  1.25it/s]Extractor Estimating: 445it [05:41,  1.27it/s]Extractor Estimating: 446it [05:41,  1.29it/s]Extractor Estimating: 447it [05:42,  1.29it/s]Extractor Estimating: 448it [05:43,  1.28it/s]Extractor Estimating: 449it [05:44,  1.24it/s]Extractor Estimating: 450it [05:45,  1.23it/s]Extractor Estimating: 451it [05:45,  1.26it/s]Extractor Estimating: 452it [05:46,  1.26it/s]Extractor Estimating: 453it [05:47,  1.28it/s]Extractor Estimating: 454it [05:48,  1.30it/s]Extractor Estimating: 455it [05:49,  1.26it/s]Extractor Estimating: 456it [05:49,  1.28it/s]Extractor Estimating: 457it [05:50,  1.29it/s]Extractor Estimating: 458it [05:51,  1.27it/s]Extractor Estimating: 459it [05:52,  1.28it/s]Extractor Estimating: 460it [05:52,  1.28it/s]Extractor Estimating: 461it [05:53,  1.29it/s]Extractor Estimating: 462it [05:54,  1.28it/s]Extractor Estimating: 463it [05:55,  1.28it/s]Extractor Estimating: 464it [05:56,  1.30it/s]Extractor Estimating: 465it [05:56,  1.29it/s]Extractor Estimating: 466it [05:57,  1.34it/s]Extractor Estimating: 467it [05:58,  1.33it/s]Extractor Estimating: 468it [05:59,  1.24it/s]Extractor Estimating: 469it [06:00,  1.21it/s]Extractor Estimating: 470it [06:00,  1.24it/s]Extractor Estimating: 471it [06:01,  1.26it/s]Extractor Estimating: 472it [06:02,  1.30it/s]Extractor Estimating: 473it [06:03,  1.31it/s]Extractor Estimating: 474it [06:03,  1.29it/s]Extractor Estimating: 475it [06:04,  1.30it/s]Extractor Estimating: 476it [06:05,  1.33it/s]Extractor Estimating: 477it [06:06,  1.34it/s]Extractor Estimating: 478it [06:06,  1.32it/s]Extractor Estimating: 479it [06:07,  1.28it/s]Extractor Estimating: 480it [06:08,  1.28it/s]Extractor Estimating: 481it [06:09,  1.28it/s]Extractor Estimating: 482it [06:10,  1.29it/s]Extractor Estimating: 483it [06:10,  1.31it/s]Extractor Estimating: 484it [06:11,  1.32it/s]Extractor Estimating: 485it [06:12,  1.31it/s]Extractor Estimating: 486it [06:13,  1.32it/s]Extractor Estimating: 487it [06:13,  1.31it/s]Extractor Estimating: 488it [06:14,  1.27it/s]Extractor Estimating: 489it [06:15,  1.29it/s]Extractor Estimating: 490it [06:16,  1.28it/s]Extractor Estimating: 491it [06:16,  1.30it/s]Extractor Estimating: 492it [06:17,  1.29it/s]Extractor Estimating: 493it [06:18,  1.25it/s]Extractor Estimating: 494it [06:19,  1.28it/s]Extractor Estimating: 495it [06:20,  1.33it/s]Extractor Estimating: 496it [06:20,  1.33it/s]Extractor Estimating: 497it [06:21,  1.28it/s]Extractor Estimating: 498it [06:22,  1.25it/s]Extractor Estimating: 499it [06:23,  1.25it/s]Extractor Estimating: 500it [06:23,  1.28it/s]Extractor Estimating: 500it [06:23,  1.30it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:43,406 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:43,410 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:43,410 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:43,410 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:43,410 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:43:44,036 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:43:44,037 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:43:44,607 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:43:45,671 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:43:45,672 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:48,579 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:48,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:48,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:48,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:43:48,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:43:49,218 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:43:49,219 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:43:49,794 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:43:49,953 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:43:49,953 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 05:03:21,685 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 05:03:21,709 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 6015 mean pseudo reward: 0.9830891308093296
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 16725
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16825, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16825, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.312, loss:353.6387
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.338, loss:306.1175
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 49, avg_time 1.346, loss:277.5341
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 149, avg_time 1.332, loss:276.1814
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 249, avg_time 1.338, loss:277.7985
>> valid entity prec:0.6469, rec:0.6694, f1:0.6580
>> valid relation prec:0.4779, rec:0.3677, f1:0.4156
>> valid relation with NER prec:0.4779, rec:0.3677, f1:0.4156
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 98, avg_time 3.013, loss:237.0374
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 198, avg_time 1.337, loss:243.9104
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 47, avg_time 1.344, loss:241.3915
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 147, avg_time 1.339, loss:242.9700
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 247, avg_time 1.336, loss:277.0583
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6265, rec:0.6465, f1:0.6363
>> valid relation prec:0.3914, rec:0.3386, f1:0.3631
>> valid relation with NER prec:0.3914, rec:0.3386, f1:0.3631
g_step 1100, step 96, avg_time 3.011, loss:230.2784
g_step 1200, step 196, avg_time 1.330, loss:246.4490
g_step 1300, step 45, avg_time 1.328, loss:234.2227
g_step 1400, step 145, avg_time 1.349, loss:217.3867
g_step 1500, step 245, avg_time 1.341, loss:230.0835
>> valid entity prec:0.6533, rec:0.6497, f1:0.6515
>> valid relation prec:0.4433, rec:0.3463, f1:0.3888
>> valid relation with NER prec:0.4433, rec:0.3463, f1:0.3888
g_step 1600, step 94, avg_time 3.002, loss:211.8739
g_step 1700, step 194, avg_time 1.333, loss:212.6186
g_step 1800, step 43, avg_time 1.340, loss:204.7902
g_step 1900, step 143, avg_time 1.352, loss:205.0743
g_step 2000, step 243, avg_time 1.324, loss:212.5298
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6316, rec:0.6532, f1:0.6422
>> valid relation prec:0.3996, rec:0.3140, f1:0.3516
>> valid relation with NER prec:0.3996, rec:0.3140, f1:0.3516
g_step 2100, step 92, avg_time 3.012, loss:162.0883
g_step 2200, step 192, avg_time 1.335, loss:181.2200
g_step 2300, step 41, avg_time 1.332, loss:165.2484
g_step 2400, step 141, avg_time 1.331, loss:180.4503
g_step 2500, step 241, avg_time 1.333, loss:172.6550
>> valid entity prec:0.6190, rec:0.6912, f1:0.6531
>> valid relation prec:0.4039, rec:0.3503, f1:0.3752
>> valid relation with NER prec:0.4039, rec:0.3503, f1:0.3752
g_step 2600, step 90, avg_time 3.017, loss:151.4518
g_step 2700, step 190, avg_time 1.335, loss:169.9462
g_step 2800, step 39, avg_time 1.326, loss:161.9667
g_step 2900, step 139, avg_time 1.334, loss:146.1137
g_step 3000, step 239, avg_time 1.327, loss:156.6962
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6910, rec:0.6658, f1:0.6782
>> valid relation prec:0.4978, rec:0.3529, f1:0.4130
>> valid relation with NER prec:0.4978, rec:0.3529, f1:0.4130
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 88, avg_time 2.993, loss:134.6268
g_step 3200, step 188, avg_time 1.340, loss:140.8784
g_step 3300, step 37, avg_time 1.324, loss:149.9167
g_step 3400, step 137, avg_time 1.344, loss:120.0099
g_step 3500, step 237, avg_time 1.333, loss:158.0747
>> valid entity prec:0.6459, rec:0.6608, f1:0.6532
>> valid relation prec:0.4369, rec:0.3443, f1:0.3851
>> valid relation with NER prec:0.4369, rec:0.3443, f1:0.3851
g_step 3600, step 86, avg_time 2.976, loss:129.7668
g_step 3700, step 186, avg_time 1.333, loss:134.1775
g_step 3800, step 35, avg_time 1.348, loss:142.2958
g_step 3900, step 135, avg_time 1.332, loss:108.4015
g_step 4000, step 235, avg_time 1.317, loss:132.1643
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6461, rec:0.6472, f1:0.6467
>> valid relation prec:0.4176, rec:0.3500, f1:0.3808
>> valid relation with NER prec:0.4176, rec:0.3500, f1:0.3808
g_step 4100, step 84, avg_time 3.020, loss:131.1545
g_step 4200, step 184, avg_time 1.331, loss:107.7325
g_step 4300, step 33, avg_time 1.326, loss:114.1907
g_step 4400, step 133, avg_time 1.349, loss:117.5590
g_step 4500, step 233, avg_time 1.336, loss:123.1406
>> valid entity prec:0.6760, rec:0.6314, f1:0.6529
>> valid relation prec:0.4319, rec:0.3154, f1:0.3646
>> valid relation with NER prec:0.4319, rec:0.3154, f1:0.3646
g_step 4600, step 82, avg_time 3.010, loss:114.5871
g_step 4700, step 182, avg_time 1.331, loss:108.7181
g_step 4800, step 31, avg_time 1.315, loss:117.4746
g_step 4900, step 131, avg_time 1.338, loss:109.3965
g_step 5000, step 231, avg_time 1.327, loss:112.4733
learning rate was adjusted to 0.0008
>> valid entity prec:0.6864, rec:0.6433, f1:0.6642
>> valid relation prec:0.4584, rec:0.3354, f1:0.3874
>> valid relation with NER prec:0.4584, rec:0.3354, f1:0.3874
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 05:03:21 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 05:03:21 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_05-03-21_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 05:03:22 - WARNING - datasets.builder -   Using custom data configuration default-8e833b0a76d1f2c2
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8e833b0a76d1f2c2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  5.07 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 05:03:23,199 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:03:23,200 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:03:23,201 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:03:23,202 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:03:23,209 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:03:23,215 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:03:23,215 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:03:23,215 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:03:23,215 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:03:23,215 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:03:23,215 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 05:03:23,400 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:03:26,628 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 05:03:26,630 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8e833b0a76d1f2c2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.91ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.84ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.28ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.56ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.70ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  4.81ba/s]100%|██████████| 7/7 [00:01<00:00,  5.17ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.07ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.35ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.42ba/s]100%|██████████| 4/4 [00:00<00:00,  5.51ba/s]100%|██████████| 4/4 [00:00<00:00,  5.00ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  8.05ba/s] 43%|████▎     | 3/7 [00:00<00:00,  9.80ba/s] 71%|███████▏  | 5/7 [00:00<00:00,  9.96ba/s]100%|██████████| 7/7 [00:00<00:00, 11.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.96ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.17ba/s]100%|██████████| 4/4 [00:00<00:00, 11.60ba/s]
[INFO|trainer.py:414] 2023-08-29 05:03:30,119 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 05:03:30,132 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 05:03:30,132 >>   Num examples = 6015
[INFO|trainer.py:1149] 2023-08-29 05:03:30,132 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 05:03:30,132 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 05:03:30,132 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 05:03:30,132 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 05:03:30,133 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<02:54,  2.69it/s]  0%|          | 2/470 [00:00<02:31,  3.08it/s]  1%|          | 3/470 [00:00<02:24,  3.24it/s]  1%|          | 4/470 [00:01<02:20,  3.31it/s]  1%|          | 5/470 [00:01<02:18,  3.36it/s]  1%|▏         | 6/470 [00:01<02:17,  3.39it/s]  1%|▏         | 7/470 [00:02<02:15,  3.41it/s]  2%|▏         | 8/470 [00:02<02:15,  3.41it/s]  2%|▏         | 9/470 [00:02<02:14,  3.42it/s]  2%|▏         | 10/470 [00:02<02:14,  3.41it/s]  2%|▏         | 11/470 [00:03<02:14,  3.42it/s]  3%|▎         | 12/470 [00:03<02:13,  3.43it/s]  3%|▎         | 13/470 [00:03<02:13,  3.43it/s]  3%|▎         | 14/470 [00:04<02:12,  3.43it/s]  3%|▎         | 15/470 [00:04<02:12,  3.43it/s]  3%|▎         | 16/470 [00:04<02:12,  3.44it/s]  4%|▎         | 17/470 [00:05<02:11,  3.44it/s]  4%|▍         | 18/470 [00:05<02:11,  3.44it/s]  4%|▍         | 19/470 [00:05<02:11,  3.44it/s]  4%|▍         | 20/470 [00:05<02:11,  3.43it/s]  4%|▍         | 21/470 [00:06<02:11,  3.43it/s]  5%|▍         | 22/470 [00:06<02:10,  3.43it/s]  5%|▍         | 23/470 [00:06<02:10,  3.43it/s]  5%|▌         | 24/470 [00:07<02:09,  3.44it/s]  5%|▌         | 25/470 [00:07<02:09,  3.43it/s]  6%|▌         | 26/470 [00:07<02:09,  3.44it/s]  6%|▌         | 27/470 [00:07<02:09,  3.43it/s]  6%|▌         | 28/470 [00:08<02:08,  3.44it/s]  6%|▌         | 29/470 [00:08<02:08,  3.43it/s]  6%|▋         | 30/470 [00:08<02:07,  3.44it/s]  7%|▋         | 31/470 [00:09<02:07,  3.44it/s]  7%|▋         | 32/470 [00:09<02:07,  3.44it/s]  7%|▋         | 33/470 [00:09<02:07,  3.43it/s]  7%|▋         | 34/470 [00:09<02:07,  3.43it/s]  7%|▋         | 35/470 [00:10<02:06,  3.43it/s]  8%|▊         | 36/470 [00:10<02:06,  3.43it/s]  8%|▊         | 37/470 [00:10<02:06,  3.43it/s]  8%|▊         | 38/470 [00:11<02:05,  3.43it/s]  8%|▊         | 39/470 [00:11<02:05,  3.43it/s]  9%|▊         | 40/470 [00:11<02:05,  3.43it/s]  9%|▊         | 41/470 [00:12<02:04,  3.43it/s]  9%|▉         | 42/470 [00:12<02:04,  3.43it/s]  9%|▉         | 43/470 [00:12<02:04,  3.43it/s]  9%|▉         | 44/470 [00:12<02:04,  3.43it/s] 10%|▉         | 45/470 [00:13<02:03,  3.43it/s] 10%|▉         | 46/470 [00:13<02:03,  3.43it/s] 10%|█         | 47/470 [00:13<02:03,  3.43it/s] 10%|█         | 48/470 [00:14<02:03,  3.43it/s] 10%|█         | 49/470 [00:14<02:02,  3.43it/s] 11%|█         | 50/470 [00:14<02:02,  3.43it/s] 11%|█         | 51/470 [00:14<02:02,  3.43it/s] 11%|█         | 52/470 [00:15<02:01,  3.43it/s] 11%|█▏        | 53/470 [00:15<02:01,  3.42it/s] 11%|█▏        | 54/470 [00:15<02:01,  3.42it/s] 12%|█▏        | 55/470 [00:16<02:01,  3.43it/s] 12%|█▏        | 56/470 [00:16<02:00,  3.43it/s] 12%|█▏        | 57/470 [00:16<02:00,  3.43it/s] 12%|█▏        | 58/470 [00:16<02:00,  3.43it/s] 13%|█▎        | 59/470 [00:17<01:59,  3.43it/s] 13%|█▎        | 60/470 [00:17<01:59,  3.43it/s] 13%|█▎        | 61/470 [00:17<01:59,  3.43it/s] 13%|█▎        | 62/470 [00:18<01:58,  3.43it/s] 13%|█▎        | 63/470 [00:18<01:58,  3.43it/s] 14%|█▎        | 64/470 [00:18<01:58,  3.43it/s] 14%|█▍        | 65/470 [00:19<01:57,  3.43it/s] 14%|█▍        | 66/470 [00:19<01:57,  3.43it/s] 14%|█▍        | 67/470 [00:19<01:57,  3.43it/s] 14%|█▍        | 68/470 [00:19<01:57,  3.43it/s] 15%|█▍        | 69/470 [00:20<01:56,  3.43it/s] 15%|█▍        | 70/470 [00:20<01:56,  3.42it/s] 15%|█▌        | 71/470 [00:20<01:56,  3.43it/s] 15%|█▌        | 72/470 [00:21<01:56,  3.43it/s] 16%|█▌        | 73/470 [00:21<01:55,  3.43it/s] 16%|█▌        | 74/470 [00:21<01:55,  3.43it/s] 16%|█▌        | 75/470 [00:21<01:55,  3.43it/s] 16%|█▌        | 76/470 [00:22<01:54,  3.43it/s] 16%|█▋        | 77/470 [00:22<01:54,  3.43it/s] 17%|█▋        | 78/470 [00:22<01:54,  3.43it/s] 17%|█▋        | 79/470 [00:23<01:54,  3.43it/s] 17%|█▋        | 80/470 [00:23<01:53,  3.43it/s] 17%|█▋        | 81/470 [00:23<01:53,  3.43it/s] 17%|█▋        | 82/470 [00:23<01:53,  3.43it/s] 18%|█▊        | 83/470 [00:24<01:52,  3.43it/s] 18%|█▊        | 84/470 [00:24<01:52,  3.43it/s] 18%|█▊        | 85/470 [00:24<01:52,  3.43it/s] 18%|█▊        | 86/470 [00:25<01:52,  3.43it/s] 19%|█▊        | 87/470 [00:25<01:51,  3.43it/s] 19%|█▊        | 88/470 [00:25<01:51,  3.42it/s] 19%|█▉        | 89/470 [00:26<01:51,  3.42it/s] 19%|█▉        | 90/470 [00:26<01:50,  3.42it/s] 19%|█▉        | 91/470 [00:26<01:50,  3.42it/s] 20%|█▉        | 92/470 [00:26<01:50,  3.43it/s] 20%|█▉        | 93/470 [00:27<01:49,  3.43it/s] 20%|██        | 94/470 [00:27<01:49,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 05:03:57,609 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:03:57,609 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 05:03:57,609 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 58.21it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.71it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.84it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.17it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.76it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.38it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.29it/s][A
 10%|▉         | 43/438 [00:00<00:08, 47.10it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.89it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.92it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.77it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.71it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.79it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.79it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.79it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.84it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.80it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.71it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.66it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.62it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.67it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.55it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.75it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.85it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.86it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.76it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.83it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.83it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.66it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.75it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.85it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.66it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.73it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.84it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.81it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.78it/s][A
 43%|████▎     | 188/438 [00:03<00:05, 46.62it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.61it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.56it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.54it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.54it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.55it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.58it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.56it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.64it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.59it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.41it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.52it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.58it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.58it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.64it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.64it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.61it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.65it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.56it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.40it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.59it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.60it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.60it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.68it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.67it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.64it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.65it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.54it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.53it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.49it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.53it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.65it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.64it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.61it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.69it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.67it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.55it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.58it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.57it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.52it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.55it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.59it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.58it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.57it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.63it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.59it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.54it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.52it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.67it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.60it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.62it/s][A                                                
                                                 [A 20%|██        | 94/470 [00:36<01:49,  3.44it/s]
100%|██████████| 438/438 [00:09<00:00, 46.62it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:04:07,011 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 05:04:07,028 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:04:09,251 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:04:09,269 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:04:09,293 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:44<32:19,  5.17s/it] 20%|██        | 96/470 [00:44<23:07,  3.71s/it] 21%|██        | 97/470 [00:44<16:40,  2.68s/it] 21%|██        | 98/470 [00:44<12:11,  1.97s/it] 21%|██        | 99/470 [00:45<09:02,  1.46s/it] 21%|██▏       | 100/470 [00:45<06:51,  1.11s/it] 21%|██▏       | 101/470 [00:45<05:19,  1.16it/s] 22%|██▏       | 102/470 [00:46<04:14,  1.44it/s] 22%|██▏       | 103/470 [00:46<03:29,  1.75it/s] 22%|██▏       | 104/470 [00:46<02:58,  2.05it/s] 22%|██▏       | 105/470 [00:46<02:36,  2.33it/s] 23%|██▎       | 106/470 [00:47<02:20,  2.58it/s] 23%|██▎       | 107/470 [00:47<02:10,  2.78it/s] 23%|██▎       | 108/470 [00:47<02:02,  2.95it/s] 23%|██▎       | 109/470 [00:48<01:57,  3.08it/s] 23%|██▎       | 110/470 [00:48<01:53,  3.18it/s] 24%|██▎       | 111/470 [00:48<01:50,  3.25it/s] 24%|██▍       | 112/470 [00:48<01:48,  3.31it/s] 24%|██▍       | 113/470 [00:49<01:46,  3.35it/s] 24%|██▍       | 114/470 [00:49<01:45,  3.37it/s] 24%|██▍       | 115/470 [00:49<01:44,  3.39it/s] 25%|██▍       | 116/470 [00:50<01:43,  3.40it/s] 25%|██▍       | 117/470 [00:50<01:43,  3.41it/s] 25%|██▌       | 118/470 [00:50<01:43,  3.41it/s] 25%|██▌       | 119/470 [00:51<01:42,  3.42it/s] 26%|██▌       | 120/470 [00:51<01:42,  3.42it/s] 26%|██▌       | 121/470 [00:51<01:45,  3.30it/s] 26%|██▌       | 122/470 [00:51<01:44,  3.33it/s] 26%|██▌       | 123/470 [00:52<01:43,  3.36it/s] 26%|██▋       | 124/470 [00:52<01:42,  3.38it/s] 27%|██▋       | 125/470 [00:52<01:41,  3.40it/s] 27%|██▋       | 126/470 [00:53<01:40,  3.41it/s] 27%|██▋       | 127/470 [00:53<01:40,  3.42it/s] 27%|██▋       | 128/470 [00:53<01:39,  3.42it/s] 27%|██▋       | 129/470 [00:53<01:39,  3.42it/s] 28%|██▊       | 130/470 [00:54<01:39,  3.42it/s] 28%|██▊       | 131/470 [00:54<01:38,  3.43it/s] 28%|██▊       | 132/470 [00:54<01:38,  3.43it/s] 28%|██▊       | 133/470 [00:55<01:38,  3.43it/s] 29%|██▊       | 134/470 [00:55<01:37,  3.43it/s] 29%|██▊       | 135/470 [00:55<01:37,  3.43it/s] 29%|██▉       | 136/470 [00:56<01:37,  3.43it/s] 29%|██▉       | 137/470 [00:56<01:36,  3.44it/s] 29%|██▉       | 138/470 [00:56<01:36,  3.44it/s] 30%|██▉       | 139/470 [00:56<01:36,  3.43it/s] 30%|██▉       | 140/470 [00:57<01:36,  3.42it/s] 30%|███       | 141/470 [00:57<01:36,  3.42it/s] 30%|███       | 142/470 [00:57<01:35,  3.42it/s] 30%|███       | 143/470 [00:58<01:35,  3.42it/s] 31%|███       | 144/470 [00:58<01:35,  3.43it/s] 31%|███       | 145/470 [00:58<01:34,  3.43it/s] 31%|███       | 146/470 [00:58<01:34,  3.43it/s] 31%|███▏      | 147/470 [00:59<01:34,  3.43it/s] 31%|███▏      | 148/470 [00:59<01:33,  3.43it/s] 32%|███▏      | 149/470 [00:59<01:33,  3.43it/s] 32%|███▏      | 150/470 [01:00<01:33,  3.43it/s] 32%|███▏      | 151/470 [01:00<01:33,  3.42it/s] 32%|███▏      | 152/470 [01:00<01:32,  3.43it/s] 33%|███▎      | 153/470 [01:00<01:32,  3.43it/s] 33%|███▎      | 154/470 [01:01<01:32,  3.42it/s] 33%|███▎      | 155/470 [01:01<01:32,  3.42it/s] 33%|███▎      | 156/470 [01:01<01:31,  3.42it/s] 33%|███▎      | 157/470 [01:02<01:31,  3.42it/s] 34%|███▎      | 158/470 [01:02<01:31,  3.42it/s] 34%|███▍      | 159/470 [01:02<01:30,  3.42it/s] 34%|███▍      | 160/470 [01:03<01:30,  3.43it/s] 34%|███▍      | 161/470 [01:03<01:30,  3.43it/s] 34%|███▍      | 162/470 [01:03<01:29,  3.43it/s] 35%|███▍      | 163/470 [01:03<01:29,  3.43it/s] 35%|███▍      | 164/470 [01:04<01:29,  3.43it/s] 35%|███▌      | 165/470 [01:04<01:29,  3.43it/s] 35%|███▌      | 166/470 [01:04<01:28,  3.43it/s] 36%|███▌      | 167/470 [01:05<01:28,  3.43it/s] 36%|███▌      | 168/470 [01:05<01:28,  3.43it/s] 36%|███▌      | 169/470 [01:05<01:27,  3.43it/s] 36%|███▌      | 170/470 [01:05<01:27,  3.42it/s] 36%|███▋      | 171/470 [01:06<01:27,  3.42it/s] 37%|███▋      | 172/470 [01:06<01:27,  3.41it/s] 37%|███▋      | 173/470 [01:06<01:26,  3.42it/s] 37%|███▋      | 174/470 [01:07<01:26,  3.42it/s] 37%|███▋      | 175/470 [01:07<01:26,  3.42it/s] 37%|███▋      | 176/470 [01:07<01:25,  3.42it/s] 38%|███▊      | 177/470 [01:07<01:25,  3.43it/s] 38%|███▊      | 178/470 [01:08<01:25,  3.43it/s] 38%|███▊      | 179/470 [01:08<01:24,  3.43it/s] 38%|███▊      | 180/470 [01:08<01:24,  3.42it/s] 39%|███▊      | 181/470 [01:09<01:24,  3.43it/s] 39%|███▊      | 182/470 [01:09<01:24,  3.43it/s] 39%|███▉      | 183/470 [01:09<01:24,  3.42it/s] 39%|███▉      | 184/470 [01:10<01:23,  3.42it/s] 39%|███▉      | 185/470 [01:10<01:23,  3.42it/s] 40%|███▉      | 186/470 [01:10<01:22,  3.42it/s] 40%|███▉      | 187/470 [01:10<01:22,  3.42it/s] 40%|████      | 188/470 [01:11<01:22,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 05:04:41,333 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:04:41,334 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 05:04:41,334 >>   Batch size = 8
{'eval_loss': 1.0806814432144165, 'eval_runtime': 9.3836, 'eval_samples_per_second': 372.67, 'eval_steps_per_second': 46.677, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.34it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.26it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.51it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.84it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.49it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.16it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.03it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.64it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.47it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.48it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.41it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.45it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.53it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.52it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.43it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.57it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.45it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.34it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.36it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.30it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.39it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.47it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.39it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.43it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.49it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.38it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.36it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.33it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.33it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.38it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.46it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.40it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.45it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.41it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.36it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.30it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.37it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.27it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.42it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.51it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.39it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.40it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.32it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.32it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.35it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.34it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.33it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.37it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.35it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.44it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.32it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.31it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.26it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.32it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.34it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.32it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.35it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.38it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.45it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.34it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.37it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.31it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.31it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.36it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.33it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.34it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.41it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.35it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.36it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.24it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.21it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.25it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.29it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.33it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.36it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.38it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.45it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.38it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.38it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.30it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.16it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.31it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.32it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.37it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.38it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.49it/s][A                                                 
                                                 [A 40%|████      | 188/470 [01:20<01:22,  3.44it/s]
100%|██████████| 438/438 [00:09<00:00, 46.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:04:50,784 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 05:04:50,802 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:04:53,202 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:04:53,217 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:04:53,229 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:27<24:24,  5.21s/it] 40%|████      | 190/470 [01:28<17:26,  3.74s/it] 41%|████      | 191/470 [01:28<12:34,  2.70s/it] 41%|████      | 192/470 [01:28<09:10,  1.98s/it] 41%|████      | 193/470 [01:29<06:47,  1.47s/it] 41%|████▏     | 194/470 [01:29<05:08,  1.12s/it] 41%|████▏     | 195/470 [01:29<03:59,  1.15it/s] 42%|████▏     | 196/470 [01:29<03:10,  1.44it/s] 42%|████▏     | 197/470 [01:30<02:36,  1.74it/s] 42%|████▏     | 198/470 [01:30<02:13,  2.04it/s] 42%|████▏     | 199/470 [01:30<01:56,  2.33it/s] 43%|████▎     | 200/470 [01:31<01:44,  2.58it/s] 43%|████▎     | 201/470 [01:31<01:37,  2.76it/s] 43%|████▎     | 202/470 [01:31<01:31,  2.93it/s] 43%|████▎     | 203/470 [01:31<01:27,  3.07it/s] 43%|████▎     | 204/470 [01:32<01:24,  3.17it/s] 44%|████▎     | 205/470 [01:32<01:21,  3.24it/s] 44%|████▍     | 206/470 [01:32<01:20,  3.30it/s] 44%|████▍     | 207/470 [01:33<01:18,  3.34it/s] 44%|████▍     | 208/470 [01:33<01:17,  3.37it/s] 44%|████▍     | 209/470 [01:33<01:17,  3.38it/s] 45%|████▍     | 210/470 [01:34<01:16,  3.40it/s] 45%|████▍     | 211/470 [01:34<01:15,  3.41it/s] 45%|████▌     | 212/470 [01:34<01:15,  3.41it/s] 45%|████▌     | 213/470 [01:34<01:15,  3.41it/s] 46%|████▌     | 214/470 [01:35<01:14,  3.42it/s] 46%|████▌     | 215/470 [01:35<01:14,  3.43it/s] 46%|████▌     | 216/470 [01:35<01:14,  3.43it/s] 46%|████▌     | 217/470 [01:36<01:13,  3.43it/s] 46%|████▋     | 218/470 [01:36<01:13,  3.43it/s] 47%|████▋     | 219/470 [01:36<01:13,  3.43it/s] 47%|████▋     | 220/470 [01:36<01:12,  3.43it/s] 47%|████▋     | 221/470 [01:37<01:12,  3.43it/s] 47%|████▋     | 222/470 [01:37<01:12,  3.43it/s] 47%|████▋     | 223/470 [01:37<01:12,  3.42it/s] 48%|████▊     | 224/470 [01:38<01:11,  3.43it/s] 48%|████▊     | 225/470 [01:38<01:11,  3.43it/s] 48%|████▊     | 226/470 [01:38<01:11,  3.43it/s] 48%|████▊     | 227/470 [01:38<01:10,  3.43it/s] 49%|████▊     | 228/470 [01:39<01:10,  3.43it/s] 49%|████▊     | 229/470 [01:39<01:10,  3.43it/s] 49%|████▉     | 230/470 [01:39<01:09,  3.43it/s] 49%|████▉     | 231/470 [01:40<01:09,  3.43it/s] 49%|████▉     | 232/470 [01:40<01:09,  3.44it/s] 50%|████▉     | 233/470 [01:40<01:09,  3.43it/s] 50%|████▉     | 234/470 [01:41<01:08,  3.43it/s] 50%|█████     | 235/470 [01:41<01:08,  3.43it/s] 50%|█████     | 236/470 [01:41<01:08,  3.43it/s] 50%|█████     | 237/470 [01:41<01:07,  3.43it/s] 51%|█████     | 238/470 [01:42<01:07,  3.44it/s] 51%|█████     | 239/470 [01:42<01:07,  3.43it/s] 51%|█████     | 240/470 [01:42<01:07,  3.43it/s] 51%|█████▏    | 241/470 [01:43<01:06,  3.43it/s] 51%|█████▏    | 242/470 [01:43<01:06,  3.43it/s] 52%|█████▏    | 243/470 [01:43<01:06,  3.43it/s] 52%|█████▏    | 244/470 [01:43<01:05,  3.43it/s] 52%|█████▏    | 245/470 [01:44<01:05,  3.43it/s] 52%|█████▏    | 246/470 [01:44<01:05,  3.43it/s] 53%|█████▎    | 247/470 [01:44<01:04,  3.43it/s] 53%|█████▎    | 248/470 [01:45<01:04,  3.43it/s] 53%|█████▎    | 249/470 [01:45<01:04,  3.43it/s] 53%|█████▎    | 250/470 [01:45<01:04,  3.43it/s] 53%|█████▎    | 251/470 [01:45<01:03,  3.43it/s] 54%|█████▎    | 252/470 [01:46<01:03,  3.43it/s] 54%|█████▍    | 253/470 [01:46<01:03,  3.43it/s] 54%|█████▍    | 254/470 [01:46<01:02,  3.43it/s] 54%|█████▍    | 255/470 [01:47<01:02,  3.43it/s] 54%|█████▍    | 256/470 [01:47<01:02,  3.42it/s] 55%|█████▍    | 257/470 [01:47<01:02,  3.42it/s] 55%|█████▍    | 258/470 [01:48<01:01,  3.42it/s] 55%|█████▌    | 259/470 [01:48<01:01,  3.42it/s] 55%|█████▌    | 260/470 [01:48<01:01,  3.43it/s] 56%|█████▌    | 261/470 [01:48<01:01,  3.42it/s] 56%|█████▌    | 262/470 [01:49<01:00,  3.43it/s] 56%|█████▌    | 263/470 [01:49<01:00,  3.43it/s] 56%|█████▌    | 264/470 [01:49<01:00,  3.43it/s] 56%|█████▋    | 265/470 [01:50<00:59,  3.43it/s] 57%|█████▋    | 266/470 [01:50<00:59,  3.43it/s] 57%|█████▋    | 267/470 [01:50<00:59,  3.43it/s] 57%|█████▋    | 268/470 [01:50<00:59,  3.42it/s] 57%|█████▋    | 269/470 [01:51<00:58,  3.42it/s] 57%|█████▋    | 270/470 [01:51<00:58,  3.43it/s] 58%|█████▊    | 271/470 [01:51<00:59,  3.34it/s] 58%|█████▊    | 272/470 [01:52<00:58,  3.37it/s] 58%|█████▊    | 273/470 [01:52<00:58,  3.39it/s] 58%|█████▊    | 274/470 [01:52<00:57,  3.38it/s] 59%|█████▊    | 275/470 [01:53<00:57,  3.40it/s] 59%|█████▊    | 276/470 [01:53<00:56,  3.41it/s] 59%|█████▉    | 277/470 [01:53<00:56,  3.42it/s] 59%|█████▉    | 278/470 [01:53<00:56,  3.42it/s] 59%|█████▉    | 279/470 [01:54<00:55,  3.42it/s] 60%|█████▉    | 280/470 [01:54<00:55,  3.43it/s] 60%|█████▉    | 281/470 [01:54<00:55,  3.42it/s] 60%|██████    | 282/470 [01:55<00:54,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 05:05:25,180 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:05:25,180 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 05:05:25,180 >>   Batch size = 8
{'eval_loss': 1.0993143320083618, 'eval_runtime': 9.4385, 'eval_samples_per_second': 370.505, 'eval_steps_per_second': 46.406, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.31it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.41it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.65it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.58it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.36it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.06it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.77it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.42it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.28it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.35it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.31it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.48it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.52it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.50it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.56it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.43it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.28it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.19it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.24it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.27it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.33it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.42it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.47it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.51it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.47it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.30it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.30it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.26it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.27it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.31it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.31it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.42it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.43it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.47it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.44it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.35it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.33it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.36it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.31it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.34it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.37it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.40it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.42it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.35it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.44it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.41it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.36it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.26it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.22it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.32it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.33it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.41it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.44it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.39it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.42it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.43it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.35it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.35it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.28it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.30it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.24it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.27it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.40it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.38it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.38it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.32it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.37it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.38it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.26it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.29it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.33it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.35it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.46it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.40it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.38it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.40it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.40it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.36it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.15it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.32it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.36it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.35it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.40it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.38it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.38it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.40it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.47it/s][A                                                 
                                                 [A 60%|██████    | 282/470 [02:04<00:54,  3.44it/s]
100%|██████████| 438/438 [00:09<00:00, 46.47it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:05:34,638 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-29 05:05:34,659 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:05:36,795 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:05:36,810 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:05:36,817 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:11<15:59,  5.13s/it] 60%|██████    | 284/470 [02:11<11:24,  3.68s/it] 61%|██████    | 285/470 [02:12<08:12,  2.66s/it] 61%|██████    | 286/470 [02:12<05:59,  1.95s/it] 61%|██████    | 287/470 [02:12<04:25,  1.45s/it] 61%|██████▏   | 288/470 [02:12<03:20,  1.10s/it] 61%|██████▏   | 289/470 [02:13<02:35,  1.16it/s] 62%|██████▏   | 290/470 [02:13<02:04,  1.45it/s] 62%|██████▏   | 291/470 [02:13<01:42,  1.75it/s] 62%|██████▏   | 292/470 [02:14<01:26,  2.06it/s] 62%|██████▏   | 293/470 [02:14<01:15,  2.34it/s] 63%|██████▎   | 294/470 [02:14<01:08,  2.59it/s] 63%|██████▎   | 295/470 [02:14<01:02,  2.79it/s] 63%|██████▎   | 296/470 [02:15<00:58,  2.96it/s] 63%|██████▎   | 297/470 [02:15<00:56,  3.08it/s] 63%|██████▎   | 298/470 [02:15<00:54,  3.18it/s] 64%|██████▎   | 299/470 [02:16<00:52,  3.25it/s] 64%|██████▍   | 300/470 [02:16<00:51,  3.31it/s] 64%|██████▍   | 301/470 [02:16<00:50,  3.34it/s] 64%|██████▍   | 302/470 [02:16<00:49,  3.37it/s] 64%|██████▍   | 303/470 [02:17<00:49,  3.39it/s] 65%|██████▍   | 304/470 [02:17<00:48,  3.40it/s] 65%|██████▍   | 305/470 [02:17<00:48,  3.41it/s] 65%|██████▌   | 306/470 [02:18<00:48,  3.40it/s] 65%|██████▌   | 307/470 [02:18<00:47,  3.41it/s] 66%|██████▌   | 308/470 [02:18<00:47,  3.42it/s] 66%|██████▌   | 309/470 [02:19<00:47,  3.42it/s] 66%|██████▌   | 310/470 [02:19<00:46,  3.42it/s] 66%|██████▌   | 311/470 [02:19<00:46,  3.43it/s] 66%|██████▋   | 312/470 [02:19<00:46,  3.43it/s] 67%|██████▋   | 313/470 [02:20<00:45,  3.43it/s] 67%|██████▋   | 314/470 [02:20<00:45,  3.43it/s] 67%|██████▋   | 315/470 [02:20<00:45,  3.43it/s] 67%|██████▋   | 316/470 [02:21<00:44,  3.44it/s] 67%|██████▋   | 317/470 [02:21<00:44,  3.43it/s] 68%|██████▊   | 318/470 [02:21<00:44,  3.43it/s] 68%|██████▊   | 319/470 [02:21<00:44,  3.43it/s] 68%|██████▊   | 320/470 [02:22<00:43,  3.43it/s] 68%|██████▊   | 321/470 [02:22<00:43,  3.43it/s] 69%|██████▊   | 322/470 [02:22<00:43,  3.43it/s] 69%|██████▊   | 323/470 [02:23<00:42,  3.43it/s] 69%|██████▉   | 324/470 [02:23<00:42,  3.43it/s] 69%|██████▉   | 325/470 [02:23<00:42,  3.43it/s] 69%|██████▉   | 326/470 [02:23<00:41,  3.43it/s] 70%|██████▉   | 327/470 [02:24<00:41,  3.43it/s] 70%|██████▉   | 328/470 [02:24<00:41,  3.42it/s] 70%|███████   | 329/470 [02:24<00:41,  3.43it/s] 70%|███████   | 330/470 [02:25<00:40,  3.43it/s] 70%|███████   | 331/470 [02:25<00:40,  3.43it/s] 71%|███████   | 332/470 [02:25<00:40,  3.43it/s] 71%|███████   | 333/470 [02:26<00:39,  3.43it/s] 71%|███████   | 334/470 [02:26<00:39,  3.43it/s] 71%|███████▏  | 335/470 [02:26<00:39,  3.43it/s] 71%|███████▏  | 336/470 [02:26<00:39,  3.43it/s] 72%|███████▏  | 337/470 [02:27<00:38,  3.43it/s] 72%|███████▏  | 338/470 [02:27<00:38,  3.43it/s] 72%|███████▏  | 339/470 [02:27<00:38,  3.42it/s] 72%|███████▏  | 340/470 [02:28<00:37,  3.42it/s] 73%|███████▎  | 341/470 [02:28<00:37,  3.43it/s] 73%|███████▎  | 342/470 [02:28<00:37,  3.43it/s] 73%|███████▎  | 343/470 [02:28<00:37,  3.43it/s] 73%|███████▎  | 344/470 [02:29<00:36,  3.43it/s] 73%|███████▎  | 345/470 [02:29<00:36,  3.43it/s] 74%|███████▎  | 346/470 [02:29<00:36,  3.43it/s] 74%|███████▍  | 347/470 [02:30<00:35,  3.43it/s] 74%|███████▍  | 348/470 [02:30<00:35,  3.43it/s] 74%|███████▍  | 349/470 [02:30<00:35,  3.43it/s] 74%|███████▍  | 350/470 [02:31<00:35,  3.42it/s] 75%|███████▍  | 351/470 [02:31<00:34,  3.42it/s] 75%|███████▍  | 352/470 [02:31<00:34,  3.42it/s] 75%|███████▌  | 353/470 [02:31<00:34,  3.43it/s] 75%|███████▌  | 354/470 [02:32<00:33,  3.43it/s] 76%|███████▌  | 355/470 [02:32<00:33,  3.43it/s] 76%|███████▌  | 356/470 [02:32<00:33,  3.43it/s] 76%|███████▌  | 357/470 [02:33<00:32,  3.43it/s] 76%|███████▌  | 358/470 [02:33<00:32,  3.43it/s] 76%|███████▋  | 359/470 [02:33<00:32,  3.43it/s] 77%|███████▋  | 360/470 [02:33<00:32,  3.43it/s] 77%|███████▋  | 361/470 [02:34<00:32,  3.35it/s] 77%|███████▋  | 362/470 [02:34<00:32,  3.37it/s] 77%|███████▋  | 363/470 [02:34<00:31,  3.39it/s] 77%|███████▋  | 364/470 [02:35<00:31,  3.40it/s] 78%|███████▊  | 365/470 [02:35<00:30,  3.41it/s] 78%|███████▊  | 366/470 [02:35<00:30,  3.42it/s] 78%|███████▊  | 367/470 [02:35<00:30,  3.42it/s] 78%|███████▊  | 368/470 [02:36<00:29,  3.42it/s] 79%|███████▊  | 369/470 [02:36<00:29,  3.43it/s] 79%|███████▊  | 370/470 [02:36<00:29,  3.43it/s] 79%|███████▉  | 371/470 [02:37<00:28,  3.43it/s] 79%|███████▉  | 372/470 [02:37<00:28,  3.43it/s] 79%|███████▉  | 373/470 [02:37<00:28,  3.43it/s] 80%|███████▉  | 374/470 [02:38<00:28,  3.43it/s] 80%|███████▉  | 375/470 [02:38<00:27,  3.43it/s] 80%|████████  | 376/470 [02:38<00:27,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 05:06:08,741 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:06:08,741 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 05:06:08,741 >>   Batch size = 8
{'eval_loss': 1.119210958480835, 'eval_runtime': 9.4418, 'eval_samples_per_second': 370.375, 'eval_steps_per_second': 46.39, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.92it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.08it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.62it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.85it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.47it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.17it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.78it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.40it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.25it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.37it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.38it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.39it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.49it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.51it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.47it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.43it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.27it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.29it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.24it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.23it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.32it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.21it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.42it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.43it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.33it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.18it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.14it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.21it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.18it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.38it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.37it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.34it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.46it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.44it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.36it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.29it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.24it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.27it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.28it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.41it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.35it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.40it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.47it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.34it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.37it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.23it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.23it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.26it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.29it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.33it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.28it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.35it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.39it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.36it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.39it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.30it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.25it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.35it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.32it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.41it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.36it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.35it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.40it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.39it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.34it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.25it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.24it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.33it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.32it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.35it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.38it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.38it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.43it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.36it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.33it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.33it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.37it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.36it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.34it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.33it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.37it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.37it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.40it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.31it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.31it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.33it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.45it/s][A                                                 
                                                 [A 80%|████████  | 376/470 [02:48<00:27,  3.43it/s]
100%|██████████| 438/438 [00:09<00:00, 46.45it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:06:18,204 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-29 05:06:18,221 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:06:20,508 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:06:20,524 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:06:20,536 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [02:55<08:02,  5.19s/it] 80%|████████  | 378/470 [02:55<05:42,  3.72s/it] 81%|████████  | 379/470 [02:55<04:04,  2.69s/it] 81%|████████  | 380/470 [02:56<02:57,  1.97s/it] 81%|████████  | 381/470 [02:56<02:10,  1.47s/it] 81%|████████▏ | 382/470 [02:56<01:37,  1.11s/it] 81%|████████▏ | 383/470 [02:56<01:15,  1.15it/s] 82%|████████▏ | 384/470 [02:57<00:59,  1.44it/s] 82%|████████▏ | 385/470 [02:57<00:48,  1.75it/s] 82%|████████▏ | 386/470 [02:57<00:41,  2.05it/s] 82%|████████▏ | 387/470 [02:58<00:35,  2.33it/s] 83%|████████▎ | 388/470 [02:58<00:31,  2.58it/s] 83%|████████▎ | 389/470 [02:58<00:29,  2.78it/s] 83%|████████▎ | 390/470 [02:58<00:27,  2.95it/s] 83%|████████▎ | 391/470 [02:59<00:25,  3.08it/s] 83%|████████▎ | 392/470 [02:59<00:24,  3.18it/s] 84%|████████▎ | 393/470 [02:59<00:23,  3.25it/s] 84%|████████▍ | 394/470 [03:00<00:22,  3.31it/s] 84%|████████▍ | 395/470 [03:00<00:22,  3.34it/s] 84%|████████▍ | 396/470 [03:00<00:21,  3.37it/s] 84%|████████▍ | 397/470 [03:01<00:21,  3.39it/s] 85%|████████▍ | 398/470 [03:01<00:21,  3.40it/s] 85%|████████▍ | 399/470 [03:01<00:20,  3.41it/s] 85%|████████▌ | 400/470 [03:01<00:20,  3.41it/s] 85%|████████▌ | 401/470 [03:02<00:20,  3.42it/s] 86%|████████▌ | 402/470 [03:02<00:19,  3.42it/s] 86%|████████▌ | 403/470 [03:02<00:19,  3.42it/s] 86%|████████▌ | 404/470 [03:03<00:19,  3.43it/s] 86%|████████▌ | 405/470 [03:03<00:18,  3.43it/s] 86%|████████▋ | 406/470 [03:03<00:18,  3.43it/s] 87%|████████▋ | 407/470 [03:03<00:18,  3.43it/s] 87%|████████▋ | 408/470 [03:04<00:18,  3.43it/s] 87%|████████▋ | 409/470 [03:04<00:17,  3.43it/s] 87%|████████▋ | 410/470 [03:04<00:17,  3.43it/s] 87%|████████▋ | 411/470 [03:05<00:17,  3.43it/s] 88%|████████▊ | 412/470 [03:05<00:16,  3.43it/s] 88%|████████▊ | 413/470 [03:05<00:16,  3.43it/s] 88%|████████▊ | 414/470 [03:05<00:16,  3.43it/s] 88%|████████▊ | 415/470 [03:06<00:16,  3.43it/s] 89%|████████▊ | 416/470 [03:06<00:15,  3.43it/s] 89%|████████▊ | 417/470 [03:06<00:15,  3.43it/s] 89%|████████▉ | 418/470 [03:07<00:15,  3.43it/s] 89%|████████▉ | 419/470 [03:07<00:14,  3.44it/s] 89%|████████▉ | 420/470 [03:07<00:14,  3.44it/s] 90%|████████▉ | 421/470 [03:08<00:14,  3.43it/s] 90%|████████▉ | 422/470 [03:08<00:14,  3.42it/s] 90%|█████████ | 423/470 [03:08<00:13,  3.42it/s] 90%|█████████ | 424/470 [03:08<00:13,  3.43it/s] 90%|█████████ | 425/470 [03:09<00:13,  3.43it/s] 91%|█████████ | 426/470 [03:09<00:12,  3.43it/s] 91%|█████████ | 427/470 [03:09<00:12,  3.43it/s] 91%|█████████ | 428/470 [03:10<00:12,  3.43it/s] 91%|█████████▏| 429/470 [03:10<00:11,  3.43it/s] 91%|█████████▏| 430/470 [03:10<00:11,  3.43it/s] 92%|█████████▏| 431/470 [03:10<00:11,  3.43it/s] 92%|█████████▏| 432/470 [03:11<00:11,  3.43it/s] 92%|█████████▏| 433/470 [03:11<00:10,  3.42it/s] 92%|█████████▏| 434/470 [03:11<00:10,  3.42it/s] 93%|█████████▎| 435/470 [03:12<00:10,  3.43it/s] 93%|█████████▎| 436/470 [03:12<00:09,  3.43it/s] 93%|█████████▎| 437/470 [03:12<00:09,  3.43it/s] 93%|█████████▎| 438/470 [03:12<00:09,  3.43it/s] 93%|█████████▎| 439/470 [03:13<00:09,  3.43it/s] 94%|█████████▎| 440/470 [03:13<00:08,  3.43it/s] 94%|█████████▍| 441/470 [03:13<00:08,  3.43it/s] 94%|█████████▍| 442/470 [03:14<00:08,  3.43it/s] 94%|█████████▍| 443/470 [03:14<00:07,  3.43it/s] 94%|█████████▍| 444/470 [03:14<00:07,  3.42it/s] 95%|█████████▍| 445/470 [03:15<00:07,  3.42it/s] 95%|█████████▍| 446/470 [03:15<00:07,  3.43it/s] 95%|█████████▌| 447/470 [03:15<00:06,  3.43it/s] 95%|█████████▌| 448/470 [03:15<00:06,  3.42it/s] 96%|█████████▌| 449/470 [03:16<00:06,  3.43it/s] 96%|█████████▌| 450/470 [03:16<00:05,  3.43it/s] 96%|█████████▌| 451/470 [03:16<00:05,  3.43it/s] 96%|█████████▌| 452/470 [03:17<00:05,  3.43it/s] 96%|█████████▋| 453/470 [03:17<00:04,  3.43it/s] 97%|█████████▋| 454/470 [03:17<00:04,  3.43it/s] 97%|█████████▋| 455/470 [03:17<00:04,  3.43it/s] 97%|█████████▋| 456/470 [03:18<00:04,  3.43it/s] 97%|█████████▋| 457/470 [03:18<00:03,  3.43it/s] 97%|█████████▋| 458/470 [03:18<00:03,  3.43it/s] 98%|█████████▊| 459/470 [03:19<00:03,  3.43it/s] 98%|█████████▊| 460/470 [03:19<00:02,  3.43it/s] 98%|█████████▊| 461/470 [03:19<00:02,  3.42it/s] 98%|█████████▊| 462/470 [03:19<00:02,  3.42it/s] 99%|█████████▊| 463/470 [03:20<00:02,  3.42it/s] 99%|█████████▊| 464/470 [03:20<00:01,  3.43it/s] 99%|█████████▉| 465/470 [03:20<00:01,  3.43it/s] 99%|█████████▉| 466/470 [03:21<00:01,  3.43it/s] 99%|█████████▉| 467/470 [03:21<00:00,  3.43it/s]100%|█████████▉| 468/470 [03:21<00:00,  3.43it/s]100%|█████████▉| 469/470 [03:22<00:00,  3.43it/s]100%|██████████| 470/470 [03:22<00:00,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 05:06:52,454 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:06:52,454 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 05:06:52,454 >>   Batch size = 8
{'eval_loss': 1.129530906677246, 'eval_runtime': 9.4453, 'eval_samples_per_second': 370.235, 'eval_steps_per_second': 46.372, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.76it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.44it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.58it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.83it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.47it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.12it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.81it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.48it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.31it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.38it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.36it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.42it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.52it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.53it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.54it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.45it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.30it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.24it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.25it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.30it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.29it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.36it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.48it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.48it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.44it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.34it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.10it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.15it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.25it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.30it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.28it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.42it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.49it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.42it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.38it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.30it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.27it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.31it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.31it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.37it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.37it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.37it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.48it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.42it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.37it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.30it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.26it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.30it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.29it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.36it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.35it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.40it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.37it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.38it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.31it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.29it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.31it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.32it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.33it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.38it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.38it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.34it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.44it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.40it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.35it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.38it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.28it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.35it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.36it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.40it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.38it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.33it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.41it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.41it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.39it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.24it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.24it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.32it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.34it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.40it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.39it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.38it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.31it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.37it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.31it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.21it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.41it/s][A                                                 
                                                 [A100%|██████████| 470/470 [03:31<00:00,  3.44it/s]
100%|██████████| 438/438 [00:09<00:00, 46.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 05:07:01,915 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-29 05:07:01,932 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:07:04,195 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:07:04,223 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:07:04,258 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 05:07:09,503 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 05:07:09,506 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-94 (score: 1.0806814432144165).
                                                 100%|██████████| 470/470 [03:41<00:00,  3.44it/s]100%|██████████| 470/470 [03:41<00:00,  2.12it/s]
[INFO|trainer.py:1894] 2023-08-29 05:07:11,396 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 05:07:11,412 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 05:07:13,677 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 05:07:13,692 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 05:07:13,700 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:07:13,877 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,877 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,877 >>   train_loss               =     0.4374
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,877 >>   train_runtime            = 0:03:41.25
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,877 >>   train_samples            =       6015
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,877 >>   train_samples_per_second =    135.926
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:13,877 >>   train_steps_per_second   =      2.124
{'eval_loss': 1.1336784362792969, 'eval_runtime': 9.4427, 'eval_samples_per_second': 370.34, 'eval_steps_per_second': 46.385, 'epoch': 5.0}
{'train_runtime': 221.2594, 'train_samples_per_second': 135.926, 'train_steps_per_second': 2.124, 'train_loss': 0.43743049134599404, 'epoch': 5.0}
08/29/2023 05:07:13 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 05:07:13,919 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 05:07:13,920 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 05:07:13,920 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 57.85it/s]  3%|▎         | 12/438 [00:00<00:08, 51.00it/s]  4%|▍         | 18/438 [00:00<00:08, 49.14it/s]  5%|▌         | 23/438 [00:00<00:08, 48.44it/s]  6%|▋         | 28/438 [00:00<00:08, 48.01it/s]  8%|▊         | 33/438 [00:00<00:08, 47.67it/s]  9%|▊         | 38/438 [00:00<00:08, 47.43it/s] 10%|▉         | 43/438 [00:00<00:08, 47.15it/s] 11%|█         | 48/438 [00:00<00:08, 46.75it/s] 12%|█▏        | 53/438 [00:01<00:08, 46.66it/s] 13%|█▎        | 58/438 [00:01<00:08, 46.70it/s] 14%|█▍        | 63/438 [00:01<00:08, 46.80it/s] 16%|█▌        | 68/438 [00:01<00:07, 46.79it/s] 17%|█▋        | 73/438 [00:01<00:07, 46.90it/s] 18%|█▊        | 78/438 [00:01<00:07, 46.92it/s] 19%|█▉        | 83/438 [00:01<00:07, 46.93it/s] 20%|██        | 88/438 [00:01<00:07, 46.87it/s] 21%|██        | 93/438 [00:01<00:07, 46.73it/s] 22%|██▏       | 98/438 [00:02<00:07, 46.59it/s] 24%|██▎       | 103/438 [00:02<00:07, 46.58it/s] 25%|██▍       | 108/438 [00:02<00:07, 46.56it/s] 26%|██▌       | 113/438 [00:02<00:06, 46.65it/s] 27%|██▋       | 118/438 [00:02<00:06, 46.74it/s] 28%|██▊       | 123/438 [00:02<00:06, 46.78it/s] 29%|██▉       | 128/438 [00:02<00:06, 46.80it/s] 30%|███       | 133/438 [00:02<00:06, 46.86it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.79it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.73it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.58it/s] 35%|███▍      | 153/438 [00:03<00:06, 46.55it/s] 36%|███▌      | 158/438 [00:03<00:05, 46.68it/s] 37%|███▋      | 163/438 [00:03<00:05, 46.67it/s] 38%|███▊      | 168/438 [00:03<00:05, 46.75it/s] 39%|███▉      | 173/438 [00:03<00:05, 46.81it/s] 41%|████      | 178/438 [00:03<00:05, 46.84it/s] 42%|████▏     | 183/438 [00:03<00:05, 46.75it/s] 43%|████▎     | 188/438 [00:03<00:05, 46.69it/s] 44%|████▍     | 193/438 [00:04<00:05, 46.55it/s] 45%|████▌     | 198/438 [00:04<00:05, 46.55it/s] 46%|████▋     | 203/438 [00:04<00:05, 46.59it/s] 47%|████▋     | 208/438 [00:04<00:04, 46.57it/s] 49%|████▊     | 213/438 [00:04<00:04, 46.58it/s] 50%|████▉     | 218/438 [00:04<00:04, 46.63it/s] 51%|█████     | 223/438 [00:04<00:04, 46.64it/s] 52%|█████▏    | 228/438 [00:04<00:04, 46.46it/s] 53%|█████▎    | 233/438 [00:04<00:04, 46.50it/s] 54%|█████▍    | 238/438 [00:05<00:04, 46.44it/s] 55%|█████▌    | 243/438 [00:05<00:04, 46.44it/s] 57%|█████▋    | 248/438 [00:05<00:04, 46.44it/s] 58%|█████▊    | 253/438 [00:05<00:03, 46.55it/s] 59%|█████▉    | 258/438 [00:05<00:03, 46.61it/s] 60%|██████    | 263/438 [00:05<00:03, 46.57it/s] 61%|██████    | 268/438 [00:05<00:03, 46.67it/s] 62%|██████▏   | 273/438 [00:05<00:03, 46.57it/s] 63%|██████▎   | 278/438 [00:05<00:03, 46.58it/s] 65%|██████▍   | 283/438 [00:06<00:03, 46.50it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.46it/s] 67%|██████▋   | 293/438 [00:06<00:03, 46.52it/s] 68%|██████▊   | 298/438 [00:06<00:03, 46.57it/s] 69%|██████▉   | 303/438 [00:06<00:02, 46.60it/s] 70%|███████   | 308/438 [00:06<00:02, 46.53it/s] 71%|███████▏  | 313/438 [00:06<00:02, 46.60it/s] 73%|███████▎  | 318/438 [00:06<00:02, 46.58it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.46it/s] 75%|███████▍  | 328/438 [00:07<00:02, 46.44it/s] 76%|███████▌  | 333/438 [00:07<00:02, 46.43it/s] 77%|███████▋  | 338/438 [00:07<00:02, 46.44it/s] 78%|███████▊  | 343/438 [00:07<00:02, 46.55it/s] 79%|███████▉  | 348/438 [00:07<00:01, 46.60it/s] 81%|████████  | 353/438 [00:07<00:01, 46.58it/s] 82%|████████▏ | 358/438 [00:07<00:01, 46.46it/s] 83%|████████▎ | 363/438 [00:07<00:01, 46.50it/s] 84%|████████▍ | 368/438 [00:07<00:01, 46.42it/s] 85%|████████▌ | 373/438 [00:07<00:01, 46.42it/s] 86%|████████▋ | 378/438 [00:08<00:01, 46.35it/s] 87%|████████▋ | 383/438 [00:08<00:01, 46.42it/s] 89%|████████▊ | 388/438 [00:08<00:01, 46.50it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.56it/s] 91%|█████████ | 398/438 [00:08<00:00, 46.55it/s] 92%|█████████▏| 403/438 [00:08<00:00, 46.61it/s] 93%|█████████▎| 408/438 [00:08<00:00, 46.55it/s] 94%|█████████▍| 413/438 [00:08<00:00, 46.48it/s] 95%|█████████▌| 418/438 [00:08<00:00, 46.46it/s] 97%|█████████▋| 423/438 [00:09<00:00, 46.46it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.41it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.47it/s]100%|██████████| 438/438 [00:09<00:00, 46.69it/s]100%|██████████| 438/438 [00:09<00:00, 46.74it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 05:07:23,316 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:23,316 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:23,316 >>   eval_loss               =     1.0807
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:23,316 >>   eval_runtime            = 0:00:09.39
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:23,316 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:23,316 >>   eval_samples_per_second =    372.155
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:23,316 >>   eval_steps_per_second   =     46.613
[INFO|trainer_pt_utils.py:913] 2023-08-29 05:07:23,316 >>   perplexity              =     2.9467
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:29,472 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:29,479 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:29,480 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:29,480 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:29,480 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:07:30,382 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:07:30,383 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:07:30,953 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:07:31,992 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:07:31,992 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:34,851 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:34,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:34,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:34,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:07:34,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:07:35,493 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:07:35,496 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:07:36,055 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:07:36,202 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:07:36,202 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-470
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.27it/s]Extractor Predicting: 4it [00:03,  1.29it/s]Extractor Predicting: 5it [00:03,  1.30it/s]Extractor Predicting: 6it [00:04,  1.24it/s]Extractor Predicting: 7it [00:05,  1.27it/s]Extractor Predicting: 8it [00:06,  1.28it/s]Extractor Predicting: 9it [00:07,  1.26it/s]Extractor Predicting: 10it [00:07,  1.26it/s]Extractor Predicting: 11it [00:08,  1.25it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.28it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:12,  1.29it/s]Extractor Predicting: 17it [00:13,  1.30it/s]Extractor Predicting: 18it [00:14,  1.30it/s]Extractor Predicting: 19it [00:14,  1.29it/s]Extractor Predicting: 20it [00:15,  1.26it/s]Extractor Predicting: 21it [00:16,  1.26it/s]Extractor Predicting: 22it [00:17,  1.24it/s]Extractor Predicting: 23it [00:18,  1.26it/s]Extractor Predicting: 24it [00:18,  1.31it/s]Extractor Predicting: 25it [00:19,  1.29it/s]Extractor Predicting: 26it [00:20,  1.33it/s]Extractor Predicting: 27it [00:20,  1.35it/s]Extractor Predicting: 28it [00:21,  1.35it/s]Extractor Predicting: 29it [00:22,  1.34it/s]Extractor Predicting: 30it [00:23,  1.35it/s]Extractor Predicting: 31it [00:23,  1.33it/s]Extractor Predicting: 32it [00:24,  1.31it/s]Extractor Predicting: 33it [00:25,  1.32it/s]Extractor Predicting: 34it [00:26,  1.30it/s]Extractor Predicting: 35it [00:27,  1.31it/s]Extractor Predicting: 36it [00:27,  1.29it/s]Extractor Predicting: 37it [00:28,  1.28it/s]Extractor Predicting: 38it [00:29,  1.26it/s]Extractor Predicting: 39it [00:30,  1.28it/s]Extractor Predicting: 40it [00:30,  1.27it/s]Extractor Predicting: 41it [00:31,  1.24it/s]Extractor Predicting: 42it [00:32,  1.24it/s]Extractor Predicting: 43it [00:33,  1.27it/s]Extractor Predicting: 44it [00:34,  1.26it/s]Extractor Predicting: 45it [00:34,  1.27it/s]Extractor Predicting: 46it [00:35,  1.30it/s]Extractor Predicting: 47it [00:36,  1.29it/s]Extractor Predicting: 48it [00:37,  1.26it/s]Extractor Predicting: 49it [00:38,  1.25it/s]Extractor Predicting: 50it [00:38,  1.23it/s]Extractor Predicting: 51it [00:39,  1.22it/s]Extractor Predicting: 52it [00:40,  1.21it/s]Extractor Predicting: 53it [00:41,  1.23it/s]Extractor Predicting: 54it [00:42,  1.23it/s]Extractor Predicting: 55it [00:43,  1.23it/s]Extractor Predicting: 56it [00:43,  1.24it/s]Extractor Predicting: 57it [00:44,  1.24it/s]Extractor Predicting: 58it [00:45,  1.24it/s]Extractor Predicting: 59it [00:46,  1.26it/s]Extractor Predicting: 60it [00:47,  1.22it/s]Extractor Predicting: 61it [00:47,  1.21it/s]Extractor Predicting: 62it [00:48,  1.19it/s]Extractor Predicting: 63it [00:49,  1.18it/s]Extractor Predicting: 64it [00:50,  1.19it/s]Extractor Predicting: 65it [00:51,  1.18it/s]Extractor Predicting: 66it [00:52,  1.17it/s]Extractor Predicting: 67it [00:53,  1.16it/s]Extractor Predicting: 68it [00:54,  1.16it/s]Extractor Predicting: 69it [00:54,  1.15it/s]Extractor Predicting: 70it [00:55,  1.16it/s]Extractor Predicting: 71it [00:56,  1.15it/s]Extractor Predicting: 72it [00:57,  1.15it/s]Extractor Predicting: 73it [00:58,  1.13it/s]Extractor Predicting: 74it [00:59,  1.14it/s]Extractor Predicting: 75it [01:00,  1.13it/s]Extractor Predicting: 76it [01:01,  1.15it/s]Extractor Predicting: 77it [01:01,  1.15it/s]Extractor Predicting: 78it [01:02,  1.15it/s]Extractor Predicting: 79it [01:03,  1.17it/s]Extractor Predicting: 80it [01:04,  1.15it/s]Extractor Predicting: 81it [01:05,  1.13it/s]Extractor Predicting: 82it [01:06,  1.13it/s]Extractor Predicting: 83it [01:07,  1.17it/s]Extractor Predicting: 84it [01:07,  1.17it/s]Extractor Predicting: 85it [01:08,  1.18it/s]Extractor Predicting: 86it [01:09,  1.17it/s]Extractor Predicting: 87it [01:10,  1.09it/s]Extractor Predicting: 88it [01:11,  1.11it/s]Extractor Predicting: 89it [01:12,  1.14it/s]Extractor Predicting: 90it [01:13,  1.17it/s]Extractor Predicting: 91it [01:13,  1.19it/s]Extractor Predicting: 92it [01:14,  1.17it/s]Extractor Predicting: 93it [01:15,  1.20it/s]Extractor Predicting: 94it [01:16,  1.20it/s]Extractor Predicting: 95it [01:17,  1.20it/s]Extractor Predicting: 96it [01:18,  1.19it/s]Extractor Predicting: 97it [01:19,  1.20it/s]Extractor Predicting: 98it [01:19,  1.19it/s]Extractor Predicting: 99it [01:20,  1.20it/s]Extractor Predicting: 100it [01:21,  1.21it/s]Extractor Predicting: 101it [01:22,  1.22it/s]Extractor Predicting: 102it [01:23,  1.19it/s]Extractor Predicting: 103it [01:24,  1.16it/s]Extractor Predicting: 104it [01:24,  1.17it/s]Extractor Predicting: 105it [01:25,  1.19it/s]Extractor Predicting: 106it [01:26,  1.19it/s]Extractor Predicting: 107it [01:27,  1.17it/s]Extractor Predicting: 108it [01:28,  1.18it/s]Extractor Predicting: 109it [01:29,  1.18it/s]Extractor Predicting: 110it [01:30,  1.17it/s]Extractor Predicting: 111it [01:30,  1.17it/s]Extractor Predicting: 112it [01:31,  1.17it/s]Extractor Predicting: 113it [01:32,  1.18it/s]Extractor Predicting: 114it [01:33,  1.18it/s]Extractor Predicting: 115it [01:34,  1.18it/s]Extractor Predicting: 116it [01:35,  1.18it/s]Extractor Predicting: 117it [01:35,  1.21it/s]Extractor Predicting: 118it [01:36,  1.23it/s]Extractor Predicting: 119it [01:37,  1.27it/s]Extractor Predicting: 120it [01:38,  1.27it/s]Extractor Predicting: 121it [01:38,  1.26it/s]Extractor Predicting: 122it [01:39,  1.30it/s]Extractor Predicting: 123it [01:40,  1.31it/s]Extractor Predicting: 124it [01:41,  1.29it/s]Extractor Predicting: 125it [01:42,  1.30it/s]Extractor Predicting: 126it [01:42,  1.32it/s]Extractor Predicting: 127it [01:43,  1.34it/s]Extractor Predicting: 128it [01:44,  1.34it/s]Extractor Predicting: 129it [01:44,  1.35it/s]Extractor Predicting: 130it [01:45,  1.31it/s]Extractor Predicting: 131it [01:46,  1.32it/s]Extractor Predicting: 132it [01:47,  1.36it/s]Extractor Predicting: 133it [01:47,  1.37it/s]Extractor Predicting: 134it [01:48,  1.31it/s]Extractor Predicting: 135it [01:49,  1.32it/s]Extractor Predicting: 136it [01:50,  1.35it/s]Extractor Predicting: 137it [01:50,  1.36it/s]Extractor Predicting: 138it [01:51,  1.35it/s]Extractor Predicting: 139it [01:52,  1.35it/s]Extractor Predicting: 140it [01:53,  1.38it/s]Extractor Predicting: 141it [01:53,  1.39it/s]Extractor Predicting: 142it [01:54,  1.41it/s]Extractor Predicting: 143it [01:55,  1.38it/s]Extractor Predicting: 144it [01:55,  1.36it/s]Extractor Predicting: 145it [01:56,  1.51it/s]Extractor Predicting: 145it [01:56,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:41,751 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:41,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:41,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:41,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:41,756 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:09:42,347 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:09:42,348 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:09:42,931 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:09:43,950 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:09:43,950 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:46,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:46,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:46,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:46,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:09:46,815 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:09:47,460 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:09:47,461 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:09:48,035 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:09:48,180 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:09:48,180 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5919566199728875,
  "recall": 0.37460680583357164,
  "score": 0.4588441330998249,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.25it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 6it [00:04,  1.21it/s]Extractor Predicting: 7it [00:05,  1.22it/s]Extractor Predicting: 8it [00:06,  1.21it/s]Extractor Predicting: 9it [00:07,  1.20it/s]Extractor Predicting: 10it [00:08,  1.21it/s]Extractor Predicting: 11it [00:09,  1.22it/s]Extractor Predicting: 12it [00:09,  1.24it/s]Extractor Predicting: 13it [00:10,  1.22it/s]Extractor Predicting: 14it [00:11,  1.23it/s]Extractor Predicting: 15it [00:12,  1.25it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.22it/s]Extractor Predicting: 18it [00:14,  1.23it/s]Extractor Predicting: 19it [00:15,  1.21it/s]Extractor Predicting: 20it [00:16,  1.21it/s]Extractor Predicting: 21it [00:17,  1.23it/s]Extractor Predicting: 22it [00:18,  1.19it/s]Extractor Predicting: 23it [00:18,  1.21it/s]Extractor Predicting: 24it [00:19,  1.23it/s]Extractor Predicting: 25it [00:20,  1.24it/s]Extractor Predicting: 26it [00:21,  1.22it/s]Extractor Predicting: 27it [00:22,  1.21it/s]Extractor Predicting: 28it [00:22,  1.21it/s]Extractor Predicting: 29it [00:23,  1.20it/s]Extractor Predicting: 30it [00:24,  1.17it/s]Extractor Predicting: 31it [00:25,  1.18it/s]Extractor Predicting: 32it [00:26,  1.17it/s]Extractor Predicting: 33it [00:27,  1.20it/s]Extractor Predicting: 34it [00:27,  1.21it/s]Extractor Predicting: 35it [00:28,  1.21it/s]Extractor Predicting: 36it [00:29,  1.23it/s]Extractor Predicting: 37it [00:30,  1.28it/s]Extractor Predicting: 38it [00:31,  1.27it/s]Extractor Predicting: 39it [00:31,  1.26it/s]Extractor Predicting: 40it [00:32,  1.27it/s]Extractor Predicting: 41it [00:33,  1.26it/s]Extractor Predicting: 42it [00:34,  1.25it/s]Extractor Predicting: 43it [00:35,  1.25it/s]Extractor Predicting: 44it [00:35,  1.25it/s]Extractor Predicting: 45it [00:36,  1.23it/s]Extractor Predicting: 46it [00:37,  1.26it/s]Extractor Predicting: 47it [00:38,  1.25it/s]Extractor Predicting: 48it [00:39,  1.25it/s]Extractor Predicting: 49it [00:39,  1.25it/s]Extractor Predicting: 50it [00:40,  1.23it/s]Extractor Predicting: 51it [00:41,  1.24it/s]Extractor Predicting: 52it [00:42,  1.24it/s]Extractor Predicting: 53it [00:43,  1.24it/s]Extractor Predicting: 54it [00:43,  1.25it/s]Extractor Predicting: 55it [00:44,  1.22it/s]Extractor Predicting: 56it [00:45,  1.23it/s]Extractor Predicting: 57it [00:46,  1.26it/s]Extractor Predicting: 58it [00:47,  1.28it/s]Extractor Predicting: 59it [00:47,  1.28it/s]Extractor Predicting: 60it [00:48,  1.28it/s]Extractor Predicting: 61it [00:49,  1.27it/s]Extractor Predicting: 62it [00:50,  1.27it/s]Extractor Predicting: 63it [00:51,  1.26it/s]Extractor Predicting: 64it [00:51,  1.26it/s]Extractor Predicting: 65it [00:52,  1.15it/s]Extractor Predicting: 66it [00:53,  1.16it/s]Extractor Predicting: 67it [00:54,  1.20it/s]Extractor Predicting: 68it [00:55,  1.24it/s]Extractor Predicting: 69it [00:56,  1.25it/s]Extractor Predicting: 70it [00:56,  1.25it/s]Extractor Predicting: 71it [00:57,  1.24it/s]Extractor Predicting: 72it [00:58,  1.25it/s]Extractor Predicting: 73it [00:59,  1.22it/s]Extractor Predicting: 74it [01:00,  1.23it/s]Extractor Predicting: 75it [01:00,  1.23it/s]Extractor Predicting: 76it [01:01,  1.25it/s]Extractor Predicting: 77it [01:02,  1.21it/s]Extractor Predicting: 78it [01:03,  1.21it/s]Extractor Predicting: 79it [01:04,  1.22it/s]Extractor Predicting: 80it [01:05,  1.21it/s]Extractor Predicting: 81it [01:05,  1.23it/s]Extractor Predicting: 82it [01:06,  1.21it/s]Extractor Predicting: 83it [01:07,  1.22it/s]Extractor Predicting: 84it [01:08,  1.22it/s]Extractor Predicting: 85it [01:09,  1.24it/s]Extractor Predicting: 86it [01:09,  1.22it/s]Extractor Predicting: 87it [01:10,  1.22it/s]Extractor Predicting: 88it [01:11,  1.22it/s]Extractor Predicting: 89it [01:12,  1.23it/s]Extractor Predicting: 90it [01:13,  1.24it/s]Extractor Predicting: 91it [01:14,  1.24it/s]Extractor Predicting: 92it [01:14,  1.23it/s]Extractor Predicting: 93it [01:15,  1.21it/s]Extractor Predicting: 94it [01:16,  1.23it/s]Extractor Predicting: 95it [01:17,  1.22it/s]Extractor Predicting: 96it [01:18,  1.21it/s]Extractor Predicting: 97it [01:18,  1.23it/s]Extractor Predicting: 98it [01:19,  1.23it/s]Extractor Predicting: 99it [01:20,  1.24it/s]Extractor Predicting: 100it [01:21,  1.25it/s]Extractor Predicting: 101it [01:22,  1.26it/s]Extractor Predicting: 102it [01:22,  1.26it/s]Extractor Predicting: 103it [01:23,  1.25it/s]Extractor Predicting: 104it [01:24,  1.22it/s]Extractor Predicting: 105it [01:25,  1.20it/s]Extractor Predicting: 106it [01:26,  1.21it/s]Extractor Predicting: 107it [01:27,  1.21it/s]Extractor Predicting: 108it [01:27,  1.22it/s]Extractor Predicting: 109it [01:28,  1.20it/s]Extractor Predicting: 110it [01:29,  1.22it/s]Extractor Predicting: 111it [01:30,  1.22it/s]Extractor Predicting: 112it [01:31,  1.20it/s]Extractor Predicting: 113it [01:32,  1.20it/s]Extractor Predicting: 114it [01:32,  1.19it/s]Extractor Predicting: 115it [01:33,  1.17it/s]Extractor Predicting: 116it [01:34,  1.17it/s]Extractor Predicting: 117it [01:35,  1.18it/s]Extractor Predicting: 118it [01:36,  1.19it/s]Extractor Predicting: 119it [01:37,  1.19it/s]Extractor Predicting: 120it [01:37,  1.21it/s]Extractor Predicting: 121it [01:38,  1.23it/s]Extractor Predicting: 122it [01:39,  1.24it/s]Extractor Predicting: 123it [01:40,  1.25it/s]Extractor Predicting: 124it [01:41,  1.24it/s]Extractor Predicting: 125it [01:41,  1.23it/s]Extractor Predicting: 126it [01:42,  1.21it/s]Extractor Predicting: 127it [01:43,  1.21it/s]Extractor Predicting: 128it [01:44,  1.21it/s]Extractor Predicting: 129it [01:45,  1.23it/s]Extractor Predicting: 130it [01:46,  1.23it/s]Extractor Predicting: 131it [01:46,  1.24it/s]Extractor Predicting: 132it [01:47,  1.26it/s]Extractor Predicting: 133it [01:48,  1.23it/s]Extractor Predicting: 134it [01:49,  1.22it/s]Extractor Predicting: 135it [01:50,  1.22it/s]Extractor Predicting: 136it [01:50,  1.21it/s]Extractor Predicting: 137it [01:51,  1.20it/s]Extractor Predicting: 138it [01:52,  1.24it/s]Extractor Predicting: 139it [01:53,  1.23it/s]Extractor Predicting: 140it [01:54,  1.21it/s]Extractor Predicting: 141it [01:55,  1.22it/s]Extractor Predicting: 142it [01:55,  1.23it/s]Extractor Predicting: 143it [01:56,  1.23it/s]Extractor Predicting: 144it [01:57,  1.21it/s]Extractor Predicting: 145it [01:58,  1.23it/s]Extractor Predicting: 146it [01:59,  1.23it/s]Extractor Predicting: 147it [01:59,  1.26it/s]Extractor Predicting: 148it [02:00,  1.24it/s]Extractor Predicting: 149it [02:01,  1.25it/s]Extractor Predicting: 150it [02:02,  1.25it/s]Extractor Predicting: 151it [02:03,  1.28it/s]Extractor Predicting: 152it [02:03,  1.29it/s]Extractor Predicting: 153it [02:04,  1.29it/s]Extractor Predicting: 154it [02:05,  1.29it/s]Extractor Predicting: 155it [02:06,  1.28it/s]Extractor Predicting: 156it [02:06,  1.28it/s]Extractor Predicting: 157it [02:07,  1.26it/s]Extractor Predicting: 158it [02:08,  1.22it/s]Extractor Predicting: 159it [02:09,  1.24it/s]Extractor Predicting: 160it [02:10,  1.27it/s]Extractor Predicting: 161it [02:10,  1.27it/s]Extractor Predicting: 162it [02:11,  1.27it/s]Extractor Predicting: 163it [02:12,  1.29it/s]Extractor Predicting: 164it [02:13,  1.28it/s]Extractor Predicting: 165it [02:14,  1.27it/s]Extractor Predicting: 166it [02:14,  1.26it/s]Extractor Predicting: 167it [02:15,  1.26it/s]Extractor Predicting: 168it [02:16,  1.26it/s]Extractor Predicting: 169it [02:17,  1.29it/s]Extractor Predicting: 170it [02:17,  1.29it/s]Extractor Predicting: 171it [02:18,  1.27it/s]Extractor Predicting: 172it [02:19,  1.25it/s]Extractor Predicting: 173it [02:20,  1.25it/s]Extractor Predicting: 174it [02:21,  1.16it/s]Extractor Predicting: 175it [02:22,  1.21it/s]Extractor Predicting: 176it [02:22,  1.23it/s]Extractor Predicting: 177it [02:23,  1.20it/s]Extractor Predicting: 178it [02:24,  1.24it/s]Extractor Predicting: 179it [02:25,  1.23it/s]Extractor Predicting: 180it [02:26,  1.23it/s]Extractor Predicting: 181it [02:26,  1.25it/s]Extractor Predicting: 182it [02:27,  1.23it/s]Extractor Predicting: 183it [02:28,  1.25it/s]Extractor Predicting: 184it [02:29,  1.25it/s]Extractor Predicting: 185it [02:30,  1.27it/s]Extractor Predicting: 186it [02:30,  1.29it/s]Extractor Predicting: 187it [02:31,  1.33it/s]Extractor Predicting: 188it [02:32,  1.30it/s]Extractor Predicting: 189it [02:33,  1.26it/s]Extractor Predicting: 190it [02:34,  1.26it/s]Extractor Predicting: 191it [02:34,  1.24it/s]Extractor Predicting: 192it [02:35,  1.26it/s]Extractor Predicting: 193it [02:36,  1.26it/s]Extractor Predicting: 194it [02:37,  1.27it/s]Extractor Predicting: 195it [02:37,  1.29it/s]Extractor Predicting: 196it [02:38,  1.29it/s]Extractor Predicting: 197it [02:39,  1.25it/s]Extractor Predicting: 198it [02:40,  1.26it/s]Extractor Predicting: 199it [02:41,  1.24it/s]Extractor Predicting: 200it [02:41,  1.25it/s]Extractor Predicting: 201it [02:42,  1.25it/s]Extractor Predicting: 202it [02:43,  1.24it/s]Extractor Predicting: 203it [02:44,  1.24it/s]Extractor Predicting: 204it [02:45,  1.22it/s]Extractor Predicting: 205it [02:46,  1.20it/s]Extractor Predicting: 206it [02:46,  1.23it/s]Extractor Predicting: 207it [02:47,  1.23it/s]Extractor Predicting: 208it [02:48,  1.25it/s]Extractor Predicting: 209it [02:49,  1.27it/s]Extractor Predicting: 210it [02:50,  1.25it/s]Extractor Predicting: 211it [02:50,  1.24it/s]Extractor Predicting: 212it [02:51,  1.24it/s]Extractor Predicting: 213it [02:52,  1.23it/s]Extractor Predicting: 214it [02:53,  1.23it/s]Extractor Predicting: 215it [02:54,  1.22it/s]Extractor Predicting: 216it [02:54,  1.23it/s]Extractor Predicting: 217it [02:55,  1.21it/s]Extractor Predicting: 218it [02:56,  1.21it/s]Extractor Predicting: 219it [02:57,  1.21it/s]Extractor Predicting: 220it [02:58,  1.23it/s]Extractor Predicting: 221it [02:58,  1.27it/s]Extractor Predicting: 222it [02:59,  1.22it/s]Extractor Predicting: 223it [03:00,  1.20it/s]Extractor Predicting: 224it [03:01,  1.24it/s]Extractor Predicting: 225it [03:02,  1.24it/s]Extractor Predicting: 226it [03:03,  1.22it/s]Extractor Predicting: 227it [03:03,  1.23it/s]Extractor Predicting: 228it [03:04,  1.24it/s]Extractor Predicting: 229it [03:05,  1.26it/s]Extractor Predicting: 230it [03:06,  1.22it/s]Extractor Predicting: 231it [03:07,  1.23it/s]Extractor Predicting: 232it [03:07,  1.26it/s]Extractor Predicting: 233it [03:08,  1.23it/s]Extractor Predicting: 234it [03:09,  1.22it/s]Extractor Predicting: 235it [03:10,  1.23it/s]Extractor Predicting: 236it [03:11,  1.23it/s]Extractor Predicting: 237it [03:12,  1.21it/s]Extractor Predicting: 238it [03:12,  1.22it/s]Extractor Predicting: 239it [03:13,  1.22it/s]Extractor Predicting: 240it [03:14,  1.21it/s]Extractor Predicting: 241it [03:15,  1.22it/s]Extractor Predicting: 242it [03:16,  1.24it/s]Extractor Predicting: 243it [03:16,  1.21it/s]Extractor Predicting: 244it [03:17,  1.23it/s]Extractor Predicting: 245it [03:18,  1.22it/s]Extractor Predicting: 246it [03:19,  1.23it/s]Extractor Predicting: 247it [03:20,  1.23it/s]Extractor Predicting: 248it [03:21,  1.23it/s]Extractor Predicting: 249it [03:21,  1.24it/s]Extractor Predicting: 250it [03:22,  1.22it/s]Extractor Predicting: 251it [03:23,  1.24it/s]Extractor Predicting: 252it [03:24,  1.22it/s]Extractor Predicting: 253it [03:25,  1.23it/s]Extractor Predicting: 254it [03:25,  1.24it/s]Extractor Predicting: 255it [03:26,  1.24it/s]Extractor Predicting: 256it [03:27,  1.24it/s]Extractor Predicting: 257it [03:28,  1.26it/s]Extractor Predicting: 258it [03:29,  1.26it/s]Extractor Predicting: 259it [03:29,  1.28it/s]Extractor Predicting: 260it [03:30,  1.26it/s]Extractor Predicting: 261it [03:31,  1.27it/s]Extractor Predicting: 262it [03:32,  1.26it/s]Extractor Predicting: 263it [03:33,  1.26it/s]Extractor Predicting: 264it [03:33,  1.23it/s]Extractor Predicting: 265it [03:34,  1.23it/s]Extractor Predicting: 266it [03:35,  1.23it/s]Extractor Predicting: 267it [03:36,  1.24it/s]Extractor Predicting: 268it [03:37,  1.26it/s]Extractor Predicting: 269it [03:37,  1.26it/s]Extractor Predicting: 270it [03:38,  1.26it/s]Extractor Predicting: 271it [03:39,  1.26it/s]Extractor Predicting: 272it [03:40,  1.30it/s]Extractor Predicting: 273it [03:40,  1.30it/s]Extractor Predicting: 274it [03:41,  1.30it/s]Extractor Predicting: 275it [03:42,  1.28it/s]Extractor Predicting: 276it [03:43,  1.26it/s]Extractor Predicting: 277it [03:44,  1.28it/s]Extractor Predicting: 278it [03:44,  1.26it/s]Extractor Predicting: 279it [03:45,  1.27it/s]Extractor Predicting: 280it [03:46,  1.25it/s]Extractor Predicting: 281it [03:47,  1.26it/s]Extractor Predicting: 282it [03:48,  1.13it/s]Extractor Predicting: 283it [03:49,  1.16it/s]Extractor Predicting: 284it [03:49,  1.18it/s]Extractor Predicting: 285it [03:50,  1.19it/s]Extractor Predicting: 286it [03:51,  1.19it/s]Extractor Predicting: 287it [03:52,  1.21it/s]Extractor Predicting: 288it [03:53,  1.22it/s]Extractor Predicting: 289it [03:54,  1.23it/s]Extractor Predicting: 290it [03:54,  1.22it/s]Extractor Predicting: 291it [03:55,  1.22it/s]Extractor Predicting: 292it [03:56,  1.24it/s]Extractor Predicting: 293it [03:57,  1.23it/s]Extractor Predicting: 294it [03:58,  1.22it/s]Extractor Predicting: 295it [03:58,  1.25it/s]Extractor Predicting: 296it [03:59,  1.25it/s]Extractor Predicting: 297it [04:00,  1.22it/s]Extractor Predicting: 298it [04:01,  1.25it/s]Extractor Predicting: 299it [04:02,  1.24it/s]Extractor Predicting: 300it [04:02,  1.23it/s]Extractor Predicting: 301it [04:03,  1.28it/s]Extractor Predicting: 302it [04:04,  1.27it/s]Extractor Predicting: 303it [04:05,  1.29it/s]Extractor Predicting: 304it [04:06,  1.27it/s]Extractor Predicting: 305it [04:06,  1.26it/s]Extractor Predicting: 306it [04:07,  1.26it/s]Extractor Predicting: 307it [04:08,  1.24it/s]Extractor Predicting: 308it [04:09,  1.23it/s]Extractor Predicting: 309it [04:10,  1.24it/s]Extractor Predicting: 310it [04:10,  1.22it/s]Extractor Predicting: 311it [04:11,  1.21it/s]Extractor Predicting: 312it [04:12,  1.18it/s]Extractor Predicting: 313it [04:13,  1.20it/s]Extractor Predicting: 314it [04:14,  1.20it/s]Extractor Predicting: 315it [04:15,  1.23it/s]Extractor Predicting: 316it [04:15,  1.24it/s]Extractor Predicting: 317it [04:16,  1.23it/s]Extractor Predicting: 318it [04:17,  1.24it/s]Extractor Predicting: 319it [04:18,  1.23it/s]Extractor Predicting: 320it [04:19,  1.23it/s]Extractor Predicting: 321it [04:19,  1.23it/s]Extractor Predicting: 322it [04:20,  1.21it/s]Extractor Predicting: 323it [04:21,  1.25it/s]Extractor Predicting: 324it [04:22,  1.23it/s]Extractor Predicting: 325it [04:23,  1.24it/s]Extractor Predicting: 326it [04:23,  1.24it/s]Extractor Predicting: 327it [04:24,  1.26it/s]Extractor Predicting: 328it [04:25,  1.27it/s]Extractor Predicting: 329it [04:26,  1.24it/s]Extractor Predicting: 330it [04:27,  1.22it/s]Extractor Predicting: 331it [04:28,  1.22it/s]Extractor Predicting: 332it [04:28,  1.22it/s]Extractor Predicting: 333it [04:29,  1.23it/s]Extractor Predicting: 334it [04:30,  1.22it/s]Extractor Predicting: 335it [04:31,  1.23it/s]Extractor Predicting: 336it [04:32,  1.26it/s]Extractor Predicting: 337it [04:32,  1.27it/s]Extractor Predicting: 338it [04:33,  1.26it/s]Extractor Predicting: 339it [04:34,  1.23it/s]Extractor Predicting: 340it [04:35,  1.24it/s]Extractor Predicting: 341it [04:36,  1.26it/s]Extractor Predicting: 342it [04:36,  1.23it/s]Extractor Predicting: 343it [04:37,  1.24it/s]Extractor Predicting: 344it [04:38,  1.25it/s]Extractor Predicting: 345it [04:39,  1.26it/s]Extractor Predicting: 346it [04:40,  1.26it/s]Extractor Predicting: 347it [04:40,  1.25it/s]Extractor Predicting: 348it [04:41,  1.25it/s]Extractor Predicting: 349it [04:42,  1.23it/s]Extractor Predicting: 350it [04:43,  1.24it/s]Extractor Predicting: 351it [04:44,  1.22it/s]Extractor Predicting: 352it [04:45,  1.20it/s]Extractor Predicting: 353it [04:45,  1.21it/s]Extractor Predicting: 354it [04:46,  1.21it/s]Extractor Predicting: 355it [04:47,  1.22it/s]Extractor Predicting: 356it [04:48,  1.23it/s]Extractor Predicting: 357it [04:49,  1.25it/s]Extractor Predicting: 358it [04:49,  1.26it/s]Extractor Predicting: 359it [04:50,  1.25it/s]Extractor Predicting: 360it [04:51,  1.24it/s]Extractor Predicting: 361it [04:52,  1.25it/s]Extractor Predicting: 362it [04:53,  1.25it/s]Extractor Predicting: 363it [04:53,  1.27it/s]Extractor Predicting: 364it [04:54,  1.27it/s]Extractor Predicting: 365it [04:55,  1.27it/s]Extractor Predicting: 366it [04:56,  1.26it/s]Extractor Predicting: 367it [04:56,  1.25it/s]Extractor Predicting: 368it [04:57,  1.27it/s]Extractor Predicting: 369it [04:58,  1.27it/s]Extractor Predicting: 370it [04:59,  1.29it/s]Extractor Predicting: 371it [05:00,  1.28it/s]Extractor Predicting: 372it [05:00,  1.28it/s]Extractor Predicting: 373it [05:01,  1.27it/s]Extractor Predicting: 374it [05:02,  1.26it/s]Extractor Predicting: 375it [05:03,  1.27it/s]Extractor Predicting: 376it [05:04,  1.27it/s]Extractor Predicting: 377it [05:04,  1.27it/s]Extractor Predicting: 378it [05:05,  1.29it/s]Extractor Predicting: 379it [05:06,  1.25it/s]Extractor Predicting: 380it [05:07,  1.25it/s]Extractor Predicting: 381it [05:07,  1.28it/s]Extractor Predicting: 382it [05:08,  1.28it/s]Extractor Predicting: 383it [05:09,  1.24it/s]Extractor Predicting: 384it [05:10,  1.24it/s]Extractor Predicting: 385it [05:11,  1.25it/s]Extractor Predicting: 386it [05:11,  1.29it/s]Extractor Predicting: 387it [05:12,  1.29it/s]Extractor Predicting: 388it [05:13,  1.29it/s]Extractor Predicting: 389it [05:14,  1.26it/s]Extractor Predicting: 390it [05:15,  1.28it/s]Extractor Predicting: 391it [05:15,  1.28it/s]Extractor Predicting: 392it [05:16,  1.29it/s]Extractor Predicting: 393it [05:17,  1.30it/s]Extractor Predicting: 394it [05:18,  1.32it/s]Extractor Predicting: 395it [05:19,  1.17it/s]Extractor Predicting: 396it [05:19,  1.19it/s]Extractor Predicting: 397it [05:20,  1.22it/s]Extractor Predicting: 398it [05:21,  1.24it/s]Extractor Predicting: 399it [05:22,  1.27it/s]Extractor Predicting: 400it [05:22,  1.30it/s]Extractor Predicting: 401it [05:23,  1.28it/s]Extractor Predicting: 402it [05:24,  1.29it/s]Extractor Predicting: 403it [05:25,  1.27it/s]Extractor Predicting: 404it [05:26,  1.27it/s]Extractor Predicting: 405it [05:26,  1.27it/s]Extractor Predicting: 406it [05:27,  1.26it/s]Extractor Predicting: 407it [05:28,  1.26it/s]Extractor Predicting: 408it [05:29,  1.26it/s]Extractor Predicting: 409it [05:30,  1.29it/s]Extractor Predicting: 409it [05:30,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:27,031 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:27,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:27,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:27,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:27,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:15:27,768 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:15:27,769 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:15:28,082 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:15:29,164 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:15:29,164 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:30,899 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:30,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:30,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:30,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:15:30,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:15:31,653 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:15:31,654 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:15:31,919 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:15:32,078 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:15:32,078 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.36307553415588323,
  "recall": 0.24589829817588912,
  "score": 0.29321343945561695,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.02it/s]Extractor Predicting: 2it [00:01,  1.08it/s]Extractor Predicting: 3it [00:02,  1.10it/s]Extractor Predicting: 4it [00:03,  1.15it/s]Extractor Predicting: 5it [00:04,  1.15it/s]Extractor Predicting: 6it [00:05,  1.17it/s]Extractor Predicting: 7it [00:06,  1.16it/s]Extractor Predicting: 8it [00:06,  1.17it/s]Extractor Predicting: 9it [00:07,  1.22it/s]Extractor Predicting: 10it [00:08,  1.20it/s]Extractor Predicting: 11it [00:09,  1.23it/s]Extractor Predicting: 12it [00:10,  1.21it/s]Extractor Predicting: 13it [00:11,  1.22it/s]Extractor Predicting: 13it [00:11,  1.18it/s]
[INFO|configuration_utils.py:515] 2023-08-29 05:15:43,640 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:15:43,641 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 05:15:43,645 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:15:43,645 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 05:15:43,647 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 05:15:46,700 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 05:15:46,700 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 05:15:46,713 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 05:15:46,713 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 05:15:46,717 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:15:46,721 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:15:46,721 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:15:46,721 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:15:46,721 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:15:46,721 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 05:15:46,721 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.1590909090909091,
  "recall": 0.040756914119359534,
  "score": 0.0648899188876014,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 05:15:46,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:48,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:49,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:50,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:51,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:52,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:53,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:54,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:55,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:57,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:58,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:15:59,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:00,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:01,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:01,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:02,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:03,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:04,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:05,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:06,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:20<06:37, 20.92s/it][WARNING|generation_utils.py:914] 2023-08-29 05:16:07,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:08,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:09,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:10,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:11,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:12,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:13,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:14,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:14,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:16,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:17,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:17,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:18,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:19,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:20,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:21,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:22,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:23,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:24,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:25,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:39<05:46, 19.27s/it][WARNING|generation_utils.py:914] 2023-08-29 05:16:25,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:26,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:27,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:28,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:29,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:30,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:31,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:32,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:32,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:33,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:34,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:35,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:36,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:37,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:38,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:39,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:40,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:41,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:42,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:42,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:56<05:15, 18.53s/it][WARNING|generation_utils.py:914] 2023-08-29 05:16:43,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:44,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:45,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:46,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:47,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:48,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:49,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:50,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:51,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:51,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:52,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:53,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:54,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:55,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:56,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:56,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:57,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:58,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:16:59,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:00,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:14<04:52, 18.26s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:01,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:02,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:03,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:04,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:04,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:05,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:06,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:07,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:08,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:09,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:10,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:11,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:11,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:12,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:13,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:14,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:15,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:16,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:17,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:18,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:19,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:20,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:20,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:21,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:22,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:23,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:37<04:58, 19.91s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:24,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:25,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:26,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:26,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:27,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:28,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:29,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:30,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:31,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:32,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:33,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:34,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:35,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:36,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:36,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:37,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:38,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:39,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:40,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:41,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:42,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:43,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:57<04:38, 19.92s/it][WARNING|generation_utils.py:914] 2023-08-29 05:17:44,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:45,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:46,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:47,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:48,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:49,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:50,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:51,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:51,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:52,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:53,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:54,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:55,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:56,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:57,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:58,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:17:59,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:00,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:01,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:01,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:02,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:17<04:18, 19.90s/it][WARNING|generation_utils.py:914] 2023-08-29 05:18:04,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:05,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:06,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:07,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:08,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:09,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:10,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:11,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:12,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:13,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:14,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:14,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:16,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:17,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:18,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:19,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:20,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:21,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:22,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:23,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:24,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:24,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:25,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:39<04:09, 20.78s/it][WARNING|generation_utils.py:914] 2023-08-29 05:18:26,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:27,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:28,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:29,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:30,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:31,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:32,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:33,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:34,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:35,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:36,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:36,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:37,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:38,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:39,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:40,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:41,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:42,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:42,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:43,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:44,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:58<03:41, 20.15s/it][WARNING|generation_utils.py:914] 2023-08-29 05:18:45,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:46,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:47,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:48,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:49,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:49,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:50,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:51,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:52,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:53,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:54,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:55,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:56,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:57,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:57,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:58,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:18:59,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:00,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:01,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:01,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [03:15<03:12, 19.22s/it][WARNING|generation_utils.py:914] 2023-08-29 05:19:02,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:03,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:04,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:05,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:06,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:07,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:08,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:09,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:10,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:11,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:12,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:13,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:13,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:14,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:15,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:16,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:17,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:18,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:19,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:20,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:20,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:35<02:53, 19.31s/it][WARNING|generation_utils.py:914] 2023-08-29 05:19:22,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:23,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:24,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:25,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:25,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:26,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:27,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:28,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:29,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:30,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:31,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:32,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:33,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:34,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:35,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:36,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:37,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:38,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:38,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:39,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:40,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:54<02:35, 19.41s/it][WARNING|generation_utils.py:914] 2023-08-29 05:19:41,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:42,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:43,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:44,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:44,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:45,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:46,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:47,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:48,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:48,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:49,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:50,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:51,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:51,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:52,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:53,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:54,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:55,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:55,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:56,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [04:10<02:07, 18.18s/it][WARNING|generation_utils.py:914] 2023-08-29 05:19:57,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:57,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:58,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:19:59,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:00,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:01,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:02,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:03,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:04,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:05,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:06,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:06,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:07,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:08,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:09,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:10,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:11,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:12,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:13,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:14,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:28<01:49, 18.27s/it][WARNING|generation_utils.py:914] 2023-08-29 05:20:15,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:16,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:17,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:18,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:19,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:20,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:20,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:21,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:22,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:23,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:24,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:25,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:26,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:26,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:27,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:28,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:29,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:30,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:31,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:32,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:33,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:47<01:31, 18.29s/it][WARNING|generation_utils.py:914] 2023-08-29 05:20:33,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:34,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:35,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:36,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:37,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:38,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:38,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:39,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:40,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:41,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:42,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:43,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:44,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:45,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:46,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:46,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:47,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:48,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:49,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:50,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:51,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:52,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [05:06<01:14, 18.59s/it][WARNING|generation_utils.py:914] 2023-08-29 05:20:53,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:53,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:54,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:55,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:56,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:57,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:58,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:59,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:20:59,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:00,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:01,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:02,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:03,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:04,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:04,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:05,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:06,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:07,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:08,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:09,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:09,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:10,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:11,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [05:25<00:56, 18.75s/it][WARNING|generation_utils.py:914] 2023-08-29 05:21:12,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:13,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:14,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:14,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:16,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:16,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:17,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:18,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:19,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:20,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:21,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:22,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:23,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:23,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:24,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:25,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:26,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:27,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:28,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:29,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:43<00:36, 18.40s/it][WARNING|generation_utils.py:914] 2023-08-29 05:21:29,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:30,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:32,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:33,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:34,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:35,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:36,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:36,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:37,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:38,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:39,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:40,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:41,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:42,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:43,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:44,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:45,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:46,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:47,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:48,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:49,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [06:03<00:18, 18.98s/it][WARNING|generation_utils.py:914] 2023-08-29 05:21:50,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:51,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:51,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:52,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:53,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:54,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:55,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:56,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:57,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:58,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:59,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:21:59,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:01,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:02,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:02,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:03,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:05,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:05,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:07,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 05:22:08,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [06:22<00:00, 18.91s/it]Generating: 100%|██████████| 20/20 [06:22<00:00, 19.11s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:15,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:15,992 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:15,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:15,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:15,993 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:22:16,617 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:22:16,618 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:22:17,185 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:22:18,321 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:22:18,321 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:21,158 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:21,162 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:21,162 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:21,162 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:22:21,162 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:22:21,811 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:22:21,812 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:22:22,378 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:22:22,542 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:22:22,542 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 560, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.971875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 616, 'raw': 640}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9546875, 'errors': {''}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 85, 'raw': 128}
{'target': 600, 'success': 108, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 154, 'raw': 224}
{'target': 600, 'success': 178, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 224, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 272, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 339, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 387, 'raw': 544}
{'target': 600, 'success': 415, 'raw': 576}
{'target': 600, 'success': 440, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 496, 'raw': 672}
{'target': 600, 'success': 517, 'raw': 704}
{'target': 600, 'success': 544, 'raw': 736}
{'target': 600, 'success': 565, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 607, 'raw': 832}
{'prompt': 'Relation : voice type .', 'success_rate': 0.7295673076923077, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.890625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9285714285714286, 'errors': {''}}
['Relation : language of work or name . Context : Later in 1867 he came to the United States , where he wrote a translation of the French language into English . Head Entity : William Pinchot , Tail Entity : French language .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.8274456521739131, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : location .', 'success_rate': 0.9047619047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 425, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 455, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.95, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9360119047619048, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : Later in 1867 he was appointed under the position of secretary to the Secretary of State for the Treasury under George Washington , but lost his seat in 1869 under Henry George Smith , who resigned . Head Entity : Henry George Smith , Tail Entity : secretary of state .\n']
['Relation : position held . Context : Later in 1867 he was appointed under the position of secretary to the Secretary of State for the Treasury under George Washington , but lost his seat in 1869 under Henry George Smith , who resigned . Head Entity : Henry George Smith , Tail Entity : secretary of state .\n', 'Relation : position held . Context : After he was elected to the House of Representatives , he was elected mayor of Los Angeles , California , where he served as mayor for only two weeks . Head Entity : mayor , Tail Entity : city .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : position held .', 'success_rate': 0.8678977272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.842391304347826, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 626, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.978125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : religion .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.95625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 9913
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10013, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.18it/s]Extractor Estimating: 2it [00:01,  1.10it/s]Extractor Estimating: 3it [00:02,  1.15it/s]Extractor Estimating: 4it [00:03,  1.13it/s]Extractor Estimating: 5it [00:04,  1.15it/s]Extractor Estimating: 6it [00:05,  1.17it/s]Extractor Estimating: 7it [00:05,  1.22it/s]Extractor Estimating: 8it [00:06,  1.25it/s]Extractor Estimating: 9it [00:07,  1.24it/s]Extractor Estimating: 10it [00:08,  1.25it/s]Extractor Estimating: 11it [00:09,  1.28it/s]Extractor Estimating: 12it [00:09,  1.28it/s]Extractor Estimating: 13it [00:10,  1.27it/s]Extractor Estimating: 14it [00:11,  1.26it/s]Extractor Estimating: 15it [00:12,  1.26it/s]Extractor Estimating: 16it [00:13,  1.25it/s]Extractor Estimating: 17it [00:13,  1.29it/s]Extractor Estimating: 18it [00:14,  1.31it/s]Extractor Estimating: 19it [00:15,  1.31it/s]Extractor Estimating: 20it [00:16,  1.30it/s]Extractor Estimating: 21it [00:16,  1.32it/s]Extractor Estimating: 22it [00:17,  1.29it/s]Extractor Estimating: 23it [00:18,  1.29it/s]Extractor Estimating: 24it [00:19,  1.25it/s]Extractor Estimating: 25it [00:20,  1.25it/s]Extractor Estimating: 26it [00:20,  1.26it/s]Extractor Estimating: 27it [00:21,  1.28it/s]Extractor Estimating: 28it [00:22,  1.28it/s]Extractor Estimating: 29it [00:23,  1.30it/s]Extractor Estimating: 30it [00:23,  1.28it/s]Extractor Estimating: 31it [00:24,  1.27it/s]Extractor Estimating: 32it [00:25,  1.30it/s]Extractor Estimating: 33it [00:26,  1.31it/s]Extractor Estimating: 34it [00:26,  1.33it/s]Extractor Estimating: 35it [00:27,  1.33it/s]Extractor Estimating: 36it [00:28,  1.30it/s]Extractor Estimating: 37it [00:29,  1.28it/s]Extractor Estimating: 38it [00:29,  1.32it/s]Extractor Estimating: 39it [00:30,  1.27it/s]Extractor Estimating: 40it [00:31,  1.30it/s]Extractor Estimating: 41it [00:32,  1.31it/s]Extractor Estimating: 42it [00:33,  1.32it/s]Extractor Estimating: 43it [00:33,  1.33it/s]Extractor Estimating: 44it [00:34,  1.34it/s]Extractor Estimating: 45it [00:35,  1.33it/s]Extractor Estimating: 46it [00:36,  1.30it/s]Extractor Estimating: 47it [00:36,  1.34it/s]Extractor Estimating: 48it [00:37,  1.32it/s]Extractor Estimating: 49it [00:38,  1.34it/s]Extractor Estimating: 50it [00:39,  1.32it/s]Extractor Estimating: 51it [00:39,  1.37it/s]Extractor Estimating: 52it [00:40,  1.43it/s]Extractor Estimating: 53it [00:40,  1.48it/s]Extractor Estimating: 54it [00:41,  1.46it/s]Extractor Estimating: 55it [00:42,  1.51it/s]Extractor Estimating: 56it [00:42,  1.52it/s]Extractor Estimating: 57it [00:43,  1.50it/s]Extractor Estimating: 58it [00:44,  1.49it/s]Extractor Estimating: 59it [00:44,  1.53it/s]Extractor Estimating: 60it [00:45,  1.55it/s]Extractor Estimating: 61it [00:46,  1.55it/s]Extractor Estimating: 62it [00:46,  1.54it/s]Extractor Estimating: 63it [00:47,  1.56it/s]Extractor Estimating: 64it [00:48,  1.57it/s]Extractor Estimating: 65it [00:48,  1.54it/s]Extractor Estimating: 66it [00:49,  1.55it/s]Extractor Estimating: 67it [00:50,  1.52it/s]Extractor Estimating: 68it [00:50,  1.55it/s]Extractor Estimating: 69it [00:51,  1.55it/s]Extractor Estimating: 70it [00:52,  1.53it/s]Extractor Estimating: 71it [00:52,  1.58it/s]Extractor Estimating: 72it [00:53,  1.58it/s]Extractor Estimating: 73it [00:53,  1.58it/s]Extractor Estimating: 74it [00:54,  1.57it/s]Extractor Estimating: 75it [00:55,  1.57it/s]Extractor Estimating: 76it [00:56,  1.40it/s]Extractor Estimating: 77it [00:56,  1.38it/s]Extractor Estimating: 78it [00:57,  1.35it/s]Extractor Estimating: 79it [00:58,  1.27it/s]Extractor Estimating: 80it [00:59,  1.26it/s]Extractor Estimating: 81it [01:00,  1.19it/s]Extractor Estimating: 82it [01:01,  1.18it/s]Extractor Estimating: 83it [01:01,  1.20it/s]Extractor Estimating: 84it [01:02,  1.19it/s]Extractor Estimating: 85it [01:03,  1.20it/s]Extractor Estimating: 86it [01:04,  1.19it/s]Extractor Estimating: 87it [01:05,  1.21it/s]Extractor Estimating: 88it [01:06,  1.21it/s]Extractor Estimating: 89it [01:06,  1.24it/s]Extractor Estimating: 90it [01:07,  1.24it/s]Extractor Estimating: 91it [01:08,  1.25it/s]Extractor Estimating: 92it [01:09,  1.31it/s]Extractor Estimating: 93it [01:09,  1.31it/s]Extractor Estimating: 94it [01:10,  1.30it/s]Extractor Estimating: 95it [01:11,  1.27it/s]Extractor Estimating: 96it [01:12,  1.26it/s]Extractor Estimating: 97it [01:13,  1.27it/s]Extractor Estimating: 98it [01:13,  1.25it/s]Extractor Estimating: 99it [01:14,  1.22it/s]Extractor Estimating: 100it [01:15,  1.23it/s]Extractor Estimating: 101it [01:16,  1.24it/s]Extractor Estimating: 102it [01:17,  1.23it/s]Extractor Estimating: 103it [01:17,  1.23it/s]Extractor Estimating: 104it [01:18,  1.26it/s]Extractor Estimating: 105it [01:19,  1.27it/s]Extractor Estimating: 106it [01:20,  1.25it/s]Extractor Estimating: 107it [01:21,  1.25it/s]Extractor Estimating: 108it [01:21,  1.25it/s]Extractor Estimating: 109it [01:22,  1.27it/s]Extractor Estimating: 110it [01:23,  1.30it/s]Extractor Estimating: 111it [01:24,  1.30it/s]Extractor Estimating: 112it [01:24,  1.30it/s]Extractor Estimating: 113it [01:25,  1.27it/s]Extractor Estimating: 114it [01:26,  1.20it/s]Extractor Estimating: 115it [01:27,  1.20it/s]Extractor Estimating: 116it [01:28,  1.24it/s]Extractor Estimating: 117it [01:29,  1.26it/s]Extractor Estimating: 118it [01:29,  1.25it/s]Extractor Estimating: 119it [01:30,  1.23it/s]Extractor Estimating: 120it [01:31,  1.23it/s]Extractor Estimating: 121it [01:32,  1.22it/s]Extractor Estimating: 122it [01:33,  1.26it/s]Extractor Estimating: 123it [01:33,  1.24it/s]Extractor Estimating: 124it [01:34,  1.25it/s]Extractor Estimating: 125it [01:35,  1.25it/s]Extractor Estimating: 126it [01:36,  1.29it/s]Extractor Estimating: 127it [01:36,  1.31it/s]Extractor Estimating: 128it [01:37,  1.29it/s]Extractor Estimating: 129it [01:38,  1.32it/s]Extractor Estimating: 130it [01:39,  1.36it/s]Extractor Estimating: 131it [01:39,  1.38it/s]Extractor Estimating: 132it [01:40,  1.38it/s]Extractor Estimating: 133it [01:41,  1.39it/s]Extractor Estimating: 134it [01:41,  1.41it/s]Extractor Estimating: 135it [01:42,  1.41it/s]Extractor Estimating: 136it [01:43,  1.41it/s]Extractor Estimating: 137it [01:44,  1.43it/s]Extractor Estimating: 138it [01:44,  1.38it/s]Extractor Estimating: 139it [01:45,  1.36it/s]Extractor Estimating: 140it [01:46,  1.40it/s]Extractor Estimating: 141it [01:46,  1.42it/s]Extractor Estimating: 142it [01:47,  1.41it/s]Extractor Estimating: 143it [01:48,  1.44it/s]Extractor Estimating: 144it [01:49,  1.37it/s]Extractor Estimating: 145it [01:49,  1.36it/s]Extractor Estimating: 146it [01:50,  1.40it/s]Extractor Estimating: 147it [01:51,  1.38it/s]Extractor Estimating: 148it [01:52,  1.38it/s]Extractor Estimating: 149it [01:52,  1.36it/s]Extractor Estimating: 150it [01:53,  1.35it/s]Extractor Estimating: 151it [01:54,  1.31it/s]Extractor Estimating: 152it [01:54,  1.38it/s]Extractor Estimating: 153it [01:55,  1.40it/s]Extractor Estimating: 154it [01:56,  1.45it/s]Extractor Estimating: 155it [01:57,  1.40it/s]Extractor Estimating: 156it [01:57,  1.33it/s]Extractor Estimating: 157it [01:58,  1.35it/s]Extractor Estimating: 158it [01:59,  1.36it/s]Extractor Estimating: 159it [02:00,  1.37it/s]Extractor Estimating: 160it [02:00,  1.42it/s]Extractor Estimating: 161it [02:01,  1.45it/s]Extractor Estimating: 162it [02:02,  1.38it/s]Extractor Estimating: 163it [02:02,  1.40it/s]Extractor Estimating: 164it [02:03,  1.23it/s]Extractor Estimating: 165it [02:04,  1.26it/s]Extractor Estimating: 166it [02:05,  1.27it/s]Extractor Estimating: 167it [02:06,  1.26it/s]Extractor Estimating: 168it [02:06,  1.31it/s]Extractor Estimating: 169it [02:07,  1.33it/s]Extractor Estimating: 170it [02:08,  1.34it/s]Extractor Estimating: 171it [02:09,  1.33it/s]Extractor Estimating: 172it [02:09,  1.34it/s]Extractor Estimating: 173it [02:10,  1.38it/s]Extractor Estimating: 174it [02:11,  1.35it/s]Extractor Estimating: 175it [02:12,  1.33it/s]Extractor Estimating: 176it [02:13,  1.20it/s]Extractor Estimating: 177it [02:13,  1.22it/s]Extractor Estimating: 178it [02:14,  1.23it/s]Extractor Estimating: 179it [02:15,  1.27it/s]Extractor Estimating: 180it [02:16,  1.26it/s]Extractor Estimating: 181it [02:16,  1.30it/s]Extractor Estimating: 182it [02:17,  1.32it/s]Extractor Estimating: 183it [02:18,  1.31it/s]Extractor Estimating: 184it [02:19,  1.31it/s]Extractor Estimating: 185it [02:19,  1.33it/s]Extractor Estimating: 186it [02:20,  1.34it/s]Extractor Estimating: 187it [02:21,  1.33it/s]Extractor Estimating: 188it [02:22,  1.32it/s]Extractor Estimating: 189it [02:23,  1.29it/s]Extractor Estimating: 190it [02:23,  1.27it/s]Extractor Estimating: 191it [02:24,  1.25it/s]Extractor Estimating: 192it [02:25,  1.28it/s]Extractor Estimating: 193it [02:26,  1.32it/s]Extractor Estimating: 194it [02:26,  1.32it/s]Extractor Estimating: 195it [02:27,  1.33it/s]Extractor Estimating: 196it [02:28,  1.32it/s]Extractor Estimating: 197it [02:29,  1.32it/s]Extractor Estimating: 198it [02:29,  1.29it/s]Extractor Estimating: 199it [02:30,  1.28it/s]Extractor Estimating: 200it [02:31,  1.28it/s]Extractor Estimating: 201it [02:32,  1.28it/s]Extractor Estimating: 202it [02:33,  1.25it/s]Extractor Estimating: 203it [02:33,  1.28it/s]Extractor Estimating: 204it [02:34,  1.33it/s]Extractor Estimating: 205it [02:35,  1.34it/s]Extractor Estimating: 206it [02:36,  1.24it/s]Extractor Estimating: 207it [02:36,  1.28it/s]Extractor Estimating: 208it [02:37,  1.31it/s]Extractor Estimating: 209it [02:38,  1.32it/s]Extractor Estimating: 210it [02:39,  1.34it/s]Extractor Estimating: 211it [02:40,  1.26it/s]Extractor Estimating: 212it [02:40,  1.25it/s]Extractor Estimating: 213it [02:41,  1.29it/s]Extractor Estimating: 214it [02:42,  1.29it/s]Extractor Estimating: 215it [02:43,  1.31it/s]Extractor Estimating: 216it [02:43,  1.31it/s]Extractor Estimating: 217it [02:44,  1.32it/s]Extractor Estimating: 218it [02:45,  1.37it/s]Extractor Estimating: 219it [02:45,  1.39it/s]Extractor Estimating: 220it [02:46,  1.35it/s]Extractor Estimating: 221it [02:47,  1.36it/s]Extractor Estimating: 222it [02:48,  1.34it/s]Extractor Estimating: 223it [02:49,  1.34it/s]Extractor Estimating: 224it [02:49,  1.34it/s]Extractor Estimating: 225it [02:50,  1.31it/s]Extractor Estimating: 226it [02:51,  1.28it/s]Extractor Estimating: 227it [02:52,  1.26it/s]Extractor Estimating: 228it [02:52,  1.28it/s]Extractor Estimating: 229it [02:53,  1.22it/s]Extractor Estimating: 230it [02:54,  1.26it/s]Extractor Estimating: 231it [02:55,  1.29it/s]Extractor Estimating: 232it [02:56,  1.29it/s]Extractor Estimating: 233it [02:56,  1.34it/s]Extractor Estimating: 234it [02:57,  1.35it/s]Extractor Estimating: 235it [02:58,  1.31it/s]Extractor Estimating: 236it [02:59,  1.29it/s]Extractor Estimating: 237it [02:59,  1.33it/s]Extractor Estimating: 238it [03:00,  1.36it/s]Extractor Estimating: 239it [03:01,  1.35it/s]Extractor Estimating: 240it [03:02,  1.36it/s]Extractor Estimating: 241it [03:02,  1.35it/s]Extractor Estimating: 242it [03:03,  1.36it/s]Extractor Estimating: 243it [03:04,  1.38it/s]Extractor Estimating: 244it [03:04,  1.37it/s]Extractor Estimating: 245it [03:05,  1.41it/s]Extractor Estimating: 246it [03:06,  1.40it/s]Extractor Estimating: 247it [03:07,  1.41it/s]Extractor Estimating: 248it [03:07,  1.39it/s]Extractor Estimating: 249it [03:08,  1.41it/s]Extractor Estimating: 250it [03:09,  1.41it/s]Extractor Estimating: 251it [03:09,  1.39it/s]Extractor Estimating: 252it [03:10,  1.39it/s]Extractor Estimating: 253it [03:11,  1.39it/s]Extractor Estimating: 254it [03:12,  1.37it/s]Extractor Estimating: 255it [03:12,  1.40it/s]Extractor Estimating: 256it [03:13,  1.37it/s]Extractor Estimating: 257it [03:14,  1.39it/s]Extractor Estimating: 258it [03:15,  1.34it/s]Extractor Estimating: 259it [03:15,  1.30it/s]Extractor Estimating: 260it [03:16,  1.29it/s]Extractor Estimating: 261it [03:17,  1.28it/s]Extractor Estimating: 262it [03:18,  1.28it/s]Extractor Estimating: 263it [03:18,  1.29it/s]Extractor Estimating: 264it [03:19,  1.33it/s]Extractor Estimating: 265it [03:20,  1.33it/s]Extractor Estimating: 266it [03:21,  1.35it/s]Extractor Estimating: 267it [03:21,  1.31it/s]Extractor Estimating: 268it [03:22,  1.35it/s]Extractor Estimating: 269it [03:23,  1.35it/s]Extractor Estimating: 270it [03:24,  1.33it/s]Extractor Estimating: 271it [03:24,  1.36it/s]Extractor Estimating: 272it [03:25,  1.35it/s]Extractor Estimating: 273it [03:26,  1.32it/s]Extractor Estimating: 274it [03:27,  1.31it/s]Extractor Estimating: 275it [03:27,  1.31it/s]Extractor Estimating: 276it [03:28,  1.26it/s]Extractor Estimating: 277it [03:29,  1.24it/s]Extractor Estimating: 278it [03:30,  1.19it/s]Extractor Estimating: 279it [03:31,  1.17it/s]Extractor Estimating: 280it [03:32,  1.17it/s]Extractor Estimating: 281it [03:33,  1.19it/s]Extractor Estimating: 282it [03:34,  1.17it/s]Extractor Estimating: 283it [03:34,  1.18it/s]Extractor Estimating: 284it [03:35,  1.20it/s]Extractor Estimating: 285it [03:36,  1.19it/s]Extractor Estimating: 286it [03:37,  1.20it/s]Extractor Estimating: 287it [03:38,  1.19it/s]Extractor Estimating: 288it [03:38,  1.20it/s]Extractor Estimating: 289it [03:39,  1.18it/s]Extractor Estimating: 290it [03:40,  1.18it/s]Extractor Estimating: 291it [03:41,  1.19it/s]Extractor Estimating: 292it [03:42,  1.15it/s]Extractor Estimating: 293it [03:43,  1.13it/s]Extractor Estimating: 294it [03:44,  1.10it/s]Extractor Estimating: 295it [03:45,  1.14it/s]Extractor Estimating: 296it [03:46,  1.15it/s]Extractor Estimating: 297it [03:46,  1.16it/s]Extractor Estimating: 298it [03:47,  1.17it/s]Extractor Estimating: 299it [03:48,  1.15it/s]Extractor Estimating: 300it [03:49,  1.15it/s]Extractor Estimating: 301it [03:50,  1.24it/s]Extractor Estimating: 302it [03:50,  1.31it/s]Extractor Estimating: 303it [03:51,  1.36it/s]Extractor Estimating: 304it [03:52,  1.41it/s]Extractor Estimating: 305it [03:52,  1.44it/s]Extractor Estimating: 306it [03:53,  1.44it/s]Extractor Estimating: 307it [03:54,  1.44it/s]Extractor Estimating: 308it [03:54,  1.45it/s]Extractor Estimating: 309it [03:55,  1.46it/s]Extractor Estimating: 310it [03:56,  1.54it/s]Extractor Estimating: 311it [03:56,  1.62it/s]Extractor Estimating: 312it [03:57,  1.58it/s]Extractor Estimating: 313it [03:57,  1.58it/s]Extractor Estimating: 314it [03:58,  1.60it/s]Extractor Estimating: 315it [03:59,  1.57it/s]Extractor Estimating: 316it [03:59,  1.53it/s]Extractor Estimating: 317it [04:00,  1.55it/s]Extractor Estimating: 318it [04:01,  1.55it/s]Extractor Estimating: 319it [04:01,  1.47it/s]Extractor Estimating: 320it [04:02,  1.35it/s]Extractor Estimating: 321it [04:03,  1.35it/s]Extractor Estimating: 322it [04:04,  1.35it/s]Extractor Estimating: 323it [04:04,  1.40it/s]Extractor Estimating: 324it [04:05,  1.44it/s]Extractor Estimating: 325it [04:06,  1.53it/s]Extractor Estimating: 326it [04:06,  1.50it/s]Extractor Estimating: 327it [04:07,  1.42it/s]Extractor Estimating: 328it [04:08,  1.45it/s]Extractor Estimating: 329it [04:09,  1.42it/s]Extractor Estimating: 330it [04:09,  1.38it/s]Extractor Estimating: 331it [04:10,  1.38it/s]Extractor Estimating: 332it [04:11,  1.38it/s]Extractor Estimating: 333it [04:11,  1.38it/s]Extractor Estimating: 334it [04:12,  1.41it/s]Extractor Estimating: 335it [04:13,  1.39it/s]Extractor Estimating: 336it [04:14,  1.39it/s]Extractor Estimating: 337it [04:14,  1.40it/s]Extractor Estimating: 338it [04:15,  1.38it/s]Extractor Estimating: 339it [04:16,  1.35it/s]Extractor Estimating: 340it [04:17,  1.37it/s]Extractor Estimating: 341it [04:17,  1.39it/s]Extractor Estimating: 342it [04:18,  1.33it/s]Extractor Estimating: 343it [04:19,  1.37it/s]Extractor Estimating: 344it [04:20,  1.36it/s]Extractor Estimating: 345it [04:20,  1.42it/s]Extractor Estimating: 346it [04:21,  1.31it/s]Extractor Estimating: 347it [04:22,  1.30it/s]Extractor Estimating: 348it [04:23,  1.33it/s]Extractor Estimating: 349it [04:23,  1.30it/s]Extractor Estimating: 350it [04:24,  1.27it/s]Extractor Estimating: 351it [04:25,  1.31it/s]Extractor Estimating: 352it [04:26,  1.35it/s]Extractor Estimating: 353it [04:26,  1.41it/s]Extractor Estimating: 354it [04:27,  1.39it/s]Extractor Estimating: 355it [04:28,  1.41it/s]Extractor Estimating: 356it [04:28,  1.43it/s]Extractor Estimating: 357it [04:29,  1.41it/s]Extractor Estimating: 358it [04:30,  1.48it/s]Extractor Estimating: 359it [04:30,  1.47it/s]Extractor Estimating: 360it [04:31,  1.42it/s]Extractor Estimating: 361it [04:32,  1.47it/s]Extractor Estimating: 362it [04:32,  1.46it/s]Extractor Estimating: 363it [04:33,  1.48it/s]Extractor Estimating: 364it [04:34,  1.48it/s]Extractor Estimating: 365it [04:34,  1.50it/s]Extractor Estimating: 366it [04:35,  1.48it/s]Extractor Estimating: 367it [04:36,  1.51it/s]Extractor Estimating: 368it [04:36,  1.48it/s]Extractor Estimating: 369it [04:37,  1.44it/s]Extractor Estimating: 370it [04:38,  1.49it/s]Extractor Estimating: 371it [04:38,  1.47it/s]Extractor Estimating: 372it [04:39,  1.45it/s]Extractor Estimating: 373it [04:40,  1.44it/s]Extractor Estimating: 374it [04:41,  1.43it/s]Extractor Estimating: 375it [04:41,  1.44it/s]Extractor Estimating: 376it [04:42,  1.46it/s]Extractor Estimating: 377it [04:43,  1.43it/s]Extractor Estimating: 378it [04:43,  1.40it/s]Extractor Estimating: 379it [04:44,  1.38it/s]Extractor Estimating: 380it [04:45,  1.40it/s]Extractor Estimating: 381it [04:46,  1.37it/s]Extractor Estimating: 382it [04:46,  1.37it/s]Extractor Estimating: 383it [04:47,  1.33it/s]Extractor Estimating: 384it [04:48,  1.38it/s]Extractor Estimating: 385it [04:49,  1.38it/s]Extractor Estimating: 386it [04:49,  1.41it/s]Extractor Estimating: 387it [04:50,  1.34it/s]Extractor Estimating: 388it [04:51,  1.35it/s]Extractor Estimating: 389it [04:52,  1.34it/s]Extractor Estimating: 390it [04:52,  1.37it/s]Extractor Estimating: 391it [04:53,  1.37it/s]Extractor Estimating: 392it [04:54,  1.41it/s]Extractor Estimating: 393it [04:54,  1.38it/s]Extractor Estimating: 394it [04:55,  1.37it/s]Extractor Estimating: 395it [04:56,  1.36it/s]Extractor Estimating: 396it [04:57,  1.36it/s]Extractor Estimating: 397it [04:57,  1.34it/s]Extractor Estimating: 398it [04:58,  1.35it/s]Extractor Estimating: 399it [04:59,  1.39it/s]Extractor Estimating: 400it [05:00,  1.39it/s]Extractor Estimating: 401it [05:00,  1.40it/s]Extractor Estimating: 402it [05:01,  1.38it/s]Extractor Estimating: 403it [05:02,  1.40it/s]Extractor Estimating: 404it [05:02,  1.40it/s]Extractor Estimating: 405it [05:03,  1.43it/s]Extractor Estimating: 406it [05:04,  1.41it/s]Extractor Estimating: 407it [05:04,  1.44it/s]Extractor Estimating: 408it [05:05,  1.41it/s]Extractor Estimating: 409it [05:06,  1.43it/s]Extractor Estimating: 410it [05:07,  1.46it/s]Extractor Estimating: 411it [05:07,  1.43it/s]Extractor Estimating: 412it [05:08,  1.39it/s]Extractor Estimating: 413it [05:09,  1.43it/s]Extractor Estimating: 414it [05:10,  1.30it/s]Extractor Estimating: 415it [05:10,  1.32it/s]Extractor Estimating: 416it [05:11,  1.35it/s]Extractor Estimating: 417it [05:12,  1.37it/s]Extractor Estimating: 418it [05:12,  1.39it/s]Extractor Estimating: 419it [05:13,  1.41it/s]Extractor Estimating: 420it [05:14,  1.39it/s]Extractor Estimating: 421it [05:15,  1.40it/s]Extractor Estimating: 422it [05:15,  1.42it/s]Extractor Estimating: 423it [05:16,  1.44it/s]Extractor Estimating: 424it [05:17,  1.41it/s]Extractor Estimating: 425it [05:17,  1.41it/s]Extractor Estimating: 426it [05:18,  1.35it/s]Extractor Estimating: 427it [05:19,  1.33it/s]Extractor Estimating: 428it [05:20,  1.32it/s]Extractor Estimating: 429it [05:21,  1.28it/s]Extractor Estimating: 430it [05:21,  1.25it/s]Extractor Estimating: 431it [05:22,  1.27it/s]Extractor Estimating: 432it [05:23,  1.28it/s]Extractor Estimating: 433it [05:24,  1.30it/s]Extractor Estimating: 434it [05:24,  1.33it/s]Extractor Estimating: 435it [05:25,  1.36it/s]Extractor Estimating: 436it [05:26,  1.31it/s]Extractor Estimating: 437it [05:27,  1.25it/s]Extractor Estimating: 438it [05:28,  1.27it/s]Extractor Estimating: 439it [05:28,  1.26it/s]Extractor Estimating: 440it [05:29,  1.27it/s]Extractor Estimating: 441it [05:30,  1.26it/s]Extractor Estimating: 442it [05:31,  1.27it/s]Extractor Estimating: 443it [05:32,  1.25it/s]Extractor Estimating: 444it [05:32,  1.23it/s]Extractor Estimating: 445it [05:33,  1.27it/s]Extractor Estimating: 446it [05:34,  1.27it/s]Extractor Estimating: 447it [05:35,  1.30it/s]Extractor Estimating: 448it [05:35,  1.29it/s]Extractor Estimating: 449it [05:36,  1.31it/s]Extractor Estimating: 450it [05:37,  1.32it/s]Extractor Estimating: 451it [05:38,  1.36it/s]Extractor Estimating: 452it [05:38,  1.33it/s]Extractor Estimating: 453it [05:39,  1.30it/s]Extractor Estimating: 454it [05:40,  1.27it/s]Extractor Estimating: 455it [05:41,  1.27it/s]Extractor Estimating: 456it [05:42,  1.24it/s]Extractor Estimating: 457it [05:42,  1.25it/s]Extractor Estimating: 458it [05:43,  1.23it/s]Extractor Estimating: 459it [05:44,  1.25it/s]Extractor Estimating: 460it [05:45,  1.25it/s]Extractor Estimating: 461it [05:46,  1.23it/s]Extractor Estimating: 462it [05:46,  1.26it/s]Extractor Estimating: 463it [05:47,  1.24it/s]Extractor Estimating: 464it [05:48,  1.26it/s]Extractor Estimating: 465it [05:49,  1.28it/s]Extractor Estimating: 466it [05:50,  1.21it/s]Extractor Estimating: 467it [05:51,  1.16it/s]Extractor Estimating: 468it [05:51,  1.26it/s]Extractor Estimating: 469it [05:52,  1.27it/s]Extractor Estimating: 470it [05:53,  1.29it/s]Extractor Estimating: 471it [05:54,  1.29it/s]Extractor Estimating: 472it [05:54,  1.27it/s]Extractor Estimating: 473it [05:55,  1.30it/s]Extractor Estimating: 474it [05:56,  1.30it/s]Extractor Estimating: 475it [05:57,  1.28it/s]Extractor Estimating: 476it [05:57,  1.34it/s]Extractor Estimating: 477it [05:58,  1.30it/s]Extractor Estimating: 478it [05:59,  1.30it/s]Extractor Estimating: 479it [06:00,  1.32it/s]Extractor Estimating: 480it [06:00,  1.35it/s]Extractor Estimating: 481it [06:02,  1.18it/s]Extractor Estimating: 482it [06:02,  1.16it/s]Extractor Estimating: 483it [06:03,  1.21it/s]Extractor Estimating: 484it [06:04,  1.25it/s]Extractor Estimating: 485it [06:05,  1.24it/s]Extractor Estimating: 486it [06:06,  1.22it/s]Extractor Estimating: 487it [06:06,  1.23it/s]Extractor Estimating: 488it [06:07,  1.25it/s]Extractor Estimating: 489it [06:08,  1.25it/s]Extractor Estimating: 490it [06:09,  1.22it/s]Extractor Estimating: 491it [06:10,  1.27it/s]Extractor Estimating: 492it [06:10,  1.29it/s]Extractor Estimating: 493it [06:11,  1.33it/s]Extractor Estimating: 494it [06:12,  1.30it/s]Extractor Estimating: 495it [06:13,  1.23it/s]Extractor Estimating: 496it [06:13,  1.27it/s]Extractor Estimating: 497it [06:14,  1.29it/s]Extractor Estimating: 498it [06:15,  1.29it/s]Extractor Estimating: 499it [06:16,  1.30it/s]Extractor Estimating: 500it [06:17,  1.23it/s]Extractor Estimating: 500it [06:17,  1.33it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:52,505 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:52,510 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:52,510 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:52,510 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:52,510 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 05:28:53,120 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 05:28:53,121 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:28:53,672 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 05:28:54,729 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:28:54,730 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:57,673 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:57,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:57,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:57,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 05:28:57,677 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 05:28:58,323 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 05:28:58,324 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 05:28:58,878 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 05:28:59,039 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 05:28:59,040 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 08:33:57,152 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 08:33:57,216 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 7999 mean pseudo reward: 0.9750718661497205
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 16805
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16905, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16905, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.323, loss:407.5643
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.335, loss:333.2724
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.368, loss:336.2974
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 66, avg_time 1.339, loss:327.9325
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 166, avg_time 1.356, loss:314.9225
>> valid entity prec:0.6533, rec:0.6605, f1:0.6569
>> valid relation prec:0.3987, rec:0.2908, f1:0.3363
>> valid relation with NER prec:0.3987, rec:0.2908, f1:0.3363
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 266, avg_time 2.980, loss:312.1888
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 32, avg_time 1.331, loss:316.3390
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 132, avg_time 1.333, loss:291.6576
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 232, avg_time 1.341, loss:317.0867
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 332, avg_time 1.340, loss:342.6900
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6721, rec:0.6254, f1:0.6479
>> valid relation prec:0.4401, rec:0.2848, f1:0.3458
>> valid relation with NER prec:0.4401, rec:0.2848, f1:0.3458
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 98, avg_time 3.000, loss:286.6416
g_step 1200, step 198, avg_time 1.341, loss:307.9913
g_step 1300, step 298, avg_time 1.322, loss:289.8385
g_step 1400, step 64, avg_time 1.338, loss:288.1984
g_step 1500, step 164, avg_time 1.340, loss:274.4522
>> valid entity prec:0.6431, rec:0.6565, f1:0.6497
>> valid relation prec:0.4110, rec:0.3240, f1:0.3623
>> valid relation with NER prec:0.4110, rec:0.3240, f1:0.3623
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 264, avg_time 3.009, loss:280.6745
g_step 1700, step 30, avg_time 1.321, loss:284.1358
g_step 1800, step 130, avg_time 1.336, loss:260.6155
g_step 1900, step 230, avg_time 1.340, loss:269.1937
g_step 2000, step 330, avg_time 1.318, loss:275.1674
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6435, rec:0.6611, f1:0.6522
>> valid relation prec:0.4122, rec:0.2751, f1:0.3300
>> valid relation with NER prec:0.4122, rec:0.2751, f1:0.3300
g_step 2100, step 96, avg_time 2.999, loss:230.6590
g_step 2200, step 196, avg_time 1.355, loss:244.8797
g_step 2300, step 296, avg_time 1.345, loss:262.7299
g_step 2400, step 62, avg_time 1.309, loss:228.0307
g_step 2500, step 162, avg_time 1.342, loss:228.7645
>> valid entity prec:0.6509, rec:0.6406, f1:0.6457
>> valid relation prec:0.4376, rec:0.3317, f1:0.3774
>> valid relation with NER prec:0.4376, rec:0.3317, f1:0.3774
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 262, avg_time 3.013, loss:233.7791
g_step 2700, step 28, avg_time 1.360, loss:223.3580
g_step 2800, step 128, avg_time 1.327, loss:210.9912
g_step 2900, step 228, avg_time 1.351, loss:228.8000
g_step 3000, step 328, avg_time 1.333, loss:240.7370
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6500, rec:0.6155, f1:0.6323
>> valid relation prec:0.3874, rec:0.2645, f1:0.3144
>> valid relation with NER prec:0.3874, rec:0.2645, f1:0.3144
g_step 3100, step 94, avg_time 2.996, loss:199.1281
g_step 3200, step 194, avg_time 1.331, loss:203.7382
g_step 3300, step 294, avg_time 1.327, loss:217.8260
g_step 3400, step 60, avg_time 1.344, loss:194.4692
g_step 3500, step 160, avg_time 1.356, loss:206.7497
>> valid entity prec:0.6745, rec:0.6499, f1:0.6619
>> valid relation prec:0.4120, rec:0.3111, f1:0.3545
>> valid relation with NER prec:0.4120, rec:0.3111, f1:0.3545
new max entity f1 on valid!
g_step 3600, step 260, avg_time 2.997, loss:213.2994
g_step 3700, step 26, avg_time 1.334, loss:195.0762
g_step 3800, step 126, avg_time 1.327, loss:198.4329
g_step 3900, step 226, avg_time 1.320, loss:191.8699
g_step 4000, step 326, avg_time 1.343, loss:200.7517
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6561, rec:0.6543, f1:0.6552
>> valid relation prec:0.3999, rec:0.2971, f1:0.3409
>> valid relation with NER prec:0.3999, rec:0.2971, f1:0.3409
g_step 4100, step 92, avg_time 2.993, loss:170.9596
g_step 4200, step 192, avg_time 1.332, loss:187.9368
g_step 4300, step 292, avg_time 1.341, loss:188.1963
g_step 4400, step 58, avg_time 1.331, loss:168.8258
g_step 4500, step 158, avg_time 1.334, loss:157.9479
>> valid entity prec:0.6617, rec:0.6344, f1:0.6478
>> valid relation prec:0.3960, rec:0.3034, f1:0.3436
>> valid relation with NER prec:0.3960, rec:0.3034, f1:0.3436
g_step 4600, step 258, avg_time 3.015, loss:182.7172
g_step 4700, step 24, avg_time 1.333, loss:194.6937
g_step 4800, step 124, avg_time 1.337, loss:159.5397
g_step 4900, step 224, avg_time 1.320, loss:171.5483
g_step 5000, step 324, avg_time 1.342, loss:173.1153
learning rate was adjusted to 0.0008
>> valid entity prec:0.6443, rec:0.6314, f1:0.6378
>> valid relation prec:0.3801, rec:0.2997, f1:0.3351
>> valid relation with NER prec:0.3801, rec:0.2997, f1:0.3351
g_step 5100, step 90, avg_time 3.003, loss:143.5002
g_step 5200, step 190, avg_time 1.347, loss:160.1868
g_step 5300, step 290, avg_time 1.329, loss:173.7492
g_step 5400, step 56, avg_time 1.317, loss:156.0234
g_step 5500, step 156, avg_time 1.350, loss:148.8648
>> valid entity prec:0.6691, rec:0.6584, f1:0.6637
>> valid relation prec:0.4153, rec:0.3260, f1:0.3653
>> valid relation with NER prec:0.4153, rec:0.3260, f1:0.3653
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 256, avg_time 3.020, loss:155.4010
g_step 5700, step 22, avg_time 1.336, loss:163.9545
g_step 5800, step 122, avg_time 1.357, loss:138.2519
g_step 5900, step 222, avg_time 1.315, loss:146.4664
g_step 6000, step 322, avg_time 1.333, loss:157.5953
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6374, rec:0.6479, f1:0.6426
>> valid relation prec:0.3595, rec:0.2897, f1:0.3208
>> valid relation with NER prec:0.3595, rec:0.2897, f1:0.3208
g_step 6100, step 88, avg_time 3.029, loss:133.0499
g_step 6200, step 188, avg_time 1.335, loss:134.9899
g_step 6300, step 288, avg_time 1.326, loss:137.6795
g_step 6400, step 54, avg_time 1.333, loss:138.8402
g_step 6500, step 154, avg_time 1.324, loss:134.7275
>> valid entity prec:0.6596, rec:0.6545, f1:0.6570
>> valid relation prec:0.3748, rec:0.3111, f1:0.3400
>> valid relation with NER prec:0.3748, rec:0.3111, f1:0.3400
g_step 6600, step 254, avg_time 3.024, loss:141.5268
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 08:33:57 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 08:33:57 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_08-33-57_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 08:33:58 - WARNING - datasets.builder -   Using custom data configuration default-1c9d0427a6a8b3da
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-1c9d0427a6a8b3da/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 08:33:58,519 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:33:58,520 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:33:58,521 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:33:58,522 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:33:58,553 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:33:58,556 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:33:58,556 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:33:58,556 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:33:58,556 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:33:58,556 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:33:58,556 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 08:33:58,724 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:34:01,807 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 08:34:01,809 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-1c9d0427a6a8b3da/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  3.22ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.07ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.45ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.65ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.79ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.86ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.91ba/s]100%|██████████| 8/8 [00:01<00:00,  4.96ba/s]100%|██████████| 8/8 [00:01<00:00,  4.69ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.12ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.35ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.56ba/s]100%|██████████| 4/4 [00:00<00:00,  4.63ba/s]100%|██████████| 4/4 [00:00<00:00,  4.34ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.08ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.47ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 10.44ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 10.77ba/s]100%|██████████| 8/8 [00:00<00:00, 10.47ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.26ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.86ba/s]100%|██████████| 4/4 [00:00<00:00, 11.23ba/s]
[INFO|trainer.py:414] 2023-08-29 08:34:05,934 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 08:34:05,944 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 08:34:05,944 >>   Num examples = 8000
[INFO|trainer.py:1149] 2023-08-29 08:34:05,944 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 08:34:05,944 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 08:34:05,944 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 08:34:05,944 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 08:34:05,944 >>   Total optimization steps = 625
  0%|          | 0/625 [00:00<?, ?it/s]  0%|          | 1/625 [00:00<03:11,  3.26it/s]  0%|          | 2/625 [00:00<03:05,  3.36it/s]  0%|          | 3/625 [00:00<03:02,  3.40it/s]  1%|          | 4/625 [00:01<03:01,  3.42it/s]  1%|          | 5/625 [00:01<03:00,  3.43it/s]  1%|          | 6/625 [00:01<03:00,  3.43it/s]  1%|          | 7/625 [00:02<02:59,  3.43it/s]  1%|▏         | 8/625 [00:02<03:00,  3.42it/s]  1%|▏         | 9/625 [00:02<02:59,  3.43it/s]  2%|▏         | 10/625 [00:02<02:59,  3.43it/s]  2%|▏         | 11/625 [00:03<02:58,  3.44it/s]  2%|▏         | 12/625 [00:03<02:58,  3.44it/s]  2%|▏         | 13/625 [00:03<02:58,  3.44it/s]  2%|▏         | 14/625 [00:04<02:57,  3.44it/s]  2%|▏         | 15/625 [00:04<02:57,  3.44it/s]  3%|▎         | 16/625 [00:04<02:57,  3.44it/s]  3%|▎         | 17/625 [00:04<02:56,  3.44it/s]  3%|▎         | 18/625 [00:05<02:56,  3.44it/s]  3%|▎         | 19/625 [00:05<02:56,  3.43it/s]  3%|▎         | 20/625 [00:05<02:56,  3.43it/s]  3%|▎         | 21/625 [00:06<02:55,  3.43it/s]  4%|▎         | 22/625 [00:06<02:55,  3.44it/s]  4%|▎         | 23/625 [00:06<02:55,  3.44it/s]  4%|▍         | 24/625 [00:06<02:55,  3.43it/s]  4%|▍         | 25/625 [00:07<02:54,  3.44it/s]  4%|▍         | 26/625 [00:07<02:54,  3.44it/s]  4%|▍         | 27/625 [00:07<02:53,  3.44it/s]  4%|▍         | 28/625 [00:08<02:53,  3.44it/s]  5%|▍         | 29/625 [00:08<02:53,  3.44it/s]  5%|▍         | 30/625 [00:08<02:53,  3.42it/s]  5%|▍         | 31/625 [00:09<02:53,  3.43it/s]  5%|▌         | 32/625 [00:09<02:52,  3.43it/s]  5%|▌         | 33/625 [00:09<02:52,  3.43it/s]  5%|▌         | 34/625 [00:09<02:52,  3.43it/s]  6%|▌         | 35/625 [00:10<02:51,  3.44it/s]  6%|▌         | 36/625 [00:10<02:51,  3.44it/s]  6%|▌         | 37/625 [00:10<02:51,  3.44it/s]  6%|▌         | 38/625 [00:11<02:50,  3.43it/s]  6%|▌         | 39/625 [00:11<02:50,  3.43it/s]  6%|▋         | 40/625 [00:11<02:50,  3.43it/s]  7%|▋         | 41/625 [00:11<02:50,  3.42it/s]  7%|▋         | 42/625 [00:12<02:50,  3.42it/s]  7%|▋         | 43/625 [00:12<02:49,  3.43it/s]  7%|▋         | 44/625 [00:12<02:49,  3.43it/s]  7%|▋         | 45/625 [00:13<02:48,  3.44it/s]  7%|▋         | 46/625 [00:13<02:48,  3.43it/s]  8%|▊         | 47/625 [00:13<02:48,  3.43it/s]  8%|▊         | 48/625 [00:13<02:48,  3.43it/s]  8%|▊         | 49/625 [00:14<02:47,  3.43it/s]  8%|▊         | 50/625 [00:14<02:47,  3.43it/s]  8%|▊         | 51/625 [00:14<02:47,  3.43it/s]  8%|▊         | 52/625 [00:15<02:47,  3.43it/s]  8%|▊         | 53/625 [00:15<02:46,  3.43it/s]  9%|▊         | 54/625 [00:15<02:46,  3.43it/s]  9%|▉         | 55/625 [00:16<02:46,  3.42it/s]  9%|▉         | 56/625 [00:16<02:46,  3.42it/s]  9%|▉         | 57/625 [00:16<02:45,  3.42it/s]  9%|▉         | 58/625 [00:16<02:45,  3.42it/s]  9%|▉         | 59/625 [00:17<02:45,  3.42it/s] 10%|▉         | 60/625 [00:17<02:45,  3.42it/s] 10%|▉         | 61/625 [00:17<02:45,  3.42it/s] 10%|▉         | 62/625 [00:18<02:44,  3.42it/s] 10%|█         | 63/625 [00:18<02:44,  3.41it/s] 10%|█         | 64/625 [00:18<02:44,  3.41it/s] 10%|█         | 65/625 [00:18<02:43,  3.42it/s] 11%|█         | 66/625 [00:19<02:43,  3.42it/s] 11%|█         | 67/625 [00:19<02:43,  3.42it/s] 11%|█         | 68/625 [00:19<02:42,  3.42it/s] 11%|█         | 69/625 [00:20<02:42,  3.42it/s] 11%|█         | 70/625 [00:20<02:42,  3.42it/s] 11%|█▏        | 71/625 [00:20<02:42,  3.42it/s] 12%|█▏        | 72/625 [00:21<02:41,  3.42it/s] 12%|█▏        | 73/625 [00:21<02:41,  3.42it/s] 12%|█▏        | 74/625 [00:21<02:41,  3.41it/s] 12%|█▏        | 75/625 [00:21<02:41,  3.41it/s] 12%|█▏        | 76/625 [00:22<02:40,  3.42it/s] 12%|█▏        | 77/625 [00:22<02:40,  3.42it/s] 12%|█▏        | 78/625 [00:22<02:39,  3.42it/s] 13%|█▎        | 79/625 [00:23<02:39,  3.42it/s] 13%|█▎        | 80/625 [00:23<02:39,  3.42it/s] 13%|█▎        | 81/625 [00:23<02:38,  3.43it/s] 13%|█▎        | 82/625 [00:23<02:38,  3.43it/s] 13%|█▎        | 83/625 [00:24<02:38,  3.43it/s] 13%|█▎        | 84/625 [00:24<02:37,  3.43it/s] 14%|█▎        | 85/625 [00:24<02:37,  3.43it/s] 14%|█▍        | 86/625 [00:25<02:37,  3.41it/s] 14%|█▍        | 87/625 [00:25<02:37,  3.42it/s] 14%|█▍        | 88/625 [00:25<02:36,  3.42it/s] 14%|█▍        | 89/625 [00:25<02:36,  3.43it/s] 14%|█▍        | 90/625 [00:26<02:36,  3.43it/s] 15%|█▍        | 91/625 [00:26<02:35,  3.43it/s] 15%|█▍        | 92/625 [00:26<02:35,  3.43it/s] 15%|█▍        | 93/625 [00:27<02:35,  3.43it/s] 15%|█▌        | 94/625 [00:27<02:34,  3.43it/s] 15%|█▌        | 95/625 [00:27<02:34,  3.43it/s] 15%|█▌        | 96/625 [00:28<02:34,  3.43it/s] 16%|█▌        | 97/625 [00:28<02:34,  3.42it/s] 16%|█▌        | 98/625 [00:28<02:33,  3.42it/s] 16%|█▌        | 99/625 [00:28<02:33,  3.42it/s] 16%|█▌        | 100/625 [00:29<02:33,  3.43it/s] 16%|█▌        | 101/625 [00:29<02:32,  3.43it/s] 16%|█▋        | 102/625 [00:29<02:32,  3.43it/s] 16%|█▋        | 103/625 [00:30<02:32,  3.43it/s] 17%|█▋        | 104/625 [00:30<02:32,  3.43it/s] 17%|█▋        | 105/625 [00:30<02:31,  3.43it/s] 17%|█▋        | 106/625 [00:30<02:31,  3.43it/s] 17%|█▋        | 107/625 [00:31<02:31,  3.43it/s] 17%|█▋        | 108/625 [00:31<02:30,  3.43it/s] 17%|█▋        | 109/625 [00:31<02:30,  3.42it/s] 18%|█▊        | 110/625 [00:32<02:30,  3.43it/s] 18%|█▊        | 111/625 [00:32<02:30,  3.43it/s] 18%|█▊        | 112/625 [00:32<02:29,  3.43it/s] 18%|█▊        | 113/625 [00:32<02:29,  3.43it/s] 18%|█▊        | 114/625 [00:33<02:29,  3.43it/s] 18%|█▊        | 115/625 [00:33<02:28,  3.43it/s] 19%|█▊        | 116/625 [00:33<02:28,  3.43it/s] 19%|█▊        | 117/625 [00:34<02:28,  3.43it/s] 19%|█▉        | 118/625 [00:34<02:27,  3.43it/s] 19%|█▉        | 119/625 [00:34<02:27,  3.43it/s] 19%|█▉        | 120/625 [00:35<02:27,  3.43it/s] 19%|█▉        | 121/625 [00:35<02:27,  3.41it/s] 20%|█▉        | 122/625 [00:35<02:27,  3.42it/s] 20%|█▉        | 123/625 [00:35<02:26,  3.42it/s] 20%|█▉        | 124/625 [00:36<02:26,  3.43it/s] 20%|██        | 125/625 [00:36<02:26,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 08:34:42,430 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:34:42,430 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 08:34:42,430 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.40it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.59it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.84it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.07it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.69it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.28it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.86it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.44it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.40it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.45it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.47it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.61it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.68it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.67it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.66it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.56it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.37it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.31it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.27it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.36it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.49it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.56it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.65it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.56it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.53it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.37it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.33it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.28it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.25it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.45it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.53it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.53it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.60it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.55it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.49it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.45it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.33it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.32it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.38it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.43it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.49it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.54it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.55it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.48it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.39it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.35it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.38it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.51it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.46it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.46it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.53it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.51it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.50it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.45it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.37it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.40it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.41it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.42it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.45it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.48it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.48it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.46it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.43it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.40it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.40it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.40it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.42it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.44it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.40it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.42it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.40it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.37it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.35it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.34it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.30it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.44it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.51it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.41it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.47it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.38it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.36it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.38it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.37it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.30it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.34it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.38it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.58it/s][A                                                 
                                                 [A 20%|██        | 125/625 [00:45<02:26,  3.42it/s]
100%|██████████| 438/438 [00:09<00:00, 46.58it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:34:51,870 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-125
[INFO|configuration_utils.py:351] 2023-08-29 08:34:51,890 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-125/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:34:54,234 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-125/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:34:54,256 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-125/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:34:54,269 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-125/special_tokens_map.json
 20%|██        | 126/625 [00:54<47:36,  5.72s/it] 20%|██        | 127/625 [00:55<33:59,  4.10s/it] 20%|██        | 128/625 [00:55<24:28,  2.95s/it] 21%|██        | 129/625 [00:55<17:48,  2.16s/it] 21%|██        | 130/625 [00:56<13:09,  1.60s/it] 21%|██        | 131/625 [00:56<09:55,  1.20s/it] 21%|██        | 132/625 [00:56<07:38,  1.07it/s] 21%|██▏       | 133/625 [00:56<06:03,  1.35it/s] 21%|██▏       | 134/625 [00:57<04:56,  1.65it/s] 22%|██▏       | 135/625 [00:57<04:10,  1.96it/s] 22%|██▏       | 136/625 [00:57<03:37,  2.25it/s] 22%|██▏       | 137/625 [00:58<03:14,  2.51it/s] 22%|██▏       | 138/625 [00:58<02:58,  2.72it/s] 22%|██▏       | 139/625 [00:58<02:47,  2.90it/s] 22%|██▏       | 140/625 [00:58<02:39,  3.04it/s] 23%|██▎       | 141/625 [00:59<02:33,  3.15it/s] 23%|██▎       | 142/625 [00:59<02:29,  3.23it/s] 23%|██▎       | 143/625 [00:59<02:26,  3.29it/s] 23%|██▎       | 144/625 [01:00<02:24,  3.33it/s] 23%|██▎       | 145/625 [01:00<02:22,  3.36it/s] 23%|██▎       | 146/625 [01:00<02:21,  3.38it/s] 24%|██▎       | 147/625 [01:01<02:20,  3.40it/s] 24%|██▎       | 148/625 [01:01<02:19,  3.41it/s] 24%|██▍       | 149/625 [01:01<02:19,  3.41it/s] 24%|██▍       | 150/625 [01:01<02:19,  3.41it/s] 24%|██▍       | 151/625 [01:02<02:18,  3.42it/s] 24%|██▍       | 152/625 [01:02<02:18,  3.42it/s] 24%|██▍       | 153/625 [01:02<02:18,  3.42it/s] 25%|██▍       | 154/625 [01:03<02:17,  3.42it/s] 25%|██▍       | 155/625 [01:03<02:17,  3.43it/s] 25%|██▍       | 156/625 [01:03<02:16,  3.43it/s] 25%|██▌       | 157/625 [01:03<02:16,  3.43it/s] 25%|██▌       | 158/625 [01:04<02:16,  3.43it/s] 25%|██▌       | 159/625 [01:04<02:15,  3.43it/s] 26%|██▌       | 160/625 [01:04<02:15,  3.42it/s] 26%|██▌       | 161/625 [01:05<02:15,  3.43it/s] 26%|██▌       | 162/625 [01:05<02:15,  3.43it/s] 26%|██▌       | 163/625 [01:05<02:14,  3.43it/s] 26%|██▌       | 164/625 [01:05<02:14,  3.43it/s] 26%|██▋       | 165/625 [01:06<02:14,  3.43it/s] 27%|██▋       | 166/625 [01:06<02:13,  3.43it/s] 27%|██▋       | 167/625 [01:06<02:13,  3.43it/s] 27%|██▋       | 168/625 [01:07<02:13,  3.43it/s] 27%|██▋       | 169/625 [01:07<02:12,  3.43it/s] 27%|██▋       | 170/625 [01:07<02:12,  3.43it/s] 27%|██▋       | 171/625 [01:08<02:12,  3.41it/s] 28%|██▊       | 172/625 [01:08<02:12,  3.42it/s] 28%|██▊       | 173/625 [01:08<02:12,  3.42it/s] 28%|██▊       | 174/625 [01:08<02:11,  3.42it/s] 28%|██▊       | 175/625 [01:09<02:11,  3.43it/s] 28%|██▊       | 176/625 [01:09<02:10,  3.43it/s] 28%|██▊       | 177/625 [01:09<02:10,  3.43it/s] 28%|██▊       | 178/625 [01:10<02:10,  3.43it/s] 29%|██▊       | 179/625 [01:10<02:10,  3.43it/s] 29%|██▉       | 180/625 [01:10<02:09,  3.43it/s] 29%|██▉       | 181/625 [01:10<02:09,  3.43it/s] 29%|██▉       | 182/625 [01:11<02:09,  3.42it/s] 29%|██▉       | 183/625 [01:11<02:09,  3.42it/s] 29%|██▉       | 184/625 [01:11<02:08,  3.42it/s] 30%|██▉       | 185/625 [01:12<02:08,  3.43it/s] 30%|██▉       | 186/625 [01:12<02:08,  3.43it/s] 30%|██▉       | 187/625 [01:12<02:07,  3.43it/s] 30%|███       | 188/625 [01:12<02:07,  3.43it/s] 30%|███       | 189/625 [01:13<02:07,  3.41it/s] 30%|███       | 190/625 [01:13<02:07,  3.42it/s] 31%|███       | 191/625 [01:13<02:06,  3.42it/s] 31%|███       | 192/625 [01:14<02:06,  3.42it/s] 31%|███       | 193/625 [01:14<02:06,  3.43it/s] 31%|███       | 194/625 [01:14<02:05,  3.43it/s] 31%|███       | 195/625 [01:15<02:05,  3.43it/s] 31%|███▏      | 196/625 [01:15<02:05,  3.43it/s] 32%|███▏      | 197/625 [01:15<02:04,  3.43it/s] 32%|███▏      | 198/625 [01:15<02:04,  3.43it/s] 32%|███▏      | 199/625 [01:16<02:04,  3.43it/s] 32%|███▏      | 200/625 [01:16<02:04,  3.42it/s] 32%|███▏      | 201/625 [01:16<02:03,  3.43it/s] 32%|███▏      | 202/625 [01:17<02:03,  3.43it/s] 32%|███▏      | 203/625 [01:17<02:03,  3.43it/s] 33%|███▎      | 204/625 [01:17<02:02,  3.43it/s] 33%|███▎      | 205/625 [01:17<02:02,  3.43it/s] 33%|███▎      | 206/625 [01:18<02:02,  3.43it/s] 33%|███▎      | 207/625 [01:18<02:01,  3.43it/s] 33%|███▎      | 208/625 [01:18<02:01,  3.43it/s] 33%|███▎      | 209/625 [01:19<02:01,  3.43it/s] 34%|███▎      | 210/625 [01:19<02:00,  3.43it/s] 34%|███▍      | 211/625 [01:19<02:00,  3.42it/s] 34%|███▍      | 212/625 [01:19<02:00,  3.42it/s] 34%|███▍      | 213/625 [01:20<02:00,  3.43it/s] 34%|███▍      | 214/625 [01:20<01:59,  3.43it/s] 34%|███▍      | 215/625 [01:20<01:59,  3.43it/s] 35%|███▍      | 216/625 [01:21<01:59,  3.43it/s] 35%|███▍      | 217/625 [01:21<01:59,  3.43it/s] 35%|███▍      | 218/625 [01:21<01:58,  3.43it/s] 35%|███▌      | 219/625 [01:22<01:58,  3.43it/s] 35%|███▌      | 220/625 [01:22<01:58,  3.43it/s] 35%|███▌      | 221/625 [01:22<01:57,  3.43it/s] 36%|███▌      | 222/625 [01:22<01:57,  3.42it/s] 36%|███▌      | 223/625 [01:23<01:57,  3.42it/s] 36%|███▌      | 224/625 [01:23<01:57,  3.42it/s] 36%|███▌      | 225/625 [01:23<01:56,  3.42it/s] 36%|███▌      | 226/625 [01:24<01:56,  3.42it/s] 36%|███▋      | 227/625 [01:24<01:56,  3.43it/s] 36%|███▋      | 228/625 [01:24<01:55,  3.43it/s] 37%|███▋      | 229/625 [01:24<01:55,  3.43it/s] 37%|███▋      | 230/625 [01:25<01:55,  3.43it/s] 37%|███▋      | 231/625 [01:25<01:54,  3.43it/s] 37%|███▋      | 232/625 [01:25<01:54,  3.43it/s] 37%|███▋      | 233/625 [01:26<01:54,  3.42it/s] 37%|███▋      | 234/625 [01:26<01:54,  3.42it/s] 38%|███▊      | 235/625 [01:26<01:54,  3.42it/s] 38%|███▊      | 236/625 [01:26<01:53,  3.43it/s] 38%|███▊      | 237/625 [01:27<01:53,  3.43it/s] 38%|███▊      | 238/625 [01:27<01:52,  3.43it/s] 38%|███▊      | 239/625 [01:27<01:52,  3.43it/s] 38%|███▊      | 240/625 [01:28<01:52,  3.43it/s] 39%|███▊      | 241/625 [01:28<01:52,  3.42it/s] 39%|███▊      | 242/625 [01:28<01:52,  3.42it/s] 39%|███▉      | 243/625 [01:29<01:51,  3.42it/s] 39%|███▉      | 244/625 [01:29<01:51,  3.40it/s] 39%|███▉      | 245/625 [01:29<01:51,  3.41it/s] 39%|███▉      | 246/625 [01:29<01:50,  3.42it/s] 40%|███▉      | 247/625 [01:30<01:50,  3.42it/s] 40%|███▉      | 248/625 [01:30<01:50,  3.43it/s] 40%|███▉      | 249/625 [01:30<01:49,  3.43it/s] 40%|████      | 250/625 [01:31<01:49,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 08:35:37,016 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:35:37,016 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 08:35:37,016 >>   Batch size = 8
{'eval_loss': 1.1205298900604248, 'eval_runtime': 9.4237, 'eval_samples_per_second': 371.087, 'eval_steps_per_second': 46.479, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.06it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.44it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.62it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.83it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.06it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.71it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.36it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.29it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 42.87it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 43.82it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 44.60it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.21it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.59it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.86it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.11it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.16it/s][A
 21%|██        | 93/438 [00:02<00:07, 45.88it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.90it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.02it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.09it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.24it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.35it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.37it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.49it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.42it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.26it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.09it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.05it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.16it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.24it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.30it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.41it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.47it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.30it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.28it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.12it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.01it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.11it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.25it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.24it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.40it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.30it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.21it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.11it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.17it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.20it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.30it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.24it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.28it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.37it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.36it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.24it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 46.21it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.06it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.25it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.23it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.16it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.28it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.25it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.16it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.07it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.15it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.09it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.19it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.26it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.28it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.35it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.37it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.26it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.20it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.21it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.17it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.15it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.25it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.23it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.28it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.37it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.22it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.27it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.21it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.16it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.14it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.21it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.29it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.45it/s][A                                                 
                                                 [A 40%|████      | 250/625 [01:40<01:49,  3.42it/s]
100%|██████████| 438/438 [00:09<00:00, 46.45it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:35:46,519 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-250
[INFO|configuration_utils.py:351] 2023-08-29 08:35:46,533 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-250/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:35:48,885 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:35:48,905 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:35:48,919 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-250/special_tokens_map.json
 40%|████      | 251/625 [01:47<32:27,  5.21s/it] 40%|████      | 252/625 [01:48<23:12,  3.73s/it] 40%|████      | 253/625 [01:48<16:44,  2.70s/it] 41%|████      | 254/625 [01:48<12:13,  1.98s/it] 41%|████      | 255/625 [01:48<09:04,  1.47s/it] 41%|████      | 256/625 [01:49<06:52,  1.12s/it] 41%|████      | 257/625 [01:49<05:20,  1.15it/s] 41%|████▏     | 258/625 [01:49<04:15,  1.44it/s] 41%|████▏     | 259/625 [01:50<03:30,  1.74it/s] 42%|████▏     | 260/625 [01:50<02:58,  2.04it/s] 42%|████▏     | 261/625 [01:50<02:36,  2.32it/s] 42%|████▏     | 262/625 [01:50<02:21,  2.57it/s] 42%|████▏     | 263/625 [01:51<02:10,  2.78it/s] 42%|████▏     | 264/625 [01:51<02:02,  2.95it/s] 42%|████▏     | 265/625 [01:51<01:57,  3.08it/s] 43%|████▎     | 266/625 [01:52<01:53,  3.17it/s] 43%|████▎     | 267/625 [01:52<01:50,  3.25it/s] 43%|████▎     | 268/625 [01:52<01:48,  3.30it/s] 43%|████▎     | 269/625 [01:52<01:46,  3.34it/s] 43%|████▎     | 270/625 [01:53<01:45,  3.37it/s] 43%|████▎     | 271/625 [01:53<01:44,  3.39it/s] 44%|████▎     | 272/625 [01:53<01:43,  3.40it/s] 44%|████▎     | 273/625 [01:54<01:43,  3.41it/s] 44%|████▍     | 274/625 [01:54<01:43,  3.41it/s] 44%|████▍     | 275/625 [01:54<01:42,  3.41it/s] 44%|████▍     | 276/625 [01:55<01:42,  3.42it/s] 44%|████▍     | 277/625 [01:55<01:41,  3.42it/s] 44%|████▍     | 278/625 [01:55<01:41,  3.43it/s] 45%|████▍     | 279/625 [01:55<01:40,  3.43it/s] 45%|████▍     | 280/625 [01:56<01:40,  3.43it/s] 45%|████▍     | 281/625 [01:56<01:40,  3.43it/s] 45%|████▌     | 282/625 [01:56<01:39,  3.43it/s] 45%|████▌     | 283/625 [01:57<01:39,  3.43it/s] 45%|████▌     | 284/625 [01:57<01:39,  3.43it/s] 46%|████▌     | 285/625 [01:57<01:39,  3.42it/s] 46%|████▌     | 286/625 [01:57<01:39,  3.42it/s] 46%|████▌     | 287/625 [01:58<01:38,  3.42it/s] 46%|████▌     | 288/625 [01:58<01:38,  3.43it/s] 46%|████▌     | 289/625 [01:58<01:38,  3.43it/s] 46%|████▋     | 290/625 [01:59<01:37,  3.43it/s] 47%|████▋     | 291/625 [01:59<01:37,  3.43it/s] 47%|████▋     | 292/625 [01:59<01:37,  3.43it/s] 47%|████▋     | 293/625 [01:59<01:36,  3.43it/s] 47%|████▋     | 294/625 [02:00<01:36,  3.43it/s] 47%|████▋     | 295/625 [02:00<01:36,  3.43it/s] 47%|████▋     | 296/625 [02:00<01:36,  3.42it/s] 48%|████▊     | 297/625 [02:01<01:35,  3.42it/s] 48%|████▊     | 298/625 [02:01<01:35,  3.42it/s] 48%|████▊     | 299/625 [02:01<01:35,  3.42it/s] 48%|████▊     | 300/625 [02:02<01:34,  3.43it/s] 48%|████▊     | 301/625 [02:02<01:34,  3.43it/s] 48%|████▊     | 302/625 [02:02<01:34,  3.43it/s] 48%|████▊     | 303/625 [02:02<01:33,  3.43it/s] 49%|████▊     | 304/625 [02:03<01:33,  3.42it/s] 49%|████▉     | 305/625 [02:03<01:33,  3.42it/s] 49%|████▉     | 306/625 [02:03<01:33,  3.42it/s] 49%|████▉     | 307/625 [02:04<01:37,  3.27it/s] 49%|████▉     | 308/625 [02:04<01:35,  3.32it/s] 49%|████▉     | 309/625 [02:04<01:34,  3.35it/s] 50%|████▉     | 310/625 [02:05<01:33,  3.37it/s] 50%|████▉     | 311/625 [02:05<01:32,  3.39it/s] 50%|████▉     | 312/625 [02:05<01:32,  3.40it/s] 50%|█████     | 313/625 [02:05<01:31,  3.41it/s] 50%|█████     | 314/625 [02:06<01:31,  3.42it/s] 50%|█████     | 315/625 [02:06<01:30,  3.42it/s] 51%|█████     | 316/625 [02:06<01:30,  3.42it/s] 51%|█████     | 317/625 [02:07<01:29,  3.42it/s] 51%|█████     | 318/625 [02:07<01:30,  3.41it/s] 51%|█████     | 319/625 [02:07<01:29,  3.41it/s] 51%|█████     | 320/625 [02:07<01:29,  3.42it/s] 51%|█████▏    | 321/625 [02:08<01:28,  3.42it/s] 52%|█████▏    | 322/625 [02:08<01:28,  3.42it/s] 52%|█████▏    | 323/625 [02:08<01:28,  3.42it/s] 52%|█████▏    | 324/625 [02:09<01:27,  3.43it/s] 52%|█████▏    | 325/625 [02:09<01:27,  3.43it/s] 52%|█████▏    | 326/625 [02:09<01:27,  3.43it/s] 52%|█████▏    | 327/625 [02:09<01:26,  3.43it/s] 52%|█████▏    | 328/625 [02:10<01:26,  3.43it/s] 53%|█████▎    | 329/625 [02:10<01:26,  3.41it/s] 53%|█████▎    | 330/625 [02:10<01:26,  3.42it/s] 53%|█████▎    | 331/625 [02:11<01:25,  3.42it/s] 53%|█████▎    | 332/625 [02:11<01:25,  3.42it/s] 53%|█████▎    | 333/625 [02:11<01:25,  3.42it/s] 53%|█████▎    | 334/625 [02:12<01:24,  3.42it/s] 54%|█████▎    | 335/625 [02:12<01:24,  3.43it/s] 54%|█████▍    | 336/625 [02:12<01:24,  3.43it/s] 54%|█████▍    | 337/625 [02:12<01:24,  3.43it/s] 54%|█████▍    | 338/625 [02:13<01:23,  3.43it/s] 54%|█████▍    | 339/625 [02:13<01:23,  3.43it/s] 54%|█████▍    | 340/625 [02:13<01:23,  3.42it/s] 55%|█████▍    | 341/625 [02:14<01:23,  3.42it/s] 55%|█████▍    | 342/625 [02:14<01:22,  3.42it/s] 55%|█████▍    | 343/625 [02:14<01:22,  3.42it/s] 55%|█████▌    | 344/625 [02:14<01:22,  3.42it/s] 55%|█████▌    | 345/625 [02:15<01:21,  3.42it/s] 55%|█████▌    | 346/625 [02:15<01:21,  3.42it/s] 56%|█████▌    | 347/625 [02:15<01:21,  3.43it/s] 56%|█████▌    | 348/625 [02:16<01:20,  3.43it/s] 56%|█████▌    | 349/625 [02:16<01:20,  3.43it/s] 56%|█████▌    | 350/625 [02:16<01:20,  3.42it/s] 56%|█████▌    | 351/625 [02:16<01:20,  3.42it/s] 56%|█████▋    | 352/625 [02:17<01:19,  3.43it/s] 56%|█████▋    | 353/625 [02:17<01:19,  3.43it/s] 57%|█████▋    | 354/625 [02:17<01:19,  3.40it/s] 57%|█████▋    | 355/625 [02:18<01:19,  3.41it/s] 57%|█████▋    | 356/625 [02:18<01:18,  3.41it/s] 57%|█████▋    | 357/625 [02:18<01:18,  3.42it/s] 57%|█████▋    | 358/625 [02:19<01:17,  3.42it/s] 57%|█████▋    | 359/625 [02:19<01:17,  3.42it/s] 58%|█████▊    | 360/625 [02:19<01:17,  3.43it/s] 58%|█████▊    | 361/625 [02:19<01:17,  3.43it/s] 58%|█████▊    | 362/625 [02:20<01:16,  3.43it/s] 58%|█████▊    | 363/625 [02:20<01:16,  3.43it/s] 58%|█████▊    | 364/625 [02:20<01:16,  3.43it/s] 58%|█████▊    | 365/625 [02:21<01:16,  3.42it/s] 59%|█████▊    | 366/625 [02:21<01:15,  3.42it/s] 59%|█████▊    | 367/625 [02:21<01:15,  3.43it/s] 59%|█████▉    | 368/625 [02:21<01:14,  3.43it/s] 59%|█████▉    | 369/625 [02:22<01:14,  3.43it/s] 59%|█████▉    | 370/625 [02:22<01:14,  3.43it/s] 59%|█████▉    | 371/625 [02:22<01:14,  3.43it/s] 60%|█████▉    | 372/625 [02:23<01:13,  3.42it/s] 60%|█████▉    | 373/625 [02:23<01:13,  3.43it/s] 60%|█████▉    | 374/625 [02:23<01:13,  3.43it/s] 60%|██████    | 375/625 [02:23<01:12,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 08:36:29,938 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:36:29,938 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 08:36:29,938 >>   Batch size = 8
{'eval_loss': 1.1334336996078491, 'eval_runtime': 9.4886, 'eval_samples_per_second': 368.546, 'eval_steps_per_second': 46.16, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.78it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.29it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.52it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.79it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.43it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.98it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.64it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.35it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.20it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.18it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.25it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.36it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.36it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.46it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.34it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.28it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.12it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.17it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.16it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.17it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.39it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.30it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.37it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.38it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.28it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.15it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.11it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.17it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.26it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.26it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.35it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.26it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.32it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.27it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.10it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.14it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.11it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.20it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.21it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.33it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.31it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.33it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.22it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.12it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.14it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.15it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.17it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.26it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.27it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.30it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.33it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.14it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.11it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.12it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.14it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.11it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.19it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.23it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.20it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.28it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.20it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.20it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.18it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.05it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.09it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.11it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.19it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.18it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.17it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.10it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.05it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.13it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.11it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.11it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.16it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.24it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.29it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.12it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.22it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.18it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.20it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.19it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.16it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.18it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.28it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.26it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.35it/s][A                                                 
                                                 [A 60%|██████    | 375/625 [02:33<01:12,  3.43it/s]
100%|██████████| 438/438 [00:09<00:00, 46.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:36:39,423 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-375
[INFO|configuration_utils.py:351] 2023-08-29 08:36:39,443 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-375/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:36:42,000 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-375/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:36:42,159 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-375/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:36:42,288 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-375/special_tokens_map.json
 60%|██████    | 376/625 [02:43<25:29,  6.14s/it] 60%|██████    | 377/625 [02:44<18:08,  4.39s/it] 60%|██████    | 378/625 [02:44<13:00,  3.16s/it] 61%|██████    | 379/625 [02:44<09:25,  2.30s/it] 61%|██████    | 380/625 [02:44<06:55,  1.70s/it] 61%|██████    | 381/625 [02:45<05:10,  1.27s/it] 61%|██████    | 382/625 [02:45<03:58,  1.02it/s] 61%|██████▏   | 383/625 [02:45<03:07,  1.29it/s] 61%|██████▏   | 384/625 [02:46<02:31,  1.59it/s] 62%|██████▏   | 385/625 [02:46<02:06,  1.90it/s] 62%|██████▏   | 386/625 [02:46<01:49,  2.19it/s] 62%|██████▏   | 387/625 [02:46<01:36,  2.46it/s] 62%|██████▏   | 388/625 [02:47<01:28,  2.68it/s] 62%|██████▏   | 389/625 [02:47<01:22,  2.87it/s] 62%|██████▏   | 390/625 [02:47<01:17,  3.02it/s] 63%|██████▎   | 391/625 [02:48<01:14,  3.13it/s] 63%|██████▎   | 392/625 [02:48<01:12,  3.21it/s] 63%|██████▎   | 393/625 [02:48<01:10,  3.27it/s] 63%|██████▎   | 394/625 [02:49<01:09,  3.32it/s] 63%|██████▎   | 395/625 [02:49<01:08,  3.35it/s] 63%|██████▎   | 396/625 [02:49<01:07,  3.38it/s] 64%|██████▎   | 397/625 [02:49<01:07,  3.39it/s] 64%|██████▎   | 398/625 [02:50<01:06,  3.41it/s] 64%|██████▍   | 399/625 [02:50<01:06,  3.42it/s] 64%|██████▍   | 400/625 [02:50<01:05,  3.42it/s] 64%|██████▍   | 401/625 [02:51<01:05,  3.43it/s] 64%|██████▍   | 402/625 [02:51<01:05,  3.43it/s] 64%|██████▍   | 403/625 [02:51<01:04,  3.42it/s] 65%|██████▍   | 404/625 [02:51<01:04,  3.42it/s] 65%|██████▍   | 405/625 [02:52<01:04,  3.43it/s] 65%|██████▍   | 406/625 [02:52<01:03,  3.43it/s] 65%|██████▌   | 407/625 [02:52<01:03,  3.43it/s] 65%|██████▌   | 408/625 [02:53<01:03,  3.43it/s] 65%|██████▌   | 409/625 [02:53<01:02,  3.43it/s] 66%|██████▌   | 410/625 [02:53<01:02,  3.43it/s] 66%|██████▌   | 411/625 [02:53<01:02,  3.43it/s] 66%|██████▌   | 412/625 [02:54<01:02,  3.43it/s] 66%|██████▌   | 413/625 [02:54<01:01,  3.44it/s] 66%|██████▌   | 414/625 [02:54<01:01,  3.43it/s] 66%|██████▋   | 415/625 [02:55<01:01,  3.43it/s] 67%|██████▋   | 416/625 [02:55<01:00,  3.43it/s] 67%|██████▋   | 417/625 [02:55<01:00,  3.43it/s] 67%|██████▋   | 418/625 [02:56<01:00,  3.43it/s] 67%|██████▋   | 419/625 [02:56<01:00,  3.43it/s] 67%|██████▋   | 420/625 [02:56<00:59,  3.43it/s] 67%|██████▋   | 421/625 [02:56<00:59,  3.43it/s] 68%|██████▊   | 422/625 [02:57<00:59,  3.43it/s] 68%|██████▊   | 423/625 [02:57<00:58,  3.43it/s] 68%|██████▊   | 424/625 [02:57<00:58,  3.43it/s] 68%|██████▊   | 425/625 [02:58<00:58,  3.42it/s] 68%|██████▊   | 426/625 [02:58<00:58,  3.43it/s] 68%|██████▊   | 427/625 [02:58<00:57,  3.43it/s] 68%|██████▊   | 428/625 [02:58<00:57,  3.43it/s] 69%|██████▊   | 429/625 [02:59<00:57,  3.43it/s] 69%|██████▉   | 430/625 [02:59<00:56,  3.43it/s] 69%|██████▉   | 431/625 [02:59<00:56,  3.43it/s] 69%|██████▉   | 432/625 [03:00<00:56,  3.43it/s] 69%|██████▉   | 433/625 [03:00<00:55,  3.43it/s] 69%|██████▉   | 434/625 [03:00<00:55,  3.43it/s] 70%|██████▉   | 435/625 [03:00<00:55,  3.43it/s] 70%|██████▉   | 436/625 [03:01<00:55,  3.42it/s] 70%|██████▉   | 437/625 [03:01<00:54,  3.42it/s] 70%|███████   | 438/625 [03:01<00:54,  3.42it/s] 70%|███████   | 439/625 [03:02<00:54,  3.43it/s] 70%|███████   | 440/625 [03:02<00:53,  3.43it/s] 71%|███████   | 441/625 [03:02<00:53,  3.43it/s] 71%|███████   | 442/625 [03:03<00:53,  3.43it/s] 71%|███████   | 443/625 [03:03<00:53,  3.43it/s] 71%|███████   | 444/625 [03:03<00:52,  3.43it/s] 71%|███████   | 445/625 [03:03<00:52,  3.43it/s] 71%|███████▏  | 446/625 [03:04<00:52,  3.43it/s] 72%|███████▏  | 447/625 [03:04<00:51,  3.42it/s] 72%|███████▏  | 448/625 [03:04<00:51,  3.42it/s] 72%|███████▏  | 449/625 [03:05<00:51,  3.43it/s] 72%|███████▏  | 450/625 [03:05<00:51,  3.43it/s] 72%|███████▏  | 451/625 [03:05<00:50,  3.43it/s] 72%|███████▏  | 452/625 [03:05<00:50,  3.43it/s] 72%|███████▏  | 453/625 [03:06<00:50,  3.43it/s] 73%|███████▎  | 454/625 [03:06<00:49,  3.43it/s] 73%|███████▎  | 455/625 [03:06<00:49,  3.43it/s] 73%|███████▎  | 456/625 [03:07<00:49,  3.43it/s] 73%|███████▎  | 457/625 [03:07<00:48,  3.43it/s] 73%|███████▎  | 458/625 [03:07<00:48,  3.42it/s] 73%|███████▎  | 459/625 [03:07<00:48,  3.42it/s] 74%|███████▎  | 460/625 [03:08<00:48,  3.42it/s] 74%|███████▍  | 461/625 [03:08<00:47,  3.42it/s] 74%|███████▍  | 462/625 [03:08<00:47,  3.43it/s] 74%|███████▍  | 463/625 [03:09<00:47,  3.43it/s] 74%|███████▍  | 464/625 [03:09<00:46,  3.43it/s] 74%|███████▍  | 465/625 [03:09<00:46,  3.43it/s] 75%|███████▍  | 466/625 [03:10<00:46,  3.43it/s] 75%|███████▍  | 467/625 [03:10<00:46,  3.43it/s] 75%|███████▍  | 468/625 [03:10<00:45,  3.43it/s] 75%|███████▌  | 469/625 [03:10<00:45,  3.42it/s] 75%|███████▌  | 470/625 [03:11<00:45,  3.43it/s] 75%|███████▌  | 471/625 [03:11<00:44,  3.42it/s] 76%|███████▌  | 472/625 [03:11<00:44,  3.42it/s] 76%|███████▌  | 473/625 [03:12<00:44,  3.42it/s] 76%|███████▌  | 474/625 [03:12<00:44,  3.42it/s] 76%|███████▌  | 475/625 [03:12<00:43,  3.43it/s] 76%|███████▌  | 476/625 [03:12<00:43,  3.43it/s] 76%|███████▋  | 477/625 [03:13<00:43,  3.43it/s] 76%|███████▋  | 478/625 [03:13<00:42,  3.43it/s] 77%|███████▋  | 479/625 [03:13<00:42,  3.42it/s] 77%|███████▋  | 480/625 [03:14<00:42,  3.41it/s] 77%|███████▋  | 481/625 [03:14<00:42,  3.41it/s] 77%|███████▋  | 482/625 [03:14<00:41,  3.41it/s] 77%|███████▋  | 483/625 [03:14<00:41,  3.41it/s] 77%|███████▋  | 484/625 [03:15<00:41,  3.42it/s] 78%|███████▊  | 485/625 [03:15<00:40,  3.42it/s] 78%|███████▊  | 486/625 [03:15<00:40,  3.43it/s] 78%|███████▊  | 487/625 [03:16<00:42,  3.27it/s] 78%|███████▊  | 488/625 [03:16<00:41,  3.32it/s] 78%|███████▊  | 489/625 [03:16<00:40,  3.35it/s] 78%|███████▊  | 490/625 [03:17<00:39,  3.38it/s] 79%|███████▊  | 491/625 [03:17<00:39,  3.39it/s] 79%|███████▊  | 492/625 [03:17<00:39,  3.40it/s] 79%|███████▉  | 493/625 [03:17<00:38,  3.41it/s] 79%|███████▉  | 494/625 [03:18<00:38,  3.41it/s] 79%|███████▉  | 495/625 [03:18<00:38,  3.41it/s] 79%|███████▉  | 496/625 [03:18<00:37,  3.41it/s] 80%|███████▉  | 497/625 [03:19<00:37,  3.41it/s] 80%|███████▉  | 498/625 [03:19<00:37,  3.40it/s] 80%|███████▉  | 499/625 [03:19<00:37,  3.40it/s] 80%|████████  | 500/625 [03:20<00:36,  3.41it/s]                                                  80%|████████  | 500/625 [03:20<00:36,  3.41it/s][INFO|trainer.py:2140] 2023-08-29 08:37:25,958 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:37:25,958 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 08:37:25,958 >>   Batch size = 8
{'eval_loss': 1.1504931449890137, 'eval_runtime': 9.4704, 'eval_samples_per_second': 369.257, 'eval_steps_per_second': 46.25, 'epoch': 3.0}
{'loss': 0.4062, 'learning_rate': 7.5e-06, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.43it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.32it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.55it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.84it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.50it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.09it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.78it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.50it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.30it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.34it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.39it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.41it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.40it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.48it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.44it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.38it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.24it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.12it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.20it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.30it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.39it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.39it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.39it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.44it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.39it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.28it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.38it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.33it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.31it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.31it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.35it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.39it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.29it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.25it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.21it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.24it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.29it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.39it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.46it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.30it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.43it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.39it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.35it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.32it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.26it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.37it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.33it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.35it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.46it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.40it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.27it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.26it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.26it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.28it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.16it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.30it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.27it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.31it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.35it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.29it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.15it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.19it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.20it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.16it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.24it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.25it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.30it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.32it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.26it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.19it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.23it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.21it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.23it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.15it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.27it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 45.97it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.02it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.14it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.15it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.22it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.19it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.13it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.18it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.20it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.28it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.39it/s][A                                                 
                                                 [A 80%|████████  | 500/625 [03:29<00:36,  3.41it/s]
100%|██████████| 438/438 [00:09<00:00, 46.39it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:37:35,432 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-500
[INFO|configuration_utils.py:351] 2023-08-29 08:37:35,456 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-500/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:37:37,583 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:37:37,601 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:37:37,613 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-500/special_tokens_map.json
 80%|████████  | 501/625 [03:36<10:28,  5.07s/it] 80%|████████  | 502/625 [03:36<07:26,  3.63s/it] 80%|████████  | 503/625 [03:36<05:20,  2.63s/it] 81%|████████  | 504/625 [03:37<03:53,  1.93s/it] 81%|████████  | 505/625 [03:37<02:52,  1.44s/it] 81%|████████  | 506/625 [03:37<02:10,  1.09s/it] 81%|████████  | 507/625 [03:37<01:40,  1.17it/s] 81%|████████▏ | 508/625 [03:38<01:20,  1.46it/s] 81%|████████▏ | 509/625 [03:38<01:05,  1.77it/s] 82%|████████▏ | 510/625 [03:38<00:55,  2.07it/s] 82%|████████▏ | 511/625 [03:39<00:48,  2.35it/s] 82%|████████▏ | 512/625 [03:39<00:43,  2.60it/s] 82%|████████▏ | 513/625 [03:39<00:40,  2.80it/s] 82%|████████▏ | 514/625 [03:39<00:37,  2.96it/s] 82%|████████▏ | 515/625 [03:40<00:35,  3.09it/s] 83%|████████▎ | 516/625 [03:40<00:34,  3.18it/s] 83%|████████▎ | 517/625 [03:40<00:33,  3.26it/s] 83%|████████▎ | 518/625 [03:41<00:32,  3.30it/s] 83%|████████▎ | 519/625 [03:41<00:31,  3.34it/s] 83%|████████▎ | 520/625 [03:41<00:31,  3.37it/s] 83%|████████▎ | 521/625 [03:42<00:30,  3.38it/s] 84%|████████▎ | 522/625 [03:42<00:30,  3.40it/s] 84%|████████▎ | 523/625 [03:42<00:29,  3.41it/s] 84%|████████▍ | 524/625 [03:42<00:29,  3.40it/s] 84%|████████▍ | 525/625 [03:43<00:29,  3.41it/s] 84%|████████▍ | 526/625 [03:43<00:28,  3.42it/s] 84%|████████▍ | 527/625 [03:43<00:28,  3.42it/s] 84%|████████▍ | 528/625 [03:44<00:28,  3.43it/s] 85%|████████▍ | 529/625 [03:44<00:27,  3.43it/s] 85%|████████▍ | 530/625 [03:44<00:27,  3.43it/s] 85%|████████▍ | 531/625 [03:44<00:27,  3.43it/s] 85%|████████▌ | 532/625 [03:45<00:27,  3.44it/s] 85%|████████▌ | 533/625 [03:45<00:26,  3.44it/s] 85%|████████▌ | 534/625 [03:45<00:26,  3.43it/s] 86%|████████▌ | 535/625 [03:46<00:26,  3.43it/s] 86%|████████▌ | 536/625 [03:46<00:25,  3.43it/s] 86%|████████▌ | 537/625 [03:46<00:25,  3.43it/s] 86%|████████▌ | 538/625 [03:46<00:25,  3.43it/s] 86%|████████▌ | 539/625 [03:47<00:25,  3.43it/s] 86%|████████▋ | 540/625 [03:47<00:24,  3.43it/s] 87%|████████▋ | 541/625 [03:47<00:24,  3.42it/s] 87%|████████▋ | 542/625 [03:48<00:24,  3.43it/s] 87%|████████▋ | 543/625 [03:48<00:23,  3.43it/s] 87%|████████▋ | 544/625 [03:48<00:23,  3.43it/s] 87%|████████▋ | 545/625 [03:49<00:23,  3.43it/s] 87%|████████▋ | 546/625 [03:49<00:23,  3.43it/s] 88%|████████▊ | 547/625 [03:49<00:22,  3.43it/s] 88%|████████▊ | 548/625 [03:49<00:22,  3.42it/s] 88%|████████▊ | 549/625 [03:50<00:22,  3.43it/s] 88%|████████▊ | 550/625 [03:50<00:21,  3.42it/s] 88%|████████▊ | 551/625 [03:50<00:21,  3.43it/s] 88%|████████▊ | 552/625 [03:51<00:21,  3.43it/s] 88%|████████▊ | 553/625 [03:51<00:21,  3.43it/s] 89%|████████▊ | 554/625 [03:51<00:20,  3.42it/s] 89%|████████▉ | 555/625 [03:51<00:20,  3.42it/s] 89%|████████▉ | 556/625 [03:52<00:20,  3.43it/s] 89%|████████▉ | 557/625 [03:52<00:19,  3.43it/s] 89%|████████▉ | 558/625 [03:52<00:19,  3.42it/s] 89%|████████▉ | 559/625 [03:53<00:19,  3.41it/s] 90%|████████▉ | 560/625 [03:53<00:19,  3.42it/s] 90%|████████▉ | 561/625 [03:53<00:18,  3.42it/s] 90%|████████▉ | 562/625 [03:54<00:19,  3.31it/s] 90%|█████████ | 563/625 [03:54<00:18,  3.31it/s] 90%|█████████ | 564/625 [03:54<00:18,  3.35it/s] 90%|█████████ | 565/625 [03:54<00:17,  3.37it/s] 91%|█████████ | 566/625 [03:55<00:17,  3.39it/s] 91%|█████████ | 567/625 [03:55<00:17,  3.40it/s] 91%|█████████ | 568/625 [03:55<00:16,  3.41it/s] 91%|█████████ | 569/625 [03:56<00:16,  3.41it/s] 91%|█████████ | 570/625 [03:56<00:16,  3.41it/s] 91%|█████████▏| 571/625 [03:56<00:15,  3.41it/s] 92%|█████████▏| 572/625 [03:56<00:15,  3.42it/s] 92%|█████████▏| 573/625 [03:57<00:15,  3.42it/s] 92%|█████████▏| 574/625 [03:57<00:14,  3.43it/s] 92%|█████████▏| 575/625 [03:57<00:14,  3.43it/s] 92%|█████████▏| 576/625 [03:58<00:14,  3.43it/s] 92%|█████████▏| 577/625 [03:58<00:14,  3.43it/s] 92%|█████████▏| 578/625 [03:58<00:13,  3.43it/s] 93%|█████████▎| 579/625 [03:59<00:13,  3.43it/s] 93%|█████████▎| 580/625 [03:59<00:13,  3.43it/s] 93%|█████████▎| 581/625 [03:59<00:12,  3.42it/s] 93%|█████████▎| 582/625 [03:59<00:12,  3.42it/s] 93%|█████████▎| 583/625 [04:00<00:12,  3.43it/s] 93%|█████████▎| 584/625 [04:00<00:11,  3.43it/s] 94%|█████████▎| 585/625 [04:00<00:11,  3.43it/s] 94%|█████████▍| 586/625 [04:01<00:11,  3.42it/s] 94%|█████████▍| 587/625 [04:01<00:11,  3.43it/s] 94%|█████████▍| 588/625 [04:01<00:10,  3.43it/s] 94%|█████████▍| 589/625 [04:01<00:10,  3.43it/s] 94%|█████████▍| 590/625 [04:02<00:10,  3.43it/s] 95%|█████████▍| 591/625 [04:02<00:09,  3.43it/s] 95%|█████████▍| 592/625 [04:02<00:09,  3.42it/s] 95%|█████████▍| 593/625 [04:03<00:09,  3.42it/s] 95%|█████████▌| 594/625 [04:03<00:09,  3.43it/s] 95%|█████████▌| 595/625 [04:03<00:08,  3.43it/s] 95%|█████████▌| 596/625 [04:03<00:08,  3.43it/s] 96%|█████████▌| 597/625 [04:04<00:08,  3.43it/s] 96%|█████████▌| 598/625 [04:04<00:07,  3.43it/s] 96%|█████████▌| 599/625 [04:04<00:07,  3.43it/s] 96%|█████████▌| 600/625 [04:05<00:07,  3.43it/s] 96%|█████████▌| 601/625 [04:05<00:07,  3.42it/s] 96%|█████████▋| 602/625 [04:05<00:06,  3.42it/s] 96%|█████████▋| 603/625 [04:06<00:06,  3.42it/s] 97%|█████████▋| 604/625 [04:06<00:06,  3.42it/s] 97%|█████████▋| 605/625 [04:06<00:05,  3.43it/s] 97%|█████████▋| 606/625 [04:06<00:05,  3.43it/s] 97%|█████████▋| 607/625 [04:07<00:05,  3.43it/s] 97%|█████████▋| 608/625 [04:07<00:04,  3.42it/s] 97%|█████████▋| 609/625 [04:07<00:04,  3.43it/s] 98%|█████████▊| 610/625 [04:08<00:04,  3.43it/s] 98%|█████████▊| 611/625 [04:08<00:04,  3.43it/s] 98%|█████████▊| 612/625 [04:08<00:03,  3.43it/s] 98%|█████████▊| 613/625 [04:08<00:03,  3.43it/s] 98%|█████████▊| 614/625 [04:09<00:03,  3.43it/s] 98%|█████████▊| 615/625 [04:09<00:02,  3.43it/s] 99%|█████████▊| 616/625 [04:09<00:02,  3.43it/s] 99%|█████████▊| 617/625 [04:10<00:02,  3.43it/s] 99%|█████████▉| 618/625 [04:10<00:02,  3.42it/s] 99%|█████████▉| 619/625 [04:10<00:01,  3.42it/s] 99%|█████████▉| 620/625 [04:10<00:01,  3.42it/s] 99%|█████████▉| 621/625 [04:11<00:01,  3.42it/s]100%|█████████▉| 622/625 [04:11<00:00,  3.42it/s]100%|█████████▉| 623/625 [04:11<00:00,  3.43it/s]100%|█████████▉| 624/625 [04:12<00:00,  3.43it/s]100%|██████████| 625/625 [04:12<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 08:38:18,380 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:38:18,380 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 08:38:18,380 >>   Batch size = 8
{'eval_loss': 1.163827657699585, 'eval_runtime': 9.4558, 'eval_samples_per_second': 369.826, 'eval_steps_per_second': 46.321, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.95it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.25it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.61it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.74it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.42it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.11it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.66it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.27it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.20it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.16it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.23it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.39it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.37it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.45it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.38it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.32it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.17it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.06it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.97it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.14it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.29it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.35it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.41it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.41it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.24it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.17it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.03it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.08it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.20it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.21it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.27it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.31it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.32it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.32it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.24it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.16it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.15it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.03it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.11it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.22it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.29it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.34it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.27it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.22it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.07it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.17it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.22it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.24it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.12it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.19it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.28it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.28it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.24it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.13it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.07it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.16it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.18it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.22it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.23it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.28it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.30it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.16it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.16it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.13it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.15it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.21it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.21it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.21it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.20it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.26it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.20it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.16it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.03it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.12it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.21it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.24it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.28it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.36it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.20it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.20it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.12it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.10it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.11it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.12it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.24it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.16it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.44it/s][A                                                 
                                                 [A100%|██████████| 625/625 [04:21<00:00,  3.43it/s]
100%|██████████| 438/438 [00:09<00:00, 46.44it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 08:38:27,872 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-625
[INFO|configuration_utils.py:351] 2023-08-29 08:38:27,889 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-625/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:38:30,089 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-625/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:38:30,106 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-625/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:38:30,119 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-625/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 08:38:34,473 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 08:38:34,476 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-125 (score: 1.1205298900604248).
                                                 100%|██████████| 625/625 [04:30<00:00,  3.43it/s]100%|██████████| 625/625 [04:30<00:00,  2.31it/s]
[INFO|trainer.py:1894] 2023-08-29 08:38:36,343 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 08:38:36,366 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 08:38:38,620 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 08:38:38,646 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 08:38:38,662 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:38:38,849 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:38,849 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:38,849 >>   train_loss               =     0.4022
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:38,849 >>   train_runtime            = 0:04:30.39
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:38,849 >>   train_samples            =       8000
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:38,849 >>   train_samples_per_second =    147.932
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:38,849 >>   train_steps_per_second   =      2.311
{'eval_loss': 1.1715742349624634, 'eval_runtime': 9.4699, 'eval_samples_per_second': 369.275, 'eval_steps_per_second': 46.252, 'epoch': 5.0}
{'train_runtime': 270.3946, 'train_samples_per_second': 147.932, 'train_steps_per_second': 2.311, 'train_loss': 0.4021798095703125, 'epoch': 5.0}
08/29/2023 08:38:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 08:38:38,890 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 08:38:38,890 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 08:38:38,890 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 57.50it/s]  3%|▎         | 12/438 [00:00<00:08, 50.84it/s]  4%|▍         | 18/438 [00:00<00:08, 49.12it/s]  5%|▌         | 23/438 [00:00<00:08, 48.42it/s]  6%|▋         | 28/438 [00:00<00:08, 48.00it/s]  8%|▊         | 33/438 [00:00<00:08, 47.66it/s]  9%|▊         | 38/438 [00:00<00:08, 47.35it/s] 10%|▉         | 43/438 [00:00<00:08, 46.99it/s] 11%|█         | 48/438 [00:01<00:08, 46.53it/s] 12%|█▏        | 53/438 [00:01<00:08, 46.62it/s] 13%|█▎        | 58/438 [00:01<00:08, 46.53it/s] 14%|█▍        | 63/438 [00:01<00:08, 46.65it/s] 16%|█▌        | 68/438 [00:01<00:07, 46.66it/s] 17%|█▋        | 73/438 [00:01<00:07, 46.70it/s] 18%|█▊        | 78/438 [00:01<00:07, 46.73it/s] 19%|█▉        | 83/438 [00:01<00:07, 46.81it/s] 20%|██        | 88/438 [00:01<00:07, 46.70it/s] 21%|██        | 93/438 [00:01<00:07, 46.55it/s] 22%|██▏       | 98/438 [00:02<00:07, 46.48it/s] 24%|██▎       | 103/438 [00:02<00:07, 46.39it/s] 25%|██▍       | 108/438 [00:02<00:07, 46.55it/s] 26%|██▌       | 113/438 [00:02<00:06, 46.66it/s] 27%|██▋       | 118/438 [00:02<00:06, 46.66it/s] 28%|██▊       | 123/438 [00:02<00:06, 46.76it/s] 29%|██▉       | 128/438 [00:02<00:06, 46.79it/s] 30%|███       | 133/438 [00:02<00:06, 46.58it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.59it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.51it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.54it/s] 35%|███▍      | 153/438 [00:03<00:06, 46.58it/s] 36%|███▌      | 158/438 [00:03<00:06, 46.56it/s] 37%|███▋      | 163/438 [00:03<00:05, 46.56it/s] 38%|███▊      | 168/438 [00:03<00:05, 46.70it/s] 39%|███▉      | 173/438 [00:03<00:05, 46.71it/s] 41%|████      | 178/438 [00:03<00:05, 46.71it/s] 42%|████▏     | 183/438 [00:03<00:05, 46.60it/s] 43%|████▎     | 188/438 [00:04<00:05, 46.51it/s] 44%|████▍     | 193/438 [00:04<00:05, 46.37it/s] 45%|████▌     | 198/438 [00:04<00:05, 46.52it/s] 46%|████▋     | 203/438 [00:04<00:05, 46.56it/s] 47%|████▋     | 208/438 [00:04<00:04, 46.59it/s] 49%|████▊     | 213/438 [00:04<00:04, 46.66it/s] 50%|████▉     | 218/438 [00:04<00:04, 46.70it/s] 51%|█████     | 223/438 [00:04<00:04, 46.68it/s] 52%|█████▏    | 228/438 [00:04<00:04, 46.42it/s] 53%|█████▎    | 233/438 [00:04<00:04, 46.50it/s] 54%|█████▍    | 238/438 [00:05<00:04, 46.44it/s] 55%|█████▌    | 243/438 [00:05<00:04, 46.49it/s] 57%|█████▋    | 248/438 [00:05<00:04, 46.53it/s] 58%|█████▊    | 253/438 [00:05<00:03, 46.55it/s] 59%|█████▉    | 258/438 [00:05<00:03, 46.45it/s] 60%|██████    | 263/438 [00:05<00:03, 46.57it/s] 61%|██████    | 268/438 [00:05<00:03, 46.57it/s] 62%|██████▏   | 273/438 [00:05<00:03, 46.53it/s] 63%|██████▎   | 278/438 [00:05<00:03, 46.48it/s] 65%|██████▍   | 283/438 [00:06<00:03, 46.51it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.48it/s] 67%|██████▋   | 293/438 [00:06<00:03, 46.55it/s] 68%|██████▊   | 298/438 [00:06<00:03, 46.55it/s] 69%|██████▉   | 303/438 [00:06<00:02, 46.57it/s] 70%|███████   | 308/438 [00:06<00:02, 46.58it/s] 71%|███████▏  | 313/438 [00:06<00:02, 46.59it/s] 73%|███████▎  | 318/438 [00:06<00:02, 46.45it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.48it/s] 75%|███████▍  | 328/438 [00:07<00:02, 46.45it/s] 76%|███████▌  | 333/438 [00:07<00:02, 46.49it/s] 77%|███████▋  | 338/438 [00:07<00:02, 46.56it/s] 78%|███████▊  | 343/438 [00:07<00:02, 46.61it/s] 79%|███████▉  | 348/438 [00:07<00:01, 46.57it/s] 81%|████████  | 353/438 [00:07<00:01, 46.57it/s] 82%|████████▏ | 358/438 [00:07<00:01, 46.45it/s] 83%|████████▎ | 363/438 [00:07<00:01, 46.48it/s] 84%|████████▍ | 368/438 [00:07<00:01, 46.43it/s] 85%|████████▌ | 373/438 [00:07<00:01, 46.44it/s] 86%|████████▋ | 378/438 [00:08<00:01, 46.43it/s] 87%|████████▋ | 383/438 [00:08<00:01, 46.54it/s] 89%|████████▊ | 388/438 [00:08<00:01, 46.57it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.63it/s] 91%|█████████ | 398/438 [00:08<00:00, 46.49it/s] 92%|█████████▏| 403/438 [00:08<00:00, 46.48it/s] 93%|█████████▎| 408/438 [00:08<00:00, 46.43it/s] 94%|█████████▍| 413/438 [00:08<00:00, 46.13it/s] 95%|█████████▌| 418/438 [00:08<00:00, 46.22it/s] 97%|█████████▋| 423/438 [00:09<00:00, 46.28it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.38it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.40it/s]100%|██████████| 438/438 [00:09<00:00, 46.59it/s]100%|██████████| 438/438 [00:09<00:00, 46.68it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 08:38:48,299 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:48,299 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:48,300 >>   eval_loss               =     1.1205
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:48,300 >>   eval_runtime            = 0:00:09.40
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:48,300 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:48,300 >>   eval_samples_per_second =    371.642
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:48,300 >>   eval_steps_per_second   =     46.548
[INFO|trainer_pt_utils.py:913] 2023-08-29 08:38:48,300 >>   perplexity              =     3.0665
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:54,916 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:54,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:54,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:54,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:54,920 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:38:55,503 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:38:55,504 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:38:56,079 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:38:57,103 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:38:57,103 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:59,958 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:59,960 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:59,960 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:59,960 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:38:59,960 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:39:00,612 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:39:00,613 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:39:01,183 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:39:01,346 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:39:01,346 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-375
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-250
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-625
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-125
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-500
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.22it/s]Extractor Predicting: 2it [00:01,  1.34it/s]Extractor Predicting: 3it [00:02,  1.31it/s]Extractor Predicting: 4it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.31it/s]Extractor Predicting: 6it [00:04,  1.25it/s]Extractor Predicting: 7it [00:05,  1.27it/s]Extractor Predicting: 8it [00:06,  1.28it/s]Extractor Predicting: 9it [00:07,  1.25it/s]Extractor Predicting: 10it [00:07,  1.26it/s]Extractor Predicting: 11it [00:08,  1.25it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.28it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:14,  1.30it/s]Extractor Predicting: 19it [00:14,  1.28it/s]Extractor Predicting: 20it [00:15,  1.25it/s]Extractor Predicting: 21it [00:16,  1.25it/s]Extractor Predicting: 22it [00:17,  1.23it/s]Extractor Predicting: 23it [00:18,  1.25it/s]Extractor Predicting: 24it [00:18,  1.30it/s]Extractor Predicting: 25it [00:19,  1.28it/s]Extractor Predicting: 26it [00:20,  1.32it/s]Extractor Predicting: 27it [00:21,  1.33it/s]Extractor Predicting: 28it [00:21,  1.34it/s]Extractor Predicting: 29it [00:22,  1.33it/s]Extractor Predicting: 30it [00:23,  1.33it/s]Extractor Predicting: 31it [00:24,  1.32it/s]Extractor Predicting: 32it [00:24,  1.29it/s]Extractor Predicting: 33it [00:25,  1.31it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:27,  1.30it/s]Extractor Predicting: 36it [00:27,  1.27it/s]Extractor Predicting: 37it [00:28,  1.27it/s]Extractor Predicting: 38it [00:29,  1.24it/s]Extractor Predicting: 39it [00:30,  1.27it/s]Extractor Predicting: 40it [00:31,  1.26it/s]Extractor Predicting: 41it [00:32,  1.23it/s]Extractor Predicting: 42it [00:32,  1.24it/s]Extractor Predicting: 43it [00:33,  1.27it/s]Extractor Predicting: 44it [00:34,  1.26it/s]Extractor Predicting: 45it [00:35,  1.26it/s]Extractor Predicting: 46it [00:35,  1.29it/s]Extractor Predicting: 47it [00:36,  1.28it/s]Extractor Predicting: 48it [00:37,  1.26it/s]Extractor Predicting: 49it [00:38,  1.24it/s]Extractor Predicting: 50it [00:39,  1.22it/s]Extractor Predicting: 51it [00:40,  1.22it/s]Extractor Predicting: 52it [00:40,  1.20it/s]Extractor Predicting: 53it [00:41,  1.22it/s]Extractor Predicting: 54it [00:42,  1.22it/s]Extractor Predicting: 55it [00:43,  1.23it/s]Extractor Predicting: 56it [00:44,  1.24it/s]Extractor Predicting: 57it [00:45,  1.17it/s]Extractor Predicting: 58it [00:45,  1.18it/s]Extractor Predicting: 59it [00:46,  1.21it/s]Extractor Predicting: 60it [00:47,  1.19it/s]Extractor Predicting: 61it [00:48,  1.18it/s]Extractor Predicting: 62it [00:49,  1.17it/s]Extractor Predicting: 63it [00:50,  1.16it/s]Extractor Predicting: 64it [00:50,  1.17it/s]Extractor Predicting: 65it [00:51,  1.16it/s]Extractor Predicting: 66it [00:52,  1.15it/s]Extractor Predicting: 67it [00:53,  1.14it/s]Extractor Predicting: 68it [00:54,  1.14it/s]Extractor Predicting: 69it [00:55,  1.14it/s]Extractor Predicting: 70it [00:56,  1.15it/s]Extractor Predicting: 71it [00:57,  1.14it/s]Extractor Predicting: 72it [00:58,  1.14it/s]Extractor Predicting: 73it [00:58,  1.12it/s]Extractor Predicting: 74it [00:59,  1.12it/s]Extractor Predicting: 75it [01:00,  1.12it/s]Extractor Predicting: 76it [01:01,  1.14it/s]Extractor Predicting: 77it [01:02,  1.14it/s]Extractor Predicting: 78it [01:03,  1.13it/s]Extractor Predicting: 79it [01:04,  1.15it/s]Extractor Predicting: 80it [01:05,  1.13it/s]Extractor Predicting: 81it [01:06,  1.11it/s]Extractor Predicting: 82it [01:06,  1.12it/s]Extractor Predicting: 83it [01:07,  1.15it/s]Extractor Predicting: 84it [01:08,  1.16it/s]Extractor Predicting: 85it [01:09,  1.16it/s]Extractor Predicting: 86it [01:10,  1.16it/s]Extractor Predicting: 87it [01:11,  1.13it/s]Extractor Predicting: 88it [01:12,  1.14it/s]Extractor Predicting: 89it [01:12,  1.17it/s]Extractor Predicting: 90it [01:13,  1.18it/s]Extractor Predicting: 91it [01:14,  1.20it/s]Extractor Predicting: 92it [01:15,  1.17it/s]Extractor Predicting: 93it [01:16,  1.20it/s]Extractor Predicting: 94it [01:17,  1.21it/s]Extractor Predicting: 95it [01:17,  1.20it/s]Extractor Predicting: 96it [01:18,  1.20it/s]Extractor Predicting: 97it [01:19,  1.21it/s]Extractor Predicting: 98it [01:20,  1.20it/s]Extractor Predicting: 99it [01:21,  1.20it/s]Extractor Predicting: 100it [01:22,  1.22it/s]Extractor Predicting: 101it [01:22,  1.22it/s]Extractor Predicting: 102it [01:23,  1.20it/s]Extractor Predicting: 103it [01:24,  1.16it/s]Extractor Predicting: 104it [01:25,  1.18it/s]Extractor Predicting: 105it [01:26,  1.19it/s]Extractor Predicting: 106it [01:27,  1.19it/s]Extractor Predicting: 107it [01:27,  1.17it/s]Extractor Predicting: 108it [01:28,  1.19it/s]Extractor Predicting: 109it [01:29,  1.18it/s]Extractor Predicting: 110it [01:30,  1.17it/s]Extractor Predicting: 111it [01:31,  1.17it/s]Extractor Predicting: 112it [01:32,  1.17it/s]Extractor Predicting: 113it [01:33,  1.18it/s]Extractor Predicting: 114it [01:33,  1.18it/s]Extractor Predicting: 115it [01:34,  1.18it/s]Extractor Predicting: 116it [01:35,  1.18it/s]Extractor Predicting: 117it [01:36,  1.21it/s]Extractor Predicting: 118it [01:37,  1.23it/s]Extractor Predicting: 119it [01:37,  1.27it/s]Extractor Predicting: 120it [01:38,  1.28it/s]Extractor Predicting: 121it [01:39,  1.26it/s]Extractor Predicting: 122it [01:40,  1.30it/s]Extractor Predicting: 123it [01:40,  1.31it/s]Extractor Predicting: 124it [01:41,  1.29it/s]Extractor Predicting: 125it [01:42,  1.30it/s]Extractor Predicting: 126it [01:43,  1.31it/s]Extractor Predicting: 127it [01:43,  1.34it/s]Extractor Predicting: 128it [01:44,  1.33it/s]Extractor Predicting: 129it [01:45,  1.26it/s]Extractor Predicting: 130it [01:46,  1.24it/s]Extractor Predicting: 131it [01:47,  1.26it/s]Extractor Predicting: 132it [01:47,  1.32it/s]Extractor Predicting: 133it [01:48,  1.33it/s]Extractor Predicting: 134it [01:49,  1.28it/s]Extractor Predicting: 135it [01:50,  1.30it/s]Extractor Predicting: 136it [01:50,  1.33it/s]Extractor Predicting: 137it [01:51,  1.34it/s]Extractor Predicting: 138it [01:52,  1.33it/s]Extractor Predicting: 139it [01:53,  1.33it/s]Extractor Predicting: 140it [01:53,  1.37it/s]Extractor Predicting: 141it [01:54,  1.37it/s]Extractor Predicting: 142it [01:55,  1.40it/s]Extractor Predicting: 143it [01:56,  1.36it/s]Extractor Predicting: 144it [01:56,  1.34it/s]Extractor Predicting: 145it [01:57,  1.49it/s]Extractor Predicting: 145it [01:57,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:07,547 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:07,552 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:07,552 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:07,552 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:07,552 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:41:08,150 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:41:08,152 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:41:08,717 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:41:09,751 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:41:09,751 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:12,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:12,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:12,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:12,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:41:12,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:41:13,211 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:41:13,212 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:41:13,785 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:41:13,935 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:41:13,935 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.557032590051458,
  "recall": 0.3714612525021447,
  "score": 0.44570252187339165,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.24it/s]Extractor Predicting: 3it [00:02,  1.24it/s]Extractor Predicting: 4it [00:03,  1.22it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 6it [00:04,  1.20it/s]Extractor Predicting: 7it [00:05,  1.21it/s]Extractor Predicting: 8it [00:06,  1.21it/s]Extractor Predicting: 9it [00:07,  1.20it/s]Extractor Predicting: 10it [00:08,  1.21it/s]Extractor Predicting: 11it [00:09,  1.22it/s]Extractor Predicting: 12it [00:09,  1.24it/s]Extractor Predicting: 13it [00:10,  1.22it/s]Extractor Predicting: 14it [00:11,  1.23it/s]Extractor Predicting: 15it [00:12,  1.25it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.23it/s]Extractor Predicting: 18it [00:14,  1.23it/s]Extractor Predicting: 19it [00:15,  1.21it/s]Extractor Predicting: 20it [00:16,  1.20it/s]Extractor Predicting: 21it [00:17,  1.22it/s]Extractor Predicting: 22it [00:18,  1.18it/s]Extractor Predicting: 23it [00:18,  1.20it/s]Extractor Predicting: 24it [00:19,  1.22it/s]Extractor Predicting: 25it [00:20,  1.23it/s]Extractor Predicting: 26it [00:21,  1.21it/s]Extractor Predicting: 27it [00:22,  1.20it/s]Extractor Predicting: 28it [00:23,  1.13it/s]Extractor Predicting: 29it [00:24,  1.14it/s]Extractor Predicting: 30it [00:24,  1.13it/s]Extractor Predicting: 31it [00:25,  1.15it/s]Extractor Predicting: 32it [00:26,  1.15it/s]Extractor Predicting: 33it [00:27,  1.18it/s]Extractor Predicting: 34it [00:28,  1.19it/s]Extractor Predicting: 35it [00:29,  1.20it/s]Extractor Predicting: 36it [00:29,  1.22it/s]Extractor Predicting: 37it [00:30,  1.26it/s]Extractor Predicting: 38it [00:31,  1.25it/s]Extractor Predicting: 39it [00:32,  1.25it/s]Extractor Predicting: 40it [00:33,  1.26it/s]Extractor Predicting: 41it [00:33,  1.25it/s]Extractor Predicting: 42it [00:34,  1.24it/s]Extractor Predicting: 43it [00:35,  1.24it/s]Extractor Predicting: 44it [00:36,  1.24it/s]Extractor Predicting: 45it [00:37,  1.22it/s]Extractor Predicting: 46it [00:37,  1.24it/s]Extractor Predicting: 47it [00:38,  1.24it/s]Extractor Predicting: 48it [00:39,  1.24it/s]Extractor Predicting: 49it [00:40,  1.23it/s]Extractor Predicting: 50it [00:41,  1.22it/s]Extractor Predicting: 51it [00:41,  1.23it/s]Extractor Predicting: 52it [00:42,  1.23it/s]Extractor Predicting: 53it [00:43,  1.23it/s]Extractor Predicting: 54it [00:44,  1.24it/s]Extractor Predicting: 55it [00:45,  1.21it/s]Extractor Predicting: 56it [00:46,  1.22it/s]Extractor Predicting: 57it [00:46,  1.25it/s]Extractor Predicting: 58it [00:47,  1.27it/s]Extractor Predicting: 59it [00:48,  1.27it/s]Extractor Predicting: 60it [00:49,  1.27it/s]Extractor Predicting: 61it [00:49,  1.25it/s]Extractor Predicting: 62it [00:50,  1.25it/s]Extractor Predicting: 63it [00:51,  1.24it/s]Extractor Predicting: 64it [00:52,  1.24it/s]Extractor Predicting: 65it [00:53,  1.23it/s]Extractor Predicting: 66it [00:54,  1.21it/s]Extractor Predicting: 67it [00:54,  1.23it/s]Extractor Predicting: 68it [00:55,  1.26it/s]Extractor Predicting: 69it [00:56,  1.27it/s]Extractor Predicting: 70it [00:57,  1.26it/s]Extractor Predicting: 71it [00:58,  1.24it/s]Extractor Predicting: 72it [00:58,  1.25it/s]Extractor Predicting: 73it [00:59,  1.22it/s]Extractor Predicting: 74it [01:00,  1.23it/s]Extractor Predicting: 75it [01:01,  1.23it/s]Extractor Predicting: 76it [01:02,  1.25it/s]Extractor Predicting: 77it [01:02,  1.21it/s]Extractor Predicting: 78it [01:03,  1.21it/s]Extractor Predicting: 79it [01:04,  1.22it/s]Extractor Predicting: 80it [01:05,  1.22it/s]Extractor Predicting: 81it [01:06,  1.23it/s]Extractor Predicting: 82it [01:07,  1.21it/s]Extractor Predicting: 83it [01:07,  1.22it/s]Extractor Predicting: 84it [01:08,  1.22it/s]Extractor Predicting: 85it [01:09,  1.24it/s]Extractor Predicting: 86it [01:10,  1.23it/s]Extractor Predicting: 87it [01:11,  1.22it/s]Extractor Predicting: 88it [01:11,  1.23it/s]Extractor Predicting: 89it [01:12,  1.23it/s]Extractor Predicting: 90it [01:13,  1.24it/s]Extractor Predicting: 91it [01:14,  1.24it/s]Extractor Predicting: 92it [01:15,  1.23it/s]Extractor Predicting: 93it [01:16,  1.21it/s]Extractor Predicting: 94it [01:16,  1.23it/s]Extractor Predicting: 95it [01:17,  1.22it/s]Extractor Predicting: 96it [01:18,  1.21it/s]Extractor Predicting: 97it [01:19,  1.23it/s]Extractor Predicting: 98it [01:20,  1.23it/s]Extractor Predicting: 99it [01:20,  1.24it/s]Extractor Predicting: 100it [01:21,  1.24it/s]Extractor Predicting: 101it [01:22,  1.25it/s]Extractor Predicting: 102it [01:23,  1.26it/s]Extractor Predicting: 103it [01:24,  1.24it/s]Extractor Predicting: 104it [01:24,  1.22it/s]Extractor Predicting: 105it [01:25,  1.20it/s]Extractor Predicting: 106it [01:26,  1.21it/s]Extractor Predicting: 107it [01:27,  1.20it/s]Extractor Predicting: 108it [01:28,  1.22it/s]Extractor Predicting: 109it [01:29,  1.20it/s]Extractor Predicting: 110it [01:29,  1.22it/s]Extractor Predicting: 111it [01:30,  1.22it/s]Extractor Predicting: 112it [01:31,  1.20it/s]Extractor Predicting: 113it [01:32,  1.20it/s]Extractor Predicting: 114it [01:33,  1.19it/s]Extractor Predicting: 115it [01:34,  1.17it/s]Extractor Predicting: 116it [01:35,  1.16it/s]Extractor Predicting: 117it [01:35,  1.18it/s]Extractor Predicting: 118it [01:36,  1.19it/s]Extractor Predicting: 119it [01:37,  1.20it/s]Extractor Predicting: 120it [01:38,  1.21it/s]Extractor Predicting: 121it [01:39,  1.22it/s]Extractor Predicting: 122it [01:39,  1.24it/s]Extractor Predicting: 123it [01:40,  1.25it/s]Extractor Predicting: 124it [01:41,  1.24it/s]Extractor Predicting: 125it [01:42,  1.23it/s]Extractor Predicting: 126it [01:43,  1.21it/s]Extractor Predicting: 127it [01:43,  1.21it/s]Extractor Predicting: 128it [01:44,  1.21it/s]Extractor Predicting: 129it [01:45,  1.23it/s]Extractor Predicting: 130it [01:46,  1.23it/s]Extractor Predicting: 131it [01:47,  1.24it/s]Extractor Predicting: 132it [01:47,  1.25it/s]Extractor Predicting: 133it [01:48,  1.23it/s]Extractor Predicting: 134it [01:49,  1.12it/s]Extractor Predicting: 135it [01:50,  1.14it/s]Extractor Predicting: 136it [01:51,  1.16it/s]Extractor Predicting: 137it [01:52,  1.16it/s]Extractor Predicting: 138it [01:53,  1.21it/s]Extractor Predicting: 139it [01:54,  1.21it/s]Extractor Predicting: 140it [01:54,  1.19it/s]Extractor Predicting: 141it [01:55,  1.20it/s]Extractor Predicting: 142it [01:56,  1.21it/s]Extractor Predicting: 143it [01:57,  1.22it/s]Extractor Predicting: 144it [01:58,  1.20it/s]Extractor Predicting: 145it [01:58,  1.22it/s]Extractor Predicting: 146it [01:59,  1.22it/s]Extractor Predicting: 147it [02:00,  1.25it/s]Extractor Predicting: 148it [02:01,  1.23it/s]Extractor Predicting: 149it [02:02,  1.24it/s]Extractor Predicting: 150it [02:02,  1.25it/s]Extractor Predicting: 151it [02:03,  1.27it/s]Extractor Predicting: 152it [02:04,  1.28it/s]Extractor Predicting: 153it [02:05,  1.28it/s]Extractor Predicting: 154it [02:06,  1.28it/s]Extractor Predicting: 155it [02:06,  1.27it/s]Extractor Predicting: 156it [02:07,  1.27it/s]Extractor Predicting: 157it [02:08,  1.25it/s]Extractor Predicting: 158it [02:09,  1.21it/s]Extractor Predicting: 159it [02:10,  1.23it/s]Extractor Predicting: 160it [02:10,  1.26it/s]Extractor Predicting: 161it [02:11,  1.26it/s]Extractor Predicting: 162it [02:12,  1.26it/s]Extractor Predicting: 163it [02:13,  1.27it/s]Extractor Predicting: 164it [02:14,  1.27it/s]Extractor Predicting: 165it [02:14,  1.25it/s]Extractor Predicting: 166it [02:15,  1.24it/s]Extractor Predicting: 167it [02:16,  1.24it/s]Extractor Predicting: 168it [02:17,  1.25it/s]Extractor Predicting: 169it [02:18,  1.28it/s]Extractor Predicting: 170it [02:18,  1.29it/s]Extractor Predicting: 171it [02:19,  1.25it/s]Extractor Predicting: 172it [02:20,  1.25it/s]Extractor Predicting: 173it [02:21,  1.25it/s]Extractor Predicting: 174it [02:22,  1.27it/s]Extractor Predicting: 175it [02:22,  1.29it/s]Extractor Predicting: 176it [02:23,  1.29it/s]Extractor Predicting: 177it [02:24,  1.25it/s]Extractor Predicting: 178it [02:25,  1.27it/s]Extractor Predicting: 179it [02:25,  1.25it/s]Extractor Predicting: 180it [02:26,  1.25it/s]Extractor Predicting: 181it [02:27,  1.26it/s]Extractor Predicting: 182it [02:28,  1.25it/s]Extractor Predicting: 183it [02:29,  1.26it/s]Extractor Predicting: 184it [02:29,  1.26it/s]Extractor Predicting: 185it [02:30,  1.28it/s]Extractor Predicting: 186it [02:31,  1.30it/s]Extractor Predicting: 187it [02:32,  1.33it/s]Extractor Predicting: 188it [02:32,  1.31it/s]Extractor Predicting: 189it [02:33,  1.27it/s]Extractor Predicting: 190it [02:34,  1.26it/s]Extractor Predicting: 191it [02:35,  1.24it/s]Extractor Predicting: 192it [02:36,  1.26it/s]Extractor Predicting: 193it [02:36,  1.27it/s]Extractor Predicting: 194it [02:37,  1.28it/s]Extractor Predicting: 195it [02:38,  1.29it/s]Extractor Predicting: 196it [02:39,  1.29it/s]Extractor Predicting: 197it [02:40,  1.25it/s]Extractor Predicting: 198it [02:40,  1.26it/s]Extractor Predicting: 199it [02:41,  1.24it/s]Extractor Predicting: 200it [02:42,  1.25it/s]Extractor Predicting: 201it [02:43,  1.25it/s]Extractor Predicting: 202it [02:44,  1.24it/s]Extractor Predicting: 203it [02:44,  1.24it/s]Extractor Predicting: 204it [02:45,  1.22it/s]Extractor Predicting: 205it [02:46,  1.20it/s]Extractor Predicting: 206it [02:47,  1.23it/s]Extractor Predicting: 207it [02:48,  1.23it/s]Extractor Predicting: 208it [02:49,  1.25it/s]Extractor Predicting: 209it [02:49,  1.27it/s]Extractor Predicting: 210it [02:50,  1.25it/s]Extractor Predicting: 211it [02:51,  1.24it/s]Extractor Predicting: 212it [02:52,  1.24it/s]Extractor Predicting: 213it [02:53,  1.23it/s]Extractor Predicting: 214it [02:53,  1.22it/s]Extractor Predicting: 215it [02:54,  1.22it/s]Extractor Predicting: 216it [02:55,  1.22it/s]Extractor Predicting: 217it [02:56,  1.21it/s]Extractor Predicting: 218it [02:57,  1.22it/s]Extractor Predicting: 219it [02:58,  1.21it/s]Extractor Predicting: 220it [02:58,  1.23it/s]Extractor Predicting: 221it [02:59,  1.27it/s]Extractor Predicting: 222it [03:00,  1.22it/s]Extractor Predicting: 223it [03:01,  1.20it/s]Extractor Predicting: 224it [03:02,  1.24it/s]Extractor Predicting: 225it [03:02,  1.24it/s]Extractor Predicting: 226it [03:03,  1.22it/s]Extractor Predicting: 227it [03:04,  1.23it/s]Extractor Predicting: 228it [03:05,  1.23it/s]Extractor Predicting: 229it [03:06,  1.25it/s]Extractor Predicting: 230it [03:06,  1.21it/s]Extractor Predicting: 231it [03:07,  1.22it/s]Extractor Predicting: 232it [03:08,  1.25it/s]Extractor Predicting: 233it [03:09,  1.22it/s]Extractor Predicting: 234it [03:10,  1.21it/s]Extractor Predicting: 235it [03:11,  1.22it/s]Extractor Predicting: 236it [03:11,  1.23it/s]Extractor Predicting: 237it [03:12,  1.21it/s]Extractor Predicting: 238it [03:13,  1.22it/s]Extractor Predicting: 239it [03:14,  1.22it/s]Extractor Predicting: 240it [03:15,  1.21it/s]Extractor Predicting: 241it [03:15,  1.22it/s]Extractor Predicting: 242it [03:16,  1.24it/s]Extractor Predicting: 243it [03:17,  1.22it/s]Extractor Predicting: 244it [03:18,  1.23it/s]Extractor Predicting: 245it [03:19,  1.22it/s]Extractor Predicting: 246it [03:20,  1.13it/s]Extractor Predicting: 247it [03:21,  1.15it/s]Extractor Predicting: 248it [03:21,  1.17it/s]Extractor Predicting: 249it [03:22,  1.19it/s]Extractor Predicting: 250it [03:23,  1.19it/s]Extractor Predicting: 251it [03:24,  1.21it/s]Extractor Predicting: 252it [03:25,  1.20it/s]Extractor Predicting: 253it [03:25,  1.22it/s]Extractor Predicting: 254it [03:26,  1.22it/s]Extractor Predicting: 255it [03:27,  1.23it/s]Extractor Predicting: 256it [03:28,  1.23it/s]Extractor Predicting: 257it [03:29,  1.25it/s]Extractor Predicting: 258it [03:29,  1.25it/s]Extractor Predicting: 259it [03:30,  1.27it/s]Extractor Predicting: 260it [03:31,  1.25it/s]Extractor Predicting: 261it [03:32,  1.26it/s]Extractor Predicting: 262it [03:33,  1.25it/s]Extractor Predicting: 263it [03:33,  1.25it/s]Extractor Predicting: 264it [03:34,  1.22it/s]Extractor Predicting: 265it [03:35,  1.22it/s]Extractor Predicting: 266it [03:36,  1.22it/s]Extractor Predicting: 267it [03:37,  1.23it/s]Extractor Predicting: 268it [03:38,  1.25it/s]Extractor Predicting: 269it [03:38,  1.25it/s]Extractor Predicting: 270it [03:39,  1.24it/s]Extractor Predicting: 271it [03:40,  1.25it/s]Extractor Predicting: 272it [03:41,  1.28it/s]Extractor Predicting: 273it [03:41,  1.28it/s]Extractor Predicting: 274it [03:42,  1.29it/s]Extractor Predicting: 275it [03:43,  1.26it/s]Extractor Predicting: 276it [03:44,  1.25it/s]Extractor Predicting: 277it [03:45,  1.27it/s]Extractor Predicting: 278it [03:45,  1.25it/s]Extractor Predicting: 279it [03:46,  1.25it/s]Extractor Predicting: 280it [03:47,  1.23it/s]Extractor Predicting: 281it [03:48,  1.24it/s]Extractor Predicting: 282it [03:49,  1.23it/s]Extractor Predicting: 283it [03:50,  1.23it/s]Extractor Predicting: 284it [03:50,  1.24it/s]Extractor Predicting: 285it [03:51,  1.23it/s]Extractor Predicting: 286it [03:52,  1.22it/s]Extractor Predicting: 287it [03:53,  1.23it/s]Extractor Predicting: 288it [03:54,  1.24it/s]Extractor Predicting: 289it [03:54,  1.24it/s]Extractor Predicting: 290it [03:55,  1.24it/s]Extractor Predicting: 291it [03:56,  1.24it/s]Extractor Predicting: 292it [03:57,  1.25it/s]Extractor Predicting: 293it [03:58,  1.25it/s]Extractor Predicting: 294it [03:58,  1.23it/s]Extractor Predicting: 295it [03:59,  1.25it/s]Extractor Predicting: 296it [04:00,  1.26it/s]Extractor Predicting: 297it [04:01,  1.22it/s]Extractor Predicting: 298it [04:02,  1.25it/s]Extractor Predicting: 299it [04:02,  1.25it/s]Extractor Predicting: 300it [04:03,  1.24it/s]Extractor Predicting: 301it [04:04,  1.28it/s]Extractor Predicting: 302it [04:05,  1.27it/s]Extractor Predicting: 303it [04:05,  1.30it/s]Extractor Predicting: 304it [04:06,  1.28it/s]Extractor Predicting: 305it [04:07,  1.27it/s]Extractor Predicting: 306it [04:08,  1.26it/s]Extractor Predicting: 307it [04:09,  1.25it/s]Extractor Predicting: 308it [04:10,  1.23it/s]Extractor Predicting: 309it [04:10,  1.24it/s]Extractor Predicting: 310it [04:11,  1.23it/s]Extractor Predicting: 311it [04:12,  1.22it/s]Extractor Predicting: 312it [04:13,  1.19it/s]Extractor Predicting: 313it [04:14,  1.20it/s]Extractor Predicting: 314it [04:15,  1.20it/s]Extractor Predicting: 315it [04:15,  1.23it/s]Extractor Predicting: 316it [04:16,  1.24it/s]Extractor Predicting: 317it [04:17,  1.23it/s]Extractor Predicting: 318it [04:18,  1.24it/s]Extractor Predicting: 319it [04:19,  1.23it/s]Extractor Predicting: 320it [04:19,  1.23it/s]Extractor Predicting: 321it [04:20,  1.23it/s]Extractor Predicting: 322it [04:21,  1.22it/s]Extractor Predicting: 323it [04:22,  1.26it/s]Extractor Predicting: 324it [04:23,  1.24it/s]Extractor Predicting: 325it [04:23,  1.25it/s]Extractor Predicting: 326it [04:24,  1.24it/s]Extractor Predicting: 327it [04:25,  1.26it/s]Extractor Predicting: 328it [04:26,  1.27it/s]Extractor Predicting: 329it [04:27,  1.24it/s]Extractor Predicting: 330it [04:27,  1.22it/s]Extractor Predicting: 331it [04:28,  1.21it/s]Extractor Predicting: 332it [04:29,  1.21it/s]Extractor Predicting: 333it [04:30,  1.22it/s]Extractor Predicting: 334it [04:31,  1.22it/s]Extractor Predicting: 335it [04:32,  1.22it/s]Extractor Predicting: 336it [04:32,  1.25it/s]Extractor Predicting: 337it [04:33,  1.27it/s]Extractor Predicting: 338it [04:34,  1.26it/s]Extractor Predicting: 339it [04:35,  1.24it/s]Extractor Predicting: 340it [04:35,  1.25it/s]Extractor Predicting: 341it [04:36,  1.26it/s]Extractor Predicting: 342it [04:37,  1.23it/s]Extractor Predicting: 343it [04:38,  1.24it/s]Extractor Predicting: 344it [04:39,  1.25it/s]Extractor Predicting: 345it [04:39,  1.26it/s]Extractor Predicting: 346it [04:40,  1.26it/s]Extractor Predicting: 347it [04:41,  1.25it/s]Extractor Predicting: 348it [04:42,  1.25it/s]Extractor Predicting: 349it [04:43,  1.23it/s]Extractor Predicting: 350it [04:44,  1.24it/s]Extractor Predicting: 351it [04:45,  1.11it/s]Extractor Predicting: 352it [04:46,  1.12it/s]Extractor Predicting: 353it [04:46,  1.15it/s]Extractor Predicting: 354it [04:47,  1.16it/s]Extractor Predicting: 355it [04:48,  1.19it/s]Extractor Predicting: 356it [04:49,  1.20it/s]Extractor Predicting: 357it [04:50,  1.22it/s]Extractor Predicting: 358it [04:50,  1.24it/s]Extractor Predicting: 359it [04:51,  1.23it/s]Extractor Predicting: 360it [04:52,  1.22it/s]Extractor Predicting: 361it [04:53,  1.23it/s]Extractor Predicting: 362it [04:54,  1.23it/s]Extractor Predicting: 363it [04:54,  1.24it/s]Extractor Predicting: 364it [04:55,  1.25it/s]Extractor Predicting: 365it [04:56,  1.25it/s]Extractor Predicting: 366it [04:57,  1.24it/s]Extractor Predicting: 367it [04:58,  1.23it/s]Extractor Predicting: 368it [04:58,  1.25it/s]Extractor Predicting: 369it [04:59,  1.25it/s]Extractor Predicting: 370it [05:00,  1.28it/s]Extractor Predicting: 371it [05:01,  1.26it/s]Extractor Predicting: 372it [05:02,  1.26it/s]Extractor Predicting: 373it [05:02,  1.26it/s]Extractor Predicting: 374it [05:03,  1.25it/s]Extractor Predicting: 375it [05:04,  1.26it/s]Extractor Predicting: 376it [05:05,  1.25it/s]Extractor Predicting: 377it [05:06,  1.26it/s]Extractor Predicting: 378it [05:06,  1.27it/s]Extractor Predicting: 379it [05:07,  1.23it/s]Extractor Predicting: 380it [05:08,  1.23it/s]Extractor Predicting: 381it [05:09,  1.26it/s]Extractor Predicting: 382it [05:10,  1.26it/s]Extractor Predicting: 383it [05:10,  1.22it/s]Extractor Predicting: 384it [05:11,  1.23it/s]Extractor Predicting: 385it [05:12,  1.24it/s]Extractor Predicting: 386it [05:13,  1.27it/s]Extractor Predicting: 387it [05:14,  1.29it/s]Extractor Predicting: 388it [05:14,  1.28it/s]Extractor Predicting: 389it [05:15,  1.25it/s]Extractor Predicting: 390it [05:16,  1.27it/s]Extractor Predicting: 391it [05:17,  1.27it/s]Extractor Predicting: 392it [05:17,  1.27it/s]Extractor Predicting: 393it [05:18,  1.28it/s]Extractor Predicting: 394it [05:19,  1.30it/s]Extractor Predicting: 395it [05:20,  1.27it/s]Extractor Predicting: 396it [05:21,  1.27it/s]Extractor Predicting: 397it [05:21,  1.28it/s]Extractor Predicting: 398it [05:22,  1.28it/s]Extractor Predicting: 399it [05:23,  1.29it/s]Extractor Predicting: 400it [05:24,  1.32it/s]Extractor Predicting: 401it [05:24,  1.29it/s]Extractor Predicting: 402it [05:25,  1.30it/s]Extractor Predicting: 403it [05:26,  1.28it/s]Extractor Predicting: 404it [05:27,  1.28it/s]Extractor Predicting: 405it [05:28,  1.28it/s]Extractor Predicting: 406it [05:28,  1.27it/s]Extractor Predicting: 407it [05:29,  1.27it/s]Extractor Predicting: 408it [05:30,  1.27it/s]Extractor Predicting: 409it [05:31,  1.28it/s]Extractor Predicting: 409it [05:31,  1.23it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:54,516 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:54,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:54,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:54,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:54,521 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:46:55,138 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:46:55,139 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:46:55,704 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:46:56,759 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:46:56,759 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:59,683 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:59,687 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:59,687 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:59,687 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:46:59,687 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:47:00,326 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:47:00,327 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:47:00,920 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:47:01,075 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:47:01,075 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3446170184521439,
  "recall": 0.2645470294507286,
  "score": 0.29931972789115646,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.11it/s]Extractor Predicting: 2it [00:01,  1.11it/s]Extractor Predicting: 3it [00:02,  1.12it/s]Extractor Predicting: 4it [00:03,  1.16it/s]Extractor Predicting: 5it [00:04,  1.16it/s]Extractor Predicting: 6it [00:05,  1.16it/s]Extractor Predicting: 7it [00:06,  1.16it/s]Extractor Predicting: 8it [00:06,  1.18it/s]Extractor Predicting: 9it [00:07,  1.22it/s]Extractor Predicting: 10it [00:08,  1.20it/s]Extractor Predicting: 11it [00:09,  1.23it/s]Extractor Predicting: 12it [00:10,  1.20it/s]Extractor Predicting: 13it [00:10,  1.22it/s]Extractor Predicting: 13it [00:10,  1.18it/s]
[INFO|configuration_utils.py:515] 2023-08-29 08:47:13,445 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:47:13,446 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 08:47:13,452 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:47:13,452 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 08:47:13,453 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 08:47:16,536 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 08:47:16,541 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 08:47:16,553 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 08:47:16,553 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 08:47:16,561 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:16,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:16,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:16,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:16,578 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:16,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 08:47:16,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.16022099447513813,
  "recall": 0.042212518195050945,
  "score": 0.06682027649769585,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 08:47:16,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:17,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:18,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:19,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:20,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:21,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:22,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:23,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:24,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:25,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:26,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:27,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:28,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:29,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:30,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:31,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:31,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:32,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:33,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:34,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:18<05:55, 18.72s/it][WARNING|generation_utils.py:914] 2023-08-29 08:47:35,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:36,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:37,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:38,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:39,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:40,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:41,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:41,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:42,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:43,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:44,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:45,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:46,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:47,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:48,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:49,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:50,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:50,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:51,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:52,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:36<05:31, 18.42s/it][WARNING|generation_utils.py:914] 2023-08-29 08:47:53,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:54,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:55,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:56,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:57,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:58,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:47:59,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:00,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:01,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:02,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:03,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:03,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:04,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:05,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:06,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:07,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:08,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:09,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:10,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:11,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:55<05:13, 18.45s/it][WARNING|generation_utils.py:914] 2023-08-29 08:48:12,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:13,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:14,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:15,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:15,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:16,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:17,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:18,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:19,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:20,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:21,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:22,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:23,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:24,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:25,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:26,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:27,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:28,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:29,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:29,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:13<04:55, 18.49s/it][WARNING|generation_utils.py:914] 2023-08-29 08:48:30,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:31,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:32,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:33,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:34,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:35,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:35,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:36,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:37,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:38,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:39,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:40,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:41,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:42,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:42,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:43,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:44,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:45,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:46,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:48,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:48,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:49,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:50,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:51,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:52,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:36<04:59, 19.95s/it][WARNING|generation_utils.py:914] 2023-08-29 08:48:53,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:54,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:55,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:55,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:56,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:57,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:58,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:48:59,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:00,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:00,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:01,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:02,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:03,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:04,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:05,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:06,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:06,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:08,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:09,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:10,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:11,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:55<04:34, 19.58s/it][WARNING|generation_utils.py:914] 2023-08-29 08:49:12,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:12,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:13,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:14,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:15,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:16,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:17,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:18,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:18,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:19,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:20,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:21,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:22,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:23,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:24,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:25,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:26,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:27,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:28,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:29,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:30,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:14<04:11, 19.34s/it][WARNING|generation_utils.py:914] 2023-08-29 08:49:31,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:32,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:33,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:34,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:34,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:35,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:36,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:37,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:38,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:39,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:40,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:41,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:42,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:42,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:43,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:44,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:45,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:46,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:47,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:48,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:49,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:50,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:51,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:35<04:00, 20.08s/it][WARNING|generation_utils.py:914] 2023-08-29 08:49:52,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:53,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:54,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:55,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:56,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:57,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:57,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:58,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:49:59,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:00,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:01,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:02,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:03,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:04,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:05,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:05,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:06,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:07,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:08,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:09,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:53<03:31, 19.24s/it][WARNING|generation_utils.py:914] 2023-08-29 08:50:10,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:11,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:11,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:12,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:13,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:14,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:15,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:16,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:17,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:17,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:18,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:19,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:20,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:21,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:22,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:22,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:23,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:24,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:25,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:26,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [03:10<03:05, 18.54s/it][WARNING|generation_utils.py:914] 2023-08-29 08:50:27,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:27,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:28,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:29,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:30,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:31,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:32,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:32,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:33,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:34,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:35,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:36,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:37,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:37,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:38,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:39,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:40,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:41,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:42,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:43,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:44,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:28<02:45, 18.39s/it][WARNING|generation_utils.py:914] 2023-08-29 08:50:45,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:46,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:47,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:47,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:48,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:50,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:51,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:51,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:52,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:53,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:54,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:55,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:56,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:57,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:58,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:50:59,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:00,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:01,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:02,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:03,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:04,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:48<02:31, 18.99s/it][WARNING|generation_utils.py:914] 2023-08-29 08:51:05,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:06,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:06,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:07,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:09,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:09,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:10,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:11,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:12,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:12,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:13,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:14,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:15,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:16,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:16,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:17,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:18,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:19,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:19,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [04:03<02:04, 17.84s/it][WARNING|generation_utils.py:914] 2023-08-29 08:51:20,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:21,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:22,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:23,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:24,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:25,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:26,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:27,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:27,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:28,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:29,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:30,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:31,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:32,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:33,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:33,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:34,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:35,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:36,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:37,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:21<01:46, 17.74s/it][WARNING|generation_utils.py:914] 2023-08-29 08:51:38,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:39,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:39,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:40,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:41,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:42,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:43,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:44,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:44,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:45,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:46,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:47,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:48,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:49,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:50,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:51,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:51,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:52,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:53,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:54,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:38<01:27, 17.56s/it][WARNING|generation_utils.py:914] 2023-08-29 08:51:55,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:56,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:57,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:57,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:58,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:51:59,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:00,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:01,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:01,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:02,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:03,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:04,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:05,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:06,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:06,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:07,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:08,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:09,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:10,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:11,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:12,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:56<01:10, 17.57s/it][WARNING|generation_utils.py:914] 2023-08-29 08:52:12,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:13,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:14,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:15,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:16,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:17,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:17,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:18,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:19,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:19,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:20,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:21,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:22,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:23,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:24,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:25,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:26,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:26,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:27,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:28,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [05:12<00:51, 17.21s/it][WARNING|generation_utils.py:914] 2023-08-29 08:52:29,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:30,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:31,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:31,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:32,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:33,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:34,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:35,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:36,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:36,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:37,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:38,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:39,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:40,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:41,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:41,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:42,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:43,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:44,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:45,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:29<00:34, 17.09s/it][WARNING|generation_utils.py:914] 2023-08-29 08:52:46,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:46,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:47,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:48,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:49,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:50,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:51,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:52,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:53,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:54,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:55,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:56,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:57,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:58,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:52:59,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:00,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:01,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:02,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:03,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:04,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:48<00:17, 17.68s/it][WARNING|generation_utils.py:914] 2023-08-29 08:53:05,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:06,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:06,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:07,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:08,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:09,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:10,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:11,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:12,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:13,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:14,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:14,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:15,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:16,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:17,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:18,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:19,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:20,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:20,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 08:53:21,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [06:05<00:00, 17.59s/it]Generating: 100%|██████████| 20/20 [06:05<00:00, 18.29s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:28,249 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:28,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:28,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:28,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:28,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:53:28,538 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:53:28,539 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:53:28,793 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:53:29,860 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:53:29,860 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:31,970 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:31,978 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:31,978 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:31,979 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:53:31,979 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:53:32,307 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:53:32,308 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:53:32,984 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:53:33,143 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:53:33,143 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 438, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9765625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9421875, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.965625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 228, 'raw': 288}
{'target': 600, 'success': 250, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 324, 'raw': 416}
{'target': 600, 'success': 350, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 399, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 495, 'raw': 640}
{'target': 600, 'success': 518, 'raw': 672}
{'target': 600, 'success': 540, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : voice type .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 589, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : competition class .', 'success_rate': 0.9136904761904762, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : field of work .', 'success_rate': 0.9092261904761905, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.8247282608695652, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : location .', 'success_rate': 0.9390625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9300595238095238, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9330357142857143, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 577, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.9107142857142857, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 409, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 472, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 566, 'raw': 576}
{'target': 600, 'success': 597, 'raw': 608}
{'target': 600, 'success': 629, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9828125, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : religion .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.96875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 8675
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8775, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.17it/s]Extractor Estimating: 2it [00:01,  1.21it/s]Extractor Estimating: 3it [00:02,  1.23it/s]Extractor Estimating: 4it [00:03,  1.25it/s]Extractor Estimating: 5it [00:04,  1.24it/s]Extractor Estimating: 6it [00:04,  1.31it/s]Extractor Estimating: 7it [00:05,  1.33it/s]Extractor Estimating: 8it [00:06,  1.32it/s]Extractor Estimating: 9it [00:07,  1.28it/s]Extractor Estimating: 10it [00:07,  1.28it/s]Extractor Estimating: 11it [00:08,  1.28it/s]Extractor Estimating: 12it [00:09,  1.25it/s]Extractor Estimating: 13it [00:10,  1.24it/s]Extractor Estimating: 14it [00:11,  1.24it/s]Extractor Estimating: 15it [00:11,  1.28it/s]Extractor Estimating: 16it [00:12,  1.31it/s]Extractor Estimating: 17it [00:13,  1.30it/s]Extractor Estimating: 18it [00:14,  1.25it/s]Extractor Estimating: 19it [00:14,  1.26it/s]Extractor Estimating: 20it [00:15,  1.29it/s]Extractor Estimating: 21it [00:16,  1.33it/s]Extractor Estimating: 22it [00:17,  1.30it/s]Extractor Estimating: 23it [00:18,  1.29it/s]Extractor Estimating: 24it [00:18,  1.33it/s]Extractor Estimating: 25it [00:19,  1.30it/s]Extractor Estimating: 26it [00:20,  1.20it/s]Extractor Estimating: 27it [00:21,  1.23it/s]Extractor Estimating: 28it [00:22,  1.27it/s]Extractor Estimating: 29it [00:22,  1.25it/s]Extractor Estimating: 30it [00:23,  1.25it/s]Extractor Estimating: 31it [00:24,  1.26it/s]Extractor Estimating: 32it [00:25,  1.28it/s]Extractor Estimating: 33it [00:25,  1.31it/s]Extractor Estimating: 34it [00:26,  1.33it/s]Extractor Estimating: 35it [00:27,  1.27it/s]Extractor Estimating: 36it [00:28,  1.22it/s]Extractor Estimating: 37it [00:29,  1.25it/s]Extractor Estimating: 38it [00:29,  1.27it/s]Extractor Estimating: 39it [00:30,  1.28it/s]Extractor Estimating: 40it [00:31,  1.30it/s]Extractor Estimating: 41it [00:32,  1.30it/s]Extractor Estimating: 42it [00:32,  1.29it/s]Extractor Estimating: 43it [00:33,  1.29it/s]Extractor Estimating: 44it [00:34,  1.30it/s]Extractor Estimating: 45it [00:35,  1.31it/s]Extractor Estimating: 46it [00:35,  1.32it/s]Extractor Estimating: 47it [00:36,  1.33it/s]Extractor Estimating: 48it [00:37,  1.30it/s]Extractor Estimating: 49it [00:38,  1.30it/s]Extractor Estimating: 50it [00:39,  1.32it/s]Extractor Estimating: 51it [00:39,  1.39it/s]Extractor Estimating: 52it [00:40,  1.47it/s]Extractor Estimating: 53it [00:40,  1.46it/s]Extractor Estimating: 54it [00:41,  1.48it/s]Extractor Estimating: 55it [00:42,  1.46it/s]Extractor Estimating: 56it [00:42,  1.49it/s]Extractor Estimating: 57it [00:43,  1.55it/s]Extractor Estimating: 58it [00:44,  1.57it/s]Extractor Estimating: 59it [00:44,  1.48it/s]Extractor Estimating: 60it [00:45,  1.48it/s]Extractor Estimating: 61it [00:46,  1.47it/s]Extractor Estimating: 62it [00:46,  1.51it/s]Extractor Estimating: 63it [00:47,  1.55it/s]Extractor Estimating: 64it [00:48,  1.60it/s]Extractor Estimating: 65it [00:48,  1.56it/s]Extractor Estimating: 66it [00:49,  1.63it/s]Extractor Estimating: 67it [00:49,  1.63it/s]Extractor Estimating: 68it [00:50,  1.64it/s]Extractor Estimating: 69it [00:51,  1.63it/s]Extractor Estimating: 70it [00:51,  1.63it/s]Extractor Estimating: 71it [00:52,  1.66it/s]Extractor Estimating: 72it [00:52,  1.67it/s]Extractor Estimating: 73it [00:53,  1.61it/s]Extractor Estimating: 74it [00:54,  1.56it/s]Extractor Estimating: 75it [00:54,  1.56it/s]Extractor Estimating: 76it [00:55,  1.42it/s]Extractor Estimating: 77it [00:56,  1.34it/s]Extractor Estimating: 78it [00:57,  1.30it/s]Extractor Estimating: 79it [00:58,  1.29it/s]Extractor Estimating: 80it [00:59,  1.28it/s]Extractor Estimating: 81it [00:59,  1.25it/s]Extractor Estimating: 82it [01:00,  1.21it/s]Extractor Estimating: 83it [01:01,  1.22it/s]Extractor Estimating: 84it [01:02,  1.21it/s]Extractor Estimating: 85it [01:03,  1.09it/s]Extractor Estimating: 86it [01:04,  1.06it/s]Extractor Estimating: 87it [01:05,  1.10it/s]Extractor Estimating: 88it [01:06,  1.13it/s]Extractor Estimating: 89it [01:07,  1.15it/s]Extractor Estimating: 90it [01:07,  1.18it/s]Extractor Estimating: 91it [01:08,  1.21it/s]Extractor Estimating: 92it [01:09,  1.18it/s]Extractor Estimating: 93it [01:10,  1.19it/s]Extractor Estimating: 94it [01:11,  1.22it/s]Extractor Estimating: 95it [01:11,  1.24it/s]Extractor Estimating: 96it [01:12,  1.25it/s]Extractor Estimating: 97it [01:13,  1.25it/s]Extractor Estimating: 98it [01:14,  1.27it/s]Extractor Estimating: 99it [01:15,  1.25it/s]Extractor Estimating: 100it [01:15,  1.24it/s]Extractor Estimating: 101it [01:16,  1.22it/s]Extractor Estimating: 102it [01:17,  1.21it/s]Extractor Estimating: 103it [01:18,  1.22it/s]Extractor Estimating: 104it [01:19,  1.28it/s]Extractor Estimating: 105it [01:19,  1.26it/s]Extractor Estimating: 106it [01:20,  1.31it/s]Extractor Estimating: 107it [01:21,  1.30it/s]Extractor Estimating: 108it [01:22,  1.29it/s]Extractor Estimating: 109it [01:22,  1.32it/s]Extractor Estimating: 110it [01:23,  1.28it/s]Extractor Estimating: 111it [01:24,  1.26it/s]Extractor Estimating: 112it [01:25,  1.23it/s]Extractor Estimating: 113it [01:26,  1.19it/s]Extractor Estimating: 114it [01:27,  1.21it/s]Extractor Estimating: 115it [01:27,  1.25it/s]Extractor Estimating: 116it [01:28,  1.28it/s]Extractor Estimating: 117it [01:29,  1.28it/s]Extractor Estimating: 118it [01:30,  1.23it/s]Extractor Estimating: 119it [01:30,  1.25it/s]Extractor Estimating: 120it [01:31,  1.17it/s]Extractor Estimating: 121it [01:32,  1.20it/s]Extractor Estimating: 122it [01:33,  1.20it/s]Extractor Estimating: 123it [01:34,  1.19it/s]Extractor Estimating: 124it [01:35,  1.24it/s]Extractor Estimating: 125it [01:35,  1.24it/s]Extractor Estimating: 126it [01:36,  1.32it/s]Extractor Estimating: 127it [01:37,  1.32it/s]Extractor Estimating: 128it [01:38,  1.33it/s]Extractor Estimating: 129it [01:38,  1.34it/s]Extractor Estimating: 130it [01:39,  1.34it/s]Extractor Estimating: 131it [01:40,  1.37it/s]Extractor Estimating: 132it [01:41,  1.35it/s]Extractor Estimating: 133it [01:41,  1.41it/s]Extractor Estimating: 134it [01:42,  1.40it/s]Extractor Estimating: 135it [01:43,  1.39it/s]Extractor Estimating: 136it [01:43,  1.42it/s]Extractor Estimating: 137it [01:44,  1.41it/s]Extractor Estimating: 138it [01:45,  1.43it/s]Extractor Estimating: 139it [01:45,  1.40it/s]Extractor Estimating: 140it [01:46,  1.42it/s]Extractor Estimating: 141it [01:47,  1.39it/s]Extractor Estimating: 142it [01:48,  1.40it/s]Extractor Estimating: 143it [01:48,  1.39it/s]Extractor Estimating: 144it [01:49,  1.39it/s]Extractor Estimating: 145it [01:50,  1.42it/s]Extractor Estimating: 146it [01:50,  1.39it/s]Extractor Estimating: 147it [01:51,  1.38it/s]Extractor Estimating: 148it [01:52,  1.35it/s]Extractor Estimating: 149it [01:53,  1.30it/s]Extractor Estimating: 150it [01:54,  1.33it/s]Extractor Estimating: 151it [01:54,  1.37it/s]Extractor Estimating: 152it [01:55,  1.41it/s]Extractor Estimating: 153it [01:56,  1.45it/s]Extractor Estimating: 154it [01:56,  1.47it/s]Extractor Estimating: 155it [01:57,  1.43it/s]Extractor Estimating: 156it [01:58,  1.44it/s]Extractor Estimating: 157it [01:58,  1.42it/s]Extractor Estimating: 158it [01:59,  1.44it/s]Extractor Estimating: 159it [02:00,  1.45it/s]Extractor Estimating: 160it [02:00,  1.40it/s]Extractor Estimating: 161it [02:01,  1.43it/s]Extractor Estimating: 162it [02:02,  1.42it/s]Extractor Estimating: 163it [02:03,  1.43it/s]Extractor Estimating: 164it [02:03,  1.43it/s]Extractor Estimating: 165it [02:04,  1.41it/s]Extractor Estimating: 166it [02:05,  1.37it/s]Extractor Estimating: 167it [02:05,  1.38it/s]Extractor Estimating: 168it [02:06,  1.39it/s]Extractor Estimating: 169it [02:07,  1.41it/s]Extractor Estimating: 170it [02:08,  1.38it/s]Extractor Estimating: 171it [02:08,  1.39it/s]Extractor Estimating: 172it [02:09,  1.46it/s]Extractor Estimating: 173it [02:10,  1.41it/s]Extractor Estimating: 174it [02:10,  1.42it/s]Extractor Estimating: 175it [02:11,  1.43it/s]Extractor Estimating: 176it [02:12,  1.39it/s]Extractor Estimating: 177it [02:13,  1.36it/s]Extractor Estimating: 178it [02:13,  1.34it/s]Extractor Estimating: 179it [02:14,  1.35it/s]Extractor Estimating: 180it [02:15,  1.38it/s]Extractor Estimating: 181it [02:16,  1.34it/s]Extractor Estimating: 182it [02:16,  1.36it/s]Extractor Estimating: 183it [02:17,  1.35it/s]Extractor Estimating: 184it [02:18,  1.35it/s]Extractor Estimating: 185it [02:19,  1.33it/s]Extractor Estimating: 186it [02:19,  1.34it/s]Extractor Estimating: 187it [02:20,  1.37it/s]Extractor Estimating: 188it [02:21,  1.37it/s]Extractor Estimating: 189it [02:21,  1.38it/s]Extractor Estimating: 190it [02:22,  1.36it/s]Extractor Estimating: 191it [02:23,  1.35it/s]Extractor Estimating: 192it [02:24,  1.32it/s]Extractor Estimating: 193it [02:24,  1.31it/s]Extractor Estimating: 194it [02:25,  1.33it/s]Extractor Estimating: 195it [02:26,  1.35it/s]Extractor Estimating: 196it [02:27,  1.32it/s]Extractor Estimating: 197it [02:27,  1.33it/s]Extractor Estimating: 198it [02:28,  1.28it/s]Extractor Estimating: 199it [02:29,  1.27it/s]Extractor Estimating: 200it [02:30,  1.30it/s]Extractor Estimating: 201it [02:31,  1.32it/s]Extractor Estimating: 202it [02:31,  1.32it/s]Extractor Estimating: 203it [02:32,  1.34it/s]Extractor Estimating: 204it [02:33,  1.39it/s]Extractor Estimating: 205it [02:33,  1.41it/s]Extractor Estimating: 206it [02:34,  1.43it/s]Extractor Estimating: 207it [02:35,  1.41it/s]Extractor Estimating: 208it [02:36,  1.39it/s]Extractor Estimating: 209it [02:36,  1.43it/s]Extractor Estimating: 210it [02:37,  1.42it/s]Extractor Estimating: 211it [02:38,  1.31it/s]Extractor Estimating: 212it [02:39,  1.30it/s]Extractor Estimating: 213it [02:39,  1.34it/s]Extractor Estimating: 214it [02:40,  1.36it/s]Extractor Estimating: 215it [02:41,  1.32it/s]Extractor Estimating: 216it [02:42,  1.34it/s]Extractor Estimating: 217it [02:42,  1.35it/s]Extractor Estimating: 218it [02:43,  1.32it/s]Extractor Estimating: 219it [02:44,  1.35it/s]Extractor Estimating: 220it [02:44,  1.39it/s]Extractor Estimating: 221it [02:45,  1.35it/s]Extractor Estimating: 222it [02:46,  1.32it/s]Extractor Estimating: 223it [02:47,  1.35it/s]Extractor Estimating: 224it [02:47,  1.33it/s]Extractor Estimating: 225it [02:48,  1.34it/s]Extractor Estimating: 226it [02:49,  1.35it/s]Extractor Estimating: 227it [02:50,  1.34it/s]Extractor Estimating: 228it [02:50,  1.37it/s]Extractor Estimating: 229it [02:51,  1.35it/s]Extractor Estimating: 230it [02:52,  1.38it/s]Extractor Estimating: 231it [02:53,  1.39it/s]Extractor Estimating: 232it [02:53,  1.38it/s]Extractor Estimating: 233it [02:54,  1.42it/s]Extractor Estimating: 234it [02:55,  1.42it/s]Extractor Estimating: 235it [02:55,  1.45it/s]Extractor Estimating: 236it [02:56,  1.45it/s]Extractor Estimating: 237it [02:57,  1.47it/s]Extractor Estimating: 238it [02:57,  1.46it/s]Extractor Estimating: 239it [02:58,  1.44it/s]Extractor Estimating: 240it [02:59,  1.44it/s]Extractor Estimating: 241it [02:59,  1.46it/s]Extractor Estimating: 242it [03:00,  1.47it/s]Extractor Estimating: 243it [03:01,  1.42it/s]Extractor Estimating: 244it [03:02,  1.41it/s]Extractor Estimating: 245it [03:02,  1.42it/s]Extractor Estimating: 246it [03:03,  1.42it/s]Extractor Estimating: 247it [03:04,  1.48it/s]Extractor Estimating: 248it [03:04,  1.45it/s]Extractor Estimating: 249it [03:05,  1.43it/s]Extractor Estimating: 250it [03:06,  1.43it/s]Extractor Estimating: 251it [03:06,  1.41it/s]Extractor Estimating: 252it [03:07,  1.37it/s]Extractor Estimating: 253it [03:08,  1.36it/s]Extractor Estimating: 254it [03:09,  1.42it/s]Extractor Estimating: 255it [03:09,  1.42it/s]Extractor Estimating: 256it [03:10,  1.42it/s]Extractor Estimating: 257it [03:11,  1.39it/s]Extractor Estimating: 258it [03:12,  1.38it/s]Extractor Estimating: 259it [03:12,  1.37it/s]Extractor Estimating: 260it [03:13,  1.38it/s]Extractor Estimating: 261it [03:14,  1.33it/s]Extractor Estimating: 262it [03:15,  1.31it/s]Extractor Estimating: 263it [03:15,  1.34it/s]Extractor Estimating: 264it [03:16,  1.35it/s]Extractor Estimating: 265it [03:17,  1.36it/s]Extractor Estimating: 266it [03:18,  1.30it/s]Extractor Estimating: 267it [03:18,  1.32it/s]Extractor Estimating: 268it [03:19,  1.34it/s]Extractor Estimating: 269it [03:20,  1.33it/s]Extractor Estimating: 270it [03:21,  1.31it/s]Extractor Estimating: 271it [03:21,  1.33it/s]Extractor Estimating: 272it [03:22,  1.33it/s]Extractor Estimating: 273it [03:23,  1.32it/s]Extractor Estimating: 274it [03:24,  1.33it/s]Extractor Estimating: 275it [03:24,  1.33it/s]Extractor Estimating: 276it [03:25,  1.25it/s]Extractor Estimating: 277it [03:26,  1.22it/s]Extractor Estimating: 278it [03:27,  1.18it/s]Extractor Estimating: 279it [03:28,  1.19it/s]Extractor Estimating: 280it [03:29,  1.21it/s]Extractor Estimating: 281it [03:30,  1.13it/s]Extractor Estimating: 282it [03:31,  1.12it/s]Extractor Estimating: 283it [03:31,  1.13it/s]Extractor Estimating: 284it [03:32,  1.15it/s]Extractor Estimating: 285it [03:33,  1.15it/s]Extractor Estimating: 286it [03:34,  1.11it/s]Extractor Estimating: 287it [03:35,  1.10it/s]Extractor Estimating: 288it [03:36,  1.14it/s]Extractor Estimating: 289it [03:37,  1.15it/s]Extractor Estimating: 290it [03:38,  1.15it/s]Extractor Estimating: 291it [03:38,  1.15it/s]Extractor Estimating: 292it [03:39,  1.15it/s]Extractor Estimating: 293it [03:40,  1.15it/s]Extractor Estimating: 294it [03:41,  1.18it/s]Extractor Estimating: 295it [03:42,  1.18it/s]Extractor Estimating: 296it [03:43,  1.14it/s]Extractor Estimating: 297it [03:44,  1.15it/s]Extractor Estimating: 298it [03:45,  1.08it/s]Extractor Estimating: 299it [03:46,  1.11it/s]Extractor Estimating: 300it [03:46,  1.11it/s]Extractor Estimating: 301it [03:47,  1.21it/s]Extractor Estimating: 302it [03:48,  1.33it/s]Extractor Estimating: 303it [03:48,  1.38it/s]Extractor Estimating: 304it [03:49,  1.47it/s]Extractor Estimating: 305it [03:50,  1.49it/s]Extractor Estimating: 306it [03:50,  1.49it/s]Extractor Estimating: 307it [03:51,  1.51it/s]Extractor Estimating: 308it [03:52,  1.53it/s]Extractor Estimating: 309it [03:52,  1.55it/s]Extractor Estimating: 310it [03:53,  1.49it/s]Extractor Estimating: 311it [03:53,  1.52it/s]Extractor Estimating: 312it [03:54,  1.51it/s]Extractor Estimating: 313it [03:55,  1.49it/s]Extractor Estimating: 314it [03:56,  1.45it/s]Extractor Estimating: 315it [03:56,  1.43it/s]Extractor Estimating: 316it [03:57,  1.46it/s]Extractor Estimating: 317it [03:58,  1.48it/s]Extractor Estimating: 318it [03:58,  1.47it/s]Extractor Estimating: 319it [03:59,  1.47it/s]Extractor Estimating: 320it [04:00,  1.39it/s]Extractor Estimating: 321it [04:00,  1.47it/s]Extractor Estimating: 322it [04:01,  1.51it/s]Extractor Estimating: 323it [04:02,  1.50it/s]Extractor Estimating: 324it [04:02,  1.55it/s]Extractor Estimating: 325it [04:03,  1.59it/s]Extractor Estimating: 326it [04:04,  1.51it/s]Extractor Estimating: 327it [04:04,  1.44it/s]Extractor Estimating: 328it [04:05,  1.42it/s]Extractor Estimating: 329it [04:06,  1.45it/s]Extractor Estimating: 330it [04:06,  1.49it/s]Extractor Estimating: 331it [04:07,  1.39it/s]Extractor Estimating: 332it [04:08,  1.41it/s]Extractor Estimating: 333it [04:09,  1.41it/s]Extractor Estimating: 334it [04:09,  1.44it/s]Extractor Estimating: 335it [04:10,  1.42it/s]Extractor Estimating: 336it [04:11,  1.40it/s]Extractor Estimating: 337it [04:12,  1.36it/s]Extractor Estimating: 338it [04:12,  1.41it/s]Extractor Estimating: 339it [04:13,  1.38it/s]Extractor Estimating: 340it [04:14,  1.43it/s]Extractor Estimating: 341it [04:14,  1.43it/s]Extractor Estimating: 342it [04:15,  1.44it/s]Extractor Estimating: 343it [04:16,  1.45it/s]Extractor Estimating: 344it [04:16,  1.46it/s]Extractor Estimating: 345it [04:17,  1.44it/s]Extractor Estimating: 346it [04:18,  1.48it/s]Extractor Estimating: 347it [04:18,  1.56it/s]Extractor Estimating: 348it [04:19,  1.48it/s]Extractor Estimating: 349it [04:20,  1.44it/s]Extractor Estimating: 350it [04:20,  1.41it/s]Extractor Estimating: 351it [04:21,  1.43it/s]Extractor Estimating: 352it [04:22,  1.44it/s]Extractor Estimating: 353it [04:23,  1.45it/s]Extractor Estimating: 354it [04:23,  1.48it/s]Extractor Estimating: 355it [04:24,  1.47it/s]Extractor Estimating: 356it [04:24,  1.50it/s]Extractor Estimating: 357it [04:25,  1.50it/s]Extractor Estimating: 358it [04:26,  1.51it/s]Extractor Estimating: 359it [04:26,  1.52it/s]Extractor Estimating: 360it [04:27,  1.56it/s]Extractor Estimating: 361it [04:28,  1.51it/s]Extractor Estimating: 362it [04:28,  1.49it/s]Extractor Estimating: 363it [04:29,  1.53it/s]Extractor Estimating: 364it [04:30,  1.47it/s]Extractor Estimating: 365it [04:30,  1.46it/s]Extractor Estimating: 366it [04:31,  1.41it/s]Extractor Estimating: 367it [04:32,  1.47it/s]Extractor Estimating: 368it [04:33,  1.46it/s]Extractor Estimating: 369it [04:33,  1.46it/s]Extractor Estimating: 370it [04:34,  1.47it/s]Extractor Estimating: 371it [04:35,  1.48it/s]Extractor Estimating: 372it [04:35,  1.50it/s]Extractor Estimating: 373it [04:36,  1.50it/s]Extractor Estimating: 374it [04:36,  1.55it/s]Extractor Estimating: 375it [04:37,  1.53it/s]Extractor Estimating: 376it [04:38,  1.51it/s]Extractor Estimating: 377it [04:39,  1.45it/s]Extractor Estimating: 378it [04:39,  1.46it/s]Extractor Estimating: 379it [04:40,  1.47it/s]Extractor Estimating: 380it [04:41,  1.47it/s]Extractor Estimating: 381it [04:41,  1.47it/s]Extractor Estimating: 382it [04:42,  1.54it/s]Extractor Estimating: 383it [04:43,  1.50it/s]Extractor Estimating: 384it [04:43,  1.47it/s]Extractor Estimating: 385it [04:44,  1.49it/s]Extractor Estimating: 386it [04:45,  1.50it/s]Extractor Estimating: 387it [04:45,  1.49it/s]Extractor Estimating: 388it [04:46,  1.47it/s]Extractor Estimating: 389it [04:47,  1.41it/s]Extractor Estimating: 390it [04:47,  1.45it/s]Extractor Estimating: 391it [04:48,  1.47it/s]Extractor Estimating: 392it [04:49,  1.46it/s]Extractor Estimating: 393it [04:49,  1.44it/s]Extractor Estimating: 394it [04:50,  1.41it/s]Extractor Estimating: 395it [04:51,  1.45it/s]Extractor Estimating: 396it [04:52,  1.41it/s]Extractor Estimating: 397it [04:52,  1.39it/s]Extractor Estimating: 398it [04:53,  1.37it/s]Extractor Estimating: 399it [04:54,  1.39it/s]Extractor Estimating: 400it [04:55,  1.40it/s]Extractor Estimating: 401it [04:55,  1.42it/s]Extractor Estimating: 402it [04:56,  1.42it/s]Extractor Estimating: 403it [04:57,  1.43it/s]Extractor Estimating: 404it [04:57,  1.44it/s]Extractor Estimating: 405it [04:58,  1.34it/s]Extractor Estimating: 406it [04:59,  1.33it/s]Extractor Estimating: 407it [05:00,  1.39it/s]Extractor Estimating: 408it [05:00,  1.46it/s]Extractor Estimating: 409it [05:01,  1.44it/s]Extractor Estimating: 410it [05:02,  1.46it/s]Extractor Estimating: 411it [05:02,  1.46it/s]Extractor Estimating: 412it [05:03,  1.43it/s]Extractor Estimating: 413it [05:04,  1.39it/s]Extractor Estimating: 414it [05:04,  1.40it/s]Extractor Estimating: 415it [05:05,  1.40it/s]Extractor Estimating: 416it [05:06,  1.36it/s]Extractor Estimating: 417it [05:07,  1.39it/s]Extractor Estimating: 418it [05:07,  1.36it/s]Extractor Estimating: 419it [05:08,  1.40it/s]Extractor Estimating: 420it [05:09,  1.36it/s]Extractor Estimating: 421it [05:10,  1.39it/s]Extractor Estimating: 422it [05:10,  1.39it/s]Extractor Estimating: 423it [05:11,  1.40it/s]Extractor Estimating: 424it [05:12,  1.40it/s]Extractor Estimating: 425it [05:12,  1.42it/s]Extractor Estimating: 426it [05:13,  1.40it/s]Extractor Estimating: 427it [05:14,  1.36it/s]Extractor Estimating: 428it [05:15,  1.35it/s]Extractor Estimating: 429it [05:15,  1.40it/s]Extractor Estimating: 430it [05:16,  1.43it/s]Extractor Estimating: 431it [05:17,  1.39it/s]Extractor Estimating: 432it [05:17,  1.39it/s]Extractor Estimating: 433it [05:18,  1.37it/s]Extractor Estimating: 434it [05:19,  1.41it/s]Extractor Estimating: 435it [05:20,  1.43it/s]Extractor Estimating: 436it [05:20,  1.41it/s]Extractor Estimating: 437it [05:21,  1.31it/s]Extractor Estimating: 438it [05:22,  1.30it/s]Extractor Estimating: 439it [05:23,  1.33it/s]Extractor Estimating: 440it [05:23,  1.33it/s]Extractor Estimating: 441it [05:24,  1.35it/s]Extractor Estimating: 442it [05:25,  1.36it/s]Extractor Estimating: 443it [05:26,  1.34it/s]Extractor Estimating: 444it [05:26,  1.33it/s]Extractor Estimating: 445it [05:27,  1.33it/s]Extractor Estimating: 446it [05:28,  1.31it/s]Extractor Estimating: 447it [05:29,  1.33it/s]Extractor Estimating: 448it [05:29,  1.34it/s]Extractor Estimating: 449it [05:30,  1.34it/s]Extractor Estimating: 450it [05:31,  1.36it/s]Extractor Estimating: 451it [05:32,  1.34it/s]Extractor Estimating: 452it [05:32,  1.34it/s]Extractor Estimating: 453it [05:33,  1.35it/s]Extractor Estimating: 454it [05:34,  1.31it/s]Extractor Estimating: 455it [05:35,  1.33it/s]Extractor Estimating: 456it [05:35,  1.39it/s]Extractor Estimating: 457it [05:36,  1.35it/s]Extractor Estimating: 458it [05:37,  1.36it/s]Extractor Estimating: 459it [05:37,  1.38it/s]Extractor Estimating: 460it [05:38,  1.30it/s]Extractor Estimating: 461it [05:39,  1.31it/s]Extractor Estimating: 462it [05:40,  1.33it/s]Extractor Estimating: 463it [05:41,  1.30it/s]Extractor Estimating: 464it [05:41,  1.33it/s]Extractor Estimating: 465it [05:42,  1.27it/s]Extractor Estimating: 466it [05:43,  1.25it/s]Extractor Estimating: 467it [05:44,  1.28it/s]Extractor Estimating: 468it [05:45,  1.25it/s]Extractor Estimating: 469it [05:45,  1.27it/s]Extractor Estimating: 470it [05:46,  1.31it/s]Extractor Estimating: 471it [05:47,  1.37it/s]Extractor Estimating: 472it [05:47,  1.36it/s]Extractor Estimating: 473it [05:48,  1.34it/s]Extractor Estimating: 474it [05:49,  1.32it/s]Extractor Estimating: 475it [05:50,  1.33it/s]Extractor Estimating: 476it [05:51,  1.30it/s]Extractor Estimating: 477it [05:51,  1.33it/s]Extractor Estimating: 478it [05:52,  1.29it/s]Extractor Estimating: 479it [05:53,  1.30it/s]Extractor Estimating: 480it [05:54,  1.27it/s]Extractor Estimating: 481it [05:54,  1.31it/s]Extractor Estimating: 482it [05:55,  1.31it/s]Extractor Estimating: 483it [05:56,  1.26it/s]Extractor Estimating: 484it [05:57,  1.31it/s]Extractor Estimating: 485it [05:57,  1.33it/s]Extractor Estimating: 486it [05:58,  1.32it/s]Extractor Estimating: 487it [05:59,  1.33it/s]Extractor Estimating: 488it [06:00,  1.33it/s]Extractor Estimating: 489it [06:00,  1.33it/s]Extractor Estimating: 490it [06:01,  1.37it/s]Extractor Estimating: 491it [06:02,  1.31it/s]Extractor Estimating: 492it [06:03,  1.31it/s]Extractor Estimating: 493it [06:03,  1.33it/s]Extractor Estimating: 494it [06:04,  1.32it/s]Extractor Estimating: 495it [06:05,  1.35it/s]Extractor Estimating: 496it [06:06,  1.37it/s]Extractor Estimating: 497it [06:06,  1.36it/s]Extractor Estimating: 498it [06:07,  1.38it/s]Extractor Estimating: 499it [06:08,  1.34it/s]Extractor Estimating: 500it [06:09,  1.31it/s]Extractor Estimating: 500it [06:09,  1.35it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:54,250 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:54,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:54,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:54,253 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:54,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 08:59:54,536 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 08:59:54,537 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:59:54,794 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 08:59:55,839 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:59:55,839 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:57,580 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:57,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:57,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:57,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 08:59:57,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 08:59:57,921 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 08:59:57,922 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 08:59:58,597 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 08:59:58,747 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 08:59:58,747 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 12:47:22,829 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 12:47:22,848 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9999 mean pseudo reward: 0.9662980809782279
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 16697
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16797, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16797, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.294, loss:484.7863
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.302, loss:417.6490
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.334, loss:389.6613
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.329, loss:372.7643
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.306, loss:372.3573
>> valid entity prec:0.6808, rec:0.6556, f1:0.6680
>> valid relation prec:0.3796, rec:0.2542, f1:0.3045
>> valid relation with NER prec:0.3796, rec:0.2542, f1:0.3045
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.995, loss:366.7523
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.310, loss:369.2097
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.298, loss:379.5072
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.318, loss:339.3749
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.301, loss:362.5254
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6487, rec:0.6411, f1:0.6449
>> valid relation prec:0.4208, rec:0.2757, f1:0.3331
>> valid relation with NER prec:0.4208, rec:0.2757, f1:0.3331
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 266, avg_time 2.990, loss:367.4658
g_step 1200, step 366, avg_time 1.304, loss:363.3360
g_step 1300, step 49, avg_time 1.289, loss:345.9105
g_step 1400, step 149, avg_time 1.321, loss:342.0057
g_step 1500, step 249, avg_time 1.311, loss:344.2777
>> valid entity prec:0.6614, rec:0.6186, f1:0.6393
>> valid relation prec:0.4201, rec:0.2436, f1:0.3084
>> valid relation with NER prec:0.4201, rec:0.2436, f1:0.3084
g_step 1600, step 349, avg_time 2.975, loss:342.7192
g_step 1700, step 32, avg_time 1.304, loss:335.8116
g_step 1800, step 132, avg_time 1.307, loss:318.1941
g_step 1900, step 232, avg_time 1.333, loss:327.6791
g_step 2000, step 332, avg_time 1.290, loss:336.6344
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6703, rec:0.6529, f1:0.6615
>> valid relation prec:0.4666, rec:0.2957, f1:0.3620
>> valid relation with NER prec:0.4666, rec:0.2957, f1:0.3620
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 15, avg_time 2.981, loss:316.9565
g_step 2200, step 115, avg_time 1.319, loss:294.4487
g_step 2300, step 215, avg_time 1.311, loss:307.0572
g_step 2400, step 315, avg_time 1.300, loss:320.3642
g_step 2500, step 415, avg_time 1.315, loss:327.3582
>> valid entity prec:0.6574, rec:0.6758, f1:0.6665
>> valid relation prec:0.4817, rec:0.3389, f1:0.3979
>> valid relation with NER prec:0.4817, rec:0.3389, f1:0.3979
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 98, avg_time 2.977, loss:297.4037
g_step 2700, step 198, avg_time 1.311, loss:283.9633
g_step 2800, step 298, avg_time 1.325, loss:289.5818
g_step 2900, step 398, avg_time 1.308, loss:305.3377
g_step 3000, step 81, avg_time 1.289, loss:280.7038
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6868, rec:0.6728, f1:0.6798
>> valid relation prec:0.5004, rec:0.3206, f1:0.3908
>> valid relation with NER prec:0.5004, rec:0.3206, f1:0.3908
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3100, step 181, avg_time 3.008, loss:275.7964
g_step 3200, step 281, avg_time 1.322, loss:287.5884
g_step 3300, step 381, avg_time 1.317, loss:298.6476
g_step 3400, step 64, avg_time 1.309, loss:267.0976
g_step 3500, step 164, avg_time 1.315, loss:270.5076
>> valid entity prec:0.6729, rec:0.6680, f1:0.6704
>> valid relation prec:0.4382, rec:0.2828, f1:0.3438
>> valid relation with NER prec:0.4382, rec:0.2828, f1:0.3438
g_step 3600, step 264, avg_time 2.969, loss:274.5451
g_step 3700, step 364, avg_time 1.308, loss:276.2945
g_step 3800, step 47, avg_time 1.314, loss:262.8220
g_step 3900, step 147, avg_time 1.302, loss:246.8805
g_step 4000, step 247, avg_time 1.298, loss:271.0031
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6984, rec:0.6582, f1:0.6777
>> valid relation prec:0.4584, rec:0.2802, f1:0.3478
>> valid relation with NER prec:0.4584, rec:0.2802, f1:0.3478
g_step 4100, step 347, avg_time 2.983, loss:255.5372
g_step 4200, step 30, avg_time 1.317, loss:263.3621
g_step 4300, step 130, avg_time 1.328, loss:237.1129
g_step 4400, step 230, avg_time 1.302, loss:247.8514
g_step 4500, step 330, avg_time 1.323, loss:254.1274
>> valid entity prec:0.6749, rec:0.6771, f1:0.6760
>> valid relation prec:0.4787, rec:0.3446, f1:0.4007
>> valid relation with NER prec:0.4787, rec:0.3446, f1:0.4007
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4600, step 13, avg_time 3.000, loss:253.0613
g_step 4700, step 113, avg_time 1.313, loss:221.1933
g_step 4800, step 213, avg_time 1.316, loss:235.0627
g_step 4900, step 313, avg_time 1.333, loss:251.5187
g_step 5000, step 413, avg_time 1.308, loss:263.9791
learning rate was adjusted to 0.0008
>> valid entity prec:0.6713, rec:0.6641, f1:0.6677
>> valid relation prec:0.4259, rec:0.2854, f1:0.3418
>> valid relation with NER prec:0.4259, rec:0.2854, f1:0.3418
g_step 5100, step 96, avg_time 3.005, loss:211.2671
g_step 5200, step 196, avg_time 1.341, loss:247.2662
g_step 5300, step 296, avg_time 1.300, loss:231.0647
g_step 5400, step 396, avg_time 1.314, loss:231.6151
g_step 5500, step 79, avg_time 1.314, loss:214.4411
>> valid entity prec:0.7095, rec:0.6641, f1:0.6860
>> valid relation prec:0.4712, rec:0.3226, f1:0.3830
>> valid relation with NER prec:0.4712, rec:0.3226, f1:0.3830
new max entity f1 on valid!
g_step 5600, step 179, avg_time 2.982, loss:222.2683
g_step 5700, step 279, avg_time 1.315, loss:231.0214
g_step 5800, step 379, avg_time 1.326, loss:241.2280
g_step 5900, step 62, avg_time 1.314, loss:213.7048
g_step 6000, step 162, avg_time 1.298, loss:216.2796
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6502, rec:0.6717, f1:0.6608
>> valid relation prec:0.4374, rec:0.2705, f1:0.3343
>> valid relation with NER prec:0.4374, rec:0.2705, f1:0.3343
g_step 6100, step 262, avg_time 3.016, loss:222.8755
g_step 6200, step 362, avg_time 1.322, loss:213.7672
g_step 6300, step 45, avg_time 1.323, loss:210.6029
g_step 6400, step 145, avg_time 1.320, loss:212.9669
g_step 6500, step 245, avg_time 1.328, loss:205.1029
>> valid entity prec:0.6618, rec:0.6895, f1:0.6753
>> valid relation prec:0.4671, rec:0.3326, f1:0.3885
>> valid relation with NER prec:0.4671, rec:0.3326, f1:0.3885
g_step 6600, step 345, avg_time 3.000, loss:220.8253
g_step 6700, step 28, avg_time 1.302, loss:199.3227
g_step 6800, step 128, avg_time 1.329, loss:195.3135
g_step 6900, step 228, avg_time 1.319, loss:197.2108
g_step 7000, step 328, avg_time 1.321, loss:212.6777
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6823, rec:0.6198, f1:0.6495
>> valid relation prec:0.3794, rec:0.2293, f1:0.2859
>> valid relation with NER prec:0.3794, rec:0.2293, f1:0.2859
g_step 7100, step 11, avg_time 3.019, loss:216.7482
g_step 7200, step 111, avg_time 1.340, loss:192.4254
g_step 7300, step 211, avg_time 1.299, loss:192.0019
g_step 7400, step 311, avg_time 1.295, loss:197.3040
g_step 7500, step 411, avg_time 1.341, loss:223.7704
>> valid entity prec:0.6868, rec:0.6343, f1:0.6595
>> valid relation prec:0.3824, rec:0.2399, f1:0.2949
>> valid relation with NER prec:0.3824, rec:0.2399, f1:0.2949
g_step 7600, step 94, avg_time 3.000, loss:177.4089
g_step 7700, step 194, avg_time 1.318, loss:199.4669
g_step 7800, step 294, avg_time 1.295, loss:193.7438
g_step 7900, step 394, avg_time 1.311, loss:190.8458
g_step 8000, step 77, avg_time 1.299, loss:172.0298
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6695, rec:0.6354, f1:0.6520
>> valid relation prec:0.4148, rec:0.2714, f1:0.3281
>> valid relation with NER prec:0.4148, rec:0.2714, f1:0.3281
g_step 8100, step 177, avg_time 3.009, loss:176.2170
g_step 8200, step 277, avg_time 1.326, loss:194.9887
g_step 8300, step 377, avg_time 1.322, loss:192.6321
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 12:47:22 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 12:47:22 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_12-47-22_ctolab01.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 12:47:23 - WARNING - datasets.builder -   Using custom data configuration default-5a8790b8027a6d29
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-5a8790b8027a6d29/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 12:47:24,111 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:47:24,112 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 12:47:24,113 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 12:47:24,114 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 12:47:24,121 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:47:24,127 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:47:24,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:47:24,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:47:24,128 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:47:24,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 12:47:24,128 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 12:47:24,254 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 12:47:27,314 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 12:47:27,316 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-5a8790b8027a6d29/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.28ba/s] 20%|██        | 2/10 [00:00<00:02,  3.11ba/s] 30%|███       | 3/10 [00:00<00:01,  3.77ba/s] 40%|████      | 4/10 [00:01<00:01,  4.21ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.48ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.66ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.78ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.88ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.94ba/s]100%|██████████| 10/10 [00:02<00:00,  4.97ba/s]100%|██████████| 10/10 [00:02<00:00,  4.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.97ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.27ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.39ba/s]100%|██████████| 4/4 [00:00<00:00,  5.44ba/s]100%|██████████| 4/4 [00:00<00:00,  4.93ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  8.18ba/s] 30%|███       | 3/10 [00:00<00:00,  9.85ba/s] 50%|█████     | 5/10 [00:00<00:00, 10.15ba/s] 70%|███████   | 7/10 [00:00<00:00, 10.37ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.35ba/s]100%|██████████| 10/10 [00:00<00:00, 10.13ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.65ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.95ba/s]100%|██████████| 4/4 [00:00<00:00,  7.83ba/s]100%|██████████| 4/4 [00:00<00:00,  8.24ba/s]
[INFO|trainer.py:414] 2023-08-29 12:47:32,228 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 12:47:32,243 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 12:47:32,243 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 12:47:32,243 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 12:47:32,243 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 12:47:32,243 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 12:47:32,243 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 12:47:32,243 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<04:01,  3.22it/s]  0%|          | 2/780 [00:00<03:52,  3.35it/s]  0%|          | 3/780 [00:00<03:49,  3.39it/s]  1%|          | 4/780 [00:01<03:47,  3.42it/s]  1%|          | 5/780 [00:01<03:49,  3.38it/s]  1%|          | 6/780 [00:01<03:47,  3.40it/s]  1%|          | 7/780 [00:02<03:46,  3.42it/s]  1%|          | 8/780 [00:02<03:45,  3.43it/s]  1%|          | 9/780 [00:02<03:44,  3.43it/s]  1%|▏         | 10/780 [00:02<03:44,  3.44it/s]  1%|▏         | 11/780 [00:03<03:43,  3.44it/s]  2%|▏         | 12/780 [00:03<03:43,  3.44it/s]  2%|▏         | 13/780 [00:03<03:43,  3.44it/s]  2%|▏         | 14/780 [00:04<03:42,  3.44it/s]  2%|▏         | 15/780 [00:04<03:42,  3.43it/s]  2%|▏         | 16/780 [00:04<03:42,  3.43it/s]  2%|▏         | 17/780 [00:04<03:42,  3.43it/s]  2%|▏         | 18/780 [00:05<03:41,  3.44it/s]  2%|▏         | 19/780 [00:05<03:41,  3.44it/s]  3%|▎         | 20/780 [00:05<03:40,  3.44it/s]  3%|▎         | 21/780 [00:06<03:40,  3.44it/s]  3%|▎         | 22/780 [00:06<03:40,  3.44it/s]  3%|▎         | 23/780 [00:06<03:40,  3.44it/s]  3%|▎         | 24/780 [00:07<03:39,  3.44it/s]  3%|▎         | 25/780 [00:07<03:39,  3.44it/s]  3%|▎         | 26/780 [00:07<03:39,  3.44it/s]  3%|▎         | 27/780 [00:07<03:38,  3.44it/s]  4%|▎         | 28/780 [00:08<03:38,  3.44it/s]  4%|▎         | 29/780 [00:08<03:38,  3.44it/s]  4%|▍         | 30/780 [00:08<03:38,  3.44it/s]  4%|▍         | 31/780 [00:09<03:37,  3.44it/s]  4%|▍         | 32/780 [00:09<03:37,  3.44it/s]  4%|▍         | 33/780 [00:09<03:37,  3.44it/s]  4%|▍         | 34/780 [00:09<03:36,  3.44it/s]  4%|▍         | 35/780 [00:10<03:36,  3.44it/s]  5%|▍         | 36/780 [00:10<03:36,  3.44it/s]  5%|▍         | 37/780 [00:10<03:36,  3.44it/s]  5%|▍         | 38/780 [00:11<03:35,  3.44it/s]  5%|▌         | 39/780 [00:11<03:35,  3.44it/s]  5%|▌         | 40/780 [00:11<03:35,  3.44it/s]  5%|▌         | 41/780 [00:11<03:34,  3.44it/s]  5%|▌         | 42/780 [00:12<03:34,  3.44it/s]  6%|▌         | 43/780 [00:12<03:33,  3.44it/s]  6%|▌         | 44/780 [00:12<03:33,  3.44it/s]  6%|▌         | 45/780 [00:13<03:33,  3.44it/s]  6%|▌         | 46/780 [00:13<03:34,  3.41it/s]  6%|▌         | 47/780 [00:13<03:35,  3.39it/s]  6%|▌         | 48/780 [00:13<03:34,  3.41it/s]  6%|▋         | 49/780 [00:14<03:33,  3.42it/s]  6%|▋         | 50/780 [00:14<03:32,  3.43it/s]  7%|▋         | 51/780 [00:14<03:32,  3.43it/s]  7%|▋         | 52/780 [00:15<03:32,  3.43it/s]  7%|▋         | 53/780 [00:15<03:31,  3.43it/s]  7%|▋         | 54/780 [00:15<03:31,  3.43it/s]  7%|▋         | 55/780 [00:16<03:31,  3.44it/s]  7%|▋         | 56/780 [00:16<03:30,  3.44it/s]  7%|▋         | 57/780 [00:16<03:30,  3.44it/s]  7%|▋         | 58/780 [00:16<03:30,  3.44it/s]  8%|▊         | 59/780 [00:17<03:30,  3.43it/s]  8%|▊         | 60/780 [00:17<03:29,  3.43it/s]  8%|▊         | 61/780 [00:17<03:29,  3.44it/s]  8%|▊         | 62/780 [00:18<03:29,  3.43it/s]  8%|▊         | 63/780 [00:18<03:28,  3.44it/s]  8%|▊         | 64/780 [00:18<03:28,  3.43it/s]  8%|▊         | 65/780 [00:18<03:28,  3.43it/s]  8%|▊         | 66/780 [00:19<03:28,  3.43it/s]  9%|▊         | 67/780 [00:19<03:27,  3.43it/s]  9%|▊         | 68/780 [00:19<03:27,  3.43it/s]  9%|▉         | 69/780 [00:20<03:27,  3.42it/s]  9%|▉         | 70/780 [00:20<03:27,  3.43it/s]  9%|▉         | 71/780 [00:20<03:26,  3.43it/s]  9%|▉         | 72/780 [00:20<03:26,  3.43it/s]  9%|▉         | 73/780 [00:21<03:26,  3.43it/s]  9%|▉         | 74/780 [00:21<03:25,  3.43it/s] 10%|▉         | 75/780 [00:21<03:25,  3.43it/s] 10%|▉         | 76/780 [00:22<03:25,  3.43it/s] 10%|▉         | 77/780 [00:22<03:24,  3.43it/s] 10%|█         | 78/780 [00:22<03:24,  3.43it/s] 10%|█         | 79/780 [00:23<03:24,  3.43it/s] 10%|█         | 80/780 [00:23<03:24,  3.43it/s] 10%|█         | 81/780 [00:23<03:23,  3.43it/s] 11%|█         | 82/780 [00:23<03:23,  3.43it/s] 11%|█         | 83/780 [00:24<03:23,  3.43it/s] 11%|█         | 84/780 [00:24<03:22,  3.43it/s] 11%|█         | 85/780 [00:24<03:22,  3.43it/s] 11%|█         | 86/780 [00:25<03:22,  3.43it/s] 11%|█         | 87/780 [00:25<03:22,  3.43it/s] 11%|█▏        | 88/780 [00:25<03:21,  3.43it/s] 11%|█▏        | 89/780 [00:25<03:21,  3.43it/s] 12%|█▏        | 90/780 [00:26<03:21,  3.43it/s] 12%|█▏        | 91/780 [00:26<03:20,  3.43it/s] 12%|█▏        | 92/780 [00:26<03:20,  3.43it/s] 12%|█▏        | 93/780 [00:27<03:20,  3.43it/s] 12%|█▏        | 94/780 [00:27<03:19,  3.43it/s] 12%|█▏        | 95/780 [00:27<03:19,  3.43it/s] 12%|█▏        | 96/780 [00:27<03:19,  3.44it/s] 12%|█▏        | 97/780 [00:28<03:19,  3.43it/s] 13%|█▎        | 98/780 [00:28<03:18,  3.43it/s] 13%|█▎        | 99/780 [00:28<03:18,  3.43it/s] 13%|█▎        | 100/780 [00:29<03:18,  3.43it/s] 13%|█▎        | 101/780 [00:29<03:17,  3.43it/s] 13%|█▎        | 102/780 [00:29<03:17,  3.43it/s] 13%|█▎        | 103/780 [00:30<03:17,  3.43it/s] 13%|█▎        | 104/780 [00:30<03:17,  3.43it/s] 13%|█▎        | 105/780 [00:30<03:16,  3.43it/s] 14%|█▎        | 106/780 [00:30<03:16,  3.43it/s] 14%|█▎        | 107/780 [00:31<03:16,  3.43it/s] 14%|█▍        | 108/780 [00:31<03:15,  3.43it/s] 14%|█▍        | 109/780 [00:31<03:15,  3.43it/s] 14%|█▍        | 110/780 [00:32<03:15,  3.43it/s] 14%|█▍        | 111/780 [00:32<03:15,  3.43it/s] 14%|█▍        | 112/780 [00:32<03:14,  3.43it/s] 14%|█▍        | 113/780 [00:32<03:14,  3.43it/s] 15%|█▍        | 114/780 [00:33<03:14,  3.43it/s] 15%|█▍        | 115/780 [00:33<03:13,  3.43it/s] 15%|█▍        | 116/780 [00:33<03:13,  3.43it/s] 15%|█▌        | 117/780 [00:34<03:13,  3.43it/s] 15%|█▌        | 118/780 [00:34<03:13,  3.43it/s] 15%|█▌        | 119/780 [00:34<03:12,  3.43it/s] 15%|█▌        | 120/780 [00:34<03:12,  3.42it/s] 16%|█▌        | 121/780 [00:35<03:12,  3.42it/s] 16%|█▌        | 122/780 [00:35<03:12,  3.42it/s] 16%|█▌        | 123/780 [00:35<03:12,  3.42it/s] 16%|█▌        | 124/780 [00:36<03:11,  3.42it/s] 16%|█▌        | 125/780 [00:36<03:11,  3.43it/s] 16%|█▌        | 126/780 [00:36<03:11,  3.42it/s] 16%|█▋        | 127/780 [00:37<03:10,  3.43it/s] 16%|█▋        | 128/780 [00:37<03:10,  3.43it/s] 17%|█▋        | 129/780 [00:37<03:10,  3.42it/s] 17%|█▋        | 130/780 [00:37<03:10,  3.42it/s] 17%|█▋        | 131/780 [00:38<03:09,  3.42it/s] 17%|█▋        | 132/780 [00:38<03:09,  3.42it/s] 17%|█▋        | 133/780 [00:38<03:08,  3.43it/s] 17%|█▋        | 134/780 [00:39<03:08,  3.42it/s] 17%|█▋        | 135/780 [00:39<03:08,  3.43it/s] 17%|█▋        | 136/780 [00:39<03:07,  3.43it/s] 18%|█▊        | 137/780 [00:39<03:07,  3.43it/s] 18%|█▊        | 138/780 [00:40<03:07,  3.43it/s] 18%|█▊        | 139/780 [00:40<03:06,  3.43it/s] 18%|█▊        | 140/780 [00:40<03:06,  3.43it/s] 18%|█▊        | 141/780 [00:41<03:06,  3.43it/s] 18%|█▊        | 142/780 [00:41<03:06,  3.43it/s] 18%|█▊        | 143/780 [00:41<03:05,  3.43it/s] 18%|█▊        | 144/780 [00:41<03:05,  3.42it/s] 19%|█▊        | 145/780 [00:42<03:05,  3.42it/s] 19%|█▊        | 146/780 [00:42<03:05,  3.42it/s] 19%|█▉        | 147/780 [00:42<03:04,  3.43it/s] 19%|█▉        | 148/780 [00:43<03:04,  3.42it/s] 19%|█▉        | 149/780 [00:43<03:04,  3.42it/s] 19%|█▉        | 150/780 [00:43<03:03,  3.43it/s] 19%|█▉        | 151/780 [00:44<03:03,  3.43it/s] 19%|█▉        | 152/780 [00:44<03:03,  3.43it/s] 20%|█▉        | 153/780 [00:44<03:02,  3.43it/s] 20%|█▉        | 154/780 [00:44<03:02,  3.43it/s] 20%|█▉        | 155/780 [00:45<03:02,  3.43it/s] 20%|██        | 156/780 [00:45<03:02,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 12:48:17,808 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:48:17,808 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 12:48:17,808 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.82it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.32it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.44it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.82it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.40it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.11it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.89it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.43it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.41it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.35it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.49it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.48it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.50it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.53it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.60it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.45it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.29it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.23it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.30it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.33it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.34it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.46it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.44it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.50it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.43it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.37it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.24it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.19it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.33it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.27it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.35it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.06it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.61it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.58it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.50it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.36it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.26it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.33it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.43it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.37it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.31it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.40it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.52it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.45it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.38it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 44.50it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.79it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.04it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.15it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.15it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.35it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.43it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.44it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.38it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.18it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.23it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.29it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.40it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.31it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.37it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.45it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.50it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.39it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.38it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.22it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.30it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.38it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.35it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.30it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.40it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.41it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.42it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.30it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.29it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.27it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.31it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.36it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.37it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.39it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.35it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.36it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.34it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.22it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.21it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.27it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.28it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.39it/s][A                                                 
                                                 [A 20%|██        | 156/780 [00:55<03:02,  3.43it/s]
100%|██████████| 438/438 [00:09<00:00, 46.39it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:48:27,292 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 12:48:27,315 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:48:29,726 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:48:29,747 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:48:29,757 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:03<57:05,  5.50s/it] 20%|██        | 158/780 [01:03<40:48,  3.94s/it] 20%|██        | 159/780 [01:03<29:25,  2.84s/it] 21%|██        | 160/780 [01:04<21:27,  2.08s/it] 21%|██        | 161/780 [01:04<15:54,  1.54s/it] 21%|██        | 162/780 [01:04<12:00,  1.17s/it] 21%|██        | 163/780 [01:04<09:17,  1.11it/s] 21%|██        | 164/780 [01:05<07:23,  1.39it/s] 21%|██        | 165/780 [01:05<06:03,  1.69it/s] 21%|██▏       | 166/780 [01:05<05:08,  1.99it/s] 21%|██▏       | 167/780 [01:06<04:29,  2.28it/s] 22%|██▏       | 168/780 [01:06<04:01,  2.53it/s] 22%|██▏       | 169/780 [01:06<03:42,  2.74it/s] 22%|██▏       | 170/780 [01:06<03:29,  2.91it/s] 22%|██▏       | 171/780 [01:07<03:19,  3.05it/s] 22%|██▏       | 172/780 [01:07<03:12,  3.16it/s] 22%|██▏       | 173/780 [01:07<03:07,  3.23it/s] 22%|██▏       | 174/780 [01:08<03:04,  3.29it/s] 22%|██▏       | 175/780 [01:08<03:01,  3.33it/s] 23%|██▎       | 176/780 [01:08<02:59,  3.36it/s] 23%|██▎       | 177/780 [01:08<02:58,  3.38it/s] 23%|██▎       | 178/780 [01:09<02:57,  3.39it/s] 23%|██▎       | 179/780 [01:09<02:56,  3.40it/s] 23%|██▎       | 180/780 [01:09<02:56,  3.40it/s] 23%|██▎       | 181/780 [01:10<02:55,  3.41it/s] 23%|██▎       | 182/780 [01:10<02:55,  3.41it/s] 23%|██▎       | 183/780 [01:10<02:54,  3.42it/s] 24%|██▎       | 184/780 [01:11<02:54,  3.42it/s] 24%|██▎       | 185/780 [01:11<02:53,  3.43it/s] 24%|██▍       | 186/780 [01:11<02:53,  3.42it/s] 24%|██▍       | 187/780 [01:11<02:53,  3.42it/s] 24%|██▍       | 188/780 [01:12<02:52,  3.42it/s] 24%|██▍       | 189/780 [01:12<02:52,  3.42it/s] 24%|██▍       | 190/780 [01:12<02:52,  3.43it/s] 24%|██▍       | 191/780 [01:13<02:53,  3.39it/s] 25%|██▍       | 192/780 [01:13<02:52,  3.40it/s] 25%|██▍       | 193/780 [01:13<02:52,  3.40it/s] 25%|██▍       | 194/780 [01:13<02:51,  3.41it/s] 25%|██▌       | 195/780 [01:14<02:51,  3.41it/s] 25%|██▌       | 196/780 [01:14<02:50,  3.42it/s] 25%|██▌       | 197/780 [01:14<02:50,  3.43it/s] 25%|██▌       | 198/780 [01:15<02:50,  3.42it/s] 26%|██▌       | 199/780 [01:15<02:49,  3.43it/s] 26%|██▌       | 200/780 [01:15<02:49,  3.43it/s] 26%|██▌       | 201/780 [01:15<02:49,  3.43it/s] 26%|██▌       | 202/780 [01:16<02:49,  3.42it/s] 26%|██▌       | 203/780 [01:16<02:48,  3.42it/s] 26%|██▌       | 204/780 [01:16<02:48,  3.42it/s] 26%|██▋       | 205/780 [01:17<02:48,  3.42it/s] 26%|██▋       | 206/780 [01:17<02:48,  3.42it/s] 27%|██▋       | 207/780 [01:17<02:47,  3.42it/s] 27%|██▋       | 208/780 [01:18<02:47,  3.42it/s] 27%|██▋       | 209/780 [01:18<02:46,  3.42it/s] 27%|██▋       | 210/780 [01:18<02:46,  3.42it/s] 27%|██▋       | 211/780 [01:18<02:46,  3.43it/s] 27%|██▋       | 212/780 [01:19<02:45,  3.42it/s] 27%|██▋       | 213/780 [01:19<02:45,  3.42it/s] 27%|██▋       | 214/780 [01:19<02:45,  3.42it/s] 28%|██▊       | 215/780 [01:20<02:45,  3.42it/s] 28%|██▊       | 216/780 [01:20<02:44,  3.42it/s] 28%|██▊       | 217/780 [01:20<02:44,  3.42it/s] 28%|██▊       | 218/780 [01:20<02:44,  3.42it/s] 28%|██▊       | 219/780 [01:21<02:43,  3.42it/s] 28%|██▊       | 220/780 [01:21<02:43,  3.42it/s] 28%|██▊       | 221/780 [01:21<02:43,  3.42it/s] 28%|██▊       | 222/780 [01:22<02:42,  3.42it/s] 29%|██▊       | 223/780 [01:22<02:42,  3.42it/s] 29%|██▊       | 224/780 [01:22<02:42,  3.41it/s] 29%|██▉       | 225/780 [01:23<02:42,  3.42it/s] 29%|██▉       | 226/780 [01:23<02:41,  3.42it/s] 29%|██▉       | 227/780 [01:23<02:41,  3.42it/s] 29%|██▉       | 228/780 [01:23<02:41,  3.42it/s] 29%|██▉       | 229/780 [01:24<02:40,  3.43it/s] 29%|██▉       | 230/780 [01:24<02:40,  3.42it/s] 30%|██▉       | 231/780 [01:24<02:40,  3.43it/s] 30%|██▉       | 232/780 [01:25<02:40,  3.42it/s] 30%|██▉       | 233/780 [01:25<02:39,  3.43it/s] 30%|███       | 234/780 [01:25<02:39,  3.42it/s] 30%|███       | 235/780 [01:25<02:39,  3.42it/s] 30%|███       | 236/780 [01:26<02:38,  3.42it/s] 30%|███       | 237/780 [01:26<02:38,  3.42it/s] 31%|███       | 238/780 [01:26<02:38,  3.42it/s] 31%|███       | 239/780 [01:27<02:38,  3.42it/s] 31%|███       | 240/780 [01:27<02:37,  3.42it/s] 31%|███       | 241/780 [01:27<02:37,  3.42it/s] 31%|███       | 242/780 [01:27<02:37,  3.42it/s] 31%|███       | 243/780 [01:28<02:37,  3.41it/s] 31%|███▏      | 244/780 [01:28<02:37,  3.41it/s] 31%|███▏      | 245/780 [01:28<02:36,  3.42it/s] 32%|███▏      | 246/780 [01:29<02:36,  3.42it/s] 32%|███▏      | 247/780 [01:29<02:35,  3.42it/s] 32%|███▏      | 248/780 [01:29<02:35,  3.42it/s] 32%|███▏      | 249/780 [01:30<02:35,  3.42it/s] 32%|███▏      | 250/780 [01:30<02:34,  3.42it/s] 32%|███▏      | 251/780 [01:30<02:34,  3.42it/s] 32%|███▏      | 252/780 [01:30<02:34,  3.42it/s] 32%|███▏      | 253/780 [01:31<02:33,  3.42it/s] 33%|███▎      | 254/780 [01:31<02:34,  3.41it/s] 33%|███▎      | 255/780 [01:31<02:33,  3.42it/s] 33%|███▎      | 256/780 [01:32<02:33,  3.42it/s] 33%|███▎      | 257/780 [01:32<02:32,  3.42it/s] 33%|███▎      | 258/780 [01:32<02:32,  3.42it/s] 33%|███▎      | 259/780 [01:32<02:32,  3.42it/s] 33%|███▎      | 260/780 [01:33<02:31,  3.42it/s] 33%|███▎      | 261/780 [01:33<02:31,  3.42it/s] 34%|███▎      | 262/780 [01:33<02:31,  3.42it/s] 34%|███▎      | 263/780 [01:34<02:31,  3.42it/s] 34%|███▍      | 264/780 [01:34<02:31,  3.42it/s] 34%|███▍      | 265/780 [01:34<02:31,  3.40it/s] 34%|███▍      | 266/780 [01:35<02:30,  3.41it/s] 34%|███▍      | 267/780 [01:35<02:30,  3.41it/s] 34%|███▍      | 268/780 [01:35<02:29,  3.41it/s] 34%|███▍      | 269/780 [01:35<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:36<02:29,  3.42it/s] 35%|███▍      | 271/780 [01:36<02:29,  3.41it/s] 35%|███▍      | 272/780 [01:36<02:28,  3.41it/s] 35%|███▌      | 273/780 [01:37<02:28,  3.41it/s] 35%|███▌      | 274/780 [01:37<02:27,  3.42it/s] 35%|███▌      | 275/780 [01:37<02:27,  3.42it/s] 35%|███▌      | 276/780 [01:37<02:27,  3.41it/s] 36%|███▌      | 277/780 [01:38<02:27,  3.41it/s] 36%|███▌      | 278/780 [01:38<02:26,  3.42it/s] 36%|███▌      | 279/780 [01:38<02:26,  3.41it/s] 36%|███▌      | 280/780 [01:39<02:26,  3.41it/s] 36%|███▌      | 281/780 [01:39<02:25,  3.42it/s] 36%|███▌      | 282/780 [01:39<02:25,  3.41it/s] 36%|███▋      | 283/780 [01:39<02:25,  3.42it/s] 36%|███▋      | 284/780 [01:40<02:25,  3.42it/s] 37%|███▋      | 285/780 [01:40<02:24,  3.42it/s] 37%|███▋      | 286/780 [01:40<02:24,  3.42it/s] 37%|███▋      | 287/780 [01:41<02:24,  3.41it/s] 37%|███▋      | 288/780 [01:41<02:23,  3.42it/s] 37%|███▋      | 289/780 [01:41<02:23,  3.42it/s] 37%|███▋      | 290/780 [01:42<02:23,  3.42it/s] 37%|███▋      | 291/780 [01:42<02:22,  3.42it/s] 37%|███▋      | 292/780 [01:42<02:22,  3.42it/s] 38%|███▊      | 293/780 [01:42<02:22,  3.42it/s] 38%|███▊      | 294/780 [01:43<02:22,  3.42it/s] 38%|███▊      | 295/780 [01:43<02:21,  3.42it/s] 38%|███▊      | 296/780 [01:43<02:21,  3.42it/s] 38%|███▊      | 297/780 [01:44<02:21,  3.42it/s] 38%|███▊      | 298/780 [01:44<02:21,  3.40it/s] 38%|███▊      | 299/780 [01:44<02:21,  3.41it/s] 38%|███▊      | 300/780 [01:44<02:20,  3.41it/s] 39%|███▊      | 301/780 [01:45<02:20,  3.41it/s] 39%|███▊      | 302/780 [01:45<02:19,  3.42it/s] 39%|███▉      | 303/780 [01:45<02:19,  3.42it/s] 39%|███▉      | 304/780 [01:46<02:19,  3.42it/s] 39%|███▉      | 305/780 [01:46<02:18,  3.42it/s] 39%|███▉      | 306/780 [01:46<02:18,  3.42it/s] 39%|███▉      | 307/780 [01:47<02:17,  3.43it/s] 39%|███▉      | 308/780 [01:47<02:17,  3.42it/s] 40%|███▉      | 309/780 [01:47<02:17,  3.42it/s] 40%|███▉      | 310/780 [01:47<02:17,  3.42it/s] 40%|███▉      | 311/780 [01:48<02:17,  3.42it/s] 40%|████      | 312/780 [01:48<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 12:49:20,764 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:49:20,765 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 12:49:20,765 >>   Batch size = 8
{'eval_loss': 1.139360785484314, 'eval_runtime': 9.4539, 'eval_samples_per_second': 369.899, 'eval_steps_per_second': 46.33, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.92it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.14it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.37it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.71it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.26it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.99it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.70it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.35it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.38it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.35it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.40it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.41it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.50it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.45it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.51it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.37it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.23it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.27it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 44.21it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 44.98it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 45.49it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 45.87it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 45.99it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.21it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.23it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.23it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.08it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.00it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.98it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.12it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.26it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.32it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.35it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.31it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.12it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.17it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.00it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.10it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.16it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.19it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.29it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.30it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.36it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.24it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.21it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.10it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.14it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.20it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.23it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.22it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.29it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.25it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.28it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.16it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 46.11it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.16it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.16it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.27it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.19it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.33it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.31it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.30it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.22it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.15it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.04it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.01it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.14it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.15it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.17it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.24it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.13it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.17it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.19it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.17it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.12it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.12it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.26it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.27it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.20it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.16it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.14it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.13it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.12it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.15it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.12it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.13it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.33it/s][A                                                 
                                                 [A 40%|████      | 312/780 [01:58<02:16,  3.42it/s]
100%|██████████| 438/438 [00:09<00:00, 46.33it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:49:30,291 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 12:49:30,307 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:49:32,856 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:49:32,878 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:49:32,893 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:05<42:11,  5.42s/it] 40%|████      | 314/780 [02:06<30:09,  3.88s/it] 40%|████      | 315/780 [02:06<21:44,  2.81s/it] 41%|████      | 316/780 [02:06<15:51,  2.05s/it] 41%|████      | 317/780 [02:07<11:45,  1.52s/it] 41%|████      | 318/780 [02:07<08:53,  1.15s/it] 41%|████      | 319/780 [02:07<06:53,  1.12it/s] 41%|████      | 320/780 [02:07<05:28,  1.40it/s] 41%|████      | 321/780 [02:08<04:29,  1.70it/s] 41%|████▏     | 322/780 [02:08<03:48,  2.00it/s] 41%|████▏     | 323/780 [02:08<03:19,  2.29it/s] 42%|████▏     | 324/780 [02:09<02:59,  2.54it/s] 42%|████▏     | 325/780 [02:09<02:46,  2.74it/s] 42%|████▏     | 326/780 [02:09<02:35,  2.92it/s] 42%|████▏     | 327/780 [02:09<02:28,  3.05it/s] 42%|████▏     | 328/780 [02:10<02:23,  3.16it/s] 42%|████▏     | 329/780 [02:10<02:19,  3.23it/s] 42%|████▏     | 330/780 [02:10<02:16,  3.29it/s] 42%|████▏     | 331/780 [02:11<02:14,  3.33it/s] 43%|████▎     | 332/780 [02:11<02:13,  3.36it/s] 43%|████▎     | 333/780 [02:11<02:12,  3.38it/s] 43%|████▎     | 334/780 [02:11<02:11,  3.39it/s] 43%|████▎     | 335/780 [02:12<02:10,  3.40it/s] 43%|████▎     | 336/780 [02:12<02:11,  3.37it/s] 43%|████▎     | 337/780 [02:12<02:10,  3.39it/s] 43%|████▎     | 338/780 [02:13<02:10,  3.40it/s] 43%|████▎     | 339/780 [02:13<02:09,  3.41it/s] 44%|████▎     | 340/780 [02:13<02:09,  3.40it/s] 44%|████▎     | 341/780 [02:14<02:08,  3.41it/s] 44%|████▍     | 342/780 [02:14<02:08,  3.42it/s] 44%|████▍     | 343/780 [02:14<02:07,  3.42it/s] 44%|████▍     | 344/780 [02:14<02:07,  3.42it/s] 44%|████▍     | 345/780 [02:15<02:07,  3.42it/s] 44%|████▍     | 346/780 [02:15<02:06,  3.42it/s] 44%|████▍     | 347/780 [02:15<02:07,  3.40it/s] 45%|████▍     | 348/780 [02:16<02:06,  3.41it/s] 45%|████▍     | 349/780 [02:16<02:06,  3.41it/s] 45%|████▍     | 350/780 [02:16<02:05,  3.42it/s] 45%|████▌     | 351/780 [02:16<02:05,  3.42it/s] 45%|████▌     | 352/780 [02:17<02:04,  3.43it/s] 45%|████▌     | 353/780 [02:17<02:04,  3.43it/s] 45%|████▌     | 354/780 [02:17<02:04,  3.43it/s] 46%|████▌     | 355/780 [02:18<02:04,  3.43it/s] 46%|████▌     | 356/780 [02:18<02:03,  3.43it/s] 46%|████▌     | 357/780 [02:18<02:03,  3.43it/s] 46%|████▌     | 358/780 [02:19<02:03,  3.41it/s] 46%|████▌     | 359/780 [02:19<02:03,  3.42it/s] 46%|████▌     | 360/780 [02:19<02:02,  3.42it/s] 46%|████▋     | 361/780 [02:19<02:02,  3.42it/s] 46%|████▋     | 362/780 [02:20<02:02,  3.42it/s] 47%|████▋     | 363/780 [02:20<02:01,  3.42it/s] 47%|████▋     | 364/780 [02:20<02:01,  3.42it/s] 47%|████▋     | 365/780 [02:21<02:01,  3.42it/s] 47%|████▋     | 366/780 [02:21<02:00,  3.43it/s] 47%|████▋     | 367/780 [02:21<02:00,  3.43it/s] 47%|████▋     | 368/780 [02:21<02:00,  3.43it/s] 47%|████▋     | 369/780 [02:22<02:00,  3.42it/s] 47%|████▋     | 370/780 [02:22<01:59,  3.42it/s] 48%|████▊     | 371/780 [02:22<01:59,  3.43it/s] 48%|████▊     | 372/780 [02:23<01:59,  3.42it/s] 48%|████▊     | 373/780 [02:23<01:58,  3.43it/s] 48%|████▊     | 374/780 [02:23<01:58,  3.43it/s] 48%|████▊     | 375/780 [02:23<01:58,  3.43it/s] 48%|████▊     | 376/780 [02:24<01:57,  3.43it/s] 48%|████▊     | 377/780 [02:24<01:57,  3.42it/s] 48%|████▊     | 378/780 [02:24<01:57,  3.43it/s] 49%|████▊     | 379/780 [02:25<01:57,  3.42it/s] 49%|████▊     | 380/780 [02:25<01:57,  3.41it/s] 49%|████▉     | 381/780 [02:25<01:56,  3.42it/s] 49%|████▉     | 382/780 [02:26<01:56,  3.42it/s] 49%|████▉     | 383/780 [02:26<01:56,  3.42it/s] 49%|████▉     | 384/780 [02:26<01:55,  3.42it/s] 49%|████▉     | 385/780 [02:26<01:55,  3.42it/s] 49%|████▉     | 386/780 [02:27<01:55,  3.43it/s] 50%|████▉     | 387/780 [02:27<01:54,  3.43it/s] 50%|████▉     | 388/780 [02:27<01:54,  3.43it/s] 50%|████▉     | 389/780 [02:28<01:54,  3.42it/s] 50%|█████     | 390/780 [02:28<01:53,  3.42it/s] 50%|█████     | 391/780 [02:28<01:53,  3.43it/s] 50%|█████     | 392/780 [02:28<01:53,  3.43it/s] 50%|█████     | 393/780 [02:29<01:53,  3.42it/s] 51%|█████     | 394/780 [02:29<01:52,  3.42it/s] 51%|█████     | 395/780 [02:29<01:53,  3.41it/s] 51%|█████     | 396/780 [02:30<01:52,  3.41it/s] 51%|█████     | 397/780 [02:30<01:52,  3.42it/s] 51%|█████     | 398/780 [02:30<01:51,  3.42it/s] 51%|█████     | 399/780 [02:30<01:51,  3.42it/s] 51%|█████▏    | 400/780 [02:31<01:51,  3.42it/s] 51%|█████▏    | 401/780 [02:31<01:50,  3.42it/s] 52%|█████▏    | 402/780 [02:31<01:50,  3.42it/s] 52%|█████▏    | 403/780 [02:32<01:50,  3.42it/s] 52%|█████▏    | 404/780 [02:32<01:49,  3.42it/s] 52%|█████▏    | 405/780 [02:32<01:49,  3.43it/s] 52%|█████▏    | 406/780 [02:33<01:49,  3.41it/s] 52%|█████▏    | 407/780 [02:33<01:49,  3.41it/s] 52%|█████▏    | 408/780 [02:33<01:48,  3.41it/s] 52%|█████▏    | 409/780 [02:33<01:48,  3.42it/s] 53%|█████▎    | 410/780 [02:34<01:48,  3.42it/s] 53%|█████▎    | 411/780 [02:34<01:47,  3.42it/s] 53%|█████▎    | 412/780 [02:34<01:47,  3.42it/s] 53%|█████▎    | 413/780 [02:35<01:47,  3.42it/s] 53%|█████▎    | 414/780 [02:35<01:46,  3.42it/s] 53%|█████▎    | 415/780 [02:35<01:46,  3.42it/s] 53%|█████▎    | 416/780 [02:35<01:46,  3.42it/s] 53%|█████▎    | 417/780 [02:36<01:46,  3.41it/s] 54%|█████▎    | 418/780 [02:36<01:45,  3.42it/s] 54%|█████▎    | 419/780 [02:36<01:45,  3.42it/s] 54%|█████▍    | 420/780 [02:37<01:45,  3.42it/s] 54%|█████▍    | 421/780 [02:37<01:44,  3.42it/s] 54%|█████▍    | 422/780 [02:37<01:44,  3.42it/s] 54%|█████▍    | 423/780 [02:38<01:44,  3.42it/s] 54%|█████▍    | 424/780 [02:38<01:44,  3.42it/s] 54%|█████▍    | 425/780 [02:38<01:43,  3.42it/s] 55%|█████▍    | 426/780 [02:38<01:43,  3.42it/s] 55%|█████▍    | 427/780 [02:39<01:43,  3.42it/s] 55%|█████▍    | 428/780 [02:39<01:43,  3.41it/s] 55%|█████▌    | 429/780 [02:39<01:42,  3.41it/s] 55%|█████▌    | 430/780 [02:40<01:42,  3.42it/s] 55%|█████▌    | 431/780 [02:40<01:42,  3.42it/s] 55%|█████▌    | 432/780 [02:40<01:41,  3.42it/s] 56%|█████▌    | 433/780 [02:40<01:41,  3.42it/s] 56%|█████▌    | 434/780 [02:41<01:40,  3.43it/s] 56%|█████▌    | 435/780 [02:41<01:40,  3.42it/s] 56%|█████▌    | 436/780 [02:41<01:40,  3.43it/s] 56%|█████▌    | 437/780 [02:42<01:40,  3.43it/s] 56%|█████▌    | 438/780 [02:42<01:39,  3.42it/s] 56%|█████▋    | 439/780 [02:42<01:40,  3.41it/s] 56%|█████▋    | 440/780 [02:42<01:39,  3.41it/s] 57%|█████▋    | 441/780 [02:43<01:39,  3.42it/s] 57%|█████▋    | 442/780 [02:43<01:38,  3.42it/s] 57%|█████▋    | 443/780 [02:43<01:38,  3.42it/s] 57%|█████▋    | 444/780 [02:44<01:38,  3.42it/s] 57%|█████▋    | 445/780 [02:44<01:37,  3.43it/s] 57%|█████▋    | 446/780 [02:44<01:37,  3.42it/s] 57%|█████▋    | 447/780 [02:45<01:37,  3.43it/s] 57%|█████▋    | 448/780 [02:45<01:36,  3.43it/s] 58%|█████▊    | 449/780 [02:45<01:36,  3.42it/s] 58%|█████▊    | 450/780 [02:45<01:36,  3.41it/s] 58%|█████▊    | 451/780 [02:46<01:36,  3.41it/s] 58%|█████▊    | 452/780 [02:46<01:35,  3.42it/s] 58%|█████▊    | 453/780 [02:46<01:35,  3.42it/s] 58%|█████▊    | 454/780 [02:47<01:35,  3.42it/s] 58%|█████▊    | 455/780 [02:47<01:34,  3.42it/s] 58%|█████▊    | 456/780 [02:47<01:34,  3.42it/s] 59%|█████▊    | 457/780 [02:47<01:34,  3.42it/s] 59%|█████▊    | 458/780 [02:48<01:34,  3.42it/s] 59%|█████▉    | 459/780 [02:48<01:33,  3.43it/s] 59%|█████▉    | 460/780 [02:48<01:33,  3.42it/s] 59%|█████▉    | 461/780 [02:49<01:33,  3.40it/s] 59%|█████▉    | 462/780 [02:49<01:33,  3.41it/s] 59%|█████▉    | 463/780 [02:49<01:32,  3.42it/s] 59%|█████▉    | 464/780 [02:50<01:32,  3.42it/s] 60%|█████▉    | 465/780 [02:50<01:32,  3.42it/s] 60%|█████▉    | 466/780 [02:50<01:31,  3.42it/s] 60%|█████▉    | 467/780 [02:50<01:36,  3.25it/s] 60%|██████    | 468/780 [02:51<01:34,  3.30it/s][INFO|trainer.py:2140] 2023-08-29 12:50:23,524 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:50:23,524 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 12:50:23,524 >>   Batch size = 8
{'eval_loss': 1.16681706905365, 'eval_runtime': 9.4993, 'eval_samples_per_second': 368.134, 'eval_steps_per_second': 46.109, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.15it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.28it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.55it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.84it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.36it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.80it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.51it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.15it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.24it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.27it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.39it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.39it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.47it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.44it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.29it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.09it/s][A
 20%|██        | 88/438 [00:01<00:07, 45.99it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.13it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.22it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.25it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.36it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.40it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.35it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.24it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.14it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.05it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 45.99it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.16it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.15it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.28it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.33it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.35it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.31it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.04it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.12it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.09it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.13it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.13it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.21it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.22it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.18it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.26it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.21it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.09it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.14it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.11it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.27it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.28it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.31it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.18it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.20it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.24it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.26it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.20it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.19it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.22it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.31it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.27it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.22it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.22it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.18it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.22it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.04it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.21it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.17it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.30it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.23it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.18it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.14it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.12it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.10it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.01it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 45.97it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.06it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.05it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.17it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.20it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.25it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.22it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.17it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.17it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.08it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.20it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.15it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.23it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.23it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.36it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [03:00<01:34,  3.30it/s]
100%|██████████| 438/438 [00:09<00:00, 46.36it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:50:33,038 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 12:50:33,055 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:50:35,466 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:50:35,483 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:50:35,494 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:08<27:55,  5.39s/it] 60%|██████    | 470/780 [03:08<19:56,  3.86s/it] 60%|██████    | 471/780 [03:09<14:21,  2.79s/it] 61%|██████    | 472/780 [03:09<10:28,  2.04s/it] 61%|██████    | 473/780 [03:09<07:45,  1.52s/it] 61%|██████    | 474/780 [03:09<05:51,  1.15s/it] 61%|██████    | 475/780 [03:10<04:31,  1.12it/s] 61%|██████    | 476/780 [03:10<03:36,  1.41it/s] 61%|██████    | 477/780 [03:10<02:57,  1.71it/s] 61%|██████▏   | 478/780 [03:11<02:30,  2.01it/s] 61%|██████▏   | 479/780 [03:11<02:11,  2.30it/s] 62%|██████▏   | 480/780 [03:11<01:57,  2.55it/s] 62%|██████▏   | 481/780 [03:11<01:48,  2.75it/s] 62%|██████▏   | 482/780 [03:12<01:41,  2.92it/s] 62%|██████▏   | 483/780 [03:12<01:37,  3.06it/s] 62%|██████▏   | 484/780 [03:12<01:33,  3.16it/s] 62%|██████▏   | 485/780 [03:13<01:31,  3.24it/s] 62%|██████▏   | 486/780 [03:13<01:29,  3.29it/s] 62%|██████▏   | 487/780 [03:13<01:27,  3.33it/s] 63%|██████▎   | 488/780 [03:14<01:27,  3.35it/s] 63%|██████▎   | 489/780 [03:14<01:26,  3.37it/s] 63%|██████▎   | 490/780 [03:14<01:25,  3.39it/s] 63%|██████▎   | 491/780 [03:14<01:25,  3.40it/s] 63%|██████▎   | 492/780 [03:15<01:24,  3.39it/s] 63%|██████▎   | 493/780 [03:15<01:24,  3.40it/s] 63%|██████▎   | 494/780 [03:15<01:23,  3.41it/s] 63%|██████▎   | 495/780 [03:16<01:23,  3.41it/s] 64%|██████▎   | 496/780 [03:16<01:23,  3.42it/s] 64%|██████▎   | 497/780 [03:16<01:22,  3.42it/s] 64%|██████▍   | 498/780 [03:16<01:22,  3.42it/s] 64%|██████▍   | 499/780 [03:17<01:22,  3.42it/s] 64%|██████▍   | 500/780 [03:17<01:21,  3.42it/s]                                                  64%|██████▍   | 500/780 [03:17<01:21,  3.42it/s] 64%|██████▍   | 501/780 [03:17<01:21,  3.42it/s] 64%|██████▍   | 502/780 [03:18<01:21,  3.43it/s] 64%|██████▍   | 503/780 [03:18<01:21,  3.42it/s] 65%|██████▍   | 504/780 [03:18<01:20,  3.42it/s] 65%|██████▍   | 505/780 [03:18<01:20,  3.42it/s] 65%|██████▍   | 506/780 [03:19<01:20,  3.41it/s] 65%|██████▌   | 507/780 [03:19<01:19,  3.42it/s] 65%|██████▌   | 508/780 [03:19<01:19,  3.42it/s] 65%|██████▌   | 509/780 [03:20<01:19,  3.42it/s] 65%|██████▌   | 510/780 [03:20<01:18,  3.42it/s] 66%|██████▌   | 511/780 [03:20<01:18,  3.42it/s] 66%|██████▌   | 512/780 [03:21<01:18,  3.42it/s] 66%|██████▌   | 513/780 [03:21<01:17,  3.42it/s] 66%|██████▌   | 514/780 [03:21<01:17,  3.42it/s] 66%|██████▌   | 515/780 [03:21<01:17,  3.42it/s] 66%|██████▌   | 516/780 [03:22<01:17,  3.42it/s] 66%|██████▋   | 517/780 [03:22<01:16,  3.42it/s] 66%|██████▋   | 518/780 [03:22<01:16,  3.42it/s] 67%|██████▋   | 519/780 [03:23<01:16,  3.42it/s] 67%|██████▋   | 520/780 [03:23<01:15,  3.42it/s] 67%|██████▋   | 521/780 [03:23<01:15,  3.42it/s] 67%|██████▋   | 522/780 [03:23<01:15,  3.42it/s] 67%|██████▋   | 523/780 [03:24<01:15,  3.42it/s] 67%|██████▋   | 524/780 [03:24<01:14,  3.42it/s] 67%|██████▋   | 525/780 [03:24<01:14,  3.41it/s] 67%|██████▋   | 526/780 [03:25<01:14,  3.41it/s] 68%|██████▊   | 527/780 [03:25<01:14,  3.42it/s] 68%|██████▊   | 528/780 [03:25<01:13,  3.42it/s] 68%|██████▊   | 529/780 [03:26<01:13,  3.42it/s] 68%|██████▊   | 530/780 [03:26<01:13,  3.42it/s] 68%|██████▊   | 531/780 [03:26<01:12,  3.43it/s] 68%|██████▊   | 532/780 [03:26<01:12,  3.43it/s] 68%|██████▊   | 533/780 [03:27<01:12,  3.42it/s] 68%|██████▊   | 534/780 [03:27<01:11,  3.42it/s] 69%|██████▊   | 535/780 [03:27<01:11,  3.42it/s] 69%|██████▊   | 536/780 [03:28<01:11,  3.42it/s] 69%|██████▉   | 537/780 [03:28<01:11,  3.42it/s] 69%|██████▉   | 538/780 [03:28<01:10,  3.42it/s] 69%|██████▉   | 539/780 [03:28<01:10,  3.42it/s] 69%|██████▉   | 540/780 [03:29<01:10,  3.42it/s] 69%|██████▉   | 541/780 [03:29<01:09,  3.42it/s] 69%|██████▉   | 542/780 [03:29<01:09,  3.42it/s] 70%|██████▉   | 543/780 [03:30<01:09,  3.42it/s] 70%|██████▉   | 544/780 [03:30<01:08,  3.42it/s] 70%|██████▉   | 545/780 [03:30<01:08,  3.42it/s] 70%|███████   | 546/780 [03:30<01:08,  3.43it/s] 70%|███████   | 547/780 [03:31<01:08,  3.41it/s] 70%|███████   | 548/780 [03:31<01:07,  3.41it/s] 70%|███████   | 549/780 [03:31<01:07,  3.42it/s] 71%|███████   | 550/780 [03:32<01:07,  3.42it/s] 71%|███████   | 551/780 [03:32<01:06,  3.42it/s] 71%|███████   | 552/780 [03:32<01:06,  3.42it/s] 71%|███████   | 553/780 [03:33<01:06,  3.42it/s] 71%|███████   | 554/780 [03:33<01:06,  3.42it/s] 71%|███████   | 555/780 [03:33<01:05,  3.42it/s] 71%|███████▏  | 556/780 [03:33<01:05,  3.42it/s] 71%|███████▏  | 557/780 [03:34<01:05,  3.42it/s] 72%|███████▏  | 558/780 [03:34<01:05,  3.41it/s] 72%|███████▏  | 559/780 [03:34<01:04,  3.41it/s] 72%|███████▏  | 560/780 [03:35<01:04,  3.42it/s] 72%|███████▏  | 561/780 [03:35<01:04,  3.42it/s] 72%|███████▏  | 562/780 [03:35<01:03,  3.42it/s] 72%|███████▏  | 563/780 [03:35<01:03,  3.42it/s] 72%|███████▏  | 564/780 [03:36<01:03,  3.42it/s] 72%|███████▏  | 565/780 [03:36<01:02,  3.42it/s] 73%|███████▎  | 566/780 [03:36<01:02,  3.42it/s] 73%|███████▎  | 567/780 [03:37<01:02,  3.42it/s] 73%|███████▎  | 568/780 [03:37<01:01,  3.42it/s] 73%|███████▎  | 569/780 [03:37<01:01,  3.41it/s] 73%|███████▎  | 570/780 [03:38<01:01,  3.41it/s] 73%|███████▎  | 571/780 [03:38<01:01,  3.42it/s] 73%|███████▎  | 572/780 [03:38<01:00,  3.42it/s] 73%|███████▎  | 573/780 [03:38<01:00,  3.42it/s] 74%|███████▎  | 574/780 [03:39<01:00,  3.42it/s] 74%|███████▎  | 575/780 [03:39<01:00,  3.41it/s] 74%|███████▍  | 576/780 [03:39<00:59,  3.41it/s] 74%|███████▍  | 577/780 [03:40<00:59,  3.42it/s] 74%|███████▍  | 578/780 [03:40<00:59,  3.42it/s] 74%|███████▍  | 579/780 [03:40<00:58,  3.42it/s] 74%|███████▍  | 580/780 [03:40<00:58,  3.41it/s] 74%|███████▍  | 581/780 [03:41<00:58,  3.41it/s] 75%|███████▍  | 582/780 [03:41<00:58,  3.41it/s] 75%|███████▍  | 583/780 [03:41<00:57,  3.42it/s] 75%|███████▍  | 584/780 [03:42<00:57,  3.42it/s] 75%|███████▌  | 585/780 [03:42<00:57,  3.42it/s] 75%|███████▌  | 586/780 [03:42<00:56,  3.41it/s] 75%|███████▌  | 587/780 [03:42<00:56,  3.42it/s] 75%|███████▌  | 588/780 [03:43<00:56,  3.42it/s] 76%|███████▌  | 589/780 [03:43<00:55,  3.42it/s] 76%|███████▌  | 590/780 [03:43<00:55,  3.42it/s] 76%|███████▌  | 591/780 [03:44<00:55,  3.40it/s] 76%|███████▌  | 592/780 [03:44<00:55,  3.40it/s] 76%|███████▌  | 593/780 [03:44<00:54,  3.41it/s] 76%|███████▌  | 594/780 [03:45<00:54,  3.41it/s] 76%|███████▋  | 595/780 [03:45<00:54,  3.41it/s] 76%|███████▋  | 596/780 [03:45<00:53,  3.42it/s] 77%|███████▋  | 597/780 [03:45<00:53,  3.42it/s] 77%|███████▋  | 598/780 [03:46<00:53,  3.41it/s] 77%|███████▋  | 599/780 [03:46<00:52,  3.42it/s] 77%|███████▋  | 600/780 [03:46<00:52,  3.42it/s] 77%|███████▋  | 601/780 [03:47<00:52,  3.42it/s] 77%|███████▋  | 602/780 [03:47<00:52,  3.41it/s] 77%|███████▋  | 603/780 [03:47<00:51,  3.42it/s] 77%|███████▋  | 604/780 [03:47<00:51,  3.42it/s] 78%|███████▊  | 605/780 [03:48<00:51,  3.42it/s] 78%|███████▊  | 606/780 [03:48<00:50,  3.42it/s] 78%|███████▊  | 607/780 [03:48<00:50,  3.42it/s] 78%|███████▊  | 608/780 [03:49<00:50,  3.39it/s] 78%|███████▊  | 609/780 [03:49<00:50,  3.40it/s] 78%|███████▊  | 610/780 [03:49<00:49,  3.40it/s] 78%|███████▊  | 611/780 [03:50<00:49,  3.41it/s] 78%|███████▊  | 612/780 [03:50<00:49,  3.41it/s] 79%|███████▊  | 613/780 [03:50<00:49,  3.40it/s] 79%|███████▊  | 614/780 [03:50<00:50,  3.31it/s] 79%|███████▉  | 615/780 [03:51<00:49,  3.34it/s] 79%|███████▉  | 616/780 [03:51<00:48,  3.37it/s] 79%|███████▉  | 617/780 [03:51<00:48,  3.38it/s] 79%|███████▉  | 618/780 [03:52<00:47,  3.39it/s] 79%|███████▉  | 619/780 [03:52<00:47,  3.40it/s] 79%|███████▉  | 620/780 [03:52<00:46,  3.40it/s] 80%|███████▉  | 621/780 [03:52<00:46,  3.41it/s] 80%|███████▉  | 622/780 [03:53<00:46,  3.41it/s] 80%|███████▉  | 623/780 [03:53<00:45,  3.41it/s] 80%|████████  | 624/780 [03:53<00:45,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 12:51:26,153 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:51:26,153 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 12:51:26,153 >>   Batch size = 8
{'eval_loss': 1.183401346206665, 'eval_runtime': 9.4902, 'eval_samples_per_second': 368.487, 'eval_steps_per_second': 46.153, 'epoch': 3.0}
{'loss': 0.3777, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.10it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.16it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.47it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.71it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.33it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.80it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.45it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.08it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.20it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.28it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.35it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.39it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.48it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.39it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.31it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.95it/s][A
 20%|██        | 88/438 [00:01<00:07, 45.97it/s][A
 21%|██        | 93/438 [00:01<00:07, 45.98it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.07it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.18it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.23it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.29it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.28it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.34it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.11it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.16it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.08it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.09it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.17it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.26it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.27it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.36it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.27it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.11it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.10it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.17it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.04it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.13it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.21it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.23it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.34it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.18it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.21it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.14it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.05it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.07it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.11it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.13it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.26it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.24it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.23it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.13it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.23it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.13it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.11it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.10it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.06it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.22it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.18it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.24it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.12it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.12it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.10it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.11it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.14it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.94it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.25it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.30it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.24it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.30it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.13it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.17it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.16it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.10it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.08it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.02it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.21it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.22it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.22it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.12it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.16it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.16it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.11it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.12it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.09it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.16it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.38it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [04:03<00:45,  3.40it/s]
100%|██████████| 438/438 [00:09<00:00, 46.38it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:51:35,681 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 12:51:35,717 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:51:38,079 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:51:38,093 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:51:38,098 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:11<14:12,  5.50s/it] 80%|████████  | 626/780 [04:11<10:06,  3.94s/it] 80%|████████  | 627/780 [04:12<07:15,  2.84s/it] 81%|████████  | 628/780 [04:12<05:15,  2.08s/it] 81%|████████  | 629/780 [04:12<03:52,  1.54s/it] 81%|████████  | 630/780 [04:12<02:55,  1.17s/it] 81%|████████  | 631/780 [04:13<02:14,  1.11it/s] 81%|████████  | 632/780 [04:13<01:46,  1.39it/s] 81%|████████  | 633/780 [04:13<01:27,  1.69it/s] 81%|████████▏ | 634/780 [04:14<01:13,  1.99it/s] 81%|████████▏ | 635/780 [04:14<01:03,  2.27it/s] 82%|████████▏ | 636/780 [04:14<00:56,  2.53it/s] 82%|████████▏ | 637/780 [04:15<00:52,  2.74it/s] 82%|████████▏ | 638/780 [04:15<00:48,  2.92it/s] 82%|████████▏ | 639/780 [04:15<00:46,  3.05it/s] 82%|████████▏ | 640/780 [04:15<00:44,  3.16it/s] 82%|████████▏ | 641/780 [04:16<00:42,  3.23it/s] 82%|████████▏ | 642/780 [04:16<00:41,  3.29it/s] 82%|████████▏ | 643/780 [04:16<00:41,  3.33it/s] 83%|████████▎ | 644/780 [04:17<00:40,  3.36it/s] 83%|████████▎ | 645/780 [04:17<00:39,  3.38it/s] 83%|████████▎ | 646/780 [04:17<00:39,  3.39it/s] 83%|████████▎ | 647/780 [04:17<00:39,  3.40it/s] 83%|████████▎ | 648/780 [04:18<00:38,  3.40it/s] 83%|████████▎ | 649/780 [04:18<00:38,  3.41it/s] 83%|████████▎ | 650/780 [04:18<00:38,  3.41it/s] 83%|████████▎ | 651/780 [04:19<00:37,  3.42it/s] 84%|████████▎ | 652/780 [04:19<00:37,  3.42it/s] 84%|████████▎ | 653/780 [04:19<00:37,  3.42it/s] 84%|████████▍ | 654/780 [04:19<00:36,  3.42it/s] 84%|████████▍ | 655/780 [04:20<00:36,  3.42it/s] 84%|████████▍ | 656/780 [04:20<00:36,  3.43it/s] 84%|████████▍ | 657/780 [04:20<00:35,  3.42it/s] 84%|████████▍ | 658/780 [04:21<00:35,  3.43it/s] 84%|████████▍ | 659/780 [04:21<00:35,  3.41it/s] 85%|████████▍ | 660/780 [04:21<00:35,  3.41it/s] 85%|████████▍ | 661/780 [04:22<00:34,  3.42it/s] 85%|████████▍ | 662/780 [04:22<00:34,  3.42it/s] 85%|████████▌ | 663/780 [04:22<00:34,  3.42it/s] 85%|████████▌ | 664/780 [04:22<00:33,  3.42it/s] 85%|████████▌ | 665/780 [04:23<00:33,  3.42it/s] 85%|████████▌ | 666/780 [04:23<00:33,  3.42it/s] 86%|████████▌ | 667/780 [04:23<00:33,  3.42it/s] 86%|████████▌ | 668/780 [04:24<00:32,  3.42it/s] 86%|████████▌ | 669/780 [04:24<00:32,  3.42it/s] 86%|████████▌ | 670/780 [04:24<00:32,  3.42it/s] 86%|████████▌ | 671/780 [04:24<00:31,  3.42it/s] 86%|████████▌ | 672/780 [04:25<00:31,  3.43it/s] 86%|████████▋ | 673/780 [04:25<00:31,  3.42it/s] 86%|████████▋ | 674/780 [04:25<00:30,  3.42it/s] 87%|████████▋ | 675/780 [04:26<00:30,  3.42it/s] 87%|████████▋ | 676/780 [04:26<00:30,  3.42it/s] 87%|████████▋ | 677/780 [04:26<00:30,  3.42it/s] 87%|████████▋ | 678/780 [04:26<00:29,  3.42it/s] 87%|████████▋ | 679/780 [04:27<00:29,  3.42it/s] 87%|████████▋ | 680/780 [04:27<00:29,  3.42it/s] 87%|████████▋ | 681/780 [04:27<00:29,  3.41it/s] 87%|████████▋ | 682/780 [04:28<00:28,  3.41it/s] 88%|████████▊ | 683/780 [04:28<00:28,  3.42it/s] 88%|████████▊ | 684/780 [04:28<00:28,  3.42it/s] 88%|████████▊ | 685/780 [04:29<00:27,  3.42it/s] 88%|████████▊ | 686/780 [04:29<00:27,  3.42it/s] 88%|████████▊ | 687/780 [04:29<00:27,  3.42it/s] 88%|████████▊ | 688/780 [04:29<00:26,  3.42it/s] 88%|████████▊ | 689/780 [04:30<00:26,  3.42it/s] 88%|████████▊ | 690/780 [04:30<00:26,  3.42it/s] 89%|████████▊ | 691/780 [04:30<00:26,  3.42it/s] 89%|████████▊ | 692/780 [04:31<00:25,  3.42it/s] 89%|████████▉ | 693/780 [04:31<00:25,  3.42it/s] 89%|████████▉ | 694/780 [04:31<00:25,  3.42it/s] 89%|████████▉ | 695/780 [04:31<00:24,  3.42it/s] 89%|████████▉ | 696/780 [04:32<00:24,  3.42it/s] 89%|████████▉ | 697/780 [04:32<00:24,  3.41it/s] 89%|████████▉ | 698/780 [04:32<00:24,  3.41it/s] 90%|████████▉ | 699/780 [04:33<00:23,  3.42it/s] 90%|████████▉ | 700/780 [04:33<00:23,  3.42it/s] 90%|████████▉ | 701/780 [04:33<00:23,  3.42it/s] 90%|█████████ | 702/780 [04:34<00:22,  3.42it/s] 90%|█████████ | 703/780 [04:34<00:22,  3.42it/s] 90%|█████████ | 704/780 [04:34<00:22,  3.42it/s] 90%|█████████ | 705/780 [04:34<00:21,  3.42it/s] 91%|█████████ | 706/780 [04:35<00:21,  3.42it/s] 91%|█████████ | 707/780 [04:35<00:21,  3.42it/s] 91%|█████████ | 708/780 [04:35<00:21,  3.40it/s] 91%|█████████ | 709/780 [04:36<00:20,  3.41it/s] 91%|█████████ | 710/780 [04:36<00:20,  3.41it/s] 91%|█████████ | 711/780 [04:36<00:20,  3.42it/s] 91%|█████████▏| 712/780 [04:36<00:19,  3.42it/s] 91%|█████████▏| 713/780 [04:37<00:19,  3.42it/s] 92%|█████████▏| 714/780 [04:37<00:19,  3.42it/s] 92%|█████████▏| 715/780 [04:37<00:18,  3.42it/s] 92%|█████████▏| 716/780 [04:38<00:18,  3.42it/s] 92%|█████████▏| 717/780 [04:38<00:18,  3.42it/s] 92%|█████████▏| 718/780 [04:38<00:18,  3.42it/s] 92%|█████████▏| 719/780 [04:38<00:17,  3.39it/s] 92%|█████████▏| 720/780 [04:39<00:17,  3.40it/s] 92%|█████████▏| 721/780 [04:39<00:17,  3.41it/s] 93%|█████████▎| 722/780 [04:39<00:17,  3.41it/s] 93%|█████████▎| 723/780 [04:40<00:16,  3.41it/s] 93%|█████████▎| 724/780 [04:40<00:16,  3.42it/s] 93%|█████████▎| 725/780 [04:40<00:16,  3.41it/s] 93%|█████████▎| 726/780 [04:41<00:15,  3.42it/s] 93%|█████████▎| 727/780 [04:41<00:15,  3.42it/s] 93%|█████████▎| 728/780 [04:41<00:15,  3.42it/s] 93%|█████████▎| 729/780 [04:41<00:14,  3.43it/s] 94%|█████████▎| 730/780 [04:42<00:14,  3.41it/s] 94%|█████████▎| 731/780 [04:42<00:14,  3.42it/s] 94%|█████████▍| 732/780 [04:42<00:14,  3.41it/s] 94%|█████████▍| 733/780 [04:43<00:13,  3.42it/s] 94%|█████████▍| 734/780 [04:43<00:13,  3.42it/s] 94%|█████████▍| 735/780 [04:43<00:13,  3.42it/s] 94%|█████████▍| 736/780 [04:43<00:12,  3.42it/s] 94%|█████████▍| 737/780 [04:44<00:12,  3.42it/s] 95%|█████████▍| 738/780 [04:44<00:12,  3.42it/s] 95%|█████████▍| 739/780 [04:44<00:11,  3.42it/s] 95%|█████████▍| 740/780 [04:45<00:11,  3.42it/s] 95%|█████████▌| 741/780 [04:45<00:11,  3.40it/s] 95%|█████████▌| 742/780 [04:45<00:11,  3.40it/s] 95%|█████████▌| 743/780 [04:46<00:10,  3.41it/s] 95%|█████████▌| 744/780 [04:46<00:10,  3.41it/s] 96%|█████████▌| 745/780 [04:46<00:10,  3.41it/s] 96%|█████████▌| 746/780 [04:46<00:09,  3.41it/s] 96%|█████████▌| 747/780 [04:47<00:09,  3.41it/s] 96%|█████████▌| 748/780 [04:47<00:09,  3.42it/s] 96%|█████████▌| 749/780 [04:47<00:09,  3.42it/s] 96%|█████████▌| 750/780 [04:48<00:08,  3.42it/s] 96%|█████████▋| 751/780 [04:48<00:08,  3.42it/s] 96%|█████████▋| 752/780 [04:48<00:08,  3.40it/s] 97%|█████████▋| 753/780 [04:48<00:07,  3.40it/s] 97%|█████████▋| 754/780 [04:49<00:07,  3.40it/s] 97%|█████████▋| 755/780 [04:49<00:07,  3.41it/s] 97%|█████████▋| 756/780 [04:49<00:07,  3.41it/s] 97%|█████████▋| 757/780 [04:50<00:06,  3.42it/s] 97%|█████████▋| 758/780 [04:50<00:06,  3.42it/s] 97%|█████████▋| 759/780 [04:50<00:06,  3.42it/s] 97%|█████████▋| 760/780 [04:51<00:05,  3.34it/s] 98%|█████████▊| 761/780 [04:51<00:05,  3.36it/s] 98%|█████████▊| 762/780 [04:51<00:05,  3.38it/s] 98%|█████████▊| 763/780 [04:51<00:05,  3.38it/s] 98%|█████████▊| 764/780 [04:52<00:04,  3.39it/s] 98%|█████████▊| 765/780 [04:52<00:04,  3.40it/s] 98%|█████████▊| 766/780 [04:52<00:04,  3.41it/s] 98%|█████████▊| 767/780 [04:53<00:03,  3.41it/s] 98%|█████████▊| 768/780 [04:53<00:03,  3.41it/s] 99%|█████████▊| 769/780 [04:53<00:03,  3.42it/s] 99%|█████████▊| 770/780 [04:53<00:02,  3.42it/s] 99%|█████████▉| 771/780 [04:54<00:02,  3.42it/s] 99%|█████████▉| 772/780 [04:54<00:02,  3.42it/s] 99%|█████████▉| 773/780 [04:54<00:02,  3.42it/s] 99%|█████████▉| 774/780 [04:55<00:01,  3.41it/s] 99%|█████████▉| 775/780 [04:55<00:01,  3.41it/s] 99%|█████████▉| 776/780 [04:55<00:01,  3.42it/s]100%|█████████▉| 777/780 [04:55<00:00,  3.42it/s]100%|█████████▉| 778/780 [04:56<00:00,  3.42it/s]100%|█████████▉| 779/780 [04:56<00:00,  3.42it/s]100%|██████████| 780/780 [04:56<00:00,  3.42it/s][INFO|trainer.py:2140] 2023-08-29 12:52:29,121 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:52:29,121 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 12:52:29,121 >>   Batch size = 8
{'eval_loss': 1.1961575746536255, 'eval_runtime': 9.4954, 'eval_samples_per_second': 368.284, 'eval_steps_per_second': 46.128, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.54it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.17it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.38it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.76it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.33it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.01it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.72it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.29it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.23it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.16it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.25it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.19it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.25it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.22it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.20it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.15it/s][A
 20%|██        | 88/438 [00:01<00:07, 45.98it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.04it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.04it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.18it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.24it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 46.32it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.29it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.36it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.29it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.23it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.15it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.20it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.18it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.22it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.14it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.18it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.23it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.13it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.12it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 45.96it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.13it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.17it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.20it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.25it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.28it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.23it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.27it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.09it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.18it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.12it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.19it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.18it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.15it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.19it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.14it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.21it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.11it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.09it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.03it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.10it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.17it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.15it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.21it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.31it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.18it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.16it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.09it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.15it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.13it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.24it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.23it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.22it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.29it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.23it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.21it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.06it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.15it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.17it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.22it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.27it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.32it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.21it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.21it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.01it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.13it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.11it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.12it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.17it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.20it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.14it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.25it/s][A                                                 
                                                 [A100%|██████████| 780/780 [05:06<00:00,  3.42it/s]
100%|██████████| 438/438 [00:09<00:00, 46.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 12:52:38,620 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 12:52:38,637 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:52:41,070 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:52:41,087 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:52:41,101 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 12:52:45,763 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 12:52:45,765 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156 (score: 1.139360785484314).
                                                 100%|██████████| 780/780 [05:16<00:00,  3.42it/s]100%|██████████| 780/780 [05:16<00:00,  2.46it/s]
[INFO|trainer.py:1894] 2023-08-29 12:52:49,150 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 12:52:49,177 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 12:52:51,600 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 12:52:51,614 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 12:52:51,631 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 12:52:51,838 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:52:51,838 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:52:51,838 >>   train_loss               =     0.3709
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:52:51,838 >>   train_runtime            = 0:05:16.90
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:52:51,838 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:52:51,838 >>   train_samples_per_second =    157.778
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:52:51,838 >>   train_steps_per_second   =      2.461
{'eval_loss': 1.203683853149414, 'eval_runtime': 9.4777, 'eval_samples_per_second': 368.971, 'eval_steps_per_second': 46.214, 'epoch': 5.0}
{'train_runtime': 316.9004, 'train_samples_per_second': 157.778, 'train_steps_per_second': 2.461, 'train_loss': 0.37090637011405747, 'epoch': 5.0}
08/29/2023 12:52:51 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 12:52:51,883 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 12:52:51,883 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 12:52:51,883 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 57.58it/s]  3%|▎         | 12/438 [00:00<00:08, 50.94it/s]  4%|▍         | 18/438 [00:00<00:08, 49.12it/s]  5%|▌         | 23/438 [00:00<00:08, 48.41it/s]  6%|▋         | 28/438 [00:00<00:08, 47.95it/s]  8%|▊         | 33/438 [00:00<00:08, 47.61it/s]  9%|▊         | 38/438 [00:00<00:08, 47.40it/s] 10%|▉         | 43/438 [00:00<00:08, 47.15it/s] 11%|█         | 48/438 [00:01<00:08, 46.68it/s] 12%|█▏        | 53/438 [00:01<00:08, 46.60it/s] 13%|█▎        | 58/438 [00:01<00:08, 46.72it/s] 14%|█▍        | 63/438 [00:01<00:08, 46.75it/s] 16%|█▌        | 68/438 [00:01<00:07, 46.82it/s] 17%|█▋        | 73/438 [00:01<00:07, 46.88it/s] 18%|█▊        | 78/438 [00:01<00:07, 46.89it/s] 19%|█▉        | 83/438 [00:01<00:07, 46.80it/s] 20%|██        | 88/438 [00:01<00:07, 46.60it/s] 21%|██        | 93/438 [00:01<00:07, 46.47it/s] 22%|██▏       | 98/438 [00:02<00:07, 46.50it/s] 24%|██▎       | 103/438 [00:02<00:07, 46.51it/s] 25%|██▍       | 108/438 [00:02<00:07, 46.56it/s] 26%|██▌       | 113/438 [00:02<00:06, 46.74it/s] 27%|██▋       | 118/438 [00:02<00:06, 46.76it/s] 28%|██▊       | 123/438 [00:02<00:06, 46.80it/s] 29%|██▉       | 128/438 [00:02<00:06, 46.79it/s] 30%|███       | 133/438 [00:02<00:06, 46.70it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.52it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.53it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.48it/s] 35%|███▍      | 153/438 [00:03<00:06, 46.55it/s] 36%|███▌      | 158/438 [00:03<00:06, 46.65it/s] 37%|███▋      | 163/438 [00:03<00:05, 46.73it/s] 38%|███▊      | 168/438 [00:03<00:05, 46.78it/s] 39%|███▉      | 173/438 [00:03<00:05, 46.76it/s] 41%|████      | 178/438 [00:03<00:05, 46.63it/s] 42%|████▏     | 183/438 [00:03<00:05, 46.58it/s] 43%|████▎     | 188/438 [00:04<00:05, 46.47it/s] 44%|████▍     | 193/438 [00:04<00:05, 46.54it/s] 45%|████▌     | 198/438 [00:04<00:05, 46.54it/s] 46%|████▋     | 203/438 [00:04<00:05, 46.59it/s] 47%|████▋     | 208/438 [00:04<00:04, 46.66it/s] 49%|████▊     | 213/438 [00:04<00:04, 46.61it/s] 50%|████▉     | 218/438 [00:04<00:04, 46.61it/s] 51%|█████     | 223/438 [00:04<00:04, 46.60it/s] 52%|█████▏    | 228/438 [00:04<00:04, 46.52it/s] 53%|█████▎    | 233/438 [00:04<00:04, 46.50it/s] 54%|█████▍    | 238/438 [00:05<00:04, 46.48it/s] 55%|█████▌    | 243/438 [00:05<00:04, 46.49it/s] 57%|█████▋    | 248/438 [00:05<00:04, 46.58it/s] 58%|█████▊    | 253/438 [00:05<00:03, 46.65it/s] 59%|█████▉    | 258/438 [00:05<00:03, 46.64it/s] 60%|██████    | 263/438 [00:05<00:03, 46.62it/s] 61%|██████    | 268/438 [00:05<00:03, 46.55it/s] 62%|██████▏   | 273/438 [00:05<00:03, 46.48it/s] 63%|██████▎   | 278/438 [00:05<00:03, 46.47it/s] 65%|██████▍   | 283/438 [00:06<00:03, 46.44it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.52it/s] 67%|██████▋   | 293/438 [00:06<00:03, 46.51it/s] 68%|██████▊   | 298/438 [00:06<00:03, 46.56it/s] 69%|██████▉   | 303/438 [00:06<00:02, 46.64it/s] 70%|███████   | 308/438 [00:06<00:02, 46.61it/s] 71%|███████▏  | 313/438 [00:06<00:02, 46.57it/s] 73%|███████▎  | 318/438 [00:06<00:02, 46.50it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.45it/s] 75%|███████▍  | 328/438 [00:07<00:02, 46.47it/s] 76%|███████▌  | 333/438 [00:07<00:02, 46.47it/s] 77%|███████▋  | 338/438 [00:07<00:02, 46.50it/s] 78%|███████▊  | 343/438 [00:07<00:02, 46.63it/s] 79%|███████▉  | 348/438 [00:07<00:01, 46.59it/s] 81%|████████  | 353/438 [00:07<00:01, 46.59it/s] 82%|████████▏ | 358/438 [00:07<00:01, 46.57it/s] 83%|████████▎ | 363/438 [00:07<00:01, 46.47it/s] 84%|████████▍ | 368/438 [00:07<00:01, 46.49it/s] 85%|████████▌ | 373/438 [00:07<00:01, 46.43it/s] 86%|████████▋ | 378/438 [00:08<00:01, 46.47it/s] 87%|████████▋ | 383/438 [00:08<00:01, 46.55it/s] 89%|████████▊ | 388/438 [00:08<00:01, 46.56it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.53it/s] 91%|█████████ | 398/438 [00:08<00:00, 46.60it/s] 92%|█████████▏| 403/438 [00:08<00:00, 46.35it/s] 93%|█████████▎| 408/438 [00:08<00:00, 46.36it/s] 94%|█████████▍| 413/438 [00:08<00:00, 46.33it/s] 95%|█████████▌| 418/438 [00:08<00:00, 46.33it/s] 97%|█████████▋| 423/438 [00:09<00:00, 46.45it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.51it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.48it/s]100%|██████████| 438/438 [00:09<00:00, 46.68it/s]100%|██████████| 438/438 [00:09<00:00, 46.70it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 12:53:01,288 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:53:01,288 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:53:01,288 >>   eval_loss               =     1.1394
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:53:01,288 >>   eval_runtime            = 0:00:09.40
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:53:01,288 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:53:01,288 >>   eval_samples_per_second =    371.847
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:53:01,288 >>   eval_steps_per_second   =     46.574
[INFO|trainer_pt_utils.py:913] 2023-08-29 12:53:01,288 >>   perplexity              =     3.1248
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:08,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:08,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:08,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:08,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:08,101 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:53:08,731 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:53:08,732 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:53:09,305 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:53:10,325 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:53:10,325 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:13,228 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:13,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:13,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:13,237 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:53:13,238 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:53:13,895 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:53:13,896 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:53:14,511 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:53:14,672 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:53:14,672 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.33it/s]Extractor Predicting: 4it [00:03,  1.33it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 6it [00:04,  1.26it/s]Extractor Predicting: 7it [00:05,  1.28it/s]Extractor Predicting: 8it [00:06,  1.28it/s]Extractor Predicting: 9it [00:06,  1.26it/s]Extractor Predicting: 10it [00:07,  1.26it/s]Extractor Predicting: 11it [00:08,  1.25it/s]Extractor Predicting: 12it [00:09,  1.27it/s]Extractor Predicting: 13it [00:10,  1.29it/s]Extractor Predicting: 14it [00:10,  1.28it/s]Extractor Predicting: 15it [00:11,  1.29it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:13,  1.29it/s]Extractor Predicting: 18it [00:13,  1.29it/s]Extractor Predicting: 19it [00:14,  1.27it/s]Extractor Predicting: 20it [00:15,  1.25it/s]Extractor Predicting: 21it [00:16,  1.25it/s]Extractor Predicting: 22it [00:17,  1.23it/s]Extractor Predicting: 23it [00:18,  1.25it/s]Extractor Predicting: 24it [00:18,  1.30it/s]Extractor Predicting: 25it [00:19,  1.28it/s]Extractor Predicting: 26it [00:20,  1.32it/s]Extractor Predicting: 27it [00:20,  1.33it/s]Extractor Predicting: 28it [00:21,  1.33it/s]Extractor Predicting: 29it [00:22,  1.32it/s]Extractor Predicting: 30it [00:23,  1.33it/s]Extractor Predicting: 31it [00:24,  1.32it/s]Extractor Predicting: 32it [00:24,  1.29it/s]Extractor Predicting: 33it [00:25,  1.30it/s]Extractor Predicting: 34it [00:26,  1.28it/s]Extractor Predicting: 35it [00:27,  1.29it/s]Extractor Predicting: 36it [00:27,  1.27it/s]Extractor Predicting: 37it [00:28,  1.26it/s]Extractor Predicting: 38it [00:29,  1.24it/s]Extractor Predicting: 39it [00:30,  1.26it/s]Extractor Predicting: 40it [00:31,  1.26it/s]Extractor Predicting: 41it [00:32,  1.23it/s]Extractor Predicting: 42it [00:32,  1.23it/s]Extractor Predicting: 43it [00:33,  1.26it/s]Extractor Predicting: 44it [00:34,  1.25it/s]Extractor Predicting: 45it [00:35,  1.26it/s]Extractor Predicting: 46it [00:35,  1.29it/s]Extractor Predicting: 47it [00:36,  1.27it/s]Extractor Predicting: 48it [00:37,  1.25it/s]Extractor Predicting: 49it [00:38,  1.24it/s]Extractor Predicting: 50it [00:39,  1.22it/s]Extractor Predicting: 51it [00:40,  1.22it/s]Extractor Predicting: 52it [00:40,  1.20it/s]Extractor Predicting: 53it [00:41,  1.22it/s]Extractor Predicting: 54it [00:42,  1.22it/s]Extractor Predicting: 55it [00:43,  1.23it/s]Extractor Predicting: 56it [00:44,  1.24it/s]Extractor Predicting: 57it [00:45,  1.17it/s]Extractor Predicting: 58it [00:45,  1.19it/s]Extractor Predicting: 59it [00:46,  1.22it/s]Extractor Predicting: 60it [00:47,  1.19it/s]Extractor Predicting: 61it [00:48,  1.18it/s]Extractor Predicting: 62it [00:49,  1.17it/s]Extractor Predicting: 63it [00:50,  1.17it/s]Extractor Predicting: 64it [00:50,  1.18it/s]Extractor Predicting: 65it [00:51,  1.17it/s]Extractor Predicting: 66it [00:52,  1.15it/s]Extractor Predicting: 67it [00:53,  1.14it/s]Extractor Predicting: 68it [00:54,  1.14it/s]Extractor Predicting: 69it [00:55,  1.14it/s]Extractor Predicting: 70it [00:56,  1.15it/s]Extractor Predicting: 71it [00:57,  1.14it/s]Extractor Predicting: 72it [00:58,  1.14it/s]Extractor Predicting: 73it [00:58,  1.12it/s]Extractor Predicting: 74it [00:59,  1.13it/s]Extractor Predicting: 75it [01:00,  1.12it/s]Extractor Predicting: 76it [01:01,  1.14it/s]Extractor Predicting: 77it [01:02,  1.14it/s]Extractor Predicting: 78it [01:03,  1.13it/s]Extractor Predicting: 79it [01:04,  1.16it/s]Extractor Predicting: 80it [01:05,  1.14it/s]Extractor Predicting: 81it [01:06,  1.11it/s]Extractor Predicting: 82it [01:06,  1.12it/s]Extractor Predicting: 83it [01:07,  1.15it/s]Extractor Predicting: 84it [01:08,  1.16it/s]Extractor Predicting: 85it [01:09,  1.16it/s]Extractor Predicting: 86it [01:10,  1.16it/s]Extractor Predicting: 87it [01:11,  1.13it/s]Extractor Predicting: 88it [01:12,  1.14it/s]Extractor Predicting: 89it [01:12,  1.17it/s]Extractor Predicting: 90it [01:13,  1.18it/s]Extractor Predicting: 91it [01:14,  1.19it/s]Extractor Predicting: 92it [01:15,  1.17it/s]Extractor Predicting: 93it [01:16,  1.20it/s]Extractor Predicting: 94it [01:17,  1.21it/s]Extractor Predicting: 95it [01:17,  1.20it/s]Extractor Predicting: 96it [01:18,  1.19it/s]Extractor Predicting: 97it [01:19,  1.20it/s]Extractor Predicting: 98it [01:20,  1.19it/s]Extractor Predicting: 99it [01:21,  1.20it/s]Extractor Predicting: 100it [01:22,  1.22it/s]Extractor Predicting: 101it [01:22,  1.22it/s]Extractor Predicting: 102it [01:23,  1.20it/s]Extractor Predicting: 103it [01:24,  1.17it/s]Extractor Predicting: 104it [01:25,  1.18it/s]Extractor Predicting: 105it [01:26,  1.19it/s]Extractor Predicting: 106it [01:27,  1.19it/s]Extractor Predicting: 107it [01:27,  1.18it/s]Extractor Predicting: 108it [01:28,  1.19it/s]Extractor Predicting: 109it [01:29,  1.18it/s]Extractor Predicting: 110it [01:30,  1.16it/s]Extractor Predicting: 111it [01:31,  1.17it/s]Extractor Predicting: 112it [01:32,  1.17it/s]Extractor Predicting: 113it [01:33,  1.18it/s]Extractor Predicting: 114it [01:33,  1.17it/s]Extractor Predicting: 115it [01:34,  1.18it/s]Extractor Predicting: 116it [01:35,  1.18it/s]Extractor Predicting: 117it [01:36,  1.21it/s]Extractor Predicting: 118it [01:37,  1.23it/s]Extractor Predicting: 119it [01:37,  1.26it/s]Extractor Predicting: 120it [01:38,  1.28it/s]Extractor Predicting: 121it [01:39,  1.26it/s]Extractor Predicting: 122it [01:40,  1.30it/s]Extractor Predicting: 123it [01:40,  1.31it/s]Extractor Predicting: 124it [01:41,  1.29it/s]Extractor Predicting: 125it [01:42,  1.30it/s]Extractor Predicting: 126it [01:43,  1.32it/s]Extractor Predicting: 127it [01:43,  1.34it/s]Extractor Predicting: 128it [01:44,  1.34it/s]Extractor Predicting: 129it [01:45,  1.36it/s]Extractor Predicting: 130it [01:46,  1.31it/s]Extractor Predicting: 131it [01:47,  1.31it/s]Extractor Predicting: 132it [01:47,  1.36it/s]Extractor Predicting: 133it [01:48,  1.37it/s]Extractor Predicting: 134it [01:49,  1.30it/s]Extractor Predicting: 135it [01:49,  1.32it/s]Extractor Predicting: 136it [01:50,  1.35it/s]Extractor Predicting: 137it [01:51,  1.27it/s]Extractor Predicting: 138it [01:52,  1.29it/s]Extractor Predicting: 139it [01:53,  1.31it/s]Extractor Predicting: 140it [01:53,  1.35it/s]Extractor Predicting: 141it [01:54,  1.36it/s]Extractor Predicting: 142it [01:55,  1.39it/s]Extractor Predicting: 143it [01:55,  1.37it/s]Extractor Predicting: 144it [01:56,  1.34it/s]Extractor Predicting: 145it [01:57,  1.49it/s]Extractor Predicting: 145it [01:57,  1.24it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:19,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:19,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:19,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:19,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:19,960 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 12:55:20,272 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 12:55:20,273 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:55:20,952 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 12:55:21,997 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:55:21,998 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:23,288 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:23,295 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:23,296 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:23,296 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 12:55:23,296 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 12:55:23,620 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 12:55:23,621 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 12:55:23,886 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 12:55:24,039 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 12:55:24,039 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5931828242585214,
  "recall": 0.3831855876465542,
  "score": 0.4656011118832522,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.30it/s]Extractor Predicting: 2it [00:01,  1.24it/s]Extractor Predicting: 3it [00:02,  1.25it/s]Extractor Predicting: 4it [00:03,  1.23it/s]Extractor Predicting: 5it [00:04,  1.21it/s]Extractor Predicting: 6it [00:04,  1.21it/s]Extractor Predicting: 7it [00:05,  1.22it/s]Extractor Predicting: 8it [00:06,  1.22it/s]Extractor Predicting: 9it [00:07,  1.21it/s]Extractor Predicting: 10it [00:08,  1.21it/s]Extractor Predicting: 11it [00:08,  1.22it/s]Extractor Predicting: 12it [00:09,  1.25it/s]Extractor Predicting: 13it [00:10,  1.22it/s]Extractor Predicting: 14it [00:11,  1.23it/s]Extractor Predicting: 15it [00:12,  1.25it/s]Extractor Predicting: 16it [00:12,  1.26it/s]Extractor Predicting: 17it [00:13,  1.23it/s]Extractor Predicting: 18it [00:14,  1.23it/s]Extractor Predicting: 19it [00:15,  1.21it/s]Extractor Predicting: 20it [00:16,  1.21it/s]Extractor Predicting: 21it [00:17,  1.22it/s]Extractor Predicting: 22it [00:18,  1.18it/s]Extractor Predicting: 23it [00:18,  1.20it/s]Extractor Predicting: 24it [00:19,  1.22it/s]Extractor Predicting: 25it [00:20,  1.17it/s]Extractor Predicting: 26it [00:21,  1.17it/s]Extractor Predicting: 27it [00:22,  1.17it/s]Extractor Predicting: 28it [00:23,  1.18it/s]Extractor Predicting: 29it [00:23,  1.17it/s]Extractor Predicting: 30it [00:24,  1.15it/s]Extractor Predicting: 31it [00:25,  1.16it/s]Extractor Predicting: 32it [00:26,  1.16it/s]Extractor Predicting: 33it [00:27,  1.19it/s]Extractor Predicting: 34it [00:28,  1.20it/s]Extractor Predicting: 35it [00:29,  1.20it/s]Extractor Predicting: 36it [00:29,  1.22it/s]Extractor Predicting: 37it [00:30,  1.26it/s]Extractor Predicting: 38it [00:31,  1.25it/s]Extractor Predicting: 39it [00:32,  1.25it/s]Extractor Predicting: 40it [00:32,  1.26it/s]Extractor Predicting: 41it [00:33,  1.25it/s]Extractor Predicting: 42it [00:34,  1.24it/s]Extractor Predicting: 43it [00:35,  1.24it/s]Extractor Predicting: 44it [00:36,  1.24it/s]Extractor Predicting: 45it [00:37,  1.22it/s]Extractor Predicting: 46it [00:37,  1.24it/s]Extractor Predicting: 47it [00:38,  1.24it/s]Extractor Predicting: 48it [00:39,  1.24it/s]Extractor Predicting: 49it [00:40,  1.23it/s]Extractor Predicting: 50it [00:41,  1.22it/s]Extractor Predicting: 51it [00:41,  1.23it/s]Extractor Predicting: 52it [00:42,  1.23it/s]Extractor Predicting: 53it [00:43,  1.23it/s]Extractor Predicting: 54it [00:44,  1.24it/s]Extractor Predicting: 55it [00:45,  1.21it/s]Extractor Predicting: 56it [00:45,  1.21it/s]Extractor Predicting: 57it [00:46,  1.24it/s]Extractor Predicting: 58it [00:47,  1.26it/s]Extractor Predicting: 59it [00:48,  1.27it/s]Extractor Predicting: 60it [00:49,  1.27it/s]Extractor Predicting: 61it [00:49,  1.26it/s]Extractor Predicting: 62it [00:50,  1.26it/s]Extractor Predicting: 63it [00:51,  1.25it/s]Extractor Predicting: 64it [00:52,  1.25it/s]Extractor Predicting: 65it [00:53,  1.23it/s]Extractor Predicting: 66it [00:53,  1.22it/s]Extractor Predicting: 67it [00:54,  1.24it/s]Extractor Predicting: 68it [00:55,  1.27it/s]Extractor Predicting: 69it [00:56,  1.28it/s]Extractor Predicting: 70it [00:57,  1.27it/s]Extractor Predicting: 71it [00:57,  1.25it/s]Extractor Predicting: 72it [00:58,  1.27it/s]Extractor Predicting: 73it [00:59,  1.23it/s]Extractor Predicting: 74it [01:00,  1.24it/s]Extractor Predicting: 75it [01:01,  1.24it/s]Extractor Predicting: 76it [01:01,  1.26it/s]Extractor Predicting: 77it [01:02,  1.22it/s]Extractor Predicting: 78it [01:03,  1.22it/s]Extractor Predicting: 79it [01:04,  1.23it/s]Extractor Predicting: 80it [01:05,  1.22it/s]Extractor Predicting: 81it [01:06,  1.24it/s]Extractor Predicting: 82it [01:06,  1.22it/s]Extractor Predicting: 83it [01:07,  1.22it/s]Extractor Predicting: 84it [01:08,  1.23it/s]Extractor Predicting: 85it [01:09,  1.24it/s]Extractor Predicting: 86it [01:10,  1.23it/s]Extractor Predicting: 87it [01:10,  1.23it/s]Extractor Predicting: 88it [01:11,  1.23it/s]Extractor Predicting: 89it [01:12,  1.23it/s]Extractor Predicting: 90it [01:13,  1.25it/s]Extractor Predicting: 91it [01:14,  1.24it/s]Extractor Predicting: 92it [01:14,  1.23it/s]Extractor Predicting: 93it [01:15,  1.21it/s]Extractor Predicting: 94it [01:16,  1.23it/s]Extractor Predicting: 95it [01:17,  1.22it/s]Extractor Predicting: 96it [01:18,  1.21it/s]Extractor Predicting: 97it [01:19,  1.23it/s]Extractor Predicting: 98it [01:19,  1.24it/s]Extractor Predicting: 99it [01:20,  1.24it/s]Extractor Predicting: 100it [01:21,  1.25it/s]Extractor Predicting: 101it [01:22,  1.26it/s]Extractor Predicting: 102it [01:22,  1.26it/s]Extractor Predicting: 103it [01:23,  1.25it/s]Extractor Predicting: 104it [01:24,  1.22it/s]Extractor Predicting: 105it [01:25,  1.21it/s]Extractor Predicting: 106it [01:26,  1.21it/s]Extractor Predicting: 107it [01:27,  1.21it/s]Extractor Predicting: 108it [01:27,  1.22it/s]Extractor Predicting: 109it [01:28,  1.21it/s]Extractor Predicting: 110it [01:29,  1.22it/s]Extractor Predicting: 111it [01:30,  1.23it/s]Extractor Predicting: 112it [01:31,  1.11it/s]Extractor Predicting: 113it [01:32,  1.13it/s]Extractor Predicting: 114it [01:33,  1.14it/s]Extractor Predicting: 115it [01:34,  1.14it/s]Extractor Predicting: 116it [01:34,  1.14it/s]Extractor Predicting: 117it [01:35,  1.15it/s]Extractor Predicting: 118it [01:36,  1.17it/s]Extractor Predicting: 119it [01:37,  1.18it/s]Extractor Predicting: 120it [01:38,  1.19it/s]Extractor Predicting: 121it [01:39,  1.21it/s]Extractor Predicting: 122it [01:39,  1.23it/s]Extractor Predicting: 123it [01:40,  1.23it/s]Extractor Predicting: 124it [01:41,  1.23it/s]Extractor Predicting: 125it [01:42,  1.21it/s]Extractor Predicting: 126it [01:43,  1.20it/s]Extractor Predicting: 127it [01:44,  1.19it/s]Extractor Predicting: 128it [01:44,  1.20it/s]Extractor Predicting: 129it [01:45,  1.22it/s]Extractor Predicting: 130it [01:46,  1.21it/s]Extractor Predicting: 131it [01:47,  1.22it/s]Extractor Predicting: 132it [01:48,  1.24it/s]Extractor Predicting: 133it [01:48,  1.22it/s]Extractor Predicting: 134it [01:49,  1.21it/s]Extractor Predicting: 135it [01:50,  1.20it/s]Extractor Predicting: 136it [01:51,  1.20it/s]Extractor Predicting: 137it [01:52,  1.19it/s]Extractor Predicting: 138it [01:53,  1.23it/s]Extractor Predicting: 139it [01:53,  1.22it/s]Extractor Predicting: 140it [01:54,  1.19it/s]Extractor Predicting: 141it [01:55,  1.21it/s]Extractor Predicting: 142it [01:56,  1.21it/s]Extractor Predicting: 143it [01:57,  1.21it/s]Extractor Predicting: 144it [01:58,  1.19it/s]Extractor Predicting: 145it [01:58,  1.22it/s]Extractor Predicting: 146it [01:59,  1.21it/s]Extractor Predicting: 147it [02:00,  1.24it/s]Extractor Predicting: 148it [02:01,  1.22it/s]Extractor Predicting: 149it [02:02,  1.24it/s]Extractor Predicting: 150it [02:02,  1.24it/s]Extractor Predicting: 151it [02:03,  1.26it/s]Extractor Predicting: 152it [02:04,  1.28it/s]Extractor Predicting: 153it [02:05,  1.28it/s]Extractor Predicting: 154it [02:06,  1.28it/s]Extractor Predicting: 155it [02:06,  1.27it/s]Extractor Predicting: 156it [02:07,  1.27it/s]Extractor Predicting: 157it [02:08,  1.25it/s]Extractor Predicting: 158it [02:09,  1.22it/s]Extractor Predicting: 159it [02:10,  1.23it/s]Extractor Predicting: 160it [02:10,  1.26it/s]Extractor Predicting: 161it [02:11,  1.26it/s]Extractor Predicting: 162it [02:12,  1.27it/s]Extractor Predicting: 163it [02:13,  1.28it/s]Extractor Predicting: 164it [02:13,  1.28it/s]Extractor Predicting: 165it [02:14,  1.27it/s]Extractor Predicting: 166it [02:15,  1.26it/s]Extractor Predicting: 167it [02:16,  1.26it/s]Extractor Predicting: 168it [02:17,  1.26it/s]Extractor Predicting: 169it [02:17,  1.29it/s]Extractor Predicting: 170it [02:18,  1.29it/s]Extractor Predicting: 171it [02:19,  1.26it/s]Extractor Predicting: 172it [02:20,  1.25it/s]Extractor Predicting: 173it [02:21,  1.25it/s]Extractor Predicting: 174it [02:21,  1.27it/s]Extractor Predicting: 175it [02:22,  1.29it/s]Extractor Predicting: 176it [02:23,  1.30it/s]Extractor Predicting: 177it [02:24,  1.25it/s]Extractor Predicting: 178it [02:24,  1.28it/s]Extractor Predicting: 179it [02:25,  1.26it/s]Extractor Predicting: 180it [02:26,  1.26it/s]Extractor Predicting: 181it [02:27,  1.27it/s]Extractor Predicting: 182it [02:28,  1.25it/s]Extractor Predicting: 183it [02:28,  1.26it/s]Extractor Predicting: 184it [02:29,  1.27it/s]Extractor Predicting: 185it [02:30,  1.28it/s]Extractor Predicting: 186it [02:31,  1.30it/s]Extractor Predicting: 187it [02:31,  1.34it/s]Extractor Predicting: 188it [02:32,  1.31it/s]Extractor Predicting: 189it [02:33,  1.27it/s]Extractor Predicting: 190it [02:34,  1.27it/s]Extractor Predicting: 191it [02:35,  1.25it/s]Extractor Predicting: 192it [02:35,  1.27it/s]Extractor Predicting: 193it [02:36,  1.27it/s]Extractor Predicting: 194it [02:37,  1.28it/s]Extractor Predicting: 195it [02:38,  1.29it/s]Extractor Predicting: 196it [02:39,  1.29it/s]Extractor Predicting: 197it [02:39,  1.25it/s]Extractor Predicting: 198it [02:40,  1.26it/s]Extractor Predicting: 199it [02:41,  1.24it/s]Extractor Predicting: 200it [02:42,  1.25it/s]Extractor Predicting: 201it [02:43,  1.25it/s]Extractor Predicting: 202it [02:43,  1.25it/s]Extractor Predicting: 203it [02:44,  1.24it/s]Extractor Predicting: 204it [02:45,  1.22it/s]Extractor Predicting: 205it [02:46,  1.20it/s]Extractor Predicting: 206it [02:47,  1.23it/s]Extractor Predicting: 207it [02:48,  1.23it/s]Extractor Predicting: 208it [02:48,  1.26it/s]Extractor Predicting: 209it [02:49,  1.27it/s]Extractor Predicting: 210it [02:50,  1.25it/s]Extractor Predicting: 211it [02:51,  1.24it/s]Extractor Predicting: 212it [02:52,  1.24it/s]Extractor Predicting: 213it [02:52,  1.23it/s]Extractor Predicting: 214it [02:53,  1.23it/s]Extractor Predicting: 215it [02:54,  1.22it/s]Extractor Predicting: 216it [02:55,  1.23it/s]Extractor Predicting: 217it [02:56,  1.21it/s]Extractor Predicting: 218it [02:56,  1.21it/s]Extractor Predicting: 219it [02:57,  1.21it/s]Extractor Predicting: 220it [02:58,  1.23it/s]Extractor Predicting: 221it [02:59,  1.27it/s]Extractor Predicting: 222it [03:00,  1.22it/s]Extractor Predicting: 223it [03:01,  1.10it/s]Extractor Predicting: 224it [03:02,  1.16it/s]Extractor Predicting: 225it [03:02,  1.18it/s]Extractor Predicting: 226it [03:03,  1.18it/s]Extractor Predicting: 227it [03:04,  1.20it/s]Extractor Predicting: 228it [03:05,  1.21it/s]Extractor Predicting: 229it [03:06,  1.24it/s]Extractor Predicting: 230it [03:06,  1.20it/s]Extractor Predicting: 231it [03:07,  1.22it/s]Extractor Predicting: 232it [03:08,  1.24it/s]Extractor Predicting: 233it [03:09,  1.22it/s]Extractor Predicting: 234it [03:10,  1.21it/s]Extractor Predicting: 235it [03:11,  1.21it/s]Extractor Predicting: 236it [03:11,  1.22it/s]Extractor Predicting: 237it [03:12,  1.19it/s]Extractor Predicting: 238it [03:13,  1.21it/s]Extractor Predicting: 239it [03:14,  1.21it/s]Extractor Predicting: 240it [03:15,  1.20it/s]Extractor Predicting: 241it [03:16,  1.20it/s]Extractor Predicting: 242it [03:16,  1.22it/s]Extractor Predicting: 243it [03:17,  1.19it/s]Extractor Predicting: 244it [03:18,  1.20it/s]Extractor Predicting: 245it [03:19,  1.19it/s]Extractor Predicting: 246it [03:20,  1.21it/s]Extractor Predicting: 247it [03:21,  1.20it/s]Extractor Predicting: 248it [03:21,  1.20it/s]Extractor Predicting: 249it [03:22,  1.21it/s]Extractor Predicting: 250it [03:23,  1.20it/s]Extractor Predicting: 251it [03:24,  1.21it/s]Extractor Predicting: 252it [03:25,  1.20it/s]Extractor Predicting: 253it [03:26,  1.21it/s]Extractor Predicting: 254it [03:26,  1.21it/s]Extractor Predicting: 255it [03:27,  1.22it/s]Extractor Predicting: 256it [03:28,  1.20it/s]Extractor Predicting: 257it [03:29,  1.23it/s]Extractor Predicting: 258it [03:30,  1.23it/s]Extractor Predicting: 259it [03:30,  1.25it/s]Extractor Predicting: 260it [03:31,  1.23it/s]Extractor Predicting: 261it [03:32,  1.23it/s]Extractor Predicting: 262it [03:33,  1.23it/s]Extractor Predicting: 263it [03:34,  1.24it/s]Extractor Predicting: 264it [03:35,  1.20it/s]Extractor Predicting: 265it [03:35,  1.21it/s]Extractor Predicting: 266it [03:36,  1.21it/s]Extractor Predicting: 267it [03:37,  1.22it/s]Extractor Predicting: 268it [03:38,  1.23it/s]Extractor Predicting: 269it [03:39,  1.23it/s]Extractor Predicting: 270it [03:39,  1.23it/s]Extractor Predicting: 271it [03:40,  1.24it/s]Extractor Predicting: 272it [03:41,  1.27it/s]Extractor Predicting: 273it [03:42,  1.28it/s]Extractor Predicting: 274it [03:42,  1.28it/s]Extractor Predicting: 275it [03:43,  1.26it/s]Extractor Predicting: 276it [03:44,  1.24it/s]Extractor Predicting: 277it [03:45,  1.26it/s]Extractor Predicting: 278it [03:46,  1.24it/s]Extractor Predicting: 279it [03:47,  1.25it/s]Extractor Predicting: 280it [03:47,  1.23it/s]Extractor Predicting: 281it [03:48,  1.15it/s]Extractor Predicting: 282it [03:51,  1.46s/it]Extractor Predicting: 283it [03:52,  1.27s/it]Extractor Predicting: 284it [03:53,  1.13s/it]Extractor Predicting: 285it [03:54,  1.04s/it]Extractor Predicting: 286it [03:54,  1.02it/s]Extractor Predicting: 287it [03:55,  1.08it/s]Extractor Predicting: 288it [03:56,  1.13it/s]Extractor Predicting: 289it [03:57,  1.16it/s]Extractor Predicting: 290it [03:58,  1.18it/s]Extractor Predicting: 291it [03:59,  1.18it/s]Extractor Predicting: 292it [03:59,  1.21it/s]Extractor Predicting: 293it [04:00,  1.22it/s]Extractor Predicting: 294it [04:01,  1.21it/s]Extractor Predicting: 295it [04:02,  1.23it/s]Extractor Predicting: 296it [04:03,  1.24it/s]Extractor Predicting: 297it [04:03,  1.22it/s]Extractor Predicting: 298it [04:04,  1.25it/s]Extractor Predicting: 299it [04:05,  1.24it/s]Extractor Predicting: 300it [04:06,  1.23it/s]Extractor Predicting: 301it [04:06,  1.28it/s]Extractor Predicting: 302it [04:07,  1.27it/s]Extractor Predicting: 303it [04:08,  1.30it/s]Extractor Predicting: 304it [04:10,  1.13s/it]Extractor Predicting: 305it [04:14,  2.03s/it]Extractor Predicting: 306it [04:15,  1.67s/it]Extractor Predicting: 307it [04:16,  1.42s/it]Extractor Predicting: 308it [04:17,  1.24s/it]Extractor Predicting: 309it [04:17,  1.10s/it]Extractor Predicting: 310it [04:18,  1.03s/it]Extractor Predicting: 311it [04:19,  1.03it/s]Extractor Predicting: 312it [04:20,  1.06it/s]Extractor Predicting: 313it [04:21,  1.11it/s]Extractor Predicting: 314it [04:22,  1.12it/s]Extractor Predicting: 315it [04:22,  1.17it/s]Extractor Predicting: 316it [04:23,  1.10it/s]Extractor Predicting: 317it [04:24,  1.13it/s]Extractor Predicting: 318it [04:25,  1.15it/s]Extractor Predicting: 319it [04:26,  1.17it/s]Extractor Predicting: 320it [04:27,  1.18it/s]Extractor Predicting: 321it [04:28,  1.20it/s]Extractor Predicting: 322it [04:28,  1.18it/s]Extractor Predicting: 323it [04:29,  1.23it/s]Extractor Predicting: 324it [04:30,  1.21it/s]Extractor Predicting: 325it [04:31,  1.22it/s]Extractor Predicting: 326it [04:32,  1.21it/s]Extractor Predicting: 327it [04:32,  1.23it/s]Extractor Predicting: 328it [04:33,  1.25it/s]Extractor Predicting: 329it [04:34,  1.23it/s]Extractor Predicting: 330it [04:35,  1.19it/s]Extractor Predicting: 331it [04:36,  1.20it/s]Extractor Predicting: 332it [04:37,  1.20it/s]Extractor Predicting: 333it [04:37,  1.21it/s]Extractor Predicting: 334it [04:38,  1.20it/s]Extractor Predicting: 335it [04:39,  1.20it/s]Extractor Predicting: 336it [04:40,  1.24it/s]Extractor Predicting: 337it [04:41,  1.25it/s]Extractor Predicting: 338it [04:41,  1.23it/s]Extractor Predicting: 339it [04:42,  1.22it/s]Extractor Predicting: 340it [04:43,  1.23it/s]Extractor Predicting: 341it [04:44,  1.25it/s]Extractor Predicting: 342it [04:45,  1.20it/s]Extractor Predicting: 343it [04:46,  1.22it/s]Extractor Predicting: 344it [04:46,  1.23it/s]Extractor Predicting: 345it [04:47,  1.25it/s]Extractor Predicting: 346it [04:48,  1.23it/s]Extractor Predicting: 347it [05:19,  9.97s/it]Extractor Predicting: 348it [05:20,  7.22s/it]Extractor Predicting: 349it [05:21,  5.31s/it]Extractor Predicting: 350it [05:22,  3.96s/it]Extractor Predicting: 351it [05:23,  3.03s/it]Extractor Predicting: 352it [05:24,  2.38s/it]Extractor Predicting: 353it [05:24,  1.91s/it]Extractor Predicting: 354it [05:25,  1.59s/it]Extractor Predicting: 355it [05:26,  1.35s/it]Extractor Predicting: 356it [05:27,  1.19s/it]Extractor Predicting: 357it [05:28,  1.06s/it]Extractor Predicting: 358it [05:28,  1.02it/s]Extractor Predicting: 359it [05:29,  1.08it/s]Extractor Predicting: 360it [05:30,  1.11it/s]Extractor Predicting: 361it [05:31,  1.15it/s]Extractor Predicting: 362it [05:32,  1.18it/s]Extractor Predicting: 363it [05:32,  1.22it/s]Extractor Predicting: 364it [05:33,  1.23it/s]Extractor Predicting: 365it [05:34,  1.24it/s]Extractor Predicting: 366it [05:35,  1.24it/s]Extractor Predicting: 367it [05:36,  1.23it/s]Extractor Predicting: 368it [05:36,  1.26it/s]Extractor Predicting: 369it [05:37,  1.26it/s]Extractor Predicting: 370it [05:38,  1.29it/s]Extractor Predicting: 371it [05:39,  1.27it/s]Extractor Predicting: 372it [05:39,  1.27it/s]Extractor Predicting: 373it [05:40,  1.27it/s]Extractor Predicting: 374it [05:41,  1.26it/s]Extractor Predicting: 375it [05:42,  1.27it/s]Extractor Predicting: 376it [05:43,  1.27it/s]Extractor Predicting: 377it [05:43,  1.27it/s]Extractor Predicting: 378it [05:44,  1.28it/s]Extractor Predicting: 379it [05:45,  1.24it/s]Extractor Predicting: 380it [05:46,  1.24it/s]Extractor Predicting: 381it [05:47,  1.28it/s]Extractor Predicting: 382it [05:47,  1.27it/s]Extractor Predicting: 383it [05:48,  1.23it/s]Extractor Predicting: 384it [05:49,  1.24it/s]Extractor Predicting: 385it [05:50,  1.25it/s]Extractor Predicting: 386it [05:51,  1.28it/s]Extractor Predicting: 387it [05:51,  1.30it/s]Extractor Predicting: 388it [05:52,  1.29it/s]Extractor Predicting: 389it [05:53,  1.27it/s]Extractor Predicting: 390it [05:54,  1.28it/s]Extractor Predicting: 391it [05:54,  1.28it/s]Extractor Predicting: 392it [05:55,  1.29it/s]Extractor Predicting: 393it [05:56,  1.30it/s]Extractor Predicting: 394it [05:57,  1.32it/s]Extractor Predicting: 395it [05:58,  1.28it/s]Extractor Predicting: 396it [05:58,  1.28it/s]Extractor Predicting: 397it [05:59,  1.28it/s]Extractor Predicting: 398it [06:00,  1.29it/s]Extractor Predicting: 399it [06:01,  1.31it/s]Extractor Predicting: 400it [06:01,  1.33it/s]Extractor Predicting: 401it [06:02,  1.30it/s]Extractor Predicting: 402it [06:03,  1.31it/s]Extractor Predicting: 403it [06:04,  1.29it/s]Extractor Predicting: 404it [06:04,  1.28it/s]Extractor Predicting: 405it [06:05,  1.28it/s]Extractor Predicting: 406it [06:06,  1.16it/s]Extractor Predicting: 407it [06:07,  1.18it/s]Extractor Predicting: 408it [06:08,  1.21it/s]Extractor Predicting: 409it [06:09,  1.23it/s]Extractor Predicting: 409it [06:09,  1.11it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:40,900 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:40,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:40,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:40,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:40,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 13:01:41,637 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 13:01:41,638 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:01:41,906 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 13:01:42,963 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:01:42,963 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:45,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:45,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:45,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:45,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 13:01:45,868 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 13:01:46,522 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 13:01:46,523 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 13:01:47,092 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 13:01:47,255 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 13:01:47,255 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35197797420663673,
  "recall": 0.24752878834199532,
  "score": 0.29065454110326666,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.02it/s]Extractor Predicting: 2it [00:01,  1.07it/s]Extractor Predicting: 3it [00:02,  1.10it/s]Extractor Predicting: 4it [00:03,  1.15it/s]Extractor Predicting: 5it [00:04,  1.15it/s]Extractor Predicting: 6it [00:05,  1.17it/s]Extractor Predicting: 7it [00:06,  1.16it/s]Extractor Predicting: 8it [00:06,  1.18it/s]Extractor Predicting: 9it [00:07,  1.22it/s]Extractor Predicting: 10it [00:08,  1.20it/s]Extractor Predicting: 11it [00:09,  1.22it/s]Extractor Predicting: 12it [00:10,  1.20it/s]Extractor Predicting: 13it [00:11,  1.21it/s]Extractor Predicting: 13it [00:11,  1.18it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.21153846153846154,
  "recall": 0.048034934497816595,
  "score": 0.07829181494661921,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
